# The output shows the total (aggregate) profit fore ach year.
#
# To also determine the total profit summed over all years, you must add up the individual
# values yourself or run an additional query.
#
# Or, you can use ROLLUP - which provides both levels of analysis with a single query.
#
# Adding a WITH ROLLUP modifier to the GROUP BY clause causes the query to produce
# another (super-aggregate) row that shows the grant total over all year values:
#
# 		SELECT year, SUM(profit) AS profit
# 		FROM sales
# 		GROUP BY year WITH ROLLUP;
# 		+--------+---------------+
# 		| year 	| profit 		 |
# 		+--------+---------------+
# 		| 2000 	| 4525 			 |
# 		| 2001 	| 3010 			 |
# 		| NULL 	| 7535 			 |
# 		+--------+---------------+
#
# The NULL value in the year column identifies the grant total super-aggregate line.
#
# ROLLUP has a more complex effect when there are multiple GROUP by columns.
#
# In this case, each time there is a change in value in any but the last grouping
# column, the query produces an extra super-aggregate summary row.
#
# For example, without ROLLUP, a summary of the sales table based on year, country,
# and product might look like this, where the output indicates summary values only
# at the year/country/product level of analysis:
#
# 		SELECT year, country, product, SUM(profit) AS profit
# 		FROM sales
# 		GROUP BY year, country, product;
# 		+------+-----------------+-----------------+-------------+
# 		| year | country 			 | product 			 | profit 		|
# 		+------+-----------------+-----------------+-------------+
# 		| 2000 | Finland 			 | Computer 		 | 1500 			|
# 		| 2000 | Finland 			 | Phone 			 | 100 			|
# 		| 2000 | India 			 | Calculator 		 | 150 		   |
# 		| 2000 | India 			 | Computer 		 | 1200 			|
# 		| 2000 | USA 				 | Calculator 		 | 75 			|
# 		| 2000 | USA 				 | Computer 		 | 1500 			|
# 		| 2001 | Finland 			 | Phone 			 | 10 			|
# 		| 2001 | USA 				 | Calculator 		 | 50 			|
# 		| 2001 | USA 				 | Computer 		 | 2700 			|
# 		| 2001 | USA 				 | TV 				 | 250 			|
# 		+------+-----------------+-----------------+-------------+
#
# With ROLLUP added, the query produces several extra rows:
#
# 		SELECT year, country, product, SUM(profit) AS profit
# 		FROM sales
# 		GROUP BY year, country, product WITH ROLLUP;
# 		+------+-----------------+------------+-------------+
# 		| year | country 			 | product 	  | profit 		 |
# 		+------+-----------------+------------+-------------+
# 		| 2000 | Finland 			 | Computer   | 1500 		 |
# 		| 2000 | Finland 			 | Phone 	  | 100 			 |
# 		| 2000 | Finland 			 | NULL 		  | 1600 		 |
# 		| 2000 | India 			 | Calculator | 150 			 |
# 		| 2000 | India 			 | Computer   | 1200 		 |
# 		| 2000 | India 			 | NULL 		  | 1350 		 |
# 		| 2000 | USA 				 | Calculator | 75 			 |
# 		| 2000 | USA 				 | Computer   | 1500 		 |
# 		| 2000 | USA 				 | NULL 		  | 1575 		 |
# 		| 2000 | NULL 				 | NULL 		  | 4525 		 |
# 		| 2001 | Finland 			 | Phone 	  | 10 			 |
# 		| 2001 | Finland 			 | NULL 		  | 10 			 |
# 		| 2001 | USA 				 | Calculator | 50 			 |
# 		| 2001 | USA 				 | Computer   | 2700 		 |
# 		| 2001 | USA 				 | TV 		  | 250 			 |
# 		| 2001 | USA 				 | NULL 		  | 3000 		 |
# 		| 2001 | NULL 				 | NULL 		  | 3010 		 |
# 		| NULL | NULL 				 | NULL 		  | 7535 		 |
# 		+------+-----------------+------------+-------------+
#
# Now the output includes summary information at four levels of analysis, not just one:
#
# 		) Following each set of product rows for a given year and country, an extra super-aggregate summary row
# 			appears showing the total for all products.
#
# 			These rows have the product column set to NULL
#
# 		) Following each set of rows for a given year, an extra super-aggregate summary row appears showing
# 			the total for all countries and products.
#
# 			These rows have the country and products columns set to NULL
#
# 		) Finally, following all other rows, an extra super-aggregate summary row appears showing the grand total
# 			for all years, countries, and products.
#
# 			This row has the year, country and products set to NULL
#
# Previously, MySQL did not allow the use of DISTINCT or ORDER BY in a query having a WITH ROLLUP option.
#
# This restriction is lifted in MySQL 8.0.12, and later.
#
# (Bug #87450, Bug #86311, Bug#26640100, Bug#26073513)
#
# For GROUP BY --- WITH ROLLUP queries, to test whether NULL values in the result represent
# super-aggregate values, the GROUPING() function is available for use in theh select list,
# HAVING clause and (as of MySQL 8.0.12) ORDER BY clause.
#
# For example, GROUPING(year) returns 1 when NULL in the year column occurs in a super-aggregate
# row, and 0 otherwise.
#
# SImilarly, GROUPING(country) and GROUPING(product) return 1 for super-aggregate NULL values
# in the country and product columns, respectively:
#
# 		SELECT
# 			year, country, product, SUM(profit) AS profit,
# 			GROUPING(year) AS grp_year,
# 			GROUPING(country) AS grp_country,
# 			GROUPING(product) AS grp_product
# 		FROM sales
# 		GROUP BY year, country, product WITH ROLLUP;
# 		+------+------------------+----------------+---------------+-----------------+------------------+----------------+
# 		| year | country 			  | product 		 | profit 	     | grp_year   	  | grp_country 		| grp_product 	  |
# 		+------+------------------+----------------+---------------+-----------------+------------------+----------------+
# 		| 2000 | Finland 			  | Computer 		 | 1500 			  | 0 				  | 0 					| 0 				  |
# 		| 2000 | Finland 			  | Phone 			 | 100 			  | 0 				  | 0 		 			| 0 				  |
# 		| 2000 | Finland 			  | NULL 			 | 1600 			  | 0 				  | 0 					| 1 				  |
# 		| 2000 | India 			  | Calculator 	 | 150 			  | 0 				  | 0 				   | 0				  |
# 		| 2000 | India 			  | Computer 		 | 1200 			  | 0 				  | 0 					| 0 				  |
# 		| 2000 | India 			  | NULL 			 | 1350 			  | 0 				  | 0 					| 1 				  |
# 		| 2000 | USA 				  | Calculator 	 | 75 			  | 0 				  | 0 				   | 0 				  |
# 		| 2000 | USA 				  | Computer 		 | 1500 			  | 0 				  | 0 					| 0 				  |
# 		| 2000 | USA 				  | NULL 			 | 1575 			  | 0 				  | 0 					| 1 				  |
# 		| 2000 | NULL 				  | NULL 			 | 4525 			  | 0 				  | 1 					| 1 				  |
# 		| 2001 | Finland 			  | Phone 			 | 10 			  | 0 				  | 0 					| 0 				  |
# 		| 2001 | Finland 			  | NULL 			 | 10 			  | 0 				  | 0 					| 1 				  |
# 		| 2001 | USA 				  | Calculator 	 | 50 			  | 0 				  | 0 					| 0 				  |
# 		| 2001 | USA 				  | Computer 		 | 2700 			  | 0 				  | 0 					| 0 				  |
# 		| 2001 | USA 				  | TV 				 | 250 			  | 0 				  | 0 					| 0 				  |
# 		| 2001 | USA 				  | NULL 			 | 3000 			  | 0 				  | 0 					| 1 				  |
# 		| 2001 | NULL 				  | NULL 			 | 3010 			  | 0 				  | 1 					| 1 				  |
# 		| NULL | NULL 				  | NULL 			 | 7535 			  | 1 				  | 1 					| 1 				  |
# 		+------+------------------+----------------+---------------+-----------------+------------------+----------------+
#
# Instead of displaying the GROUPING()  result directly, you can use GROUPING() to substitute labels for super-aggregate
# NULL values:
#
# 		SELECT
# 			IF(GROUPING(year), 'All years', year) AS year,
# 			IF(GROUPING(country), 'All countries', country) AS country,
# 			IF(GROUPING(product), 'ALl products', product) AS product,
# 			SUM(profit) AS profit
# 		FROM sales
# 		GROUP BY year, country, product WITH ROLLUP;
# +------------+---------------------+-------------------+--------------+
# | year 		| country 				 | product 				| profit 		|
# +------------+---------------------+-------------------+--------------+
# | 2000 		| Finland 				 | Computer 			| 1500 			|
# | 2000 		| Finland 				 | Phone 				| 100 			|
# | 2000 		| Finland 				 | All products 		| 1600 			|
# | 2000 		| India 					 | Calculator 			| 150 			|
# | 2000 		| India 					 | Computer 			| 1200 			|
# | 2000 		| India 					 | All products 		| 1350 		   |
# | 2000 		| USA 					 | Calculator 			| 75 				|
# | 2000 		| USA 					 | Computer 			| 1500 		   |
# | 2000 		| USA 					 | All products 		| 1575 			|
# | 2000 		| All countries 		 | All products 		| 4525 			|
# | 2001 		| Finland 				 | Phone 				| 10 				|
# | 2001 		| Finland 				 | All products 		| 10 				|
# | 2001 		| USA 					 | Calculator 			| 50 				|
# | 2001 		| USA 					 | Computer 			| 2700 			|
# | 2001 		| USA 					 | TV 					| 250 			|
# | 2001 		| USA 					 | All Products 		| 3000 			|
# | 2001 		| All countries 		 | ALl products 		| 3010 			|
# | All years  | All countries 		 | All products 		| 7535 			|
# +------------+---------------------+-------------------+--------------+
#
# With multiple expression arguments, GROUPING() returns a result representing a bitmask
# that combines the results for each expression, with the lowest-order bit corresponding
# ot the result for the rightmost expression.
#
# For example, GROUPING(year, country, product) is evaluated like this:
#
# 		result FOR GROUPING(product)
# 	 + result FOR GROUPING(country) << 1
#   + result FOR GROUPING(year) << 2
#
# The result of such a GROUPING() is nonzero if any of the expressions represents a super-aggregate NULL,
# so you can return only the super-aggregate rows and filter out the regular grouped rows like this:
#
# 	SELECT year, country, product, SUM(profit) AS profit
# 	FROM sales
# 	GROUP BY year, country, product WITH ROLLUP
# 	HAVING GROUPING(year, country, product) <> 0;
# +--------+------------+--------+------------+
# | year   | country 	| product| profit 	 |
# +--------+------------+--------+------------+
# | 2000   | Finland 	| NULL 	| 1600 		 |
# | 2000   | India 		| NULL 	| 1350 		 |
# | 2000   | USA 			| NULL   | 1575 		 |
# | 2000   | NULL 		| NULL 	| 4525 		 |
# | 2001   | Finland 	| NULL   | 10 			 |
# | 2001   | USA 			| NULL   | 3000 		 |
# | 2001   | NULL 		| NULL 	| 3010 		 |
# | NULL   | NULL 		| NULL 	| 7535 		 |
# +--------+------------+--------+------------+
#
# The sales table contains no NULL values, so all NULL values in a ROLLUP result represent
# super-aggregate values.
#
# When the data set contains NULL values, ROLLUP summaries may contain NULL values not only
# in super-aggregate rows, but also in regular grouped rows.
#
# GROUPING() enables these to be distinguished.
#
# Suppose that table t1 contains a simple data set with two grouping factors for a
# set of quantity values, where NULL indicates something like "other" or "unknown"
#
# 		SELECT * FROM t1;
# 		+-------+----------+-----------+
# 		| name  | size 	 | quantity  | 
# 		+-------+----------+-----------+
# 		| ball  | small 	 | 10 		 |
# 		| ball  | large 	 | 20 		 |
# 		| ball  | NULL 	 | 5 			 |
# 		| hoop  | small 	 | 15 		 |
# 		| hoop  | large 	 | 5 			 |
# 		| hoop  | NULL 	 | 3 			 |
# 		+-------+----------+-----------+
#
# A simple ROLLUP operation produces these results, in which it is not so easy to distinguish
# NULL values in super-aggregate rows from NULL values in regular grouped rows:
#
# 		SELECT name, size, SUM(quantity) AS quantity
# 		FROM t1
# 		GROUP BY name, size WITH ROLLUP;
# 		+--------+-----------+----------------+
# 		| ball 	| size 		| quantity 		  |
# 		+--------+-----------+----------------+
# 		| ball 	| NULL 		| 	5 				  |
# 		| ball   | large 		|  20 			  |
# 		| ball 	| small 		|  10  			  |
# 		| ball   | NULL 		|  35 			  |
# 		| hoop   | NULL 		|  3 				  |
# 		| hoop   | large 		|  5 				  |
# 		| hoop   | small 		|  15 			  |
# 		| hoop   | NULL 		| 	23 			  |
# 		| NULL 	| NULL 		|  58 			  |
# 		+--------+-----------+----------------+
#
# Using GROUPING() to substitute labels for the super-aggregate NULL values makes the result easier
# to interpret:
#
# 		SELECT
# 			IF(GROUPING(name) = 1, 'All items', name) AS name,
# 			IF(GROUPING(size) = 1, 'All sizes', size) AS size,
# 			SUM(quantity) AS quantity
# 		FROM t1
# 		GROUP BY name, size WITH ROLLUP;
# 	+----------------+----------------+---------------+
# 	| name 			  | size 			 | quantity 	  |
# 	+----------------+----------------+---------------+
# 	| ball 			  | NULL 			 | 	5 			  |
#  | ball 			  | large 			 | 	20 		  |
# 	| ball 			  | small 			 | 	10 		  |
# 	| ball 			  | All sizes 		 | 	35 		  |
# 	| hoop 			  | NULL 			 | 	3 			  |
# 	| hoop 			  | large 			 | 	5 			  |
# 	| hoop 			  | small 			 | 	15 		  |
# 	| hoop 			  | All sizes 		 | 	23 		  |
# 	| All items 	  | All sizes 		 | 	58 		  |
# 	+----------------+----------------+---------------+
#
# OTHER CONSIDERATIONS WHEN USING ROLLUP
#
# The following discussion lists some behaviors specific to the MySQL implementation of ROLLUP.
#
# Prior to 8.0.12, when you use ROLLUP, you cannot also use an ORDER BY clause to sort the results.
# In other words, ROLLUP and ORDER BY were mutually exclusive in MySQL.
#
# However, you still have some control over sort order. To work around the restriction that prevents
# using ROLLUP with ORDER BY and achieve a specific sort order of grouped results, generate the 
# grouped result set as a derived table and apply ORDER BY to it.
#
# For example:
#
# 		SELECT * FROM
# 			(SELECT year, SUM(profit) AS profit
# 			FROM sales GROUP BY year WITH ROLLUP) AS dt
# 		ORDER BY year DESC;
# 		+-------+-----------+
# 		| year  | profit 	  |
# 		+-------+-----------+
# 		| 2001  | 3010 	  |
# 		| 2000  | 4525 	  |
# 		| NULL  | 7535 	  |
# 		+-------+-----------+
#
# As of MySQL 8.0.12, ORDER BY and ROLLUP can be used together, which enables the use
# of ORDER BY and GROUPING() to achieve a specific sort order of grouped results.
#
# For example:
#
# 		SELECT year, SUM(profit) AS profit
# 		FROM sales
# 		GROUP BY year WITH ROLLUP
# 		ORDER BY GROUPING(year) DESC;
# 		+--------+----------+
# 		| year 	| profit   |
# 		+--------+----------+
# 		| NULL 	| 7535 	  |
# 		| 2000   | 4525 	  |
# 		| 2001 	| 3010 	  |
# 		+--------+----------+
#
# In both cases, the super-aggregate summary rows sort with the rows from which
# they are calculated, and their placement depends on sort order (at the end for
# ascending sort, at the beginning for descending sort)
#
# LIMIT can be used to restrict the number of rows returned to the client.
#
# LIMIT is applied after ROLLUP, so the limit applies against the extra rows
# added by ROLLUP.
#
# For example:
#
# 		SELECT year, country, product, SUM(profit) AS profit
# 		FROM sales
# 		GROUP BY year, country, product WITH ROLLUP
# 		LIMIT 5;
# 		+---------+--------------+----------------+-------------+
# 		| year 	 | country 		 | product 			| profit 	  |
# 		+---------+--------------+----------------+-------------+
# 		| 2000    | Finland 		 | Computer 		| 1500 		  |
# 		| 2000 	 | Finland 		 | Phone 			| 100 		  |
# 		| 2000 	 | Finland 		 | NULL 				| 1600 		  |
# 		| 2000 	 | India 		 | Calculator 		| 150 		  |
# 		| 2000    | India 		 | Computer 		| 1200 		  |
# 		+---------+--------------+----------------+-------------+
#
# Using LIMIT with ROLLUP may produce results taht are more difficult to interpret,
# because there is less context for understanding the super-aggregate rows.
#
# The NULL indicators in each super-aggregate row are produced when the row is
# sent to the client.
#
# The server looks at the columns named in the GROUP BY clause following
# the leftmost one that has changed value.
#
# For any column in the result set with a name that matches any of those
# names, its value is set to NULL.
#
# (If you specify grouping columns by column position, the server identifies
# which columns to set to NULL by position)
#
# Because the NULL values in the super-aggregate rows are placed into the result
# set at such a late stage in query processing, you can test them as NULL values
# only in the select list or HAVING clause.
#
# You cannot test them as NULL values in join conditions or the WHERE clause
# to determine which rows to select.
#
# For example, you cannot add WHERE product IS NULL to the query to eliminate
# from the output all but the super-aggregate rows.
#
# The NULL values do appear as NULL on the client side and can be tested as such
# using any MySQL client programming interface.
#
# However, at this point, you cannot distinguish whether a NULL represents
# a regular grouped value or a super-aggregate value.
#
# A MySQL extension permits a column that does not appear in the GROUP BY list 
# to be named in teh select list.
#
# (For information about nonaggregated columns and GROUP BY, see SECTION 12.20.3,
# "MySQL HANDLING OF GROUP BY")
#
# In this case, the server is free to choose any value from this nonaggregated
# column in summary rows, and this includes the extra rows added by WITH ROLLUP.
#
# For example, in the following query, country is a nonaggregated column that does
# not appear in the GROUP BY list and values chosen for this column are nondeterministic:
#
# 		SELECT year, country, SUM(profit) AS profit
# 		FROM sales
# 		GROUP BY year WITH ROLLUP;
# 		+----------+-------------+-----------+
# 		| year 	  | country 	 | profit 	 |
# 		+----------+-------------+-----------+
# 		| 2000 	  | India 		 | 4525 		 |
# 		| 2001 	  | USA 			 | 3010 		 |
# 		| NULL 	  | USA 			 | 7535 		 |
# 		+----------+-------------+-----------+
#
# This behavior is permitted when the ONLY_FULL_GROUP_BY SQL mode is not enabled.
#
# If that mode is enabled, the server rejects the query as illegal because
# country is not listed in the GROUP BY clause.
#
# With ONLY_FULL_GROUP_BY enabled, you can still execute the query by using the
# ANY_VALUE() function for nondeterministic-value columns:
#
# 		SELECT year, ANY_VALUE(country) AS country, SUM(profit) AS profit
# 		FROM sales
# 		GROUP BY year WITH ROLLUP;
# 		+-------+--------------+-----------+
# 		| year  | country 	  | profit 	  |
# 		+-------+--------------+-----------+
# 		| 2000  | India 		  | 4525 	  |
# 		| 2001  | USA 			  | 3010 	  |
# 		| NULL  | USA 			  | 7535		  |
# 		+-------+--------------+-----------+
#
# 12.20.3 MYSQL HANDLING OF GROUP BY
#
# SQL92 and earlier does not permit queries for which the select list, HAVING condition,
# or ORDER BY list refer to nonaggregated columns that are not named in the GROUP BY
# clause.
#
# For example, this query is illegal in standard SQL 92 because the nonaggregated name column
# in the select list does not appear in the GROUP BY:
#
# 		SELECT o.custid, c.name, MAX(o.payment)
# 			FROM orders AS o, customers AS c
# 			WHERE o.custid = c.custid
# 			GROUP BY o.custid;
#
# For the query to be legal in SQL92, the name column must be omitted from the select list
# or named in the GROUP BY clause.
#
# SQL99 and later permits such nonaggregates per optional feature T301 if they are functionalily
# dependent on GROUP BY columns:
#
# 		if such a relationship exists between name and custid, th query is legal.
#
# This would be the case, for example, were custid a primary key of customers.
#
# MySQL implements detection of functional dependence.
#
# If the ONLY_FULL_GROUP_BY SQL mode is enabled (which it is by default), MySQL
# rejects queries for which the select list, HAVING CONDITION or ORDER BY list
# refer to nonaggregated columns that are neither named in teh GROUP BY clause
# nor are functionally dependent on them.
#
# If ONLY_FULL_GROUP_BY is disabled, a MySQL extension to the standard SQL use of
# GROUP BY permits the select list, HAVING condition, or ORDER BY list to refer to
# nonaggregated columns even if the columns are not functionally dependent on
# GROUP BY columns.
#
# This causes MySQL to accept the preceding query. In this case, the server is free
# to choose any value from each group, so unless they are the same, the values
# chosen are nondeterministic - which is probably not what you want.
#
# Furthermore, the selection of values from each group cannot be influenced by
# adding an ORDER BY clause.
#
# Result set sorting occurs after values have been chosen, and ORDER BY does not
# affect which value within each group the server chooses.
#
# Disabling ONLY_FULL_GROUP_BY is useful primarily when you know that, due to
# some property of the data, all values in each nonaggregated column not
# named in the GROUP BY are the same for each group.
#
# You can achieve the same effect without disabling ONLY_FULL_GROUP_BY by using
# ANY_VALUE() to refer to the nonaggregated column.
#
# The following discussion demonstrates functional dependence, the error message
# MySQL produces when functional dependence is absent, and ways of causing
# MySQL to accept a query in the absence of functional dependence.
#
# This query might be invalid with ONLY_FULL_GROUP_BY enabled because the nonaggregated
# address column in the select list is not named in the GROUP BY clause:
#
# 		SELECT name, address, MAX(age) FROM t GROUP BY name;
#
# The query is valid if name is a primary key of t or is a unique NOT NULL column.
#
# In such cases, MySQL recognizes that the selected column is functionally
# dependent on a grouping column.
#
# For example, if name is a primary key, its value determines the value of
# address because each group has only one value of the primary key and thus
# only one row.
#
# As a result, there is no randomness in the choice of address value in a group
# and no need to reject the query.
#
# The query is invalid if name is not a primary key of t or a unique NOT NULL column.
# In this case, no functional dependency can be inferred and an error occurs:
#
# 		SELECT name, address, MAX(age) FROM t GROUP BY name;
# 		ERROR 1055 (42000): Expression #2 of SELECT list is not in GROUP
# 		BY clause and contains nonaggregated column 'mydb.t.address' which
# 		is not functionally dependent on columns in GROUP BY clause;
# 		this is incompatible with sql_mode=only_full_group_by
#
# If you know that, for a given data set, each name value in fact uniquely
# determines the address value, address is effectively functionally dependent
# on name.
#
# To tell MySQL to accept the query, you can use the ANY_VALUE() function:
#
# 		SELECT name, ANY_VALUE(address), MAX(age) FROM t GROUP BY name;
#
# Alternatively, disable ONLY_FULL_GROUP_BY
#
# The preceding example is quite simple, however. In particular, it is unlikely you would
# group on a single primary key because every group would contain only one row.
#
# For additional examples demonstrating functional dependence in more complex queries,
# see SECTION 12.20.4, "DETECTION OF FUNCTIONAL DEPENDENCE"
#
# if a query has aggregate functions and no GROUP BY clause, it cannot have nonaggregated
# columns in the select list, HAVING condition, or ORDER BY list with ONLY_FULL_GROUP_BY enabled:
#
# 		SELECT name, MAX(age) FROM t;
# 		ERROR 1140 (42000): In aggregated query without GROUP BY, expression
# 		#1 of SELECT list contains nonaggregated column 'mydb.t.name'; this
# 		is incompatible with sql_mode=only_full_group_by
#
# Without GROUP BY, there is a single group and it is nondeterministic which name values to choose
# for the group.
#
# Here, too, ANY_VALUE() can be used, if it is immaterial which name value MySQL chooses:
#
# 		SELECT ANY_VALUE(name), MAX(age) FROM t;
#
# ONLY_FULL_GROUP_BY also affects handling of queries that use DISTINCT and ORDER BY.
#
# Consider the case of a table t with three columns c1, c2, and c3 that contains these
# rows:
#
# 		c1 c2 c3
# 		1  2  A
# 		3  4 	B
# 		1  2  C
#
# Suppose that we execute the following query, expecting the results to be ordered
# by c3:
#
# 		SELECT DISTINCT c1, c2 FROM t ORDER BY c3;
#
# To order the result, duplicates must be eliminated first.
#
# But to do so, should we keep the first row or the third?
#
# This arbitrary choice influences the retained value of c3, which in turn
# influences ordering and makes it arbitrary as well.
#
# To prevent this problem, a query that has DISTINCT and ORDER BY is rejected
# as invalid if any ORDER BY expression does not satisfy at least one of these
# conditions:
#
# 		) The expression is equal to one in the select list
#
# 		) All columns referenced by the expression and belonging to the query's selected tables
# 			are elements of the select list.
#
# Another MySQL extension to standard SQL permits references in the HAVING clause to aliased
# expressions in the select list.
#
# For example, the following query returns name values that occur only once in table orders:
#
# 		SELECT name, COUNT(name) FROM orders
# 			GROUP BY name
# 			HAVING COUNT(name) = 1;
#
# The MySQL extension permits the use of an alias in the HAVING clause for the
# aggregated column:
#
# 		SELECT name, COUNT(name) AS c FROM orders
# 			GROUP BY name
# 			HAVING c = 1;
#
# Standard SQL permits only column expressions in GROUP BY clauses, so a statement
# such as this is invalid because FLOOR(value/100) is a noncolumn expression:
#
# 		SELECT id, FLOOR(value/100)
# 			FROM tbl_name
# 			GROUP BY id, FLOOR(value/100);
#
# MySQL extends standard SQL to permit noncolumn expressions in GROUP BY clauses
# and considers the preceding statement valid.
#
# Standard SQL also does not permit alaises in GROUP BY clauses.
#
# MySQL extends standard SQL to permit aliases, so another way to write
# the query is as follows:
#
# 		SELECT id, FLOOR(value/100) AS val
# 			FROM tbl_name
# 			GROUP BY id, val;
#
# The alias val is considered a column expression in the GROUP BY clause.
#
# In the presence of a noncolumn expression in the GROUP BY clause, MySQL recognizes
# equality between that expression and expressions in the select list.
#
# This means that with ONLY_FULL_GROUP_BY SQL mode enabled, tthe query containing 
# GROUP BY id, FLOOR(value/100) is valid because that same FLOOR() expression occurs
# in the select list.
#
# However, MySQL does not try to recognize functional dependence on GROUP BY noncolumn
# expressions, so the following query is invalid with ONLY_FULL_GROUP_BY enabled,
# even though the third selected expression is a simple formula of the id column
# and the FLOOR() expression in the GROUP BY clause:
#
# 		SELECT id, FLOOR(value/100), id+FLOOR(value/100)
# 			FROM tbl_name
# 			GROUP BY id, FLOOR(value/100);
#
# A workaround is to use a derived table:
#
# 		SELECT id, F, id+F
# 		FROM
# 			(SELECT id, FLOOR(value/100) AS F
# 			FROM tbl_name
# 			GROUP BY id, FLOOR(value/100)) AS dt;
#
# 12.20.4 DETECTION OF FUNCTIONAL DEPENDENCE
#
# The following discussion provides several examples of the ways in which MySQL detects
# functional dependencies.
#
# The examples use this notation:
#
# 		{X} -> {Y}
#
# Understand this as "X uniquely determines Y"; which also means that Y is functionally dependent on X.
#
# The examples use the world database, which can be downloaded at <link>.
#
# You can find details on how to install the database on the same page.
#
# 		) FUNCTIONAL DEPENDENCIES DERIVED FROM KEYS
#
# 		) FUNCTIONAL DEPENDENCIES DERIVED FROM MULTIPLE-COLUMN KEYS AND FROM EQUALITIES
#
# 		) FUNCTIONAL DEPENDENCY SPECIAL CASES
#
# 		) FUNCTIONAL DEPENDENCIES AND VIEWS
#
# 		) COMBINATIONS OF FUNCTIONAL DEPENDENCIES
#
# FUNCTIONAL DEPENDENCIES DERIVED FROM KEYS
#
# The following query selects, for each country, a count of spoken languages:
#
# 		SELECT co.Name, COUNT(*)
# 		FROM countrylanguage cl, country co
# 		WHERE cl.CountryCode = co.Code
# 		GROUP BY co.Code;
#
# co.Code is a primary key of co, so all columns of co are functionally dependent on it,
# as expressed using this notation:
#
# 		{co.Code} -> {co.*}
#
# Thus, co.name is functionally dependent on GROUP BY columns and the query is valid.
#
# A UNIQUE index over a NOT NULL column could be used instead of a primary key and the
# same functional dependence would apply.
#
# (This is not true for a UNIQUE index that permits NULL values because it permits multiple
# NULL values and in that case uniqueness is lost)
#
# FUNCTIONAL DEPENDENCIES DERIVED FROM MULTIPLE-COLUMN KEYS AND FROM EQUALITIES
#
# This query selects, for each country, a list of all spoken languages and how many
# people speak them:
#
# 		SELECT co.Name, cl.Language,
# 		cl.Percentage * co.Population / 100.0 AS SpokenBy
# 		FROM countrylanguage cl, country co
# 		WHERE cl.CountryCode = co.Code
# 		GROUP BY cl.CountryCode, cl.Language;
#
# The pair (cl.CountryCode, cl.Language) is a two-column composite primary key
# of cl, so that column pair uniquely determines all columns of cl:
#
# 		{cl.CountryCode, cl.Language} -> {cl.*)
#
# Moreover, because of the equality in the WHERE clause:
#
# 		{cl.CountryCode} -> {co.Code}
#
# And, because co.Code is primary key of co:
#
# 		{co.Code} -> {co.*}
#
# "Uniquely determines" relationships are transitive, therefore:
#
# 		{cl.CountryCode, cl.Language} -> {cl.*,co.*}
#
# As a result, the query is valid.
#
# AS with the previous example, a UNIQUE key over NOT NULL columns could be used
# instead of a primary key.
#
# An INNER JOIN condition can be used instead of WHERE.
#
# The same functional dependencies apply:
#
# 		SELECT co.Name, cl.Language,
# 		cl.Percentage * co.Population/100.0 AS SpokenBy
# 		FROM countrylanguage cl INNER JOIN country co
# 		ON cl.CountryCode = co.Code
# 		GROUP BY cl.CountryCode, cl.Language;
#
# FUNCTIONAL DEPENDENCY SPECIAL CASES
#
# Whereas an equality test in a WHERE condition or INNER JOIN condition is symmetric,
# an equality test in an outer join condition is not, because tables play different roles.
#
# Assume that referential integrity has been accidentally broken and there exists a row
# of countrylanguage without a coresponding row in country.
#
# Consider the same query as in the previous example, but with a LEFT JOIN:
#
# 		SELECT co.Name, cl.Language,
# 		cl.Percentage * co.Population/100.0 AS SpokenBy
# 		FROM countrylanguage cl LEFT JOIN country co
# 		ON cl.CountryCode = co.Code
# 		GROUP BY cl.CountryCode, cl.Language;
#
# For a given value of cl.CountryCode, the value of co.Code in the join
# result is either found in a matching row (determined by cl.CountryCode) or is
# NULL-complemented if there is no match (also determined by cl.CountryCode).
#
# In each case, this relationship applies:
#
# 		{cl.CountryCode} -> {co.Code}
#
# cl.CountryCode is itself functionally dependent on {cl.CountryCode, cl.Language} which
# is a primary key.
#
# If in the join result co.Code is NULL-complemented, co.Name is as well.
#
# If co.Code is not NULL-complemented, then because co.Code is a primary key,
# it determines co.Name. Therefore, in all cases:
#
# 		{co.Code} -> {co.Name}
#
# Which yields:
#
# 		{cl.CountryCode, cl.Language} -> {cl.*,co.*}
#
# As a result, the query is valid.
#
# However, suppose that the tables are swapped, as in this query:
#
# 		SELECT co.Name, cl.Language,
# 		cl.Percentage * co.Population/100.0 AS SpokenBy
# 		FROM country co LEFT JOIN countrylanguage cl
# 		ON cl.CountryCode = co.Code
# 		GROUP BY cl.CountryCode, cl.Language;
#
# Now this relationship does NOT apply:
#
# 		{cl.CountryCode, cl.Language} -> {cl.*,co.*}
#
# Indeed, all NULL-complemented rows made for cl will be put into
# a single group (they have both GROUP BY columns equal to NULL), and
# inside this group the value of co.Name can vary.
#
# It is invalid, and MySQL rejects it.
#
# Functional dependence in outer joins is thus linked to whether determinant
# columns belong to the left or right side of the LEFT JOIN.
#
# Determination of functional dependence becomes more complex if htere are
# nested outer joins or the join condition does not consist entirely
# of equality comparisons.
#
# FUNCTIONAL DEPENDENCIES AND VIEWS
#
# Suppose that a view on countries produces their code, their name in uppercase,
# and how many different official languages they have:
#
# 		CREATE VIEW Country2 AS
# 		SELECT co.Code, UPPER(co.Name) AS UpperName,
# 		COUNT(cl.Language) AS OfficialLanguages
# 		FROM country AS co JOIN countrylanguage AS cl
# 		ON cl.CountryCode = co.Code
# 		WHERE cl.isOfficial = 'T'
# 		GROUP BY co.Code;
#
# This definition is valid because:
#
# 		{co.Code} -> {co.*}
#
# In the view result, the first selected column is co.Code, which is also
# the group column and thus determines all other selected expressions:
#
# 		{Country2.Code} -> {Country2.*}
#
# MySQL understands this and uses this information, as described following.
#
# This query displays countries, how many different official languages they have,
# and how many cities they have, by joining the view with the city table:
#
# 		SELECT co2.Code, co2.UpperName, co2.OfficialLanguages,
# 		COUNT(*) AS Cities
# 		FROM country2 AS co2 JOIN city ci
# 		ON ci.CountryCode = co2.Code
# 		GROUP BY co2.Code;
#
# This query is valid because, as seen previously:
#
# 		{co2.Code} -> {co2.*}
#
# MySQL is able to discover a functional dependency in the result of a view and use
# that to validate a query which uses the view.
#
# The same would be true if country2 were a derived table (or common table expression),
# as in:
#
# 		SELECT co2.Code, co2.UpperName, co2.OfficialLanguages,
# 		COUNT(*) AS Cities
# 		FROM
# 		( 
# 			SELECT co.Code, UPPER(co.Name) AS UpperName,
# 			COUNT(cl.Language) AS OfficialLanguages
# 			FROM country AS co JOIN countrylanguage AS cl
# 			ON cl.CountryCode=co.Code
# 			WHERE cl.isOfficial='T'
# 			GROUP BY co.Code
#  	) AS co2
# 		JOIN city ci ON ci.CountryCode = co2.Code
# 		GROUP BY co2.Code;
#
# COMBINATIONS OF FUNCTIONAL DEPENDENCIES:
#
# MySQL is able to combine all of hte preceding types
# of functional dependencies (key based, equality based, view based)
# to validate more complex queries.
#
# 12.21 WINDOW FUNCTIONS
#
# 12.21.1 WINDOW FUNCTION DESCRIPTIONS
# 12.21.2 WINDOW FUNCTION CONCEPTS AND SYNTAX
# 12.21.3 WINDOW FUNCTION FRAME SPECIFICATION
# 12.21.4 NAMED WINDOWS
# 12.21.5 WINDOW FUNCTION RESTRICTIONS
#
# MySQL supports window functions that, for each row from a query, perform a calculation
# using rows related to that row.
#
# The following sections discuss how to use window functions, including descriptions
# of the OVER and WINDOW clauses.
#
# The first section provides descriptions of the nonaggregate window functions.
# For descriptions of the aggregate window functions, see 12.20.1, "AGGREGATE (GROUP BY) FUNCTION DESCRIPTIONS"
#
# For information about optimization and window functions, see SECTION 8.2.1.19, "WINDOW FUNCTION OPTIMIZATION"
#
# 12.21.1 WINDOW FUNCTION DESCRIPTIONS
#
# This section describes nonaggregate window functions that, for each row from a query,
# perform a calculation using rows related to that row.
#
# Most aggregate functions also can be used as window functions; see SECTION 12.20.1,
# "AGGREGATE (GROUP BY) FUNCTION DESCRIPTIONS"
#
# For window function usage information and examples, and definitions of terms such as
# the OVER clause, window, partition, frame and peer, see SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# TABLE 12.27 WINDOW FUNCTIONS
#
# 		Name 							Description
#
# CUME_DIST() 						Cumulative distribution value
#
# DENSE_RANK() 					Rank of current row within its partition, without gaps
#
# FIRST_VALUE() 					Value of argument from first row of window frame
#
# LAG() 								Value of argument from row lagging current row within partition
#
# LAST_VALUE() 					Value of argument from last row of window frame
#
# LEAD() 							Value of argument from row leading current row within partition
#
# NTH_VALUE() 						Value of argument from N-th row of window frame
#
# NTILE() 							Bucket number of current row within its partition
#
# PERCENT_RANK() 					Percentage rank value 
#
# RANK() 							Rank of current row within its partition, with gaps
#
# ROW_NUMBER() 					Number of current row within its partition
#
# In the following function descriptions, over_clause represents the OVER clause, described
# in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# Some window functions permit a null_treatment clause that specifies how to handle
# NULL values when calculating results.
#
# This clause is optional.
#
# It is part of the SQL standard, but the MySQL implementation permits only RESPECT
# NULLS (which is also the default)
#
# This means that NULL values are considered when calculating results.
#
# IGNORE NULLS is parsed, but produces an error.
#
# 		) CUME_DIST() over_clause
#
# 			Returns the cumulative distribution of a value within a group of values,
# 			that is, the percentage of partition values less than or equal to the value in the current row.
#
# 			This represents the number of rows preceding or peer with the current row in the
# 			window ordering of the window partition divided by the total number of rows
# 			in the window partition.
#
# 			Return values range from 0 to 1
#
# 			This function should be used with ORDER BY to sort partition rows into the desired order.
#
# 			Without ORDER BY, all rows are peers and have value N/N = 1, where N is the partition size.
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 			The following query shows, for the set of values in the val column, the CUME_DIST()
# 			value for each row, as well as the percentage rank value returned by the similar
# 			PERCENT_RANK() function.
#
# 			For reference, the query also displays row numbers using ROW_NUMBER():
#
# 				SELECT
# 					val,
# 					ROW_NUMBER() OVER w AS 'row_number',
# 					CUME_DIST() OVER w AS 'cume_dist',
# 					PERCENT_RANK() OVER w AS 'percent_rank'
# 				FROM numbers
# 				WINDOW w AS (ORDER BY val);
# 				+---------+--------------------+----------------------------+-------------------+
# 				| val 	 | row_number 			 | cume_dist 						| percent_rank 	  |
# 				+---------+--------------------+----------------------------+-------------------+
# 				| 1 		 | 1 						 | 0.22 etc. 						| 0 					  |
# 				| 1 		 | 2 						 | 0.22 etc. 						| 0 					  |
# 				| 2 		 | 3 						 | 0.33 etc 						| 0.25 				  |
# 				| 3 		 | 4 						 | 0.66 etc. 						| 0.375 				  |
# 				| 3 		 | 5 						 | 0.66 etc. 						| 0.375 				  |
# 				| 3 		 | 6 						 | 0.66 etc. 						| 0.375 				  |
# 				| 4 		 | 7 						 | 0.88 etc. 						| 0.75 				  |
# 				| 4 		 | 8 						 | 0.88 etc. 						| 0.75 				  |
# 				| 5 		 | 9 						 | 1 									| 1 					  |
# 				+---------+--------------------+----------------------------+-------------------+
#
# 		) DENSE_RANK() over_clause
#
# 			Returns the rank of the current row within its partition, without gaps.
#
# 			Peers are considered ties and receive the same rank. This function assigns
# 			conesecutive ranks to peer groups; the result is that groups of size greater
# 			than one do not produce noncontiguous rank numbers.
#
# 			For an example, see the RANK() function description.
#
# 			This function should be used with ORDER BY to sort partition rows into
# 			the desired order.
#
# 			Without ORDER BY, all rows are peers.
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 		) FIRST_VALUE(expr) [null_treatment] over_clause
#
# 			Returns the value of expr from the first row of the window frame.
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 			null_treatment is as described in the section introduction.
#
# 			The following query demonstrates FIRST_VALUE(), LAST_VALUE() and two instances
# 			of NTH_VALUE():
#
# 				SELECT
# 					time, subject, val,
# 					FIRST_VALUE(val) OVER w AS 'first',
# 					LAST_VALUE(val) OVER w AS 'last',
# 					NTH_VALUE(val, 2) OVER w AS 'second',
# 					NTH_VALUE(val, 4) OVER w AS 'fourth'
# 				FROM observations
# 				WINDOW w AS (PARTITION BY subject ORDER BY time
# 								ROWS UNBOUNDED PRECEDING);
# 			+-----------------+---------------+--------+--------+-----------+---------+----------+
# 			| time 				| subject 		 | val 	 | first  | last 		 | second  | fourth 	 |
# 			+-----------------+---------------+--------+--------+-----------+---------+----------+
# 			| 07:00:00 		   | st113 			 | 10 	 | 10 	 | 10 		 | NULL 	  | NULL 	 |
# 			| 07:15:00 		   | st113 			 | 9 		 | 10 	 | 9 			 | 9 		  | NULL 	 |
# 			| 07:30:00 			| st113 			 | 25 	 | 10 	 | 25 		 | 9 		  | NULL 	 |
# 			| 07:45:00 			| st113 			 | 20 	 | 10 	 | 20 		 | 9 		  | 20 		 |
# 			| 07:00:00 			| xh458 			 | 0 		 | 0 		 | 0 			 | NULL 	  | NULL 	 |
# 			| 07:15:00 			| xh458 			 | 10 	 | 0 		 | 10 		 | 10 	  | NULL 	 |
# 			| 07:30:00 			| xh458 			 | 5 		 | 0 		 | 5 			 | 10 	  | NULL 	 |
# 			| 07:45:00 			| xh458 			 | 30 	 | 0 		 | 30 		 | 10 	  | 30 		 |
# 			| 08:00:00 			| xh458 			 | 25 	 | 0 		 | 25 		 | 10 	  | 30 		 |
# 			+-----------------+---------------+--------+--------+-----------+---------+----------+
#
# 			Each function uses the rows in the current frame, which, per the window definition shown,
# 			extends from the first partition row to the current row.
#
# 			For the NTH_VALUE() calls, the current frame does not always include the requested row,
# 			in such cases, the return value is NULL
#
# 		) LAG(expr [, N[, default]]) [null_treatment] over_clause
#
# 			Returns the value of expr from the row that lags (precedes) the current row by N rows
# 			within its partition.
#
# 			If there is no such row, the return value is default.
#
# 			For example, if N is 3, the return value is default for the first two rows.
#
# 			If N or default are missing, the defaults are 1 and NULL, respectively.
#
# 			N must be a literal nonnegative integer. If N is 0, expr is evaluated for the current row.
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 			null_treatment is as described in the section intro.
#
# 			LAG() (and the similar LEAD() function) are often used to compute differences between rows.
#
# 			The following query shows a set of time-ordered observations and, for each one,
# 			the LAG() and LEAD() values from the adjoining rows, as well as the differences
# 			between the current and adjoining rows:
#
# 				SELECT
# 					t, val,
# 					LAG(val) 			OVER w AS 'lag',
# 					LEAD(val) 			OVER w AS 'lead',
# 					val - LAG(val) 	OVER w AS 'lag diff',
# 					val - LEAD(val) 	OVER w AS 'lead diff'
# 				FROM series
# 				WINDOW w AS (ORDER BY t);
# 				+--------------------+---------+----------+----------+------------------+--------------------+
# 				| t 						| val 	 | lag 		| lead 	  | lag diff 			| lead diff 			|
# 				+--------------------+---------+----------+----------+------------------+--------------------+
# 				| 12:00:00 				| 100 	 | NULL 		| 125 	  | NULL 				| 	-25 					|
# 				| 13:00:00 				| 125 	 | 100 		| 132 	  | 25 					|  -7 					|
# 				| 14:00:00 				| 132 	 | 125 		| 145 	  | 7 					|  -13 					|
# 				| 15:00:00 				| 145 	 | 132 		| 140 	  | 13 					|  5 						|
# 				| 16:00:00 				| 140 	 | 145 		| 150 	  | -5 					|  -10 					|
# 				| 17:00:00 				| 150 	 | 140 		| 200 	  | 10 					|  -50 					|
# 				| 18:00:00 				| 200 	 | 150 		| NULL 	  | 50 					| 	NULL 					|
# 				+--------------------+---------+----------+----------+------------------+--------------------+
#
# 			In the example, the LAG() and LEAD() calls use the default N and default values of 1 and NULL, respectively.
#
# 			The first row shows what happens when there is no previous row for LAG(): The function returns the
# 			default value (in this case, NULL) 
#
# 			The last row shows the same thing when there is no next row for LEAD()
#
# 			LAG() and LEAD() also serve to compute sums rather than differences.
#
# 			Consider this data set, which contains the first few numbers of the Fibonacci series:
#
# 				SELECT n FROM fib ORDER BY n;
# 				+--------+-
# 				| n 	   |
# 				+--------+
# 				| 1 		|
# 				| 1 		|
# 				| 2 		|
# 				| 3 		|
# 				| 5 		|
# 				| 8 		|
# 				+--------+
#
# 			THe following query shows the LAG() and LEAD() values for the rows adjacent
# 			to the current row.
#
# 			It also uses those functions to add to the current row value the values
# 			from the preceding and following rows.
#
# 			The effect is to generate the next number in the Fibonacci series, and the
# 			next number after that:
#
# 				SELECT
# 					n,
# 					LAG(n, 1, 0) 		OVER w AS 'lag',
# 					LEAD(n, 1, 0) 		OVER w AS 'lead',
# 					n + LAG(n, 1, 0) 	OVER w AS 'next_n',
# 					n + LEAD(n, 1, 0) OVER w AS 'next_next_n'
# 				FROM fib
# 				WINDOW w AS (ORDER BY n);
# 				+--------+----------+---------+-----------+-------------------+
# 				| n 		| lag 	  | lead 	| next_n 	| next_next_n 		  |
# 				+--------+----------+---------+-----------+-------------------+
# 				| 1 		| 0 		  | 1 		| 	1 			| 2 					  |
# 				| 1 		| 1 		  | 2 		|  2 			| 3 					  |
# 				| 2 		| 1 		  | 3 		|  3 		   | 5 					  |
# 				| 3 		| 2 		  | 5 		|  5 			| 8 					  |
# 				| 5 		| 3 		  | 8  		|  8 			| 13 					  |
# 				| 8 		| 5 		  | 0 		|  13 		| 8 					  |
# 				+--------+----------+---------+-----------+-------------------+
#
# 			One way to generate the initial set of Fibonacci numbers is to use a recursive
# 			common table expression.
#
# 			For an example, see FIBONACCI SERIES GENERATION
#
# 		) LAST_VALUE(expr) [null_treatment] over_clause
#
# 			Returns the value of expr from the last row of the window frame
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 			null_treatment is as described in the section intro.
#
# 			For an example, see the FIRST_VALUE() function description
#
# 		) LEAD(expr [, N[, default]]) [null_treatment] over_clause
#
# 			Returns the value of expr from the row that leads (follows) the current row
# 			by N rows within its partition.
#
# 			If there is no such row, the return value is default.
#
# 			For example, if N is 3, the return value is default for the last two rows.
#
# 			If N or default are missing, the defaults are 1 and NULL, respectively.
#
# 			N must be a literal nonnegative integer. If N is 0, expr is evaluated for the
# 			current row.
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 			null_treatment is as described in this section intro
#
# 			For an example, see the LAG() function description
#
# 		) NTH_VALUE(expr, N) [from_first_last] [null_treatment] over_clause
#
# 			Returns the value of expr from the N-th row of the window frame.
# 			If there is no such row, the return value is NULL.
#
# 			N must be a literal positive integer.
#
# 			from_first_last is part of the SQL standard, but the MySQL implementation
# 			permits only FROM FIRST(which is also the default)
#
# 			This means that calculations begin at the first row of the window.
#
# 			FROM LAST is parsed, but produces an error. To obtain the same effect as
# 			FROM LAST (begin calculations at the last row of the window), use 
# 			ORDER BY to sort in reverse order.
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 			null_treatment is as described in the section intro.
#
# 			For an example, see the FIRST_VALUE() function description
#
# 		) NTILE(N) over_clause
#
# 			Divides a partition into N groups (buckets), assigns each row in the partition
# 			its bucket number, and returns the bucket number of the current row within its partition.
#
# 			For example, if N is 4, NTILE() divides rows into four buckets.
#
# 			If N is 100, NTILE() divides rows into 100 buckets.
#
# 			N must be a literal positive integer. Bucket number return values range from 1 to N.
#
# 			This function should be used with ORDER BY to sort partition rows into the
# 			desired order.
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 			The following query shows, for the set of values in the val column,, the percentile values
# 			resulting from dividing the rows into two or four groups.
#
# 			For reference, the query also displays row numbers using ROW_NUMBER():
#
# 				SELECT
# 					val,
# 					ROW_NUMBER() 	OVER w AS 'row_number',
# 					NTILE(2) 		OVER w AS 'ntile2',
# 					NTILE(4) 		OVER w AS 'ntile4'
# 				FROM numbers
# 				WINDOW w AS (ORDER BY val);
# 				+---------+---------------+----------------+------------+
# 				| val 	 | row_number 	  | ntile2 			 | ntile4 	  |
# 				+---------+---------------+----------------+------------+
# 				| 1 		 | 1 				  | 1 				 | 1 			  |
# 				| 1 		 | 2 				  | 1 				 | 1 			  |
# 				| 2 		 | 3 				  | 1 				 | 1 			  |
# 				| 3 		 | 4 				  | 1 				 | 2 			  |
# 				| 3 		 | 5 				  | 1 				 | 2 			  |
# 				| 3 		 | 6 				  | 2 				 | 3 			  |
# 				| 4 		 | 7 				  | 2 				 | 3 			  |
# 				| 4 		 | 8 				  | 2 				 | 4 			  |
# 				| 5 		 | 9 				  | 2 				 | 4 			  |
# 				+---------+---------------+-----------------+-----------+
#
# 		) PERCENT_RANK() over_clause
#
# 			Returns the percentage of partition values less than the value in the current row,
# 			excluding the highest value.
#
# 			Return values range from 0 to 1 and represent the row relative rank, calculated
# 			as the result of this formula, where rank is the row rank and rows is the number
# 			of partition rows:
#
# 				(rank - 1) / (rows - 1)
#
# 			This function should be used with ORDER BY to sort partition rows into the desired order.
#
# 			Without ORDER BY, all rows are peers.
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 			For an example, see the CUME_DIST() function description.
#
# 		) RANK() over_clause
#
# 			Returns the rank of the current row within its partition, with gaps.
#
# 			Peers are considered ties and receive the same rank.
#
# 			This function does not assign consecutive ranks to peer groups if groups
# 			of size greater than one exist; the result is noncontiguous rank numbers.
#
# 			This function should be used with ORDER BY to sort partition rows into the
# 			desired order.
#
# 			Without ORDER BY, all rows are peers.
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 			The following query shows the difference between RANK(), which produces ranks with gaps,
# 			and DENSE_RANK(), which produces ranks without gaps.
#
# 			The query shows rank values for each member of a set of values in the val column,
# 			which contains some duplicates.
#
# 			RANK() assigns peers (the duplicates) the same rank value, and the next greater value
# 			has a rank higher by the number of peers minus one.
#
# 			DENSE_RANK() also assigns peers the same rank value, but the next higher value
# 			has a rank one greater.
#
# 			For reference, the query also displays row numbers using ROW_NUMBER():
#
# 				SELECT
# 					val,
# 					ROW_NUMBER() OVER w AS 'row_number',
# 					RANK() 		 OVER w AS 'rank',
# 					DENSE_RANK() OVER w AS 'dense_rank'
# 				FROM numbers
# 				WINDOW w AS (ORDER BY val);
# 				+--------+--------------------+----------+---------------+
# 				| val 	| row_number 			| rank 	  | dense_rank 	|
# 				+--------+--------------------+----------+---------------+
# 				| 1 		| 1 						| 1 		  | 1 				|
# 				| 1 		| 2 						| 1 		  | 1 				|
# 				| 2 		| 3 						| 3 		  | 2 				|
# 				| 3 	   | 4 						| 4 		  | 3 				|
# 				| 3 		| 5 						| 4 		  | 3 				|
# 				| 3 		| 6 						| 4 		  | 3 				|
# 				| 4 		| 7 						| 7 		  | 4 				|
# 				| 4 		| 8 					   | 7 		  | 4 				|
# 				| 5 		| 9 						| 9 		  | 5 				|
# 				+--------+--------------------+----------+---------------+
#
# 		) ROW_NUMBER() over_clause
#
# 			Returns the number of the current row within its partition.
#
# 			Rows numbers range from 1 to the number of partition rows.
#
# 			ORDER BY affects the order in which rows are numbered.
#
# 			Without ORDER BY, row numbering is nondeterministic.
#
# 			ROW_NUMBER() assigns peers different row numbers. 
#
# 			To assign peers the same value, use RANK() or DENSE_RANK()
#
# 			For an example, see the RANK() function description
#
# 			over_clause is as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 12.21.2 WINDOW FUNCTION CONCEPTS AND SYNTAX
#
# This section describes how to use window functions.
#
# Examples use the same sales information data set as found in the discussion
# of the GROUPING() function in SECTION 12.20.2, "GROUP BY MODIFIERS"
#
# 		SELECT * FROM sales ORDER BY country, year, product;
# 		+----------+-------------+-------------------+-----------+
# 		| year 	  | country 	 | product 			   | profit 	|
# 		+----------+-------------+-------------------+-----------+
# 		| 2000 	  | Finland 	 | Computer 			| 1500 		|
# 		| 2000 	  | Finland 	 | Phone 				| 100 		|
# 		| 2001 	  | Finland 	 | Phone 				| 10 			|
# 		| 2000 	  | India 		 | Calculator 			| 75 			|
# 		| 2000 	  | India 		 | Calculator 			| 75 		   |
# 		| 2000 	  | India 		 | Computer 			| 1200 		|
# 		| 2000 	  | USA 			 | Calculator 			| 75 		   |
# 		| 2000 	  | USA 			 | Computer 			| 1500 		|
# 		| 2001 	  | USA 			 | Calculator 			| 50 			|
# 		| 2001 	  | USA 			 | Computer 			| 1500 		|
# 		| 2001 	  | USA 			 | Computer 			| 1200 		|
# 		| 2001 	  | USA 			 | TV 					| 150 		|
# 		| 2001 	  | USA 			 | TV 					| 100 		|
# 		+----------+-------------+-------------------+-----------+
#
# A window function performs an aggregate-like operation on a set of query rows.
#
# However, whereas an aggregate operation groups query rows into a single
# result row, a window function produces a result for each query row:
#
# 		) The row for which function evaluation occurs is called the current row
#
# 		) The query rows related to the current row over which function evaluation occurs
# 			comprise the window for the current row.
#
# For example, using the sales information table, these two queries perform aggregate
# operations that produce a single global sum for all rows taken as a group, and sums
# grouped per country:
#
# 		SELECT SUM(profit) AS total_profit
# 		FROM sales;
# 		+---------------------------+
# 		| total_profit 				 |
# 		+---------------------------+
# 		| 	7535 							 |
# 		+---------------------------+
#
# 		SELECT country, SUM(profit) AS country_profit
# 		FROM sales
# 		GROUP BY country
# 		ORDER BY country;
# 		+------------+----------------+
# 		| country 	 | country_profit |
# 		+------------+----------------+
# 		| Finland 	 | 		1610 	   |
# 		| India 		 | 		1350 	   |
# 		| USA 		 | 		4575 		|
# 		+------------+----------------+
#
# By contrast, window operations do not collapse groups of query rows to a single
# output row.
#
# Instead, they produce a result for each row.
#
# Like the preceding queries, the following query uses SUM(), but this time
# as a window function:
#
# 		SELECT
# 			year, country, product, profit,
# 			SUM(profit) OVER() AS total_profit,
# 			SUM(profit) OVER(PARTITION BY country) AS country_profit
# 		FROM sales
# 		ORDER BY country, year, product, profit;
# 		+----------+---------------+------------------+-----------+---------------------+-------------------+
# 		| year 	  | country 		| product 			 | profit 	 | total_profit 		  | country_profit 	 |
# 		+----------+---------------+------------------+-----------+---------------------+-------------------+
# 		| 2000 	  | Finland 		| Computer 			 | 1500 		 | 7535 					  | 	1610 				 |
# 		| 2000 	  | Finland 		| Phone 				 | 100 		 | 7535 					  | 	1610 				 |
# 		| 2001 	  | Finland 		| Phone 				 | 10 		 | 7535 					  | 	1610 				 |
# 		| 2000 	  | India 			| Calculator 		 | 75 		 | 7535 					  | 	1350 				 |
# 		| 2000 	  | India 			| Calculator 		 | 75 		 | 7535 					  | 	1350 				 |
# 		| 2000 	  | India 			| Computer 			 | 1200 		 | 7535 					  | 	1350 				 |
# 		| 2000 	  | USA 				| Calculator 		 | 75 		 | 7535 					  | 	4575 				 |
# 		| 2000 	  | USA 				| Computer 			 | 1500 		 | 7535 					  | 	4575 				 |
# 		| 2001 	  | USA 				| Calculator 		 | 50 		 | 7535 					  | 	4575 				 |
# 		| 2001 	  | USA 			   | Computer 			 | 1200 		 | 7535 					  | 	4575 				 |
# 		| 2001 	  | USA 				| Computer 			 | 1500 		 | 7535 					  | 	4575 				 |
# 		| 2001 	  | USA 				| TV 					 | 100 		 | 7535 					  | 	4575 				 |
#		| 2001 	  | USA 				| TV 					 | 150 		 | 7535 					  | 	4575 				 |
# 		+----------+---------------+------------------+-----------+---------------------+-------------------+
#
# Each window operation in the query is signified by inclusion of an OVER clause that specifies
# how to partition query rows into groups for processing by the window function:
#
# 		) The first OVER clause is empty, which treats the entire set of query rows as a single partition.
#
# 			The window function thus produces a global sum, but does so for each row
#
# 		) The second OVER clause partitions rows by country, producing a sum per partition (per country)
#
# 			The function produces this sum for each partition row.
#
# Window functions are permitted only in the select list and ORDER BY clause.
#
# Query result rows are determined from the FROM clause, after WHERE, GROUP BY, and 
# HAVING processing, and windowing execution occurs before ORDER BY, LIMIT and SELECT DISTINCT.
#
# The OVER clause is permitted for many aggregate functions, which therefore can be used
# as window or nonwindow functions, depending on whether the OVER clause is present or absent:
#
# 		AVG()
# 		BIT_AND()
# 		BIT_OR()
#
# 		BIT_XOR()
# 		COUNT()
# 		JSON_ARRAYAGG()
# 			
#		JSON_OBJECTAGG()
# 		MAX()
# 		MIN()
#
# 		STDDEV_POP(), STDDEV(), STD()
# 		STDDEV_SAMP()
# 		SUM()
#
# 		VAR_POP(), VARIANCE()
# 		VAR_SAMP()
#
# For details about each aggregate function, see SECTION 12.20.1, "AGGREGATE (GROUP BY) FUNCTION DESCRIPTIONS"
#
# MySQL also supports nonaggregate functions that are used only as window functions.
#
# For these, the OVER clause is mandatory:
#
# 		CUME_DIST()
#
# 		DENSE_RANK()
#
# 		FIRST_VALUE()
#
# 		LAG()
#
# 		LAST_VALUE()
#
# 		LEAD()
#
# 		NTH_VALUE()
#
# 		NTILE()
#
# 		PERCENT_RANK()
#
# 		RANK()
#
# 		ROW_NUMBER()
#
# For details about each nonaggregate function, see SECTION 12.21.1, "WINDOW FUNCTION DESCRIPTIONS"
#
# As an example of one of those nonaggregate window functions, this query uses ROW_NUMBER(),
# which produces the row number of each row within its partition.
#
# In this case, rows are numbered per country.
#
# By default, partition rows are unordered and row numbering is nondeterministic.
#
# To sort partition rows, include an ORDER BY clause within the window definition.
#
# The query uses unordered and ordered partitions (the row_num1 and row_num2 columns)
# to illustrate the difference between omitting and including ORDER BY:
#
# 		SELECT
# 			year, country, product, profit,
# 			ROW_NUMBER() OVER(PARTITION BY country) AS row_num1,
# 			ROW_NUMBER() OVER(PARTITION BY country ORDER BY year, product) AS row_num2
# 		FROM sales;
#
# +--------+------------------+------------------+---------------+----------------+--------------+
# | year   | country 			| product 			 | profit 		  | row_num1 		 | row_num2 	 |
# +--------+------------------+------------------+---------------+----------------+--------------+
# | 2000   | Finland 			| Computer 			 | 1500 			  | 2 				 | 	1 			 |
# | 2000   | Finland 			| Phone 				 | 100 			  | 1 				 | 	2 			 |
# | 2001   | Finland 			| Phone 				 | 10 			  | 3 				 | 	3 			 |
# | 2000   | India 				| Calculator 		 | 75 			  | 2 				 | 	1 			 |
# | 2000   | India 				| Calculator 		 | 75 			  | 3 				 | 	2 			 |
# | 2000   | India 				| Computer 			 | 1200 			  | 1 				 | 	3 			 |
# | 2000   | USA 					| Calculator 		 | 75 			  | 5 				 | 	1 		    |
# | 2000   | USA 					| Computer 			 | 1500 			  | 4 				 | 	2 			 |
# | 2001   | USA 					| Calculator 		 | 50 			  | 2 				 | 	3 			 |
# | 2001   | USA 					| Computer 			 | 1500 			  | 3 				 | 	4 			 |
# | 2001   | USA 					| Computer 			 | 1200 			  | 7 				 | 	5 			 |
# | 2001   | USA 					| TV 					 | 150 			  | 1 				 | 	6 			 |
# | 2001   | USA 					| TV 					 | 100 			  | 6 				 | 	7 			 |
# +--------+------------------+------------------+---------------+----------------+--------------+
#
# As mentioned previously, to use a window function (or treat an aggregate function as a window function),
# include an OVER clause following the function call.
#
# The OVER clause has two forms:
#
# 		over_clause:
# 			{OVER (window_spec) | OVER window_name}
#
# Both forms define how the window functions should process query rows.
#
# They differ in whether the window is defined directly in the OVER clause, or
# supplied by a reference to a named window defined elsewhere in the query:
#
# 		) In the first case, the window specification appears directly in the OVER clause, between the parentheses
#
# 		) In the second case, window_name is the name for a window specification defined by a WINDOW clause elsewhere
# 			in the query.
#
# 			For details, see SECTION 12.21.4, "NAMED WINDOWS"
#
# For OVER (window_spec) syntax, the window specification has several parts, all optional:
#
# 		window_spec:
# 			[window_name] [partition_clause] [order_clause] [frame_clause]
#
# If OVER() is empty, the window consists of all query rows and the window function computes
# a result using all rows.
#
# Otherwise, the clauses present within the parentheses determine which query rows are used
# to compute the function result and how they are partitioned and ordered:
#
# 		) window_name: The name of a window defined by a WINDOW clause elsewhere in the query.
#
# 			If window_name appears by itself within the OVER clause, it completely defines
# 			the window.
#
# 			If partitioning, ordering, or framing clauses are also given, they modify interpretation
# 			of the named window.
#
# 			For details, SEE SECTION 12.21.4, "NAMED WINDOWS"
#
# 		) partition_clause: A PARTITION BY clause indicates how to divide the query rows into groups.
#
# 			The window function result for a given row is based on the rows of the partition
# 			that contains the row.
#
# 			If PARTITION BY is omitted, there is a single partition consisting of all query rows.
#
# 			NOTE:
#
# 				Partitioning for window functions differs from table partitioning. For more information about
# 				table partitioning, see CHAPTER 23, PARTITIONING
#
# 			partition_clause has this syntax:
#
# 				partition_clause:
# 					PARTITION BY expr [, expr] ---
#
# 			Standard SQL requires PARTITION BY to be followed by column names only.
#
# 			A MySQL extension is to permit expressions, not just column names.
#
# 			For example, if a table contains a TIMESTAMP column named ts, standard SQL
# 			permits PARTITION BY ts but not PARTITION BY HOUR(ts), whereas MySQL permits both.
#
# 		) order_clause: An ORDER BY clause indicates how to sort rows in each partition.
#
# 			Partition rows that are equal according to the ORDER BY clause are considered peers.
#
# 			If ORDER BY is omitted, partition rows are unordered, with no procesing order implied,
# 			and all partition rows are peers.
#
# 			order_clause has this syntax:
#
# 				order_clause:
# 					ORDER BY expr [ASC|DESC] [, expr [ASC|DESC]] ---
#
# 			Each ORDER BY expression optionally can be followed by ASC or DESC to indicate sort direction.
#
# 			The default is ASC if no direction is specified, NULL values sort first for
# 			ascending sorts, last for descending sorts.
#
# 			An ORDER BY in a window definition applies within individual partitions.
#
# 			To sort the result set as a whole, include an ORDER BY at the query top level.
#
# 		) frame_clause: A frame is a subset of the current partition and the frame clause specifies
# 			how to define the subset.
#
# 			The frame clause has many subclasses of its own.
#
# 			For details, see SECTION 12.21.3, "WINDOW FUNCTION FRAME SPECIFICATION"
#
# 12.21.3 WINDOW FUNCTION FRAME SPECIFICATION
#
# The definition of a window used with a window function can include a frame clause.
#
# A frame is a subset of the current partition and the frame clause specifies
# how to define the subset.
#
# Frames are determined with respect to the current row, which enables a frame to
# move within a partition depending on the location of the current row within its
# partition.
#
# Examples:
#
# 		) By defining a frame to be all rows from the partition start to the current row,
# 			you can compute running totals for each row
#
# 		) By defining a frame as extending N rows on either side of the current row,
# 			you can compute rolling averages.
#
# The following query demonstrates the use of moving frames to compute running totals
# within each group of time-ordered level values, as well as rolling averages computed
# from the current row and the rows that immediately precede and follow it:
#
# 		SELECT
# 			time, subject, val,
# 			SUM(val) OVER (PARTITION BY subject ORDER BY time
# 								ROWS UNBOUNDED PRECEDING)
# 			AS running_total,
# 			AVG(val) OVER (PARTITION BY subject ORDER BY time
# 								ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)
# 				AS running_average
# 			FROM observations;
# 		+------------------+---------------+-------------+-------------------+-------------------+
# 		| time 				 | subject 		  | val 			 | running_total 		| running_average   |
# 		+------------------+---------------+-------------+-------------------+-------------------+
# 		| 07:00:00 			 | st113 		  | 10 			 | 		10 			| 9.5000 			  |
# 		| 07:15:00 			 | st113 		  | 9 			 | 		19 			| 14.6667 			  |
# 		| 07:30:00 			 | st113 		  | 25 			 | 		44 		   | 18.0000 			  |
# 		| 07:45:00 			 | st113 		  | 20 			 | 		64 			| 22.5000 			  |
# 		| 07:00:00 			 | xh458 		  | 0 			 | 		0 				| 5.0000 			  |
# 		| 07:15:00 			 | xh458 		  | 10 			 | 		10 			| 5.0000 			  |
# 		| 07:30:00 			 | xh458 		  | 5 			 | 		15 			| 15.0000 			  |
# 		| 07:45:00 			 | xh458 		  | 30 			 | 		45 			| 20.0000 			  |
# 		| 08:00:00 			 | xh458 		  | 25 			 | 		70 			| 27.5000 			  |
# 		+------------------+---------------+-------------+-------------------+-------------------+
#
# For the running_average column, there is no frame row preceding the first one or following the last.
#
# In these cases, AVG() computes the average of the rows that are available.
#
# Aggregate functions used as window functions operate on rows in the current row frame,
# as do these nonaggregate window functions:
#
# 		FIRST_VALUE()
# 		LAST_VALUE()
# 		NTH_VALUE()
#
# Standard SQL specifies that window functions that operate on the entire partition should
# have no frame clause.
#
# MySQL permits a frame clause for such functions but ignores it.
#
# These functions use the entire partition even if a frame is specified:
#
# 		CUME_DIST()
# 
# 		DENSE_RANK()
#
# 		LAG()
#
# 		LEAD()
#
# 		NTILE()
#
# 		PERCENT_RANK()
#
# 		RANK()
#
# 		ROW_NUMBER()
#
# The frame clause,, if given, has this syntax:
#
# 		frame_clause:
# 			frame_units frame_extent
#
# 		frame_units:
# 			{ROWS | RANGE}
#
# In the absence of a frame clause, the default frame depends on whether
# an ORDER BY clause is present, as described later in this section.
#
# The frame_units value indicates the type of relationship between the
# current row and frame rows:
#
# 		) ROWS: The frame is defined by beginning and ending row positions.
#
# 			Offsets are differences in row numbers from the current row number.
#
# 		) RANGE: The frame is defined by rows within a value range.
#
# 			Offsets are differences in row values from the current row value.
#
# The frame_extent value indicates the start and end points of the frame.
#
# You can specify just the start of hte frame (in which case the current row is implicitly the end)
# or use BETWEEN to specify both frame endpoints:
#
# 		frame_extent:
# 			{frame_start | frame_between}
#
# 		frame_between:
# 			BETWEEN frame_start AND frame_end
#
# 		frame_start, frame_end: {
# 			CURRENT ROW
# 		 | UNBOUNDED PRECEDING
# 		 | UNBOUNDED FOLLOWING
# 		 | expr PRECEDING
# 		 | expr FOLLOWING
# 		}
#
# With BETWEEN syntax, frame_start must not occur later than frame_end
#
# The permitted frame_start and frame_end values have these meanings:
#
# 		) CURRENT ROW: For ROWS, the bound is the current row. For RANGE, the bounds is the peers of the current row.
#
# 		) UNBOUNDED PRECEDING: The bound is the first partition row
#
# 		) UNBOUNDED FOLLOWING: The bound is the last partition row
#
# 		) expr PRECEDING: For ROWS, the bound is expr rows before the current row.
#
# 			For RANGE, the bound is the rows with values equal to the current row value
# 			minus expr; if the current row value is NULL, the bound is the peers of the row.
#
# 			For expr PRECEDING (and expr FOLLOWING) expr can be a ? parameter marker
# 			(for use in a prepared statement), a nonnegative numeric literal, or a temporal
# 			interval of the form INTERVAL val unit.
#
# 			For INTERVAL expressions, val specifies nonnegative interval value,
# 			and unit is a keyword indicating the units in which the value should be interpreted.
#
# 			(For details about the permitted units specifiers, see the description of the DATE_ADD()
# 			function in SECTION 12.7, "DATE AND TIME FUNCTIONS")
#
# 			RANGE on a numeric or temporal expr requires ORDER BY on a numeric or temporal expression,
# 			respectively.
#
# 			Examples of valid expr PRECEDING and expr FOLLOWING indicators:
#
# 				10 PRECEDING
# 				INTERVAL 5 DAY PRECEDING
# 				5 FOLLOWING
# 				INTERVAL '2:30' MINUTE_SECOND FOLLOWING
#
# 		) expr FOLLOWING: For ROWS, the bound is expr rows after the current row.
#
# 			For RANGE, the bound is the rows with values equal to the current row
# 			value plus expr; if the current row value is NULL, the bound is the
# 			peers of hte row.
#
# 			For permitted values of expr, see the description of expr PRECEDING
#
# The following query demonstrates FIRST_VALUE(), LAST_VALUE() and two instaces of NTH_VALUE():
#
# 		SELECT
# 			time, subject, val,
# 			FIRST_VALUE(val) 		OVER w AS 'first',
# 			LAST_VALUE(val) 		OVER w AS 'last',
# 			NTH_VALUE(val, 2) 	OVER w AS 'second',
# 			NTH_VALUE(val, 4) 	OVER w AS 'fourth'
# 		FROM observations
# 		WINDOW w AS (PARTITION BY subject ORDER BY time
# 						ROWS UNBOUNDED PRECEDING);
#
# 		+--------------+------------+---------+----------+-----------+-------------+--------+
# 		| time 			| subject 	 | val 	  | first 	 | last 	    | second 		| fourth |
# 		+--------------+------------+---------+----------+-----------+-------------+--------+
# 		| 07:00:00 		| st113 		 | 10 	  | 10 		 | 10 		 | NULL 		   | NULL 	|
# 		| 07:15:00     | st113 		 | 9 		  | 10 		 | 9 			 | 9 				| NULL   |
# 		| 07:30:00     | st113 		 | 25 	  | 10 		 | 25 		 | 9 				| NULL   |
# 		| 07:45:00 		| st113 		 | 20 	  | 10 		 | 20 		 | 9 				| 20 		|
# 		| 07:00:00  	| xh458 		 | 0 		  | 0 		 | 0 			 | NULL 		   | NULL 	|
# 		| 07:15:00     | xh458 		 | 10 	  | 0 		 | 10 		 | 10 			| NULL 	|
# 		| 07:30:00 		| xh458 		 | 5 		  | 0 		 | 5 			 | 10 			| NULL 	|
# 		| 07:45:00 	   | xh458 		 | 30 	  | 0 		 | 30 		 | 10 			| 30 		|
# 		| 08:00:00 		| xh458 		 | 25 	  | 0 		 | 25 		 | 10 			| 30 	   |
# 		+--------------+------------+---------+----------+-----------+-------------+--------+
#
# Each function uses the rows in the current frame, which, per the window definition shown,
# extends from the first partition row to the current row.
#
# For the NTH_VALUE() calls, the current frame does not always include the requested row;
# in such cases, the return value is NULL
#
# In the absence of a frame clause, the default frame depends on whether an ORDER BY clause
# is present:
#
# 		) With ORDER BY: 
#
# 			The default frame includes rows from the partition start through
# 			the current row, including all peers of the current row (rows equal to the current
# 			row according to the ORDER BY clause)
#			
# 			The default is equivalent to this frame specification:
#
# 				RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
#
# 		) Without ORDER BY:
#
# 			The default frame includes all partition rows (because, without ORDER BY, all partition
# 			rows are peers)
#
# 			The default is equivalent to this frame specification:
#
# 				RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
#
# Because the default frame differs depending on presence or absence of ORDER BY,
# adding ORDER BY to a query to get deterministic results may change the results.
#
# (For example, the values produced by SUM() might change)
#
# TO obtain the same results but ordered per ORDER BY, provide an explicit
# frame specification to be used regardless of whether ORDER BY is present.
#
# The meaning of a frame specification can be nonobvious when the current
# row value is NULL.
#
# Assuming that to be the case, these examples illustrate how various frame
# specifications apply:
#
# 		) ORDER BY X ASC RANGE BETWEEN 10 FOLLOWING AND 15 FOLLOWING
#
# 			The frame starts at NULL and stops at NULL, thus includes only rows with value NULL
#
# 		) ORDER BY X ASC RANGE BETWEEN 10 FOLLOWING AND UNBOUNDED FOLLOWING
#
# 			The frame starts at NULL and stops at the end of the partition.
#
# 			Because an ASC sort puts NULL values first, the frame is the entire partition.
#
# 		) ORDER BY X DESC RANGE BETWEEN 10 FOLLOWING AND UNBOUNDED FOLLOWING
#
# 			The frame starts at NULL and stops at the end of the partition.
#
# 			Because a DESC sort puts NULL values last, the frame is only the NULL values.
#
# 		) ORDER BY X ASC RANGE BETWEEN 10 PRECEDING AND UNBOUNDED FOLLOWING
#
# 			The frame starts at NULL and stops at teh end of the partition.
#
# 			Because an ASC sort puts NULL values first, the frame is the entire partition.
#
# 		) ORDER BY X ASC RANGE BETWEEN 10 PRECEDING AND 10 FOLLOWING
#
# 			The frame starts at NULL and stops at NULL, thus includes only rows with value NULL
#
# 		) ORDER BY X ASC RANGE BETWEEN 10 PRECEDING AND 1 PRECEDING
#
# 			The frame starts at NULL and stops at NULL, thus includes only rows with value NULL
#
# 		) ORDER BY X ASC RANGE BETWEEN UNBOUNDED PRECEDING AND 10 FOLLOWING
#
# 			The frame starts at the beginning of the partition and stops at rows with value NULL.
#
# 			Because n ASC sort puts NULL values first, the frame is only the NULL values
#
# 12.21.4 NAMED WINDOWS
#
# Windows can be defined and given names by which to refer to them in OVER clauses.
#
# To do this, use a WINDOW clause.
#
# If present in a query, the WINDOW clause falls between the positions of the
# HAVING and ORDER BY clauses, and has this syntax:
#
# 		WINDOW window_name AS (window_spec)
# 			[, window_name AS (window_spec)] ---
#
# For each window definition, window_name is the window name, and window_spec is the
# same type of window specification as given between the parentheses of an OVER
# clause, as described in SECTION 12.21.2, "WINDOW FUNCTION CONCEPTS AND SYNTAX"
#
# 		window_spec:
# 			[window_name] [partition_clause] [order_clause] [frame_clause]
#
# A WINDOW clause is useful for queries in which multiple OVER clauses would otherwise
# define the same window.
#
# Instead, you can define the window once, give it a name, and refer to the name
# in teh OVER clauses.
#
# Consider this query, which defines the same window multiple times:
#
# 		SELECT
# 			val,
# 			ROW_NUMBER() OVER (ORDER BY val) AS 'row_number',
# 			RANK() 		 OVER (ORDER BY val) AS 'rank',
# 			DENSE_RANK() OVER (ORDER BY val) AS 'dense_rank'
# 		FROM numbers;
#
# The query can be written more simply by using WINDOW to define the window once and referring
# to the window by name in the OVER clauses:
#
# 		SELECT
# 			val,
# 			ROW_NUMBER() OVER w AS 'row_number',
# 			RANK() 		 OVER w AS 'rank',
# 			DENSE_RANK() OVER w AS 'dense_rank'
# 		FROM numbers
# 		WINDOW w AS (ORDER BY val);
#
# A named window also makes it easier to experiment with the window definition
# to see the effect on query results.
#
# YOu need only modify the window definition in the WINDOW clause, rather than
# multiple OVER clause definitions.
#
# If an OVER clause uses OVER (window_name ---) rather than OVER window_name,
# the named window can be modified by the addition of other clauses.
#
# For example, this query defines a window that includes partitioning, and uses
# ORDER BY in the OVER clauses to modify the window in different ways:
#
# 		SELECT
# 			DISTINCT year, country,
# 			FIRST_VALUE(year) OVER (w ORDER BY year ASC) AS first,
# 			FIRST_VALUE(year) OVER (w ORDER BY year DESC) AS last
# 		FROM sales
# 		WINDOW w AS (PARTITION BY country);
#
# An OVER clause cna only add properties to a named window, not modify them.
#
# If the named window definition includes a partitioning, ordering or framing
# property, the OVER clause that refers to the window name cannot also include
# the same kind of property or an error occurs:
#
# 		) This construct is permitted because the window definition and the referring
# 			OVER clause do not contain the same kind of properties:
#
# 			OVER (w ORDER BY country)
# 			--- WINDOW w AS (PARTITION BY country)
#
# 		) This construct is not permitted because the OVER clause specifies PARTITION BY 
# 			for a named window that already has PARTITION BY:
#
# 			OVER (w PARTITION BY year)
# 			--- WINDOW w AS (PARTITION BY country)
#
# The definition of a named window can itself begin with a window_name.
#
# In such cases, forward and backward references are permitted, but not cycles:
#
# 		) This is permitted; it contains forward and backward references but no cycles:
#
# 			WINDOW w1 AS (w2), w2 AS (), w3 AS (w1)
#
# 		) This is not permitted because it contains a cycle:
#
# 			WINDOW w1 AS (w2), w2 AS (w3), w3 AS (w1)
#
# 12.21.5 WINDOW FUNCTION RESTRICTIONS
#
# The SQL standard imposes a constraint on window functions that they cannot
# be used in UPDATE or DELETE statements to update rows.
#
# Using such functions in a subquery of these statements (to select rows)
# is permitted.
#
# MySQL does not support these window function features:
#
# 		) DISTINCT syntax for aggregate window functions
#
# 		) Nested window functions
#
# 		) Dynamic frame endpoints that depend on the value of the current row
#
# THe parser recognizes these window constructs which nevertheless are not
# supported:
#
# 		) The GROUPS frame units specifier is parsed, but produces an error.
#
# 			Only ROWS and RANGE are supported.
#
# 		) The EXCLUDE clause for frame specification is parsed, but produces an error.
#
# 		) IGNORE NULLS is parsed, but produces an error. Only RESPECT NULLS
# 			is supported.
#
# 		) FROM LAST is parsed, but produces an error. Only FROM FIRST is supported.
#
# 12.22 INTERNAL FUNCTIONS
#
# TABLE 12.28 INTERNAL FUNCTIONS
#
# NAME 								Desc
#
# CAN_ACCESS_COLUMN() 			Internal use only
#
# CAN_ACCESS_DATABASE() 		Internal
#
# CAN_ACCESS_TABLE() 			Internal
#
# CAN_ACCESS_VIEW() 				Internal
#
# GET_DD_COLUMN_PRIVILEGES() 	Internal
#
# GET_DD_CREATE_OPTIONS() 		Internal
#
# GET_DD_INDEX_SUB_PART_LENGTH() Internal
#
# INTERNAL_AUTO_INCREMENT() 	Internal
#
# INTERNAL_AVG_ROW_LENGTH() 	Internal
#
# INTERNAL_CHECK_TIME() 		Internal
#
# INTERNAL_CHECKSUM() 			Internal
#
# INTERNAL_DATA_FREE() 			Internal
#
# INTERNAL_DATA_LENGTH() 		Internal
#
# INTERNAL_DD_CHAR_LENGTH() 	Internal
#
# INTERNAL_GET_COMMENT_OR_ERROR() Internal
#
# INTERNAL_GET_VIEW_WARNING_OR_ERROR() Internal
#
# INTERNAL_INDEX_COLUMN_CARDINALITY() Internal
#
# INTERNAL_INDEX_LENGTH() 		Internal
#
# INTERNAL_KEYS_DISABLED() 	INternal
#
# INTERNAL_MAX_DATA_LENGTH() 	Internal
#
# INTERNAL_TABLE_ROWS() 		internal
#
# INTERNAL_UPDATE_TIME() 		internal
#
# The functions listed in this section are intended only for internal use by teh server.
#
# Attempts by users to invoke them, results in an error.
#
# 		) CAN_ACCESS_COLUMN(ARGS)
#
# 		) CAN_ACCESS_DATABASE(ARGS)
#
# 		) CAN_ACCESS_TABLE(ARGS)
#
# 		) CAN_ACCESS_VIEW(ARGS)
#
# 		) GET_DD_COLUMN_PRIVILEGES(ARGS)
#
# 		) GET_DD_CREATE_OPTIONS(ARGS)
#
# 		) GET_DD_INDEX_SUB_PART_LENGTH(ARGS)
#
# 		) INTERNAL_AUTO_INCREMENT(ARGS)
#
# 		) INTERNAL_AVG_ROW_LENGTH(ARGS)
#
# 		) INTERNAL_CHECK_TIME(ARGS)
#
# 		) INTERNAL_CHECKSUM(ARGS)
#
# 		) INTERNAL_DATA_FREE(ARGS)
#
# 		) INTERNAL_DATA_LENGTH(ARGS)
#
# 		) INTERNAL_DD_CHAR_LENGTH(ARGS)
#
# 		) INTERNAL_GET_COMMENT_OR_ERROR(ARGS)
#
# 		) INTERNAL_GET_VIEW_WARNING_OR_ERROR(ARGS)
#
# 		) INTERNAL_INDEX_COLUMN_CARDINALITY(ARGS)
#
# 		) INTERNAL_INDEX_LENGTH(ARGS)
#
# 		) INTERNAL_KEYS_DISABLED(ARGS)
#
# 		) INTERNAL_MAX_DATA_LENGTH(ARGS)
#
# 		) INTERNAL_TABLE_ROWS(ARGS)
#
# 		) INTERNAL_UPDATE_TIME(ARGS)
#
# 		) IS_VISIBLE_DD_OBJECT(ARGS)
#
# 12.23 MISCELLANEOUS FUNCTIONS
#
# TABLE 12.29 MISCELLANEOUS FUNCTIONS
#
# 		NAME 									DESCRIPTION
# ANY_VALUE() 			Suppress ONLY_FULL_GROUP_BY value rejection
#
# BIN_TO_UUID() 		Convert binary UUID to string
#
# DEFAULT() 			Return the default value for a table column
#
# GROUPING() 			Distinguish super-aggregate ROLLUP rows from regular rows
#
# INET_ATON() 			Return the numeric value of an IP address
#
# INET_NTOA() 			Return the IP address from a numeric value
#
# INET6_ATON() 		Return the numeric value of an IPv6 address
#
# INET6_NTOA() 		Return the IPv6 address from a numeric value
#
# IS_IPV4() 			Whether argument is an IPv4 address
#
# IS_IPV4_COMPAT() 	Whether argument is an IPv4-compatible address
#
# IS_IPV4_MAPPED() 	Whether argument is an IPV4-mapped address
#
# IS_IPV6() 			Whether argument is an IPv6 address
#
# IS_UUID() 			Whether argument is a valid UUID
#
# MASTER_POS_WAIT() 	Block until the slave has read and applied all updates up to teh specific position
#
# NAME_CONST() 		Cause the column to have the given name
#
# RAND() 				Return a random floating-point value
#
# SLEEP() 				Sleep for a number of seconds
#
# UUID() 				Returns a Universal Unique Identifier (UUID)
#
# UUID_SHORT() 		Return an integer-valued universal identifier
#
# UUID_TO_BIN() 		Convert string UUID to binary
#
# VALUES() 				Defines the values to be used during an INSERT
#
# 		) ANY_VALUE(arg)
#
# 			THis function is useful for GROUP BY queries when the ONLY_FULL_GROUP_BY SQL mode is enabled,
# 			for cases when MySQL rejects a query that you know is valid for reasons that MySQL
# 			cannot determine.
#
# 			The function return value  and type are the same as the return value and type of its argument,
# 			but the function result is not checked for the ONLY_FULL_GROUP_BY SQL mode.
#
# 			For example, if name is a nonindexed column, the following query fails with
# 			ONLY_FULL_GROUP_BY enabled:
#
# 				SELECT name, address, MAX(age) FROM t GROUP BY name;
# 				ERROR 1055 (42000): Expression #2 of SELECT list is not
# 				in GROUP BY clause and contains nonaggregated column
# 				'mydb.t.address' which is not functionally dependent
# 				on columns in GROUP BY clause; this is incompatible
# 				with sql_mode=only_full_group_by
#
# 			THe failure occurs because address is a nonaggregated column that is
# 			neither named among GROUP BY columns nor functionally dependent on them.
#
# 			As a result, the address value for rows within each name group is nondeterminsitic.
#
# 			There are multiple ways to cause MySQL to accept the query:
#
# 				) Alter the table to make name a primary key or a unique NOT NULL column.
#
# 					This enables MySQL to determine that address is functionally dependent
# 					on name;
#
# 					That is, address is uniquely determined by name. (This technique is inapplicable if NULL
# 					must be permitted as a valid name value)
#
# 				) Use ANY_VALUE() to refer to address:
#
# 					SELECT name, ANY_VALUE(address), MAX(age) FROM t GROUP BY name;
#
# 					In this case, MySQL ignores teh nondeterminism of address values within
# 					each name group and accepts the query.
#
# 					This may be useful if you simply do not care which value of a nonaggregated
# 					column is chosen for each group.
#
# 					ANY_VALUE() is not an aggregate function, unlike functions such as SUM()
# 					or COUNT()
#
# 					It simply acts to suppress the test for nondeterminism
#
# 				) Disable ONLY_FULL_GROUP_BY.
#
# 					This is equivalent to using ANY_VALUE() with ONLY_FULL_GROUP_BY enabled,
# 					as described in previous time.
#
# 			ANY_VALUE() is also useful if funcitonal dependence exists between columns but
# 			MySQL cannot determine it.
#
# 			The following query is valid because age is functionally dependent
# 			on the grouping column age-1, but MySQL cannot tell that and rejects
# 			the query with ONLY_FULL_GROUP_BY enabled:
#
# 				SELECT age FROM t GROUP BY age-1;
#
# 			To cause MySQL to accept the query, use ANY_VALUE():
#
# 				SELECT ANY_VALUE(age) FROM t GROUP BY age-1;
#
# 			ANY_VALUE() can be used for queries that refer to aggregate functions
# 			in the absence of a GROUP BY clause:
#
# 				SELECT name, MAX(age) FROM t;
# 				ERROR 1140 (42000): In aggregated query without GROUP BY, expression
# 				#1 of SELECT list contains nonaggregated column 'mydb.t.name'; this
# 				is incompatible with sql_mode=only_full_group_by
#
# 			Without GROUP BY, there is a single group and it is nondeterminsitic which name value
# 			to choose for the group.
#
# 			ANY_VALUE() tells MySQL to accept the query:
#
# 				SELECT ANY_VALUE(name), MAX(age) FROM t;
#
# 			It may be that, due to some property of a given data set, you know that a selected
# 			nonaggregated column is effectively functionally dependent on a GROUP BY column.
#
# 			For example, an application may enforce uniqueness of one column with respect
# 			to another.
#
# 			In this case, using ANY_VALUE() for hte effectively functioanlly dependent
# 			column may make sense.
#
# 			For additional discussion, see SECTION 12.20.3, "MySQL HANDLING OF GROUP BY"
#
# 		) BIN_TO_UUID(binary uuid), BIN_TO_UUID(binary uuid, swap flag)
#
# 			BIN_TO_UUID() is the inverse of UUID_TO_BIN()
#
# 			It converts a binary UUID to a string UUID and returns the result.
#
# 			The binary value should be a UUID as a VARBINARY(16) value
#
# 			The return value is a utf8 string of five hexadecimal numbers separated
# 			by dashes.
#
# 			(For details about this format, see the UUID() function description)
#
# 			If the UUID argument is NULL, the return value is NULL.
#
# 			If any argument is invalid, an error occurs.
#
# 			BIN_TO_UUID() takes one orw two arguments:
#
# 				) The one-argument form takes a binary UUID value.
#
# 					the UUID value is assumed not to have its time-low and time-high
# 					parts swapped.
#
# 					The string result is in the same order as the binary argument.
#
# 				) The two argument form takes a binary UUID value and a swap-flag value:
#
# 					) If swap_flag is 0, the two-argument form is equivalent to the one-argument form.
#
# 						The string result is in the same order as the binary argument.
#
# 					) If swap_flag is 1, the UUID value is assumed to have its time-low and time-high parts
# 						swapped.
#
# 						These parts are swapped back to their original position in the result value.
#
# 				For usage examples and information about time-part swapping, see the UUID_TO_BIN() function description.
#
# 		) DEFAULT(col name)
#
# 			Returns the default value for a table column. An error results if the column has no default value.
#
# 			The use of DEFAULT(col name) to specify the default value for a named column is permitted
# 			only for columns that have a literal default value, not for columns that have an expression
# 			default value.
#
# 				UPDATE t SET i = DEFAULT(i)+1 WHERE id < 100;
#
# 		) FORMAT(X,D)
#
# 			Formats the number X to a format like '#,###,###.##', rounded to D decimal places,
# 			and returns the result as a string.
#
# 			For details, see SECTION 12.5, "STRING FUNCTIONS"
#
# 		) GROUPING(expr [, expr] ---)
#
# 			For GROUP BY queries that include a WITH ROLLUP modifier, the ROLLUP operation
# 			produces super-aggregate output rows where NULL represents the set of all values.
#
# 			The GROUPING() function enables you to distinguish NULL values for super-aggregate rows
# 			from NULL values in regular grouped rows.
#
# 			GROUPING() is permitted only in the select list or HAVING clause.
#
# 			Each argument to GROUPING() must be an expression that exactly matches an expression
# 			in the GROUP BY clause.
#
# 			The expression cannot be a positional specifier.
#
# 			For each expression, GROUPING() produces 1 if the expression value in the
# 			current row is a NULL representing a super-aggregate value.
#
# 			Otherwise, GROUPING() produces 0, indicating that the expression value
# 			is a NULL for a regular result row or is not NULL.
#
# 			Suppose that table t1 contains these rows, where NULL indicates something
# 			like "other" or "unknown"
#
# 				SELECT * FROM t1;
# 				+--------+------------+-----------------+
# 				| name   | size 		 | quantity 		 |
# 				+--------+------------+-----------------+
# 				| ball   | small 		 | 10 				 |
# 				| ball   | large 		 | 20 				 |
# 				| ball   | NULL 		 | 5 					 |
# 				| hoop   | small 		 | 15 				 |
# 				| hoop   | large 		 | 5 					 |
# 				| hoop   | NULL 		 | 3 					 |
# 				+--------+------------+-----------------+
#
# 			A summary of the table without WITH ROLLUP looks like this:
#
# 				SELECT name, size, SUM(quantity) AS quantity
# 				FROM t1
# 				GROUP BY name, size;
# 				+--------+---------+-----------+
# 				| name   | size 	 | quantity  |
# 				+--------+---------+-----------+
# 				| ball   | small   | 10 		 |
# 				| ball   | large   | 20 		 |
# 				| ball   | NULL    | 5 			 |
# 				| hoop   | small   | 15 		 |
# 				| hoop   | large   | 5 			 |
# 				| hoop   | NULL 	 | 3 			 |
# 				+--------+---------+-----------+
#
# 			The result contains NULL values, but those do not represent super-aggregate rows because the
# 			query does not include WITH ROLLUP.
#
# 			adding WITH ROLLUP produces super-aggregate summary rows containing additional NULL values.
#
# 			However, without comparing this result to the previous one, it is not easy to see
# 			which NULL values occur in super-aggregate rows and which occur in regular grouped rows:
#
# 				SELECT name, size, SUM(quantity) AS quantity
# 				FROM t1
# 				GROUP BY name, size WITH ROLLUP;
# 				+--------+--------+----------------+
# 				| name   | size 	| quantity 		  |
# 				+--------+--------+----------------+
# 				| ball   | NULL   | 	5 				  |
# 				| ball   | large  |  20 			  |
# 				| ball   | small  |  10 			  |
# 				| ball   | NULL   | 	35 			  |
# 				| hoop   | NULL 	| 	3 				  |
# 				| hoop   | large  |  5 				  |
# 				| hoop   | small  |  15 			  |
# 				| hoop   | NULL   | 	23 			  |
# 				| NULL   | NULL   | 	58 			  |
# 				+--------+--------+----------------+
#
# 			To distinguish NULL values in super-aggregate rows from those in regular grouped rows,
# 			use GROUPING(), which returns 1 only for super-aggregate NULL values:
#
# 				SELECT
# 					name, size, SUM(quantity) AS quantity,
# 					GROUPING(name) AS grp_name,
# 					GROUPING(size) AS grp_size
# 				FROM t1
# 				GROUP BY name, size WITH ROLLUP;
# 				+-------+--------+------------------+-----------------+--------------------+
# 				| name  | size   | quantity 			| grp_name 			| grp_size 				|
# 				+-------+--------+------------------+-----------------+--------------------+
# 				| ball  | NULL   | 		5 				| 			0 			| 		0 					|
# 				| ball  | large  | 		20 			| 			0 			| 		0 					|
# 				| ball  | small  | 		10 			| 			0 			| 		0 					|
# 				| ball  | NULL   | 		35 			| 			0 			| 		1 					|
# 				| hoop  | NULL   | 		3 				| 			0 			| 		0 					|
# 				| hoop  | large  | 		5 				| 			0 			| 		0 					|
# 				| hoop  | small  | 		15 			| 			0 			| 		0 					|
# 				| hoop  | NULL   | 		23 			| 			0 			| 		1 					|
# 				| NULL  | NULL   | 		58 			| 			1 			| 		1 					|
# 				+-------+--------+------------------+-----------------+--------------------+
#
# 			Common uses for GROUPING():
#
# 				) Substitute a label for super-aggregate NULL values:
#
# 					SELECT
# 						IF(GROUPING(name) = 1, 'All items', name) AS name,
# 						IF(GROUPING(size) = 1, 'All sizes', size) AS size,
# 						SUM(quantity) AS quantity
# 					FROM t1
# 					GROUP BY name, size WITH ROLLUP;
# 					+-----------------+-------------------+---------------+
# 					| name 				| size 				  | quantity 		|
# 					+-----------------+-------------------+---------------+
# 					| ball 				| NULL 				  | 5 				|
# 					| ball 				| large 				  | 20 				|
# 					| ball 				| small 				  | 10 			   |
# 					| ball 				| All sizes 		  | 35 				|
# 					| hoop 				| NULL 				  | 3 				|
# 					| hoop 				| large 				  | 5 				|
# 					| hoop 				| small 				  | 15 			 	|
# 					| hoop 				| All sizes 		  | 23 				|
# 					| All items 		| All sizes 		  | 58 				|
# 					+-----------------+-------------------+---------------+
#
# 				) Return only super-aggregate lines by filtering out the regular grouped lines:
#
# 					SELECT name, size, SUM(quantity) AS quantity
# 					FROM t1
# 					GROUP BY name, size WITH ROLLUP
# 					HAVING GROUPING(name) = 1 OR GROUPING(size) = 1;
# 					+-----------+-----------+-----------------+
# 					| name 		| size 		| quantity 			|
# 					+-----------+-----------+-----------------+
# 					| ball 		| NULL 		| 35 					|
# 					| hoop      | NULL 		| 23 					|
# 					| NULL 		| NULL 		| 58 					|
# 					+-----------+-----------+-----------------+
#
# 			GROUPING() permits multiple expression arguments. In this case, the GROUPING()
# 			return value represents a bitmask combined from the results for each expression,
# 			where the lowest-order bit corresponds to the result for the rightmost expression.
#
# 			For example, with three expression arguments, GROUPING(expr1, expr2, expr3) is evaluated
# 			like this:
#
# 				result for GROUPING(expr3)
# 			 + result for GROUPING(expr2) << 1
# 			 + result for GROUPING(expr1) << 2
#
# 			The following query shows how GROUPING() results for single arguments combine for 
# 			a multiple-argument call to produce a bitmask value:
#
# 				SELECT 
# 					name, size, SUM(quantity) AS quantity,
# 					GROUPING(name) AS grp_name,
# 					GROUPING(size) AS grp_size,
# 				GROUPING(name, size) AS grp_all
# 				FROM t1
# 				GROUP BY name, size WITH ROLLUP;
# 				+--------+------------+------------------+-------------+----------+--------------+
# 				| name   | size 		 | quantity 		  | grp_name    | grp_size | grp_all 		|
# 				+--------+------------+------------------+-------------+----------+--------------+
# 				| ball   | NULL 		 | 5 					  | 0 			 | 	0 		| 	0 			   |
# 				| ball   | large 		 | 20 				  | 0 			 | 	0 		|  0 				|
# 				| ball   | small 		 | 10 				  | 0 			 | 	0 		|  0 			   |
# 				| ball   | NULL 		 | 35 				  | 0 			 | 	1 		| 	1 				|
# 				| hoop   | NULL 		 | 3 					  | 0 			 | 	0 		|  0 				|
# 				| hoop   | large 		 | 5 					  | 0 			 | 	0 		| 	0 				|
# 				| hoop 	| small 		 | 15 				  | 0 			 | 	0 		|  0 				|
# 				| hoop   | NULL 		 | 23 				  | 0 			 | 	1 		| 	1 				|
# 				| NULL 	| NULL 		 | 58 				  | 1 			 | 	1 		|  3 				|
# 				+--------+------------+------------------+-------------+----------+--------------+
#
# 			With multiple expression arguments, the GROUPING() return value is nonzero if any expression
# 			represents a super-aggregate value.
#
# 			Multiple-argument GROUPING() syntax thus provides a simpler way to write the earlier query
# 			that returned only super-aggregate rows, by using a single multiple-argument GROUPING()
# 			call rather than multiple single-argument calls:
#
# 				SELECT name, size, SUM(quantity) AS quantity
# 				FROM t1
# 				GROUP BY name, size WITH ROLLUP
# 				HAVING GROUPING(name, size) <> 0;
# 				+--------+--------+--------------+
# 				| name   | size   | quantity 		|
# 				+--------+--------+--------------+
# 				| ball   | NULL   | 35 				|
# 				| hoop   | NULL   | 23 				|
# 				| NULL   | NULL   | 58 				|
# 				+--------+--------+--------------+
#
# 			Use of GROUPING() is subject to these limitations:
#
# 				) Do not use subquery GROUP BY expressions as GROUPING() arguments because
# 					matching might fail.
#
# 					For example, matching fails for this query:
#
# 						SELECT GROUPING((SELECT MAX(name) FROM t1))
# 						FROM t1
# 						GROUP BY (SELECT MAX(name) FROM t1) WITH ROLLUP;
# 						ERROR 3580 (HY000): Argument #1 of GROUPING function is not in GROUP BY
#
# 				) GROUP BY literal expressions should not be used within a HAVING clause as
# 					GROUPING() arguments.
#
# 					Due to differences between when the optimizer evaluates GROUP BY and HAVING,
# 					matching may succeed but GROUPING() evaluation does not produce the expected
# 					result.
#
# 					Consider this query:
#
# 						SELECT a AS f1, 'w' AS f2
# 						FROM t
# 						GROUP BY f1, f2 WITH ROLLUP
# 						HAVING GROUPING(f2) = 1;
#
# 					GROUPING() is evaluated earlier for the literal constant expression than for the
# 					HAVING clause as a whole and returns 0.
#
# 					To check whether a query such as this is affected, use EXPLAIN and look for
# 					Impossible having in the Extra column.
#
# 					For more information about WITH ROLLUP and GROUPING(), see SECTION 12.20.2, "GROUP BY MODIFIERS"
#
# 		) INET_ATON(expr)
#
# 			Given the dotted-quad representation of an IPv4 network address as a string, returns an integer that
# 			represents the numeric value of the address in network byte order (big endian) 
#
# 			INET_ATON() returns NULL if it does not understand its argument.
#
# 				SELECT INET_ATON('10.0.5.9');
# 					-> 167773449
#
# 			For this example, the return value is calculated as 10x256^3 + 0x256^2+5x256+9
#
# 			INET_ATON() may or may not return a non-NULL result for short-form IP addresses (such as '127.1'
# 			as a representation of '127.0.0.1')
#
# 			Because of this, INET_ATON() a should not be used for such addresses.
#
# 				NOTE:
#
# 					To store values generated by INET_ATON(), use an INT UNSIGNED column rather than
# 					INT, which is signed.
#
# 					If you use a signed column, values corresponding to IP addresses for which the
# 					first octet is greater than 127 cannot be stored correctly.
#
# 					See SECTION 11.2.6, "OUT-OF-RANGE AND OVERFLOW HANDLING"
#
# 		) INET_NTOA(expr)
#
# 			Given a numeric IPv4 network address in network byte order, returns the dotted-quad string
# 			representation of the address as a string in the connection character set.
#
# 			INET_NTOA() returns NULL if it does not understand its argument.
#
# 				SELECT INET_NTOA(16777349);
# 					-> '10.0.5.9'
#
# 		) INET6_ATON(expr)
#
# 			Given an IPv6 or IPv4 network address as a string, returns a binary string
# 			that represents the numeric value of the address in network byte order
# 			(big endian)
#
# 			Because numeric-format IPv6 addresses require more bytes than the largest
# 			integer type, the representation returned by this function has the VARBINARY
# 			data type:
#
# 				VARBINARY(16) for IPv6 addresses and VARBINARY(4) for IPv4 addresses.
#
# 			If the argument is not a valid address, INET6_ATON() returns NULL
#
# 			The following examples use HEX() to display the INET6_ATON() result in
# 			printable form:
#
# 				SELECT HEX(INET6_ATON('fde::5a55:caff:fefa:9089'));
# 					-> 'FDFE000000000000005A55CAFFFEFA9089'
# 				SELECT HEX(INET6_ATON('10.0.5.9'));
# 					-> '0A000509'
#
# 			INET6_ATON() observes several constraints on valid arguments.
#
# 			These are given in the following list along with examples.
#
# 				) A trailing zone ID is not permitted, as in fe80::3%1 or fe80::3%eth0
#
# 				) A trailing network mask is not permitted, as in 2001:45f:3:ba::/64 or 198.51.100.0/24
#
# 				) For values representing IPv4 addresses, only classless addresses are supported.
#
# 					Classful addresses such as 198.51.1 are rejected
#
# 					A trailing port number is not permitted, as in 198.51.100.2:8080
#
# 					Hexadecimal numbers in address components are not permitted, as in 198.0xa0.1.2
#
# 					Octal numbers are not supported:
#
# 						198.51.010.1 is treated as 198.51.10.1, not 198.51.8.1
#
# 					These IPv4 constraints also apply to IPv6 addresses that have IPv4 address
# 					parts, such as IPv4-compatible or IPv4-mapped addresses.
#
# 			To convert an IPv4 address expr represented in numeric form as an INT value to an
# 			IPv6 address represented in numeric form as a VARBINARY value, use this expression:
#
# 				INET6_ATON(INET_NTOA(expr))
#
# 			For example:
#
# 				SELECT HEX(INET6_ATON(INET_NTOA(167773449)));
# 					-> '0A000509'
#
# 		) INET6_NTOA(expr)
#
# 			Given an IPv6 or IPv4 network address represented in numeric form as a binary string,
# 			returns the string representation of the address as a string in the connection char set.
#
# 			If the argument is not a valid address, INET6_NTOA() returns NULL
#
# 			INET6_NTOA() has these properties:
#
# 				) It does not use operating system functions to perform conversions, thus the output string is platform independent.
#
# 				) The return string has a maximum length of 39(4x8 + 7)
#
# 					Given this statement:
#
# 						CREATE TABLE t AS SELECT INET6_NTOA(expr) AS c1;
#
# 					The resulting table would have this definition:
#
# 						CREATE TABLE t (c1 VARCHAR(39) CHARACTER SET utf8 DEFAULT NULL);
#
# 				) The return string uses lowercase letters for IPv6 addresses.
#
# 					SELECT INET6_NTOA(INET6_ATON('fdfe::5a55:caff:fefa:9089'));
# 						-> 'fdfe::5a55:caff:fefa:9089'
#
# 					SELECT INET6_NTOA(INET6_ATON('10.0.5.9'));
# 						-> '10.0.5.9'
#
# 					SELECT INET6_NTOA(UNHEX('FDFE0000000000000000000005A55CAFFFEFA9089'));
# 						-> 'fdfe::5a55:caff:fefa:9089'
#
# 					SELECT INET6_NTOA(UNHEX('0A000509'));
# 						-> '10.0.5.9'
#
# 			) IS_IPV4(expr)
#
# 				Returns 1 if the argument is a valid IPv4 address specified as a string. Otherwise 0
#
# 					SELECT IS_IPV4('10.0.5.9'), IS_IPV4('10.0.5.256');
# 						-> 1, 0
#
# 				For a given argument, if IS_IPV4() returns 1, INET_ATON() (and INET6_ATON()) will return
# 				non-NULL
#
# 				THe converse statement is not true: In some cases, INET_ATON() returns non-NULL
# 				when IS_IPV4() returns 0
#
# 				As implied by the preceding remarks, IS_IPV4() is more strict than INET_ATON() about what
# 				constitutes a valid IPV4 address, so it may be useful for applications that need to
# 				perform strong checks against invalid values.
#
# 				Alternatively, use INET6_ATON() to convert IPv4 addresses to internal form and check for
# 				a NULL result (which indicates an invalid address)
#
# 				INET6_ATON() is equally strong as IS_IPV4() about checking IPv4 addresses.
#
# 			) IS_IPV4_COMPAT(expr)
#
# 				This function takes an IPv6 address represented in numeric form as a binary string,
# 				as returned by INET6_ATON()
#
# 				It returns 1 if the argument is a valid IPv4-compatible IPv6 address, 0 otherwise.
#
# 				IPv4-compatible addresses have the form ::ipv4_address
#
# 					SELECT IS_IPV4_COMPAT(INET6_ATON('::10.0.5.9'));
# 						-> 1
# 					SELECT IS_IPV4_COMPAT(INET6_ATON('::ffff:10.0.5.9'));
# 						-> 0
#
# 				The IPv4 part of an IPv4-compatible address can also be represented using hexadecimal notation.
#
# 				For example, 198.51.100.1, has this raw hexadecimal value:
#
# 					SELECT HEX(INET6_ATON('198.51.100.1'));
# 						-> 'C6336401'
#
# 				Expressed in IPv4-compatible form, ::198.51.100.1 is equivalent to
# 				::c0a8:0001 or (without leading zeros) ::c0a8:1
#
# 					SELECT
# 						IS_IPV4_COMPAT(INET6_ATON('::198.51.100.1')),
# 						IS_IPV4_COMPAT(INET6_ATON('::c0a8:0001')),
# 						IS_IPV4_COMPAT(INET6_ATON('::c0a8:1'));
# 							-> 1, 1, 1
#
# 			) IS_IPV4_MAPPED(expr)
#
# 				This function takes an IPv6 address represented in numeric form as a binary string,
# 				as returned by INET6_ATON()
#
# 				It returns 1 if the argument is a valid IPV4-mapped IPV6 address, 0 otherwise.
#
# 				IPv4-mapped addresses have the form ::ffff:ipv4_address
#
# 					SELECT IS_IPV4_MAPPED(INET6_ATON('::10.0.5.9'));
# 						-> 0
# 					SELECT IS_IPV4_MAPPED(INET6_ATON('::ffff:10.0.5.9'));
# 						-> 1
#
# 				As with IS_IPV4_COMPAT() the IPv4 part of an IPv4-mapped address can
# 				also be represented using hexadecimal notation:
#
# 					SELECT
# 						IS_IPV4_MAPPED(INET6_ATON('::ffff:198.51.100.1')),
# 						IS_IPV4_MAPPED(INET6_ATON('::ffff:c0a8:0001')),
# 						IS_IPV4_MAPPED(INET6_ATON('::ffff:c0a8:1'));
# 							-> 1, 1, 1
#
# 			) IS_IPV6(expr)
#
# 				Returns 1 if the argument is a valid IPv6 address specified as a string, 0 otherwise.
#
# 				This function does not consider IPv4 addresses to be valid IPv6 addresses.
#
# 					SELECT IS_IPV6('10.0.5.9'), IS_IPV6('::1');
# 						-> 0, 1
#
# 				For a given argument, if IS_IPV6() returns 1, INET6_ATON() will return non-NULL
#
# 			) IS_UUID(string uuid)
#
# 				Returns 1 if the argument is a valid string-format UUID, 0 if the argument is not a valid
# 				UUID, and NULL if the argument is NULL
#
# 				"Valid" means that the value is in a format that can be parsed.
#
# 				That is, has the correct length and contains only the permitted characters
# 				(hexadecimal digits in any lettercase and optionally, dashes and curly braces)
#
# 				This format is most common:
#
# 					aaaaaaaaaa-bbbb-cccc-dddd-eeeeeeeeee
#
# 				These other formats are also permitted:
#
# 					aaaaaaaabbbbccccddddeeeeeeeeee
# 					{aaaaaaaaaa-bbbb-cccc-dddd-eeeeeeeeee}
#
# 				For the meanings of fields within the value, see the UUID() function description.
#
# 					SELECT IS_UUID('6ccd780c-baba-1026-9564-5b8c656024db');
# 					+-------------------------------------------------------+
# 					| IS_UUID('6ccd780c-baba-1026-9564-5b8c656024db') 		  |
# 					+-------------------------------------------------------+
# 					| 									1 									  |
# 					+-------------------------------------------------------+
#
# 					SELECT IS_UUID('6CCD780C-BABA-1026-9564-5B8C656024DB');
# 					+-------------------------------------------------------+
# 					| IS_UUID('6ccd780cbaba102695645b8c656024db') 			  |
# 					+-------------------------------------------------------+
# 					| 									1 									  |
# 					+-------------------------------------------------------+
#
# 					SELECT IS_UUID('6ccd780cbaba102695645b8c656024db');
# 					+-------------------------------------------------------+
# 					| IS_UUID('6ccd780cbaba102695645b8c656024db') 			  |
# 					+-------------------------------------------------------+
# 					| 									1 									  |
# 					+-------------------------------------------------------+
#
# 					SELECT IS_UUID('{6ccd780c-baba-1026-9564-5b8c656024db}');
# 					+----------------------------------------------------+
# 					| IS_UUID('{6ccd780c-baba-1026-9564-5b8c656024db}')  |
# 					+----------------------------------------------------+
# 					| 							1 										  |
# 					+----------------------------------------------------+
#
# 					SELECT IS_UUID('6ccd780c-baba-1026-9564-5b8c6560');
# 					+------------------------------------------------+
# 					| IS_UUID('6ccd780c-baba-1026-9564-5b8c6560') 	 |
# 					+------------------------------------------------+
# 					| 							0 									 |
# 					+------------------------------------------------+
#
# 					SELECT IS_UUID(RAND());
# 					+----------------------+
# 					| IS_UUID(RAND()) 	  |
# 					+----------------------+
# 					| 				0 			  |
# 					+----------------------+
#
# 		) MASTER_POS_WAIT(log name, log pos [, timeout][, channel])
#
# 			This function is useful for control of master/slave synchronization.
#
# 			It blocks until the slave has read and applied all updates up to the specified
# 			position in the master log.
#
# 			The return value is the number of log events the slave had to wait for advance
# 			to the specified position.
#
# 			The function returns NULL if the slave SQL thread is not started, the slave's
# 			master information is not initialized, the arguments are incorrect, or an error
# 			occurs.
#
# 			It returns -1 if the timeout has been exceeded.
#
# 			If the slave SQL thread stops while MASTER_POS_WAIT() is waiting, the function
# 			returns NULL.
#
# 			If the slave is past the specified position, the function returns immediately.
#
# 			On a multithreaded slave, the function waits until expiry of the limit set by the
# 			slave_checkpoint_group or slave_checkpoint_period system variable, when the checkpoint
# 			operation is called to update the status of the slave.
#
# 			Depending on the setting for the system variables, the function might therefore
# 			return some time after the specified position was reached.
#
# 			If a timeout value is specified, MASTER_POS_WAIT() stops waiting when timeout
# 			seconds have elapsed.
#
# 			timeout must be greater than 0; a zero or negative timeout means no timeout.
#
# 			The optional channel value enables you to name which replication channel the function
# 			applies to.
#
# 			See SECTION 17.2.3, "REPLICATION CHANNELS" for more information.
#
# 			This function is unsafe for statement-based replication. A warning is logged if you
# 			use this function when binlog_format is set to STATEMENT.
#
# 		) NAME_CONST(name, value)
#
# 			Returns the given value. When used to produce a result set column, NAME_CONST() causes
# 			the column to have the given name.
#
# 			The arguments should be constants.
#
# 				SELECT NAME_CONST('myname', 14);
# 				+----------+
# 				| myname   |
# 				+----------+
# 				| 14 		  |
# 				+----------+
#
# 			This function is for internal use only.
#
# 			THe server uses it when writing statements from stored programs that contain
# 			references to local program variables, as described in SECTION 24.7, "BINARY LOGGING OF STORED PROGRAMS"
#
# 			You might see this function in the output from mysqlbinlog
#
# 			For your applications, you can obtain exactly the same result as in the example just shown
# 			by using simple aliasing, like this:
#
# 				SELECT 14 AS myname;
# 				+-------------------+
# 				| myname 			  |
# 				+-------------------+
# 				| 		14 			  |
# 				+-------------------+
# 				1 row in set (0.00 sec)
#
# 			See SECTION 13.2.10, "SELECT SYNTAX" for more information about column aliases.
#
# 		) SLEEP(duration)
#
# 			SLeeps (pauses) for the number of seconds given by the duration argument, then returns 0.
#
# 			The duration may have a fractional part.
#
# 			If the argument is NULL or negative, SLEEP() produces a warning, or an error
# 			in strict SQL mode.
#
# 			When sleep returns normally (without interupption), it returns 0:
#
# 				SELECT SLEEP(1000);
# 				+-----------------------+
# 				| SLEEP(1000) 				|
# 				+-----------------------+
# 				| 			0 					|
# 				+-----------------------+
#
# 			When SLEEP() is the only thing invoked by a query that is interuppted, it returns
# 			1 and the query itself returns no error.
#
# 			This is true whether the query is killed or times out:
#
# 				) This statement is interrupted using KILL_QUERY from another session:
#
# 					SELECT SLEEP(1000);
# 					+------------------+
# 					| SLEEP(1000) 		 |
# 					+------------------+
# 					| 			1 			 |
# 					+------------------+
#
# 				) This statement is interrupted by timing out:
#
# 					SELECT /*+ MAX_EXECUTION_TIME(1) */ SLEEP(1000);
# 					+-------------------+
# 					| SLEEP(1000) 		  |
# 					+-------------------+
# 					| 			1 			  |
# 					+-------------------+
#
# 			When SLEEP() is only part of a query that is interrupted, the query returns an error:
#
# 				) This statement is interrupted using KILL_QUERY from another session:
#
# 					SELECT 1 FROM t1 WHERE SLEEP(1000);
# 					ERROR 1317 (70100): Query execution was interrupted
#
# 				) This statement is interrupted by timing out:
#
# 					SELECT /*+ MAX_EXECUTION_TIME(1000) */ 1 FROM t1 WHERE SLEEP(1000);
# 					ERROR 3024 (HY000): Query execution was interrupted, maximum statement
# 					execution time exceeded
#
# 			This function is unsafe for statement-based replication.
#
# 			A warning is logged if you use this function when binlog_format is set
# 			to STATEMENT.
#
# 		) UUID()
#
# 			Returns a Universal Unique Identifier (UUID) generated according to RFC 4122,
# 			"A Universally Unique Identifier (UUID) URN Namespace" (<link>)
#
# 			A UUID is designed as a number that is globally unique in space and time.
#
# 			Two calls to UUID() are expected to generate two different values, even if these
# 			calls are performed on two separate devices not connected to each other.
#
# 				WARNING
#
# 					Although UUID() values are intended to be unique, they are not necessarily unguessable
# 					or unpredictable.
#
# 					If unpreidctability is required, UUID() values should be generated some other way.
#
# 			UUID() returns a value that conforms to UUID version 1 as described in RFC 4122.
#
# 			The value is a 128-bit number represented as a utf8 string of five hexadecimal numbers in
# 			aaaaaaaa-bbbb-cccc-dddd-eeeeeeee format:
#
# 				) The first three numbers are generated from the low, middle and high parts of a timestamp.
#
# 					The high part also includes the UUID version number.
#
# 				) The fourth number preserves temporal uniqueness in case the timestamp value loses monotonicity
# 					(for example, due to daylight saving time)
#
# 				) The fifth number is an IEEE 802 node number that provides spatial uniqueness.
#
# 				A random number is substituted if the latter is not available (for example, because the host device
# 				has no Ethernet card, or it is unknown how to find the hardware address of an interface on the host
# 				operating system)
#
# 				In this case, spatial uniqueness cannot be guaranteed.
#
# 				Nevertheless, a collision should have very low probability.
#
# 				The MAC address of an interface is taken into account only on FreeBSD and Linux
#
# 				On other operating systems, MySQL uses a randomly generated 48-bit number.
#
# 					SELECT UUID();
# 						-> '6ccd780c-baba-1026-9564-5b8c656024db'
#
# 			To convert between string and binary UUID values, use the UUID_TO_BIN() and
# 			BIN_TO_UUID() functions.
#
# 			TO check whether a string is a valid UUID value, use the IS_UUID() function.
#
# 			NOTE:
#
# 				UUID() does not work with statement-based replication
#
# 		) UUID_SHORT()
#
# 			Returns a "short" universal identifier as a 64-bit unsigned integer.
#
# 			Values returned by UUID_SHORT() differ from the string-format 128-bit
# 			identifiers returned by the UUID() function and have different uniqueness
# 			properties.
#
# 			The value of UUID_SHORT() is guaranteed to be unique if the following conditions hold:
#
# 				) The server_id value of the current server is between 0 and 255 and is unique among your
# 					set of master and slave servers
#
# 				) You do not set back the system time for your server host between mysqld restarts
#
# 				) You invoke UUID_SHORT() on average fewer than 16 million times per second between mysqld restarts
#
# 			The UUID_SHORT() return value is constructed this way:
#
# 				(server_id & 255) << 56
# 			 + (server_startup_time_in_seconds << 24)
# 			 + incremented_variable++;
#
# 				SELECT UUID_SHORT();
# 					-> 92395783831158784
#
# 			NOTE:
#
# 				UUID_SHORT() does not work with statement-based replication
#
# 		) UUID_TO_BIN(string uuid), UUID_TO_BIN(string uuid, swap flag)
#
# 			Converts a string UUID to a binary UUID and returns the result.
#
# 			(The IS_UUID() function description lists the permitted string UUID formats)
#
# 			THe return binary UUID is a VARBINARY(16) value
#
# 			If the UUID argument is NULL, the return value is NULL.
#
# 			If any argument is invalid, an error occurs.
#
# 			UUID_TO_BIN() takes one or two arguments:
#
# 				) The one-argument form takes a string UUID value. The binary result is in the same
# 					order as the string argument.
#
# 				) The two-argument form takes a string UUID value and a flag value:
#
# 					) If swap_flag is 0, the two-argument form is equivalent to the one-argument form.
#
# 						The binary result is in the same order as the string argument.
#
# 					) If swap_flag is 1, the format of the return value differs:
#
# 						The time-low and time-high parts (the first and third groups of hexadecimal digits,
# 						respectively) are swapped.
#
# 						This moves the more rapidly varying part to the right and can improve indexing efficiency
# 						if the result is stored in an indexed column.
#
# 			Time-part swapping assumes the use of UUID version 1 values, such as are generated by the UUID()
# 			function.
#
# 			For UUID values produced by other means that do not follow version 1 format, time-part swapping
# 			provides no benefit.
#
# 			For details about version 1 format, see the UUID() function description.
#
# 			Suppose that you have the following string UUID value:
#
# 				SET @uuid = '6ccd780c-baba-1026-9564-5b8c656024db';
#
# 			To convert the string UUID to binary with or without time-part swapping,
# 			use UUID_TO_BIN():
#
# 				SELECT HEX(UUID_TO_BIN(@uuid));
# 				+-------------------------------------+
# 				| HEX(UUID_TO_BIN(@uuid)) 				  |
# 				+-------------------------------------+
# 				| 6CCD780CBABA102695645B8C656024DB 	  |
# 				+-------------------------------------+
#
# 				SELECT HEX(UUID_TO_BIN(@uuid, 0));
# 				+-------------------------------------+
# 				| HEX(UUID_TO_BIN(@uuid, 0)) 			  |
# 				+-------------------------------------+
# 				| 6CCD780CBABA102695-> ETC 			  |
# 				+-------------------------------------+
#
# 				SELECT HEX(UUID_TO_BIN(@uuid, 1));
# 				+-------------------------------------+
# 				| HEX(UUID_TO_BIN(@uuid, 1)) 			  |
# 				+-------------------------------------+
# 				| 1026BABA6CCD780C95645B8C656024DB 	  |
# 				+-------------------------------------+
#
# 			To convert a binary UUID returned by UUID_TO_BIN() to a string UUID,
# 			use BIN_TO_UUID() 
#
# 			If you produce a binary UUID by calling UUID_TO_BIN() with a second
# 			argument of 1 to swap time parts, you should also pass a second argument
# 			of 1 to BIN_TO_UUID() to unswap the time parts when converting the binary
# 			UUID back to a string UUID:
#
# 				SELECT BIN_TO_UUID(UUID_TO_BIN(@uuid));
# 				+---------------------------------------+
# 				| BIN_TO_UUID(UUID_TO_BIN(@uuid)) 		 |
# 				+---------------------------------------+
# 				| 6ccd780c-baba-1026-9564-5b8c656024db  |
# 				+---------------------------------------+
#
# 				SELECT BIN_TO_UUID(UUID_TO_BIN(@uuid,0),0);
# 				+---------------------------------------+
# 				| BIN_TO_UUID(UUID_TO_BIN(@uuid,0),0) 	 |
# 				+---------------------------------------+
# 				| 6ccd780c-baba-1026-9564-5b8c656024db  |
# 				+---------------------------------------+
#
# 				SELECT BIN_TO_UUID(UUID_TO_BIN(@uuid,1),1);
# 				+---------------------------------------+
# 				| BIN_TO_UUID(UUID_TO_BIN(@uuid,1),1) 	 |
# 				+---------------------------------------+
# 				| 6ccd780c-baba-1026-9564-5b8c656024db  |
# 				+---------------------------------------+
#
# 			If the use of time-part swapping is not the same for the conversion
# 			in both directions, the original UUID will not be recovered properly:
#
# 				SELECT BIN_TO_UUID(UUID_TO_BIN(@uuid,0),1);
# 				+---------------------------------------+
# 				| BIN_TO_UUID(UUID_TO_BIN(@uuid,0),1)   |
# 				+---------------------------------------+
# 				| baba1026-780c-6ccd-9564-5b8c656024db  |
# 				+---------------------------------------+
#
# 				SELECT BIN_TO_UUID(UUID_TO_BIN(@uuid,1),0);
# 				+---------------------------------------+
# 				| BIN_TO_UUID(UUID_TO_BIN(@uuid,1),0)   |
# 				+---------------------------------------+
# 				| 1026baba-6ccd-780c-9564-5b8c656024db  |
# 				+---------------------------------------+
#
# 		) VALUES(col name)
#
# 			In an INSERT_---_ON_DUPLICATE_KEY_UPDATE statement, you can use the VALUES(col_name)
# 			function in the UPDATE clause to refer to column values from the INSERT portion of the
# 			statement.
#
# 			In other words, VALUES(col_name) in the UPDATE clause refers to the value of col_name
# 			that would be inserted, had no duplicate-key conflict occurred.
#
# 			This function is especially useful in multiple-row inserts
#
# 			The VALUES() function is meaningful only in the ON DUPLICATE KEY UPDATE clause of
# 			INSERT statements and returns NULL otherwise.
#
# 			See SECTION 13.2.6.2, "INSERT --- ON DUPLICATE KEY UPDATE SYNTAX"
#
# 				INSERT INTO table (a,b,c) VALUES (1,2,3),(4,5,6)
# 					-> ON DUPLICATE KEY UPDATE c=VALUES(a)+VALUES(b);
#
# 12.24 PRECISION MATH
#
# 12.24.1 TYPES OF NUMERIC VALUES
# 12.24.2 DECIMAL DATA TYPE CHARACTERISTICS
# 12.24.3 EXPRESSION HANDLING
# 12.24.4 ROUNDING BEHAVIOR
# 12.24.5 PRECISION MATH EXAMPLES
#
# MySQL provides support for precision math: numeric value handling that results in extremely
# accurate results and a high degree control over invalid values.
#
# Precision math is based on these two features:
#
# 		) SQL modes that control how strict the server is about accepting or rejecting invalid data.
#
# 		) The MySQL library for fixed-point arithmetic
#
# These features have several implications for numeric operations and provide a high degree of
# compliance with standard SQL:
#
# 		) Precise calculations: For exact-value numbers, calculations do not introduce floating-point errors.
#
# 			Instead, exact precision is used.
#
# 			For example, MySQL treats a number such as .0001 as an exact value rather than as an approximation,
# 			and summing it 10,000 times produces a result of exactly 1, not a value that is merely "close" to 1.
#
# 		) Well-defined rounding behavior. For exact-value numbers, the result of ROUND() depends on its argument,
# 			not on environmental factors such as how the underlying C library works.
#
# 		) Platform independence: Operations on exact numeric values are the same across different platforms such as Windows and Unix.
#
# 		) Control over handling of invalid values: Overflow and division by zero are detectable and can be treated as errors.
#
# 			For example, you can treat a value that is too large for a column as an error rather than having the value
# 			truncated to lie within the range of the columns data type.
#
# 			SImilarly, you can treat division by zero as an error rather than as an operation that produces a result
# 			of NULL.
#
# 			The choice of which approach to take is determined by the setting of the server SQL mode.
#
# The following discussion covers several aspects of how precision math works, including possible incomatibilities
# with older applications.
#
# At the end, some examples are given that demonstrate how MySQL handles numeric operations precisely.
#
# For information about controlling the SQL mode, see SECTION 5.1.11, "SERVER SQL MODES"
#
# 12.24.1 TYPES OF NUMERIC VALUES
#
# The scope of precision math for exact-value operations includes the exact-value data types
# (integer and DECIMAL types) and exact-value numeric literals.
#
# Approximate-value data types and numeric literals are handled as floating-point numbers.
#
# Exact-value numeric literals have an integer part or fractional part, or both.
#
# They may be signed. Examples: 1, .2, 3.4, -5, -6.78, +9.10
#
# Approximate-value numeric literals are represented in scientific notation with a mantissa
# and exponent.
#
# Either or both parts may be signed. Examples: 1.2E3, 1.2E-3, -1.2E3, -1.2E-3
#
# Two numbers that look similar may be treated differently.
#
# For example, 2.34 is an exact-value (fixed-point) number, whereas 2.34E0 is an
# approximate value (floating-point) number
#
# The DECIMAL data type is a fixed-point type and calculations are exact.
#
# In MySQL, the DECIMAL type has several synonyms: NUMERIC, DEC, FIXED.
#
# The integer types also are exact-value types.
#
# The FLOAT and DOUBLE data types are floating-point types and calculations
# are approximate.
#
# In MySQL, types that are synonymous with FLOAT or DOUBLE are DOUBLE_PRECISION
# and REAL.
#
# 12.24.2 DECIMAL DATA TYPE CHARACTERISTICS
#
# This section discusses the characteristics of the DECIMAL data type (and its synonyms),
# with particular regard to the following topics:
#
# 		) Maximum number of digits
#
# 		) Storage format
#
# 		) Storage requirements
#
# 		) The nonstandard MySQL extension to the upper range of DECIMAL columns
#
# The declaration syntax for a DECIMAL column is DECIMAL(M,D)
#
# The ranges of values for the arguments are as follows:
#
# 		) M is the maximum number of digits (the precision). Has a range of 1 to 65
#
# 		) D is the number of digits to the right of the decimal point (the scale). Range of 0 to 30, must be no larger than M.
#
# If D is omitted, the default is 0. If M is omitted, the default is 10.
#
# The maximum value of 65 for M means that calculations on DECIMAL values are accurate up to
# 65 digits.
#
# This limit of 65 digits of precision also applies to exact-value numeric literals,
# so the maximum range of such literals differs from before.
#
# Values for DECIMAL columns are stored using a binary format that packs nine decimal
# digits into 4 bytes.
#
# The storage requirements for the integer and fractional parts of each value are determined
# separately.
#
# Each multiple of nine digits requires 4 bytes, and any remaining digits left over require some
# fraction of 4 bytes.
#
# The storage required for remaining digits is given by the following table.
#
# LEFTOVER DIGITS 				NUMBER OF BYTES
#
# 0 									0
#
# 1-2 								1
#
# 3-4 								2
#
# 5-6 								3
#
# 7-9 								4
#
# For example, a DECIMAL (18,9) column has nine digits on either side of the decimal point,
# so the integer part and the fractional part each require 4 bytes.
#
# A DECIMAL(20,6) column has fourteen integer digits and six fractional digits.
#
# The integer digits require four bytes for nine of the digits and 3 bytes for the remaining
# five digits.
#
# THe six fractional digits require 3 bytes.
#
# DECIMAL columns do not store a leading + character or - character or leading 0 digits.
#
# If you insert +0003.1 into a DECIMAL(5,1) column, it is stored as 3.1
#
# For negative numbers, a literal - character is not stored.
#
# DECIMAL columns do not permit values larger than the range implied by the column definition.
#
# For example, a DECIMAL(3,0) column supports a range of -999 to 999
#
# A DECIMAL(M,D) column permits up to M - D digits to the left of the decimal point.
#
# The SQL standard requires that the precision of NUMERIC(M,D) be exactly M digits.
#
# For DECIMAL(M,D), the standard requires a precision of at least M digits but permits
# more.
#
# In MySQL, DECIMAL(M,D) and NUMERIC(M,D) are the same, and both have a precision
# of exactly M digits.
#
# For a full explanation of the internal format of DECIMAL values, see the file
# strings/decimal.c in a MySQL source distrib.
#
# The format is explained (with an example) in the decimal2bin() function.
#
# 12.24.3 EXPRESSION HANDLING
#
# With precision math, exact-value numbers are used as given whenever possible.
#
# For example, numbers in comparisons are used exactly as given without a change in
# value.
#
# In strict SQL mode, for INSERT into a column with an exact data type (DECIMAL or integer),
# a number is inserted with its exact value if it is within the column range.
#
# When retrieved, the value should be the same as what was inserted.
#
# (If strict SQL mode is not enabled, truncation for INSERT is permissible)
#
# Handling of a numeric expression depends on what kind of values the expression contains:
#
# 		) If any approximate values are present, the expression is approximate and is evaluated using floating-point arithmetic
#
# 		) If no approximate value are present, the expression contains only exact values.
#
# 			If any exact value contains a fractional part (a value following the decimal point),
# 			the expression is evaluated using DECIMAL exact arithmetic and has a precision of 65 digits.
#
# 			The term "exact" is subject to the limits of what can be represented in binary.
#
# 			For example, 1.0/3.0 can be approximated in decimal notation as .333---, but not written
# 			as an exact number, so (1.0/3.0)*3.0 does not evaluate to exactly 1.0
#
# 		) Otherwise, the expression contains only integer values.
#
# 			The expression is exact and is evaluated using integer arithmetic and has a precision
# 			the same as BIGINT(64 bits)
#
# If a numeric expression contains any strings, they are converted to double-precision floating-point
# values and the expression is approximate.
#
# Inserts into numeric columns are affected by the SQL mode, which is controlled by the sql_mode system
# variable.
#
# (See SECTION 5.1.11, "SERVER SQL MODES")
#
# The following discussion mentions strict mode (selected by the STRICT_ALL_TABLES or STRICT_TRANS_TABLES
# mode values) and ERROR_FOR_DIVISION_BY_ZERO
#
# To turn on all restrictions, you can simply use TRADITIONAL mode, which includes both strict mode values
# and ERROR_FOR_DIVISION_BY_ZERO:
#
# 		SET sql_mode='TRADITIONAL';
#
# If a number is inserted inot an exact type column (DECIMAL or integer), it is inserted with its exact
# value if it is within the column range and precision.
#
# If the value has too many digits in the fractional part, rounding occurs and a note is generated.
#
# Rounding is done as described in SECTION 12.24.4, "ROUNDING BEHAVIOR"
#
# Truncation due to rounding of the fractional part is not an error, even in strict mode.
#
# If te value has too many digits in the integer part, it is too large (out of range) and is
# handled as follows:
#
# 		) If strict mode is not enabled, the value is truncated to the nearest legal value and a warning is generated.
#
# 		) If strict mode is enabled, an overflow error occurs.
#
# Underflow is not detected, so underflow handling is undefined.
#
# For inserts of strings into numeric columns, conversion from string to number is handled as follows
# if the string has nonnumeric contents:
#
# 		) A string that does not begin with a number cannot be used as a number and pproduces an error in strict mode,
# 			or a warning otherwise.
#
# 			This includes the empty string.
#
# 		) A string that begins with a number can be converted, but the trailing nonnumeric portion is truncated.
#
# 		If the truncated portion contains anything other than spaces, this produces an error in strict mode,
# 		or a warning otherwise.
#
# By default, division by zero produces a result of NULL and no warning.
#
# By setting the SQL mode appropriately, division by zero can be restricted.
#
# With the ERROR_FOR_DIVISION_BY_ZERO SQL mode enabled, MySQL handles division by zero differently:
#
# 		) if strict mode is not enabled, a warning occurs
#
# 		) If strict mode is enabled, inserts and updates involving division by zero are prohibited, and an error occurs.
#
# In other words, inserts and updates involving expressions that perform division by zero can be treated as errors,
# but this requires ERROR_FOR_DIVISION_BY_ZERO in addition to strict mode.
#
# Suppose that we have this statement:
#
# 		INSERT INTO t SET i = 1/0;
#
# This is what happens for combinations of strict and ERROR_FOR_DIVISION_BY_ZERO modes.
#
# 		sql_mode VALUE 				RESULT
#
# ''(Default) 							No warning, no error; i is set to NULL
#
# strict 								No warning, no error; i is set to NULL
#
# ERROR_FOR_DIVISION_BY_ZERO 		Warning, no error; i is set to NULL
#
# strict, ERROR_FOR_DIVISION_BY_ZERO Error condition; no row is inserted.
#
# 12.24.4 ROUNDING BEHAVIOR
#
# This section discusses precision math rounding for the ROUND() function and for
# inserts into columns with exact-value types (DECIMAL and integer)
#
# The ROUND() function rounds differently depending on whether its argument is exact
# or approximate:
#
# 		) For exact-value numbers, ROUND() uses the "round half up" rule:
#
# 			A value with a fractional part of .5 or greater is rounded up to the next
# 			integer if positive or down to the next integer if negative.
#
# 			(IN other words, it is rounded away from zero)
#
# 			A value with a fractional part less than .5 is rounded down to the next
# 			integer if positive or up to the next integer if negative.
#
# 			(In other words, it is rounded toward zero)
#
# 		) For approximate-value numbers, the result depends on the C library.
#
# 			On many systems, this means that ROUND() uses the "round to nearest even" rule:
#
# 				A value with a fractional part exactly half way between two integers is rounded
# 				to the nearest even integer.
#
# The following example shows how rounding differs for exact and approximate values:
#
# 		SELECT ROUND(2.5), ROUND(25E-1);
# 		+------------+------------------+
# 		| ROUND(2.5) | ROUND(25E-1) 	  |
# 		+------------+------------------+
# 		| 3 			 | 	2 				  |
# 		+------------+------------------+
#
# For inserts into a DECIMAL or integer column, the target is an exact data type, so rounding
# uses "round half away from zero", regardless of whether the value to be inserted is exact
# or approximate:
#
# 		CREATE TABLE t (d DECIMAL(10,0));
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		INSERT INTO t VALUES(2.5),(2.5E0);
# 		Query OK, 2 rows affected, 2 warnings (0.00 sec)
# 		Records: 2 Duplicates: 0 Warnings: 2
#
# 		SHOW WARNINGS;
# 		+---------+----------+------------------------------------------------+
# 		| Level   | Code     | Message 													 |
# 		+---------+----------+------------------------------------------------+
# 		| Note 	 | 1265 		| Data truncated for column 'd' at row 1 			 |
# 		| Note 	 | 1265 		| Data truncated for column 'd' at row 2 			 |
# 		+---------+----------+------------------------------------------------+
# 		2 rows in set (0.00 sec)
#
# 		SELECT d FROM t;
# 		+-----------+
# 		| d 			|
# 		+-----------+
# 		| 3 			|
# 		| 3 			|
# 		+-----------+
# 		2 rows in set (0.00 sec)
#
# The SHOW_WARNINGS statement displays the notes that are generated by truncation due to rounding
# of the fractional part.
#
# Such truncation is not an error, even in strict SQL mode (see SECTION 12.24.3, "EXPRESSION HANDLING")
#
# 12.24.5 PRECISION MATH EXAMPLES
#
# This section provides some examples that show precision math query results in MySQL.
#
# These examples demonstrates the principles described in SECTION 12.24.3, "EXPRESSION HANDLING",
# and SECTION 12.24.4, "ROUNDING BEHAVIOR"
#
# EXAMPLE 1.
#
# Numbers are used with their exact values as given when possible:
#
# 		SELECT (.1 + .2) = 3;
# 		+---------------------+
# 		| (.1 + .2) = .3      |
# 		+---------------------+
# 		| 				1 			 |
# 		+---------------------+
#
# For floating-point values, results are inexact:
#
# 		SELECT (.1E0 + .2E0) = .3E0;
# 		+-----------------------------+
# 		| (.1E0 + .2E0) = .3E0 			|
# 		+-----------------------------+
# 		| 					0 					|
# 		+-----------------------------+
#
# ANother way to see the difference in exact and approximate value handling is to add
# a small number to a sum many times.
#
# Consider the following stored procedure, which adds .0001 to a variable 1.000 times
#
# CREATE PROCEDURE p ()
# BEGIN
# 		DECLARE i INT DEFAULT 0;
# 		DECLARE d DECIMAL(10,4) DEFAULT 0;
# 		DECLARE f FLOAT DEFAULT 0;
# 		WHILE i < 10000 DO
# 			SET d = d + .0001;
# 			SET f = f + .0001E0;
# 			SET i = i + 1;
# 		END WHILE;
# 		SELECT d, f;
# END;
#
# The sum for both d and f logically should be 1, but that is true only for decimal calculation.
#
# The floating-point calculation introduces small errors:
#
# 		+-------------+-------------------------------+
# 		| d 			  | 	f 									 |
# 		+-------------+-------------------------------+
# 		| 1.0000      | 0.99999999999991 				 |
# 		+-------------+-------------------------------+
#
# Example 2
#
# Multiplication is performed with the scale required by standard SQL.
#
# That is, for two numbers X1 and X2 that have scale S1 and S2, the scale
# of the result is S1 + S2:
#
# 		SELECT .01 * .01;
# 		+---------------+
# 		| .01 * .01 	 |
# 		+---------------+
# 		| 0.0001 		 |
# 		+---------------+
#
# Example 3
#
# Rounding behavior for exact-value numbers is well-defined:
#
# 		Rounding behavior (for example, with the ROUND() function) is independent of the implementation
# 		of the underlying C library, which means that results are consistent from platform to platform.
#
# 			) Rounding for exact-value columns (DECIMAL and integer) and exact-valued numbers uses the "round half away from zero" rule.
#
# 				A value with a fractional part of .5 or greater is rounded away from zero to the nearest
# 				integer, as shown here:
#
# 					SELECT ROUND(2.5), ROUND(-2.5);
# 					+---------------+------------------+
# 					| ROUND(2.5)    | ROUND(-2.5) 	  |
# 					+---------------+------------------+
# 					| 3 				 | -3 				  |
# 					+---------------+------------------+
#
# 			) Rounding for floating-point values uses the C library, which on many systems uses the
# 				"round to nearest even" rule.
#
# 				A value with a fractional part exactly half way between two integers is rounded to
# 				the nearest even integer:
#
# 					SELECT ROUND(2.5E0), ROUND(-2.5E0);
# 					+-------------+-------------------+
# 					| ROUND(2.5E0)| ROUND(-2.5E0) 	 |
# 					+-------------+-------------------+
# 					| 		2 		  | 		-2 			 |
# 					+-------------+-------------------+
#
# Example 4
#
# In strict mode, inserting a value that is out of range for a column causes an error,
# rather than truncation to a legal value.
#
# When MySQL is not running in strict mode, truncation to a legal value occurs:
#
# 		SET sql_mode='';
# 		Query OK,, 0 rows affected (0.00 sec)
#
# 		CREATE TABLE t (i TINYINT);
# 		Query OK, 0 rows affected (0.01 sec)
#
# 		INSERT INTO t SET i = 128;
# 		Query OK, 1 row affected, 1 warning (0.00 sec)
#
# 		SELECT i FROM t;
# 		+----------+
# 		| 	i 		  |
# 		+----------+
# 		| 127 	  |
# 		+----------+
#
# However, an error occurs if strict mode is in effect:
#
# 		SET sql_mode='STRICT_ALL_TABLES';
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		CREATE TABLE t (i TINYINT);
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		INSERT INTO t SET i = 128;
# 		ERROR 1264 (22003): Out of range value adjusted for column 'i' at row 1
#
# 		SELECT i FROM t;
# 		Empty set (0.00 sec)
#
# Example 5
#
# In strict mode and with ERROR_FOR_DIVISION_BY_ZERO set, division by zero causes
# an error, not a result of NULL.
#
# In nonstrict mode, division by zero has a result of NULL:
#
# 		SET sql_mode='';
# 		Query OK, 0 rows affected (0.01 sec)
#
# 		CREATE TABLE t (i TINYINT);
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		INSERT INTO t SET i = 1 / 0;
# 		Query OK, 1 row affected (0.00 sec)
#
# 		SELECT i FROM t;
# 		+-----------+
# 		| i 			|
# 		+-----------+
# 		| NULL 		|
# 		+-----------+
# 		1 row in set (0.03 sec)
#
# However, division by zero is an error if the proper SQL modes are in effect:
#
# 		SET sql_mode='STRICT_ALL_TABLES, ERROR_FOR_DIVISION_BY_ZERO';
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		CREATE TABLE t (i TINYINT);
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		INSERT INTO t SET = 1 / 0;
# 		ERROR 1365 (22012): Division by 0
#
# 		SELECT i FROM t;
# 		Empty set (0.01 sec)
#
# Example 6
#
# Exact-value literals are evaluted as exact values.
#
# Approximate-value literals are evaluated using floating-point, but exact-value
# literals are handled as DECIMAL:
#
# 		CREATE TABLE t SELECT 2.5 AS a, 25E-1 AS b;
# 		Query OK, 1 row affected (0.01 sec)
# 		Records: 1 Duplicates: 0 Warnings: 0
#
# 		DESCRIBE t;
# 		+---------+--------------------------------------+----------+----------+-----------------------+-----------+
# 		| Field 	 | Type 											 | Null 		| Key 	  | Default 				  | Extra 	  |
# 		+---------+--------------------------------------+----------+----------+-----------------------+-----------+
# 		| a 		 | decimal(2,1) unsigned 					 | NO       | 			  | 0.0 						  | 			  |
# 		| b 		 | double 										 | NO 		| 			  | 0 						  | 			  |
# 		+---------+--------------------------------------+----------+----------+-----------------------+-----------+
#
# Example 7
#
# If the argument to an aggregate function is an exact numeric type, the result is also an exact numeric type,
# with a scale at least that of the argument.
#
# Consider these statements:
#
# 		CREATE TABLE t (i INT, d DECIMAL, f FLOAT);
# 		INSERT INTO t VALUES(1,1,1);
# 		CREATE TABLE y SELECT AVG(i), AVG(d), AVG(f) FROM t;
#
# THe result is a double only for the floating-point argument.
#
# For exact type arguments, the result is also an exact type:
#
# 		DESCRIBE y;
# 		+----------+---------------------------+-------+---------+---------------+--------------+
# 		| Field 	  | Type 							| Null  | Key 	   | Default 		 | Extra 		 |
# 		+----------+---------------------------+-------+---------+---------------+--------------+
# 		| AVG(i)   | decimal(14,4) 				| YES   | 			| NULL 			 | 				 |
# 		| AVG(d)   | decimal(14,4) 				| YES   | 		   | NULL 			 | 				 |
# 		| AVG(f)   | double 							| YES   | 			| NULL 			 | 				 |
# 		+----------+---------------------------+-------+---------+---------------+--------------+
#
# The result is a double only for the floating-point argument.
#
# for exact type arguments, the result is also an exact type.
#
# CHAPTER 13 SQL STATEMENT SYNTAX
#
# TABLE OF CONTENTS
#
# 13.1 DATA DEFINITION STATEMENTS
# 13.2 DATA MANIPULATION STATEMENTS
# 13.3 TRANSACTIONAL AND LOCKING STATEMENTS
# 13.4 REPLICATION STATEMENTS
# 13.5 PREPARED SQL STATEMENT SYNTAX
# 13.6 COMPOUND-STATEMENT SYNTAX
# 13.7 DATABASE ADMINISTRATION STATEMENTS
# 13.8 UTILITY STATEMENTS
#
# This chapter describes the syntax for the SQL statements supported by MySQL.
#
# 13.1 DATA DEFINITION STATEMENTS
#
# 13.1.1 ATOMIC DATA DEFINITION STATEMENT SUPPORT
# 13.1.2 ALTER DATABASE SYNTAX
# 13.1.3 ALTER EVENT SYNTAX
# 13.1.4 ALTER FUNCTION SYNTAX
# 
# 13.1.5 ALTER INSTANCE SYNTAX
# 13.1.6 ALTER LOGFILE GROUP SYNTAX
# 13.1.7 ALTER PROCEDURE SYNTAX
# 13.1.8 ALTER SERVER SYNTAX
#
# 13.1.9 ALTER TABLE SYNTAX
# 13.1.10 ALTER TABLESPACE SYNTAX
# 13.1.11 ALTER VIEW SYNTAX
# 13.1.12 CREATE DATABASE SYNTAX
#
# 13.1.13 CREATE EVENT SYNTAX
# 13.1.14 CREATE FUNCTION SYNTAX
# 13.1.15 CREATE INDEX SYNTAX
# 13.1.16 CREATE LOGFILE GROUP SYNTAX
#
# 13.1.17 CREATE PROCEDURE AND CREATE FUNCTION SYNTAX
# 13.1.18 CREATE SERVER SYNTAX
# 13.1.19 CREATE SPATIAL REFERENCE SYSTEM SYNTAX
# 13.1.20 CREATE TABLE SYNTAX
#
# 13.1.21 CREATE TABLESPACE SYNTAX
# 13.1.22 CREATE TRIGGER SYNTAX
# 13.1.23 CREATE VIEW SYNTAX
# 13.1.24 DROP DATABASE SYNTAX
#
# 13.1.25 DROP EVENT SYNTAX
# 13.1.26 DROP FUNCTION SYNTAX
# 13.1.27 DROP INDEX SYNTAX
# 13.1.28 DROP LOGFILE GROUP SYNTAX
#
# 13.1.29 DROP PROCEDURE AND DROP FUNCTION SYNTAX
# 13.1.30 DROP SERVER SYNTAX
# 13.1.31 DROP SPATIAL REFERENCE SYSTEM SYNTAX
# 13.1.32 DROP TABLE SYNTAX
#
# 13.1.33 DROP TABLESPACE SYNTAX
# 13.1.34 DROP TRIGGER SYNTAX
# 13.1.35 DROP VIEW SYNTAX
# 13.1.36 RENAME TABLE SYNTAX
#
# 13.1.37 TRUNCATE TABLE SYNTAX
#
# 13.1.1 ATOMIC DATA DEFINITION STATEMENT SUPPORT
#
# MySQL 8.0 supports atomic Data Definition Language (DDL) statements.
#
# This feature is referred to as atomic DDL.
#
# AN atomic DDL statement combines the data dictionary updates, storage engine operations,
# and binary log writes associated with a DDL operation into a single, atomic transaction.
#
# The transaction is either committed, with applicable changes persisted to the data dictionary,
# storage engine and binary log, or is rolled back, even if the server halts during the operation.
#
# Atomic DDL is made possible by the introduction of the MySQL data dictionary in MySQL 8.0
#
# In earlier MySQL versions, metadata was stored in metadata files, nontransactional tables
# and storage engine-specific dictionaries, which necessitated intermediate commits.
#
# Centralized, transactional metadata storage provided by the MySQL data dictionary
# removed this barrier, making it possible to restructure DDL statement operations into
# atomic transactions.
#
# The atomic DDL feature is described under the following topics in this section:
#
# 		) Supported DDL statements
#
# 		) Atomic DDL characteristics
#
# 		) Changes in DDL Statement Behavior
#
# 		) Storage Engine Support
#
# 		) Viewing DDL Logs
#
# SUPPORTED DDL STATEMENTS
#
# The atomic DDL feature supports both table and non-table DDL statements.
#
# Table-related DDL operations require storage engine support, whereas non-table
# DDL operations do not.
#
# Currently, only the InnoDB storage engine supports atomic DDL.
#
# 		) Supported table DDL statements include CREATE ALTER, and DROP statements for databases,
# 			tablespaces, tables and indexes, and the TRUNCATE_TABLE statement.
#
# 		) Supported non-table DDL statements include:
#
# 			) CREATE and DROP statements, and, if applicable, ALTER statements for stored programs,
# 				triggers, views and user-defined functions (UDFs)
#
# 			) Account management statements: CREATE, ALTER, DROP and if applicable, RENAME
# 				statements for users and roles, as well as GRANT and REVOKE statements.
#
# The following statements are not supported by the atomic DDL feature:
#
# 		) Table-related DDL statements that involve a storage engine other than InnoDB
#
# 		) INSTALL_PLUGIN and UNINSTALL_PLUGIN statements
#
# 		) INSTALL_COMPONENT and UNINSTALL_COMPONENT statements
#
# 		) CREATE_SERVER, ALTER_SERVER and DROP_SERVER statements
#
# ATOMIC DDL CHARACTERISTICS
#
# The characteristics of atomic DDL statements include the following:
#
# 		) Metadata updates, binary log writes, and storage engine operations, where applicable, are combined into a single transaction.
#
# 		) There are no intermediate commits at the SQL layer during the DDL operation
#
# 		) Where applicable:
#
# 			) The state of data dictionary, routine, event and UDF caches is consistent with the status
# 				of the DDL operation, meaning that caches are updated to reflect whether or not the
# 				DDL operation was completed successfully or rolled back.
#
# 			) The storage engine method involved in a DDL operation do not perform intermediate commits,
# 				and the storage engine registers itself as part of the DDL transaction.
#
# 			) The storage engine supports redo and rollback of DDL operations, which is performed in the
# 				Post-DDL phase of the DDL operation.
#
# 		) The visible behavior of DDL operations is atomic, which changes the behavior of some DDL statements.
#
# 			See CHANGES IN DDL STATEMENT BEHAVIOR
#
# NOTE:
#
# 		DDL statements, atomic or otherwise, implicitly end any transaction that is active in the current session,
# 		as if you had done a COMMIT before executing the statement.
#
# 		This means that DDL statements cannot be performed within another transaction, within transaction control
# 		statements such as START TRANSACTION --- COMMIT, or combined with other statements within the same transaction.
#
# CHANGES IN DDL STATEMENT BEHAVIOR
#
# This section describes changes in DDL statement behavior due to the introduction of atomic DDL support.
#
# 		) DROP TABLE operations are fully atomic if all named tables use an atomic DDL-supported storage engine.
#
# 			The statement either drops all tables successfully or is rolled back.
#
# 			DROP TABLE fails with an error if a named table does not exist, and no changes are made, regardless
# 			of the storage engine.
#
# 			This change in behavior is demonstrated in the following example, where the DROP TABLE statement
# 			fails because a named table does not exist:
#
# 				CREATE TABLE t1 (c1 INT);
# 				DROP TABLE t1, t2;
# 				ERROR 1051 (42S02): Unknown table 'test.2'
# 				SHOW TABLES;
# 				+--------------------+
# 				| Tables_in_test 		|
# 				+--------------------+
# 				| t1 					   |
# 				+--------------------+
#
# 			Prior to the introduction of atomic DDL, DROP TABLE reports an error for the named table
# 			that does not exist but succeeds for the named table that does exist:
#
# 				CREATE TABLE t1 (c1 INT);
# 				DROP TABLE t1, t2;
# 				ERROR 1051 (42S02): Unknown table 'test.t2'
# 				SHOW TABLES;
# 				Empty set (0.00 sec)
#
# 			NOTE:
#
# 				Due to this change in behavior, a partially completed DROP TABLE statement on a MySQL 5.7
# 				master fails when replicated on a MySQL 8.0 slave
#
# 				To avoid this failure scenario, use IF EXISTS syntax in DROP TABLE statements to prevent errors
# 				from occurring for tables that do not exist.
#
# 		) DROP_DATABASE is atomic if all tables use an atomic DDL-supported storage engine.
#
# 			The statement either drops all objects successfully or is rolled back.
#
# 			However, removal of the database directory from the file system occurs last
# 			and is not part of the atomic transaction.
#
# 			If removal of the database directory fails due to a file system error
# 			or server halt, the DROP DATABASE transaction is not rolled back.
#
# 		) For tables that do not use an atomic DDL-supported storage engine, table deletion
# 			occur outside of the atomic DROP_TABLE or DROP_DATABASE transaction.
#
# 			Such table deletions are written to the binary log individually, which limits the
# 			discrepancy between the storage engine, data dictionary, and binary log to one table
# 			at most in the case of an interrupted DROP_TABLE or DROP_DATABASE operation.
#
# 			For operations that drop multiple tables, the tables that do not use an atomic
# 			DDL-supported storage engine are dropped before tables that do.
#
# 		) CREATE_TABLE, ALTER_TABLE, RENAME_TABLE, TRUNCATE_TABLE, CREATE_TABLESPACE,
# 			and DROP_TABLESPACE operations for tables that use an atomic DDL-supported
# 			storage engine are either fully committed or rolled back if the server halts
# 			during their operation.
#
# 			In earlier MySQL releases, interruption of these operations could cause discrepancies
# 			between the storage engine, data dictionary and binary log, or leave behind
# 			orphan files.
#
# 			RENAME_TABLE operations are only atomic if all named tables use an atomic
# 			DDL-supported storage engine.
#
# 		) DROP_VIEW fails if a named view does not exist, and no changes are made.
#
# 			The change in behavior is demonstrated in this example, where the
# 			DROP_VIEW statement fails because a named view does not exist:
#
# 				CREATE VIEW test.viewA AS SELECT * FROM t;
# 				DROP VIEW test.viewA, test.viewB;
# 				ERROR 1051 (42S02): Unknown table 'test.viewB'
# 				SHOW FULL TABLES IN test WHERE TABLE_TYPE LIKE 'VIEW';
# 				+-----------------+-------------------+
# 				| Tables_in_test  | Table_type 		  |
# 				+-----------------+-------------------+
# 				| viewA 				| VIEW 				  |
# 				+-----------------+-------------------+
#
# 			Prior to the introduction of atomic DDL, DROP_VIEW returns an error for
# 			the named view that does not exist but succeeds for the named view that
# 			does exist:
#
# 				CREATE VIEW test.viewA AS SELECT * FROM t;
# 				DROP VIEW test.viewA, test.viewB;
# 				ERROR 1051 (42S02): Unknown table 'test.viewB'
# 				SHOW FULL TABLES IN test WHERE TABLE_TYPE LIKE 'VIEW';
# 				Empty set (0.00 sec)
#
# 			NOTE:
#
# 				Due to this change in behavior, a partially completed DROP_VIEW
# 				on a MySQL 5.7 master fails when replicated on a MySQL 8.0 slave.
#
# 				To avoid this failure scenario, use IF EXISTS syntax in DROP_VIEW
# 				statements to prevent an error from occurring for views that do not
# 				exist.
#
# 		) Partial execution of account management statements is no longer permitted.
#
# 			Account management statements either succeed for all named users or roll back
# 			and have no effect if an error occurs.
#
# 			In earlier MySQL versions, account management statements that name multiple
# 			users could succeed for some users and fail for others.
#
# 			The change in behavior is demonstrated in this example, where the second
# 			CREATE_USER statement returns an error but fails because it cannot succeed
# 			for all named users:
#
# 				CREATE USER userA;
# 				CREATE USER userA, userB;
# 				ERROR 1396 (HY000): Operation CREATE USER failed for 'userA'@'%'
# 				SELECT User FROM mysql.user WHERE User LIKE 'user%';
# 				+-------------+
# 				| User 		  |
# 				+-------------+
# 				| userA 		  |
# 				+-------------+
#
# 			Prior to the introduction of atomic DDL, the second CREATE USER statement
# 			returns an error for the named user that does not exist but succeeds
# 			for the named user that does exist:
#
# 				CREATE USER userA;
# 				CREATE USER userA, userB;
# 				ERROR 1396 (HY000): Operation CREATE USER failed for 'userA'@'%'
# 				SELECT User FROM mysql.user WHERE User LIKE 'user%';
# 				+----------+
# 				| User 	  |
# 				+----------+
# 				| userA 	  |
# 				| userB    |
# 				+----------+
#
# 			NOTE:
#
# 				Due to this change in behavior, partially completed account management
# 				statements on a MySQL 5.7 master fail when replicated on a MySQL 8.0 Slave.
#
# 				To avoid this failure scenario, use IF EXISTS or IF NOT EXISTS syntax,
# 				as appropriate, in account management statements to prevent errors related
# 				to named users.
#
# STORAGE ENGINE SUPPORT
#
# Currently, only the InnoDB storage engine supports atomic DDL.
#
# Storage engines that do not support atomic DDL are exempted from DDL
# atomicity.
#
# DDL operations involving exempted storage engines remain capable
# of introducing inconsistencies that can occur when operations are interuppted
# or only partially completed.
#
# To support redo and rollback of DDL operations, InnoDB writes DDL logs to
# the mysql.innodb_ddl_log table, which is a hidden data dictionary table that
# resides in the mysql.ibd data dictionary tablespace.
#
# To view DDL logs that are written to the mysql.innodb_ddl_log table during
# a DDL operation, enable the innodb_print_ddl_logs configuration option.
#
# For more information, see VIEWING DDL LOGS.
#
# NOTE:
#
# 		THe redo logs for changes to the mysql.innodb_ddl_log table are flushes to disk
# 		immediately regardless of the innodb_flush_log_at_trx_commit setting.
#
# 		Flushing the redo logs immediately avoids situations where data files are modified
# 		by DDL operations but the redo logs for changes to the mysql.innodb_ddl_log
# 		table resulting from those operations are not persisted to disk.
#
# 		SUch a situation could cause errors during rollback or recovery.
#
# The InnoDB storage engine executes DDL operations in phases.
#
# DDL operations such as ALTER_TABLE may perform the Prepare and Perform
# phases multiple times prior to the Commit phase.
#
# 		1. Prepare: Create hte required objects and write the DDL logs to the
# 			mysql.innodb_ddl_log table
#
# 			The DDL logs define how to roll forward and roll back the DDL
# 			operation
#
# 		2. Perform: Perform the DDL operation. For example, perform a create routine for a CREATE TABLE operation.
#
# 		3. Commit: Update the data dictionary and commit the data dictionary transaction
#
# 		4. Post-DDL: Replay and remove DDL logs from the mysql.innodb_ddl_log table.
#
# 			To ensure that rollback can be performed safely without introducing inconsistencies,
# 			file operations such as renaming or removing data files are performed in
# 			this final phase.
#
# 			This phase also removes dynamic metadata from the mysql.innodb_dynamic_metadata
# 			data dictionary table for DROP_TABLE, TRUNCATE_TABLE and other DDL operations
# 			that rebuild the table.
#
# DDL logs are replayed and removed from the mysql.innodb_ddl_log table during the
# Post-DDL phase, regardless of whether the transaction is committed or rolled back.
#
# DDL logs should only remain in the mysql.innodb_ddl_log table if the server is halted
# during a DDL operation.
#
# In this case, the DDL logs are replayed and removed after recovery.
#
# In a recovery situation, a DDL transaction may be committed or rolled back when
# the server is restarted.
#
# If the data dictionary transaction that was performed during the Commit phase of a
# DDL operation is present in the redo log and binary log, the operation is considered
# successful and is rolled forward.
#
# Otherwise, the incomplete data dictionary transaction is rolled back when InnoDB
# replays data dictionary redo logs, and the DDL transaction is rolled back.
#
# VIEWING DDL LOGS
#
# To view DDL logs that are written to the mysql.innodb_ddl_log data dictionary
# table during atomic DDL operations that involve the InnoDB storage engine, enable
# innodb_print_ddl_logs to have MySQL write the DDL logs to stderr.
#
# Depending on the host operating system and MySQL configuration, stderr may be the
# error log, terminal or console window.
#
# See SECTION 5.4.2.2, "DEFAULT ERROR LOG DESTINATION CONFIGURATION"
#
# InnoDB writes DDL logs to the mysql.innodb_ddl_log table to support redo and
# rollback of DDL operations.
#
# The mysql.innodb_ddl_log table is a hidden data dictionary table that resides
# in the mysql.ibd data dictionary tablespace.
#
# Like other hidden data dictionary tables, the mysql.innodb_ddl_log table cannot
# be accessed directly in non-debug versions of MySQL.
#
# (See SECTION 14.1, "DATA DICTIONARY SCHEMA")
#
# The structure of the mysql.innodb_ddl_log table corresponds to this definition:
#
# 		CREATE TABLE mysql.innodb_ddl_log (
# 			id BIGINT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
# 			thread_id BIGINT UNSIGNED NOT NULL,
# 			type INT UNSIGNED NOT NULL,
# 			space_id INT UNSIGNED,
# 			page_no INT UNSIGNED,
# 			index_id BIGINT UNSIGNED,
# 			table_id BIGINT UNSIGNED,
# 			old_file_path VARCHAR(512) COLLATE UTF8_BIN,
# 			new_file_path VARCHAR(512) COLLATE UTF8_BIN,
# 			KEY(thread_id)
# 		);
#
# 		) id: A unique identifier for a DDL log record
#
# 		) thread_id: Each DDL log record is assigned a thread_id which is used to replay and remove
# 			DDL logs that belong to a particular DDL transaction.
#
# 			DDL transactions that involve multiple data file operations generate multiple DDL log records.
#
# 		) type: The DDL operation type. Types include FREE (drop an index tree), DELETE (delete a file),
# 			RENAME (rename a file), or DROP (drop metadata from the mysql.innodb_dynamic_metadata data dictionary
# 			table)
#
# 		) space_id: The tablespace ID
#
# 		) page_no: A page that contains allocation information; an index tree root page, for example.
#
# 		) index_id: The index ID
#
# 		) table_id: The table ID
#
# 		) old_file_path: The old tablespace file path. Used by DDL operations that create or drop tablespace
# 			files; also used by DDL operations that rename a tablespace.
#
# 		) new_file_path: The new tablespace file path. Used by DDL operations that rename tablespace files.
#
# THis example demonstrates enabling innodb_print_ddl_logs to view DDL logs written to stderr for
# a CREATE TABLE operation.
#
# 		SET GLOBAL innodb_print_ddl_logs=1;
# 		CREATE TABLE t1 (c1 INT) ENGINE = InnoDB;
#
# 		[Note] [000000] InnoDB: DDL log insert : [DDL record: DELETE SPACE, id=18, thread_id=7,
# 		space_id=5, old_file_path=./test/t1.ibd]
# 		[Note] [000000] InnoDB: DDL log delete : by id 18
#
# 		[Note] [000000] InnoDB: DDL log insert : [DDL record: REMOVE CACHE, id=19, thread_id=7,
# 		table_id=1058, new_file_path=test/t1]
# 		[Note] [000000] InnoDB: DDL log delete : by id 19
#
# 		[Note] [000000] InnoDB: DDL log insert : [DDL record: FREE, id=20, thread_id=7,
# 		space_id=5, index_id=132, page_no=4]
# 		[Note] [000000] InnoDB: DDL log delete : by id 20
#
# 		[Note] [000000] InnoDB: DDL log post ddl : begin for thread id : 7
# 		[Note] [000000] InnoDB: DDL log post ddl : end for thread id : 7
#
# 13.1.2 ALTER DATABASE SYNTAX
#
# ALTER {DATABASE | SCHEMA} [db_name]
# 		alter_specification ---
#
# alter_specification:
# 		[DEFAULT] CHARACTER SET [=] charset_name
# 	 | [DEFAULT] COLLATE [=] collation_name
#
# ALTER_DATABASE enables you to change the overall characteristics of a database.
#
# These characteristics are stored in the data dictionary.
#
# To use ALTER_DATABASE, you need the ALTER privilege on the database.
#
# ALTER_SCHEMA is a synonym for ALTER_DATABASE
#
# The database name can be omitted from the first syntax, in which case the statement
# applies to the default database.
#
# NATIONAL LANGUAGE CHARACTERISTICS
#
# The CHARACTER SET clause changes the default database character set.
#
# The COLLATE clause changes the default database collation. CHAPTER 10, CHARACTER SETS,
# COLLATIONS,, UNICODE discusses char sets and collation names.
#
# You can see what character sets and collations are available using, respectively,
# the SHOW_CHARACTER_SET and SHOW_COLLATION statements.
#
# See SECTION 13.7.6.3, "SHOW CHARACTER SET SYNTAX", and SECTION 13.7.6.4, "SHOW COLLATION SYNTAX"
# for more information
#
# If you change the default character set or collation for a database, stored routines that use the
# database defaults must be dropped and recreated so that they use the new defaults.
#
# (in a stored routine, variables with character data types use the database defaults if
# the character set or collation are not specified explicitly.
#
# See SECTION 13.1.17, "CREATE PROCEDURE AND CREATE FUNCTION SYNTAX")
#
# 13.1.3 ALTER EVENT SYNTAX
#
# ALTER
# 		[DEFINER = { user | CURRENT_USER }]
# 		EVENT event_name
# 		[ON SCHEDULE schedule]
# 		[ON COMPLETION [NOT] PRESERVE]
# 		[RENAME TO new_event_name]
# 		[ENABLE | DISABLE | DISABLE ON SLAVE]
# 		[COMMENT 'string']
# 		[DO event_body]
#
# The ALTER_EVENT statement changes one or more of the characteristics of an existing event
# without the need to drop and recreate it.
#
# THe syntax for each of the DEFINER, ON SCHEDULE, ON COMPLETION, ENABLE / DISABLE and DO clauses
# is exactly the same as when used with CREATE_EVENT
#
# (See SECTION 13.1.13, "CREATE EVENT SYNTAX")
#
# Any user can alter an event defined on a database for which that user has the EVENT privilege.
#
# When a user executes a successful ALTER_EVENT statement, that user becomes the definer
# for the affected event.
#
# ALTER_EVENT works only with an existing event:
#
# 		ALTER EVENT no_such_event
# 			ON SCHEDULE
# 				EVERY '2:3' DAY_HOUR;
# 		ERROR 1517 (HY000): Unknown event 'no_such_event'
#
# In each of the following examples, assume that hte event named myevent is defined
# as shown here:
#
# 		CREATE EVENT myevent
# 			ON SCHEDULE
# 				EVERY 6 HOUR
# 			COMMENT 'A sample comment.'
# 			DO
# 				UPDATE myschema.mytable SET mycol = mycol + 1;
#
# The following statement changes the schedule for myevent from once every six hours
# starting immediately to once every twelve hours, starting four hours from the time
# the statement is run:
#
# 		ALTER EVENT myevent
# 			ON SCHEDULE
# 				EVERY 12 HOUR
# 			STARTS CURRENT_TIMESTAMP + INTERVAL 4 HOUR;
#
# It is possible to change multiple characteristics of an event in a single statement.
#
# This example changes the SQL statement executed by myevent to one that deletes all
# records from mytable;
#
# It also changes the schedule for the event such that it executes once, one day
# after this ALTER_EVENT statement is run.
#
# 		ALTER EVENT myevent
# 			ON SCHEDULE
# 				AT CURRENT_TIMESTAMP + INTERVAL 1 DAY
# 			DO
# 				TRUNCATE TABLE myschema.mytable;
#
# Specify the options in an ALTER_EVENT statement only for those characteristics
# that you want to change;
#
# Omitted options keep their existing values.
#
# This includes any default values for CREATE_EVENT such as ENABLE.
#
# To disable myevent, use this ALTER_EVENT statement:
#
# 		ALTER EVENT myevent
# 			DISABLE;
#
# The ON SCHEDULE clause may use expressions involving built-in MySQL functions and user variables
# to obtain any of the timestamp or interval values which it contains.
#
# You cannot use stored routines or user-defined functions in such expressions, and you cannot
# use any table references;
#
# However, you can use SELECT FROM DUAL. This is true for both ALTER_EVENT and CREATE_EVENT
# statements.
#
# References to stored routines, user-defined functions, and tables in such cases are specifically
# not permitted, and fail with an error (see Bug #22830)
#
# Although an ALTER_EVENT statement that contains another ALTER_EVENT statement in its DO clause
# appears to succeed, when the server attempts to execute the resulting scheduled event, the execution
# fails with an error.
#
# To rename an event, use the ALTER_EVENT statement's RENAME TO clause.
#
# This statement renames the event myevent to yourevent:
#
# 		ALTER EVENT myevent
# 			RENAME TO yourevent;
#
# You can also move an event to a different database using ALTER EVENT --- RENAME TO ---
# and db_name.event_name notation, as shown here:
#
# 		ALTER EVENT olddb.myevent
# 			RENAME TO newdb.myevent;
#
# To execute the previous statement, the user executing it must have the EVENT privilege
# on both the olddb and newdb databases.
#
# NOTE:
#
# 		There is no RENAME EVENT statement
#
# The value DISABLE ON SLAVE is used on a replication slave instead of ENABLE or DISABLE
# to indicate an event that was created on the master and replicated to the slave, but that
# is not executed on the slave.
#
# Normally, DISABLE ON SLAVE is set automatically as required; however, there are some circumstances
# under which you may want or need to change it manually.
#
# See SECTION 17.4.1.16, "REPLICATION OF INVOKED FEATURES", for more information.
#
# 13.1.4 ALTER FUNCTION SYNTAX
#
# ALTER FUNCTION func_name [characteristic ...]
#
# characteristic:
# 		COMMENT 'string'
# 	 | LANGUAGE SQL
# 	 | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }
# 	 | SQL SECURITY { DEFINER | INVOKER }
#
# This statement can be used to change the characteristics of a stored function.
#
# More than one change may be specified in an ALTER_FUNCTION statement.
#
# However, you cannot change the parameters or body of a stored function
# using this statement; to make such changes, you must drop and re-create
# the function using DROP_FUNCTION and CREATE_FUNCTION
#
# You must have the ALTER_ROUTINE privilege for the function.
#
# (That privilege is granted automatically to the function creator)
#
# If binary logging is enabled, the ALTER_FUNCTION statement might also require
# the SUPER privilege, as described in SECTION 24.7, "BINARY LOGGING OF STORED PROGRAMS"
#
# 13.1.5 ALTER INSTANCE SYNTAX
#
# ALTER INSTANCE ROTATE INNODB MASTER KEY
#
# ALTER INSTANCE defines actions applicable to a MySQL server instance.
#
# The ALTER INSTANCE ROTATE INNODB MASTER KEY statement is used to rotate the master encryption
# key used for InnoDB tablespace encryption.
#
# A keyring plugin must be installed and configured to use this statement.
#
# By default, the MySQL server loads the keyring_file plugin.
#
# Key rotation requires the ENCRYPTION_KEY_ADMIN or SUPER privilege
#
# ALTER INSTANCE ROTATE INNODB MASTER KEY supports concurrent DML.
#
# However, it cannot be run concurrently with CREATE_TABLE_---_ENCRYPTION
# or ALTER_TABLE_---_ENCRYPTION operations, and locks are taken to prevent
# conflicts that could arise from concurrent execution of these statements.
#
# If one of the conflicting statements is running, it must complete before 
# another can proceed.
#
# ALTER INSTANCE actions are written to the binary log so that they can be 
# executed on replicated servers.
#
# For additional ALTER INSTANCE ROTATE INNODB MASTER KEY usage information,
# see SECTION 15.6.3.9, "TABLESPACE ENCRYPTION"
#
# For information about the keyring_file plugin, see SECTION 6.5.4, "THE MYSQL KEYRING"
#
# 13.1.6 ALTER LOGFILE GROUP SYNTAX
#
# ALTER LOGFILE GROUP logfile_group
# 		ADD UNDOFILE 'file_name'
# 		[INITIAL_SIZE [=] size]
# 		[WAIT]
# 		ENGINE [=] engine_name
#
# This statement adds an UNDO file named 'file_name' to an existing log file group
# logfile_group.
#
# An ALTER_LOGFILE_GROUP statement has one and only one ADD UNDOFILE clause.
#
# No DROP UNDOFILE clause is currently supported.
#
# NOTE:
#
# 		All NDB Cluster Disk Data objects share the same namespace.
#
# 		This means that each Disk Data object must be uniquely named
# 		(and not merely each Disk Data object of a given type)
#
# 		For example, you cannot have a tablespace and an undo log file
# 		with the same name, or an undo log file and a data file with the
# 		same name.
#
# The optional INITIAL_SIZE parameter sets the UNDO file's initial size in bytes;
# if not specified, the initial size defaults to 134217728 (128 MB)
#
# You may optionally follow size with a one-letter abbreviation for an order
# of magnitude, similar to those used in my.cnf 
#
# Generally, this is one of the letters M (megabytes) or G (gigabytes)
#
# (Bug #13116514, Bug #16104705, Bug #62858)
#
# On 32-bit systems, the maximum supported value for INITIAL_SIZE is
# 4294967296 (4 GB) (Bug #29186)
#
# The minimum allowed value for INITIAL_SIZE is 1048576 (1 MB) (Bug #29574)
#
# NOTE:
#
# 		WAIT is parsed but otherwise ignored. 
# 		This keyword currently has no effect, and is intended for future expansion.
#
# The ENGINE parameter (required) determines the storage engine which is used by this
# log file group, with engine_name being the name of the storage engine.
#
# Currently, the only accepted values for engine_name are "NDBCLUSTER" and "NDB"
#
# The two values are equivalent
#
# Here is an example, which assumes that the log file group lg_3 has already been
# created using CREATE_LOGFILE_GROUP (see SECTION 13.1.16, "CREATE LOGFILE GROUP SYNTAX"):
#
# 		ALTER LOGFILE GROUP lg_3
# 			ADD UNDOFILE 'undo_10.dat'
# 			INITIAL_SIZE=32M
# 			ENGINE=NDBCLUSTER;
#
# When ALTER_LOGFILE_GROUP is used with ENGINE = NDBCLUSTER (alternatively, ENGINE = NDB), an UNDO log file
# is created on each NDB Cluster data node.
#
# You can verify that the UNDO files were created and obtain information about them by querying
# the INFORMATION_SCHEMA.FILES table.
#
# For example:
#
# 		SELECT FILE_NAME, LOGFILE_GROUP_NUMBER, EXTRA
# 		FROM INFORMATION_SCHEMA.FILES
# 		WHERE LOGFILE_GROUP_NAME = 'lg_3';
# 		+-------------+------------------------------+-------------------+
# 		| FILE_NAME   | LOGFILE_GROUP_NUMBER 			| EXTRA 				  |
# 		+-------------+------------------------------+-------------------+
# 		| newdata.dat | 	0 									| CLUSTER_NODE=3    |
# 		| newdata.dat | 	0 									| CLUSTER_NODE=4 	  |
# 		| undo_10.dat | 	11 								| CLUSTER_NODE=3    |
# 		| undo_10.dat | 	11 								| CLUSTER_NODE=4	  |
# 		+-------------+------------------------------+-------------------+
#
# (See SECTION 25.10, "THE INFORMATION_SCHEMA FILES TABLE")
#
# Memory used for UNDO_BUFFER_SIZE comes from the global pool whose size is determined
# by the value of the SharedGlobalMemory data node configuration parameter.
#
# This includes any default value implied for this option by the setting of the
# InitialLogFileGroup data node configuration parameter.
#
# ALTER_LOGFILE_GROUP is useful only with Disk Data storage for NDB Cluster.
# For more information, see SECTION 22.5.13, "NDB CLUSTER DISK DATA TABLES"
#
# 13.1.7 ALTER PROCEDURE SYNTAX
#
# ALTER PROCEDURE proc_name [characteristic ---]
#
# characteristic:
# 		COMMENT 'string'
# 	 | LANGUAGE SQL
# 	 | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }
# 	 | SQL SECURITY { DEFINER | INVOKER }
#
# This statement can be used to change the characteristics of a stored procedure.
#
# More than one change may be specified in an ALTER_PROCEDURE statement.
#
# However, you cannot change the parameters or body of a stored procedure
# using this statement; to make such changes, you must drop and re-create
# the procedure using DROP_PROCEDURE and CREATE_PROCEDURE.
#
# You must have the ALTER_ROUTINE privilege for the procedure.
#
# By default, that privilege is granted automatically to the procedure creator.
#
# This behavior can be changed by disabling the automatic_sp_privileges system 
# variable.				
#
# See SECTION 24.2.2, "STORED ROUTINES AND MYSQL PRIVILEGES"
#
# 13.1.8 ALTER SERVER SYNTAX
#
# ALTER SERVER server_name
# 		OPTIONS (option [, option] ---)
#
# Alters the server information for server_name, adjusting any of the options
# permitted in the CREATE_SERVER statement.
#
# The corresponding fields in the mysql.servers table are updated accordingly.
#
# This statement requires the SUPER privilege.
#
# For example, to update the USER option:
#
# 		ALTER SERVER s OPTIONS (USER 'sally');
#
# ALTER SERVER causes an implicit commit. See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# ALTER SERVER is not written to the binary log, regardless of the logging format that
# is in use.
#
# 13.1.9 ALTER TABLE SYNTAX
#
# 13.1.9.1 ALTER TABLE PARTITION OPERATIONS
# 13.1.9.2 ALTER TABLE AND GENERATED COLUMNS
# 13.1.9.3 ALTER TABLE EXAMPLES
#
# 		ALTER TABLE tbl_name
# 			[alter_specification [, alter_specification] ---]
# 			[partition_options]
#
# 		alter_specification:
# 			table_options
# 		 | ADD [COLUMN] col_name column_definition
# 				 [FIRST | AFTER col_name]
# 		 | ADD [COLUMN] (col_name column_definition ---)
# 		 | ADD {INDEX|KEY} [index_name]
# 				 [index_type] (key_part, ---) [index_option] ---
# 		 | ADD [CONSTRAINT [symbol]] PRIMARY KEY
# 				 [index_type] (key_part, ---) [index_option] ---
# 		 | ADD [CONSTRAINT [symbol]]
# 				 UNIQUE [INDEX|KEY] [index_name]
# 				 [index_type] (key_part, ---) [index_option] ---
# 		 | ADD FULLTEXT [INDEX|KEY] [index_name]
# 				(key_part,---) [index_option] ---
# 		 | ADD SPATIAL [INDEX|KEY] [index_name]
# 				(key_part,---) [index_option] ---
# 		 | ADD [CONSTRAINT [symbol]]
# 				FOREIGN KEY [index_name] (col_name, ---)
# 				reference_definition
# 		 | ALGORITHM [=] {DEFAULT|INSTANT|INPLACE|COPY}
# 		 | ALTER [COLUMN] col_name {SET DEFAULT literal | DROP DEFAULT}
# 		 | ALTER INDEX index_name {VISIBLE | INVISIBLE}
# 		 | CHANGE [COLUMN] old_col_name new_col_name column_definition
# 				[FIRST|AFTER col_name]
# 		 | [DEFAULT] CHARACTER SET [=] charset_name [COLLATE [=] collation_name]
# 		 | CONVERT TO CHARACTER SET charset_name [COLLATE collation_name]
# 		 | {DISABLE|ENABLE} KEYS
# 		 | {DISCARD|IMPORT} TABLESPACE
# 		 | DROP [COLUMN] col_name
# 		 | DROP {INDEX|KEY} index_name
# 		 | DROP PRIMARY KEY
# 		 | DROP FOREIGN KEY fk_symbol
# 		 | FORCE
# 		 | LOCK [=] {DEFAULT|NONE|SHARED|EXCLUSIVE}
# 		 | MODIFY [COLUMN] col_name column_definition
# 				[FIRST | AFTER col_name]
# 		 | ORDER BY col_name [, col_name] ---
# 		 | RENAME COLUMN old_col_name TO new_col_name
# 		 | RENAME {INDEX|KEY} old_index_name TO new_index_name
# 		 | RENAME [TO|AS] new_tbl_name
# 		 | {WITHOUT|WITH} VALIDATION
# 		 | ADD PARTITION (partition_definition)
# 		 | DROP PARTITION partition_names
# 		 | DISCARD PARTITION {partition_names | ALL} TABLESPACE
# 		 | IMPORT PARTITION {partition_names | ALL} TABLESPACE
# 		 | TRUNCATE PARTITION {partition_names | ALL}
# 		 | COALESCE PARTITION number
# 		 | REORGANIZE PARTITION partition_names INTO (partition_definitions)
# 		 | EXCHANGE PARTITION partition_name WITH TABLE tbl_name [{WITH|WITHOUT} VALIDATION]
# 		 | ANALYZE PARTITION {partition_names | ALL}
# 		 | CHECK PARTITION {partition_names | ALL}
# 		 | OPTIMIZE PARTITION {partition_names | ALL}
# 		 | REBUILD PARTITION {partition_names | ALL}
# 		 | REPAIR PARTITION {partition_names | ALL}
# 		 | REMOVE PARTITIONING
# 		 | UPGRADE PARTITIONING
#
# 		key_part: {col_name [(length)] | (expr)} [ASC | DESC]
#
# 		index_type:
# 			USING {BTREE | HASH}
#
# 		index_option:
# 			KEY_BLOCK_SIZE [=] value
# 		 | index_type
# 		 | WITH PARSER parser_name
# 		 | COMMENT 'string'
# 		 | {VISIBILE | INVISIBLE}
#
# 		table_options:
# 			table_option [[,] table_option] ---
#
# 		table_option:
# 			AUTO_INCREMENT [=] value
# 		 | AVG_ROW_LENGTH [=] value
# 		 | [DEFAULT] CHARACTER SET [=] charset_name
# 		 | CHECKSUM [=] {0 | 1}
# 		 | [DEFAULT] COLLATE [=] collation_name
# 		 | COMMENT [=] 'string'
# 		 | COMPRESSION [=] {'ZLIB'|'LZ4'|'NONE'}
# 		 | CONNECTION [=] 'connect_string'
# 		 | {DATA|INDEX} DIRECTORY [=] 'absolute path to directory'
# 		 | DELAY_KEY_WRITE [=] {0 | 1}
# 		 | ENCRYPTION [=] {'Y' | 'N'}
# 		 | ENGINE [=] engine_name
# 		 | INSERT_METHOD [=] { NO | FIRST | LAST }
# 		 | KEY_BLOCK_SIZE [=] value
# 		 | MAX_ROWS [=] value
# 		 | MIN_ROWS [=] value
# 		 | PACK_KEYS [=] {0 | 1 | DEFAULT}
# 		 | PASSWORD [=] 'string'
# 		 | ROW_FORMAT [=] {DEFAULT|DYNAMIC|FIXED|COMPRESSED|REDUNDANT|COMPACT}
# 		 | STATS_AUTO_RECALC [=] {DEFAULT|0|1}
# 		 | STATS_PERSISTENT [=] {DEFAULT|0|1}
# 		 | STATS_SAMPLE_PAGES [=] value
# 		 | TABLESPACE tablespace_name [STORAGE {DISK|MEMORY|DEFAULT}]
# 		 | UNION [=] (tbl_name[,tbl_name] ---)
#
# 		partition_options:
# 			(see CREATE TABLE options)
#
# ALTER_TABLE changes the structure of a table. For example, you can add or delete columns,
# create or destroy indexes, change the type of existing columns, or rename columns
# or the table itself.
#
# You can also change characteristics such as the storage engine used for the table or the
# table comment.
#
# 		) To use ALTER_TABLE, you need ALTER, CREATE and INSERT privileges for the table.
#
# 			Renaming a table requires ALTER and DROP on the old table, ALTER, CREATE, and
# 			INSERT on the new table.
#
# 		) Following the table name, specify the alterations to be made. If none are given, ALTER_TABLE does nothing.
#
# 		) The syntax for many of the permissible alterations is similar to clauses of the CREATE_TABLE statement.
#
# 			column_definition clauses use the same syntax for ADD and CHANGE as for CREATE_TABLE 
#
# 			For more information, see SECTION 13.1.20, "CREATE TABLE SYNTAX"
#
# 		) The word COLUMN is optional and can be omitted, except for RENAME COLUMN (to distinguish a column-renaming
# 			operation from the RENAME table-renaming operation)
#
# 		) Multiple ADD, ALTER, DROP, and CHANGE clauses are permitted in a single ALTER_TABLE statement,
# 			separated by commas.
#
# 			This is a MySQL extension to standard SQL, which permits only one of each clause per ALTER_TABLE
# 			statement.
#
# 			For example, to drop multiple columns in a single statement, do this:
#
# 				ALTER TABLE t2 DROP COLUMN c, DROP COLUMN d;
#
# 		) If a storage engine does not support an attempted ALTER_TABLE operation, a warning may result.
#
# 		Such warnings can be displayed with SHOW_WARNINGS.
#
# 		See SECTION 13.7.6.40, "SHOW WARNINGS SYNTAX"
#
# 		For information on troubleshooting ALTER_TABLE, see SECTION B.6.6.1, "PROBLEMS WITH ALTER TABLE"
#
# 		) For information about generated columns, see SECTION 13.1.9.2, "ALTER TABLE AND GENERATED COLUMNS"
#
# 		) For usage examples, see SECTION 13.1.9.3, "ALTER TABLE EXAMPLES"
#
# 		) With the mysql_info() C API function, you can find out how many rows were copied by ALTER_TABLE.
#
# 			See SECTION 28.7.7.36, "MYSQL_INFO()"
#
# There are several additional aspects to the ALTER TABLE statement, described under the following
# topics in this section:
#
# 		) TABLE OPTIONS
#
# 		) PERFORMANCE AND SPACE REQUIREMENTS
#
# 		) CONCURRENCY CONTROL
#
# 		) ADDING AND DROPPING COLUMNS
#
# 		) RENAMING, REDEFINING, AND REORDERING COLUMNS
#
# 		) PRIMARY KEYS AND INDEXES
#
# 		) FOREIGN KEYS
#
# 		) CHANGING THE CHARACTER SET
#
# 		) DISCARDING AND IMPORTING INNODB TABLESPACES
#
# 		) ROW ORDER FOR MYISAM TABLES
#
# 		) PARTITIONING OPTIONS
#
# TABLE OPTIONS
#
# table_options signifies table options of the kind that can be used in the CREATE_TABLE
# statement, such as ENGINE, AUTO_INCREMENT, AVG_ROW_LENGTH, MAX_ROWS, ROW_FORMAT or TABLESPACE.
#
# For descriptions of all table options, see SECTION 13.1.20, "CREATE TABLE SYNTAX"
#
# However, ALTER_TABLE ignores DATA DIRECTORY and INDEX DIRECTORY when given as table options.
#
# ALTER_TABLE permits them only as partitioning options, and requires that you have the FILE privilege.
#
# Use of table options with ALTER_TABLE provides a convenient way of altering single table characteristics.
#
# For example:
#
# 		) If t1 is currently not an InnoDB table, this statement changes its storage engine to InnoDB:
#
# 			ALTER TABLE t1 ENGINE = InnoDB;
#
# 			) See SECTION 15.6.1.3, "CONVERTING TABLES FROM MYISAM TO INNODB" for considerations when switching
# 				tables to the InnoDB storage engine.
#
# 			) When you specify an ENGINE clause, ALTER_TABLE rebuilds the table.
#
# 				This is true even if the table already has the specified storage engine.
#
# 			) Running ALTER_TABLE_tbl_name_ENGINE=INNODB on an existing InnoDB table performs
# 				a "null" ALTER_TABLE operation, which can be used to defragment an InnoDB table,
# 				as described in SECTION 15.11.4, "DEFRAGMENTING A TABLE"
#
# 				Running ALTER_TABLE_tbl_name_FORCE on an InnoDB table performs the same function
#
# 			) ALTER_TABLE_tbl_name_ENGINE=INNODB and ALTER_TABLE_tbl_name_FORCE use online DDL.
#
# 				For more information, see SECTION 15.12, "INNODB AND ONLINE DDL"
#
# 			) The outcome of attempting to change the storage engine of a table is affected
# 				by whether the desired storage engine is available and the setting of the
# 				NO_ENGINE_SUBSTITUTION SQL mode, as described in SECTION 5.1.11, "SERVER SQL MODES"
#
# 			) To prevent inadvertent loss of data, ALTER_TABLE cannot be used to change the storage
# 				engine of a table to MERGE or BLACKHOLE
#
# 		) To change the InnoDB table to use compressed row-storage format:
#
# 			ALTER TABLE t1 ROW_FORMAT = COMPRESSED;
#
# 		) To enable or disable encryption for an InnoDB table in a file-per-table tablespace:
#
# 			ALTER TABLE t1 ENCRYPTION='Y';
# 			ALTER TABLE t1 ENCRYPTION='N';
#
# 			A keyring plugin must be installed and configured to use the ENCRYPTION option.
#
# 			For more information, see SECTION 15.6.3.9, "TABLESPACE ENCRYPTION"
#
# 		) To reset the current auto-increment value:
#
# 			ALTER TABLE t1 AUTO_INCREMENT = 13;
#
# 			You cannot reset the counter to a value less than or equal to the value that is
# 			currently in use.
#
# 			For both InnoDB and MyISAM, if the value is less than or equal to the maximum value
# 			currently in the AUTO_INCREMENT column, the value is reset to the current maximum
# 			AUTO_INCREMENT column value plus one.
#
# 		) To change the default table character set:
#
# 			ALTER TABLE t1 CHARACTER SET = utf8;
#
# 			See also CHANGING THE CHARACTER SET
#
# 		) To add(or change) a table comment:
#
# 			ALTER TABLE t1 COMMENT = 'New table comment';
#
# 		) Use ALTER TABLE with the TABLESPACE option to move InnoDB tables between existing
# 			general tablespaces, file-per-table tablespaces, and the system tablespace.
#
# 			See MOVING TABLES BETWEEN TABLESPACES USING ALTER TABLE
#
# 			) ALTER TABLE --- TABLESPACE operations always cause a full table rebuild, even if
# 				the TABLESPACE attribute has not changed from its previous value.
#
# 			) ALTER TABLE --- TABLESPACE syntax does not support moving a table from a temporary
# 				tablespace to a persistent tablespace.
#
# 			) The DATA DIRECTORY clause, which is supported with CREATE_TABLE_---_TABLESPACE,
# 				is not supported with ALTER TABLE --- TABLESPACE, and is ignored if specified.
#
# 			) For more information about the capabilities and limitations of the TABLESPACE option, see CREATE_TABLE
#
# 		) MySQL NDB Cluster 8.0 supports setting NDB_TABLE options for controlling a table's partition balance
# 			(fragment count type), read-from-any-replica capability, full replication, or any combination of these,
# 			as part of the table comment for an ALTER TABLE statement in the same manner as for CREATE_TABLE,
# 			as shown in this example:
#
# 				ALTER TABLE t1 COMMENT = "NDB_TABLE=READ_BACKUP=0,PARTITION_BALANCE=FOR_RA_BY_NODE";
#
# 			Bear in mind that ALTER TABLE --- COMMENT --- discards any existing comment for the table.
#
# 			See SETTING NDB_TABLE OPTIONS, for additional information and examples.
#
# To verify that the table options were changed as intended, use SHOW_CREATE_TABLE,
# or query the INFORMATION_SCHEMA.TABLES table.
#
# PERFORMANCE AND SPACE REQUIREMENTS
#
# ALTER_TABLE operations are processed using one of the following algorithms:
#
# 		) COPY: Operations are performed on a copy of the original table, and table data is copied
# 			from the original table to the new table row by row.
#
# 			Concurrent DML is not permitted.
#
# 		) INPLACE: Operations avoid copying table data but may rebuild the table in place.
#
# 			An exclusive metadata lock on the table may be taken briefly during preparation and
# 			execution phases of the operation.
#
# 			Typically, concurrent DML is supported.
#
# 		) INSTANT: Operations only modify metadata in the data dictionary.
#
# 			No exclusive metadata locks are taken on the table during preparation and execution,
# 			and table data is unaffected, making operations instantaneous.
#
# 			Concurrent DML is permitted. (Introduced in MySQL 8.0.12)
#
# The ALGORITHM clause is optional. If the ALGORITHM clause is omitted, MySQL uses ALGORITHM=INSTANT
# for storage engines and ALTER_TABLE clauses that support it.
#
# Otherwise, ALGORITHM=INPLACE is used. If ALGORITHM=INPLACE is not supported,
# ALGORITHM=COPY is used.
#
# Specifying an ALGORITHM clause requires the operation to use the specified algorithm for clauses
# and storage engines that support it, or fail with an error otherwise.
#
# Specifying ALGORITHM=DEFAULT is the same as omitting the ALGORITHM clause.
#
# ALTER_TABLE operations that use the COPY algorithm wait for other operations that
# are modifying the table to complete.
#
# After alterations are applied to the table copy, data is copied over, the original
# table is deleted, and the table copy is renamed to the name of the original table.
#
# While the ALTER_TABLE operation executes, the original table is readable by other
# sessions (with the exception noted shortly)
#
# Updates and writes to the table started after the ALTER_TABLE operation begins are
# stalled until the new table is ready, then are automatically redirected to the new table.
#
# The temporary copy of the table is created in the database directory of the original
# table unless it is a RENAME TO operation that moves the table to a database that 
# resides in a different directory.
#
# The exception referred to earlier is that ALTER_TABLE blocks reads (not just writes)
# at the point where it is ready to clear outdated table structures from the table and
# table definition caches.
#
# At this point, it must acquire an exclusive lock.
#
# To do so, it waits for the current readers to finish, and blocks new reads and writes.
#
# An ALTER_TABLE operation that uses the COPY algorithm prevents concurrent DML operations.
#
# Concurrent queries are still allowed. That is, a table-copying operationg always includes
# at least the concurrency restrictions of LOCK=SHARED (allow queries but not DML)
#
# You can further restrict concurrency for operations that support the LOCK clause
# by specifying LOCK=EXCLUSIVE, which prevents DML and queries.
#
# For more information, see CONCURRENCY CONTROL
#
# To force use of the COPY algorithm for an ALTER_TABLE operation that would otherwise
# not use it, specify ALGORITHM=COPY or enable the old_alter_table system variable.
#
# If there is a conflict between the old_alter_table setting and an ALGORITHM
# clause with a value other than DEFAULT, the ALGORITHM clause takes precedence.
#
# For InnnoDB tables, an ALTER_TABLE operation that uses the COPY algorithm on a table
# that resides in a shared tablespace can increase the amount of space used by the tablespace.
#
# Such operations require as much additional space as the data in the table plus indexes.
#
# For a table residing in a shared tablespace, the additional space used during the operation
# is not released back to the operating system as it is for a table that resides in a file-per-table
# tablespace.
#
# For information about space requirements for online DDL operations, see SECTION 15.12.3, "ONLINE DDL SPACE REQUIREMENTS"
#
# ALTER_TABLE operations that support the INPLACE algorithm include:
#
# 		) ALTER TABLE operations supported by the InnoDB online DDL feature. See SECTION 15.12.1, "ONLINE DDL OPERATIONS"
#
# 		) Renaming a table. MySQL renames files that correspond to the table tbl_name without making a copy.
#
# 			(You can also use the RENAME_TABLE statement to rename tables. See SECTION 13.1.36, "RENAME TABLE SYNTAX")
#
# 			Privileges granted specifically for the renamed table are not migrated to the new name.
#
# 			They must be changed manually.
#
# 		) Operations that only modify table metadata. These operations are immediate because the server does
# 			not touch table contents.
#
# 			Metadata-only operations include:
#
# 			) Renaming a column
#
# 			) Changing the default value of a column (except for NDB tables)
#
# 			) Modifying the definition of an ENUM or SET column by adding new enumeration or set members
# 				to the end of the list of valid member values, as long as the storage size of the data
# 				type does not change.
#
# 				For example, adding a member to a SET column that has 8 members changes the required
# 				storage per value from 1 byte to 2 bytes; this requires a table copy.
#
# 				Adding members in the middle of the list causes renumbering of existing members,
# 				which requires a table copy.
#
# 			) Changing the definition of a spatial column to remove the SRID attribute.
#
# 				(Adding or changing an SRID attribute does require a rebuild and cannot be done
# 				in place because the server must verify that all values have the specified SRID value)
#
# 			) As of MySQL 8.0.14, changing a column character set, when these conditions apply:
#
# 				) The column data type is CHAR, VARCHAR, a TEXT type or ENUM
#
# 				) The character set change is from utf8mb3 to utf8mb4, or any character set to binary.
#
# 				) There is no index on the column.
#
# 			) As of MySQL 8.0.14, changing a generated column, when these conditions apply:
#
# 				) For InnoDB tables, statements that modify generated stored columns but do not change
# 					their type, expression, or nullability.
#
# 				) For non-InnoDB tables, statements that modify generated stored or virtual columns but do not
# 					change their type, expression or nullability.
#
# 				An example of such a change is a change to the column comment.
#	
# 			) Renaming an index
#
# 			) Adding or dropping a secondary index, for InnoDB and NDB tables. See SECTION 15.12.1, "ONLINE DDL OPERATIONS"
#
# 			) For NDB tables, operations that add and drop indexes on variable-width columns.
#
# 				These operations occur online, without table copying and without blocking concurrent
# 				DML actions for most of their duration.
#
# 				See SECTION 22.5.14, "ONLINE OPERATIONS WITH ALTER TABLE IN NDB CLUSTER"
#
# 			) Modifying index visibility with an ALTER INDEX operation
#
# 			) Column modifications of tables containing generated columns that depend on columns with a DEFAULT
# 				value if the modified columns are not involved in the generated column expressions.
#
# 				For example, changing the NULL property of a separate column can be done in place without a table rebuild.
#
# 		ALTER TABLE operations that support the INSTANT algorithm include:
#
# 			) Adding a column. This feature is referred to as "INSTANT ADD COLUMN"
#
# 				Limitations apply. See SECTION 15.12.1, "ONLINE DDL OPERATIONS"
#
# 			) Adding or dropping a virtual column
#
# 			) Adding or dropping a column default value
#
# 			) Modifying the definition of an ENUM or SET column. 
#
# 				The same restrictions apply as described above for ALGORITHM=INSTANT
#
# 			) Changing the index type
#
# 			) Renaming a table. The same restrictions apply as described above for ALGORITHM=INSTANT
#
# For more information about operations that support ALGORITHM=INSTANT, see SECTION 15.12.1, "ONLINE DDL OPERATIONS"
#
# ALTER_TABLE upgrades MySQL 5.5 temporal columns to 5.6 format for ADD COLUMN, CHANGE COLUMN, MODIFY COLUMN,
# ADD INDEX and FORCE operations.
#
# This conversion cannot be done using the INPLACE algorithm because the table must be rebuilt,
# so specifying ALGORITHM=INPLACE in these cases results in an error.
#
# Specify ALGORITHM=COPY if necessary.
#
# If an ALTER TABLE operation on a multicolumn index used to partition a table by KEY changes
# the order of the columns, it can only be performed using ALGORITHM=COPY
#
# The WITHOUT VALIDATION and WITH VALIDATION clauses affect whether ALTER_TABLE performs an
# in-place operation for virtual generated column modifications.
#
# See SECTION 13.1.9.2, "ALTER TABLE AND GENERATED COLUMNS"
#
# NDB Cluster 8.0 supports online operations using the same ALGORITHM=INPLACE syntax used with
# the standard MySQL server.
#
# See SECTION 22.5.14, "ONLINE OPERATIONS WITH ALTER TABLE IN NDB CLUSTER", for more information.
#
# ALTER TABLE with DISCARD --- PARTITION --- TABLESPACE or IMPORT --- PARTITION --- TABLESPACE
# does not create any temporary tables or temporary partition files.
#
# ALTER TABLE with ADD PARTITION, DROP PARTITION, COALESCE PARTITION, REBUILD PARTITION,
# or REORGANIZE PARTITION does not create temporary tables (except when used with NDB tables);
#
# However, these operations can and do create temporary partition files.
#
# ADD or DROP operations for RANGE or LIST partitions are immediate operations or nearly so.
#
# ADD or COALESCE operations for HASH or KEY partitions copy data between all partitions;
# unless LINEAR HASH or LINEAR KEY was used; this is effectively the same as creating
# a new table, although the ADD or COALESCE operation is performed partition by partition.
#
# REORGANIZE operations copy only changed partitions and do not touch unchanged ones.
#
# For MyISAM tables, you can speed up index re-creation (the slowest part of the alteration
# process) by setting the myisam_sort_buffer_size system variable to a high value.
#
# CONCURRENCY CONTROL
#
# For ALTER_TABLE operations that support it, you can use the LOCK clause to control
# the level of concurrent reads and writes on a table while it is being altered.
#
# Specifying a non-default value for this clause enables you to require a certain amount
# of concurrent access or exclusivity during the alter operation, and halts the operation
# if the requested degree of locking is not available.
#
# Only LOCK = DEFAULT is permitted for operations that use ALGORITHM=INSTANT
#
# The other LOCK clause parameters are not applicable.
#
# The parameters for the LOCK clause are:
#
# 		) LOCK = DEFAULT
#
# 			Maximum level of concurrency for the given ALGORITHM clause (if any) and ALTER TABLE
# 			operation:
#
# 				Permit concurrent reads and writes if supported.
#
# 			If not, permit concurrent reads if supported.
#
# 			If not, enforce exclusive access.
#
# 		) LOCK = NONE
#
# 			If supported, permit concurrent reads and writes.
#
# 			Otherwise, an error occurs.
#
# 		) LOCK = SHARED
#
# 			If supported, permit concurrent reads but block writes.
#
# 			Writes are blocked even if concurrent writes are supported by the storage engine
# 			for the given ALGORITHM clause (if any) and ALTER TABLE operation.
#
# 			If concurrent reads are not supported, an error occurs.
#
# 		) LOCK = EXCLUSIVE
#
# 			Enforce exclusive access.
#
# 			This is done even if concurrent reads/writes are supported by the storage engine
# 			for the given ALGORITHM clause (if any) and ALTER TABLE operation.
#
# ADDING AND DROPPING COLUMNS
#
# Use ADD to add new columns to a table, and DROP to remove existing columns.
#
# DROP col_name is a MySQL extension to standard SQL.
#
# To add a column at a specific position within a table row, use FIRST or AFTER
# col_name.
#
# The default is to add the column last.
#
# If a table contains only one column, that column cannot be dropped.
#
# If what you intend is to remove the table, use the DROP_TABLE statement instead.
#
# If columns are dropped from a table, the columns are also removed from any index of
# which they are a part.
#
# If all columns that make up an index are dropped, the index is dropped as well.
#
# If you use CHANGE or MODIFY to shorten a column for which an index exists on the
# column, and the resulting column length is less than the index length, MySQL shortens
# the index automatically.
#
# For ALTER TABLE --- ADD, if the column has an expression default value that uses a 
# nondeterministic function, the statement may produce a warning or error.
#
# For details, see SECTION 17.1.3.6, "RESTRICTIONS ON REPLICATION WITH GTIDS"
#
# RENAMING, REDEFINING, AND REORDERING COLUMNS
#
# The CHANGE, MODIFY, RENAME COLUMN and ALTER clauses enable the names and definitions of
# existing columns to be altered.
#
# They have these comparative characteristics:
#
# 		) CHANGE:
#
# 			) Can rename a column and change its definition, or both
#
# 			) Has more capability than MODIFY or RENAME COLUMN, but at the expense of
# 				convenience for some operations.
#
# 				CHANGE requires naming the column twice if not renaming it, and requires
# 				respecifying the column definition if only renaming it.
#
# 			) With FIRST or AFTER, can reorder columns
#
# 		) MODIFY:
#
# 			) Can change a column definition but not its name.
#
# 			) More convenient than CHANGE to change a column definition without renaming it
#
# 			) With FIRST or AFTER, can reorder columns
#
# 		) RENAME COLUMN:
#
# 			) Can change a column name but not its definition
#
# 			) More convenient than CHANGE to rename a column without changing its definition
#
# 		) ALTER: Used only to change a column default value
#
# CHANGE is a MySQL extension to standard SQL. MODIFY and RENAME COLUMN are MySQL
# extensions for Oracle compatibility.
#
# To alter a column to change both its name and definition, use CHANGE, specifying
# the old and new names and the new definition.
#
# For example, to rename an INT NOT NULL column from a to b and change its definition
# to use the BIGINT data type while retaining the NOT NULL attribute, do this:
#
# 		ALTER TABLE t1 CHANGE a b BIGINT NOT NULL;
#
# To change a column definition but not its name, use CHANGE or MODIFY.
#
# With CHANGE, the syntax requires two column names, so you must specify
# the same name twice to leave the name unchanged.
#
# For example, to change the definition of column b, do this:
#
# 		ALTER TABLE t1 CHANGE b b INT NOT NULL;
#
# MODIFY is more convenient to change the definition without changing the name
# because it requires the column name only once:
#
# 		ALTER TABLE t1 MODIFY b INT NOT NULL;
#
# To change a column name but not its definition, use CHANGE or RENAME COLUMN.
#
# With CHANGE, the syntax requires a column definition, so to leave the definition
# unchanged, you must respecify the definition the column currently has.
#
# For example, to rename an INT NOT NULL column from b to a, do this:
#
# 		ALTER TABLE t1 CHANGE b a INT NOT NULL;
#
# RENAME COLUMN is more convenient to change the name without changing the definition
# because it requires only the old and new names:
#
# 		ALTER TABLE t1 RENAME COLUMN b TO a;
#
# In general, you cannot rename a column to a name that already exists in the table.
#
# However, this is sometimes not the case, such as when you swap names or move them
# through a cycle.
#
# If a table has columns named a,b and c, these are valid operations:
#
# 		-- swap a and b
# 		ALTER TABLE t1 RENAME COLUMN a TO b,
# 						   RENAME COLUMN b TO a;
# 		-- "rotate" a, b, c through a cycle
# 		ALTER TABLE t1 RENAME COLUMN a TO b,
# 							RENAME COLUMN b TO c,
# 							RENAME COLUMN c TO a;
#
# For column definition changes using CHANGE or MODIFY, the definition must include
# the data type and all attributes that should apply to the new column, other than
# index attributes such as PRIMARY KEY or UNIQUE.
#
# Attributes present in the original definition but not specified for the new definition
# are not carried forward.
#
# Suppose that a column col1 is defined as INT UNSIGNED DEFAULT 1 COMMENT 'my column'
# and you modify the column as follows, intending to change only INT to BIGINT:
#
# 		ALTER TABLE t1 MODIFY col1 BIGINT;
#
# That statement changes the data type from INT to BIGINT, but it also drops the UNSIGNED,
# DEFAULT and COMMENT attributes.
#
# To retain them, the statement must include them explicitly:
#
# 		ALTER TABLE t1 MODIFY col1 BIGINT UNSIGNED DEFAULT 1 COMMENT 'my column';
#
# For data type changes using CHANGE or MODIFY, MySQL tries to convert existing columns
# values to the new type as well as possible.
#
# WARNING:
#
# 		This conversion may result in alteration of data.
#
# 		For example, if you shorten a string column, values may be truncated.
#
# 		To prevent the operation from succeeding if conversions to the new data type
# 		would result in loss of data, enable strict SQL mode before using ALTER_TABLE
#
# 		(see SECTION 5.1.11, "SERVER SQL MODES")
#
# If you use CHANGE or MODIFY to shorten a column for which an index exists on the
# column, and the resulting column length is less than the index length, MySQL shortens
# the index automatically.
#
# For columns renamed by CHANGE or RENAME COLUMN, MySQL automatically renames these
# references to the renamed column:
#
# 		) Indexes that refer to the old column, including invisible indexes and disabled by MyISAM indexes.
#
# 		) Foreign keys that refer to the old column
#
# For columns renamed by CHANGE or RENAME COLUMN, MySQL does not automatically rename these references
# to the renamed column:
#
# 		) Generated column and partition expressions that refer to the renamed column.
#
# 			You must use CHANGE to redefine such expressions in the same ALTER_TABLE statement
# 			as the one that renames the column.
#
# 		) Views and stored programs that refer to the renamed column. 
#
# 			You must manually alter the definition of these objects to refer to the 
# 			new column name.
#
# To reorder columns within a table, use FIRST and AFTER in CHANGE or MODIFY operations.
#
# ALTER --- SET DEFAULT or ALTER --- DROP DEFAULT specify a new default value for a column
# or remove the old default value, respectively.
#
# If the old default is removed and the column can be NULL, the new default is NULL.
#
# If the column cannot be NULL, MySQL assigns a default value as described in SECTION 11.7,
# "DATA TYPE DEFAULT VALUES"
#
# PRIMARY KEYS AND INDEXES
#
# DROP PRIMARY KEY drops the primary key.
#
# If there is no primary key, an error occurs.
#
# For information about the performance characteristics of primary keys, especially
# for InnoDB tables, see SECTION 8.3.2, "PRIMARY KEY OPTIMIZATION"
#
# If you add a UNIQUE INDEX or PRIMARY KEY to a table, MySQL stores it before any
# nonunique index to permit detection of duplicate keys as early as possible.
#
# DROP_INDEX removes an index. This is a MySQL extension to standard SQL.
#
# See SECTION 13.1.27, "DROP INDEX SYNTAX"
#
# To determine index names, use SHOW INDEX FROM tbl_name
#
# Some storage engines permit you to specify an index type when creating an index.
#
# The syntax for the index_type specifier is USING type_name.
#
# For details about USING, see SECTION 13.1.15, "CREATE INDEX SYNTAX"
#
# The preferred position is after the column list. Support for use of the
# option before the column list will be removed in a future MySQL release.
#
# index_option values specify additional options for an index. USING is one such option.
#
# For details about permissible index_option values, see SECTION 13.1.15, "CREATE INDEX SYNTAX"
#
# RENAME INDEX old_index_name TO new_index_name renames an index.
#
# This is a MySQL extension to standard SQL. The content of the table remains unchanged.
#
# old_index_name must be the name of an existing index in the table that is not dropped
# by the same ALTER_TABLE statement.
#
# new_index_name is the new index name, which cannot duplicate the name of an index
# in the resulting table after changes have been applied.
#
# Neither index name can be PRIMARY.
#
# If you use ALTER_TABLE on a MyISAM table, all nonunique indexes are created in a 
# separate batch (as for REPAIR_TABLE)
#
# This should make ALTER_TABLE much faster when you have many indexes.
#
# For MyISAM tables, key updating can be controlled explicitly. Use ALTER TABLE --- DISABLE KEYS
# to tell MySQL to stop updating nonunique indexes.
#
# Then use ALTER TABLE --- ENABLE KEYS to re-create missing indexes.
#
# MyISAM does this with a special algorithm that is much faster than inserting keys
# one by one, so disabling keys before performing bulk insert operations should give a 
# considerable speedup.
#
# Using ALTER TABLE --- DISABLE KEYS requires the INDEX privilege in addition to the
# privileges mentioned earlier.
#
# While the nonunique indexes are disabled, they are ignored for statements such as
# SELECT and EXPLAIN that otherwise would use them.
#
# After an ALTER_TABLE statement, it may be necessary to run ANALYZE_TABLE to update
# index cardinality information.
#
# See SECTION 13.7.6.22, "SHOW INDEX SYNTAX"
#
# The ALTER INDEX operation permits an index to be made visible or invisible.
#
# An invisible index is not used by the optimizer.
#
# Modification of index visibility applies to indexes other than primary keys
# (either explicit or implicit)
#
# This feature is storage engine neutral (supported for any engine)
#
# FOr more information, see SECTION 8.3.12, "INVISIBLE INDEXES"
#
# FOREIGN KEYS
#
# The FOREIGN KEY and REFERENCES clauses are suppported by the InnoDB and NDB
# storage engines, which implement ADD [CONSTRAINT [symbol]] FOREIGN KEY [index_name] (---) REFERENCES --- (---)
#
# See SECTION 15.6.1.5, "InnoDB AND FOREIGN KEY CONSTRAINTS"
#
# For other storage engines, the clauses are parsed but ignored.
#
# The CHECK clause is parsed but ignored by all storage engines.
#
# See SECTION 13.1.20, "CREATE TABLE SYNTAX"
#
# The reason for accepting but ignoring syntax clauses is for compatibility,
# to make it easier to port code from other SQL servers, and to run applications
# that create tables with references.
#
# See SECTION 1.8.2, "MYSQL DIFFERENCES FROM STANDARD SQL"
#
# For ALTER_TABLE, unlike CREATE_TABLE, ADD FOREIGN KEY ignores index_name if given
# and uses an automatically generated foreign key name.
#
# As a workaround, include the CONSTRAINT clause to specify the foreign key name:
#
# 		ADD CONSTRAINT name FOREIGN KEY (---) ---
#
# IMPORTANT:
#
# 		MySQL silently ignores inline REFERENCES specifications, where the references
# 		are defined as part of the column specification.
#
# 		MySQL accepts only REFERENCES clauses defined as part of a separate FOREIGN KEY specification.
#
# NOTE:
#
# 		Partitioned InnoDB tables do not support foreign keys.
#
# 		This restriction does not apply to NDB tables, including those explicitly partitioned
# 		by [LINEAR] KEY
#
# 		For more information, see SECTION 23.6.2, "PARTITIONING LIMITATIONS RELATING TO STORAGE ENGINES"
#
# MySQL Server and NDB Cluster both support the use of ALTER_TABLE to drop foreign keys:
#
# 		ALTER TABLE tbl_name DROP FOREIGN KEY fk_symbol;
#
# Adding and dropping a foreign key in the same ALTER_TABLE statement is supported for ALTER_TABLE_---_ALGORITHM=INPLACE
# but not for ALTER_TABLE_---_ALGORITHM=COPY
#
# The server prohibits changes to foreign key columns that have the potential to cause loss of referential
# integrity.
#
# It also prohibits changes to the data type of such columns that may be unsafe.
#
# For example, changing VARCHAR(20) to VARCHAR(30) is permitted, but changing it to
# VARCHAR(1024) is not - because that alters hte number of length bytes required to store
# individual values.
#
# A workaround is to use ALTER_TABLE_---_DROP_FOREIGN_KEY before changing the column
# definition and ALTER_TABLE_---_ADD_FOREIGN_KEY afterward.
#
# ALTER TABLE tbl_name RENAME new_tbl_name changes internally generated foreign key
# constraint names and user-defined foreign key constraint names that contain the string
# "tbl_name_ibfk_" to reflect the new table name.
#
# InnoDB interprets foreign key constraint names that contain the string "tbl_name_ibfk_"
# as internally generated names.
#
# CHANGING THE CHARACTER SET
#
# To change the table default character set and all character columns (CHAR, VARCHAR, TEXT)
# to a new character set, use a statement like this:
#
# 		ALTER TABLE tbl_name CONVERT TO CHARACTER SET charset_name;
#
# The statement also changes the collation of all character columns.
#
# If you specify no COLLATE clause to indicate which collation to use, the statement
# uses default collation for the character set.
#
# If this collation is inappropriate for the intended table use (for example, if it would
# change from a case-sensitive collation to a case-insensitive collation), specify
# a collation explicitly.
#
# For a column that has a data type of VARCHAR or one of the TEXT types, CONVERT TO CHARACTER SET
# changes the data type as necessary to ensure that the new column is long
# enough to store as many chars as the original column.
#
# For example, a TEXT column has two length bytes, which stores the byte-length of values in the
# column, up to a maximum of 65,535
#
# For a latin1 TEXT column, each character requires a single byte, so the column can store up to
# 65,535 characters
#
# If the column is converted to utf8, each char might require up to three bytes, for a maximum
# possible length of 3 x 65,535 = 196,605 bytes.
#
# That length does not fit in a TEXT column's length bytes, so MySQL converts
# the data type to MEDIUMTEXT, which is the smallest string type for which the length
# bytes can record a value of 196,605.
#
# Similarly, a VARCHAR column might be converted to MEDIUMTEXT
#
# To avoid data type changes of the type just described, do not use CONVERT TO CHARACTER SET
# 
# Instead, use MODIFY to change individual columns. for example:
#
# 		ALTER TABLE t MODIFY latin1_text_col TEXT CHARACTER SET utf8;
# 		ALTER TABLE t MODIFY latin1_varchar_col VARCHAR(M) CHARACTER SET utf8;
#
# If you specify CONVERT TO CHARACTER SET binary, the CHAR, VARCHAR and TEXT columns are converted
# to their corresponding binary string types (BINARY, VARBINARY, BLOB)
#
# This means that the columns no longer will have a character set and a subsequent CONVERT TO
# operation will not apply to them.
#
# If charset_name is DEFAULT in a CONVERT TO CHARACTER SET operation, the character set named
# by the character_set_database system variable is used.
#
# WARNING:
#
# 		The CONVERT TO operation converts column values between the original and named character sets.
#
# 		This is not what  you want, if you have a column in one character set (like latin1) but the
# 		stored values actually use some other, incompatible character set (like utf8).
#
# 		In this case, you have to do the following for each such column:
#
# 			ALTER TABLE t1 CHANGE c1 c1 BLOB;
# 			ALTER TABLE t1 CHANGE c1 c1 TEXT CHARACTER SET utf8;
#
# 		The reason this works, is that there is no conversion when you convert to or from BLOB columns.
#
# To change only the default character set for a table, use this statement:
#
# 		ALTER TABLE tbl_name DEFAULT CHARACTER SET charset_name;
#
# THe word DEFAULT is optional. The default character set is the character set that is
# used if you do not specify the char set for columns that you add to a table later 
# (for example, with ALTER TABLE --- ADD column)
#
# When the foreign_key_checks system variable is enabled, which is the default string, character set
# conversion is not permitted on tables that include a character string column used in a
# foreign key constraint.
#
# The workaround is to disable foreign_key_checks before performing the character set conversion.
#
# You must perform the conversion on both tables involved in the foreign key constraint
# before re-enabling foreign_key_checks
#
# If you re-enable foreign_key_checks after converting only one of the tables, an ON DELETE
# CASCADE or ON UPDATE CASCADE operation could corrupt data in the referencing table due to
# implicit conversion that occurs during these operations.
#
# (Bug #45290, Bug #74816)
#
# DISCARDING AND IMPORTING INNODB TABLESPACES
#
# An InnoDB table created in its own file-per-table tablespace can be discarded and imported
# using the DISCARD TABLESPACE and IMPORT TABLESPACE options
#
# These options can be used to import a file-per-table tablespace from a backup or to copy
# a file-per-table tablespace from one database server to another.
#
# See SECTION 15.6.3.7, "COPYING TABLESPACES TO ANOTHER INSTANCE"
#
# ROW ORDER FOR MYISAM TABLES
#
# ORDER BY enables you to create the new table with the rows in a specific order.
#
# This option is useful primarily when you know that you query the rows in a certain
# order most of the time.
#
# By using this option after major changes to the table, you might be able to get higher
# performance.
#
# In some cases, it might make sorting easier for MySQL if the table is in order by the
# column that you want to order it by later.
#
# NOTE:
#
# 		The table does not remain in the specified order after inserts and deletes
#
# ORDER BY syntax permits one or more column names to be specified for sorting, each of which
# optionally can be followed by ASC or DESC to indicate ascending or descending sort order,
# respectively.
#
# The default is ascending order. 
#
# Only column names are permitted as sort criteria; 
# arbitrary expressions are not permitted.
#
# This clause should be given last after any other clauses.
#
# ORDER BY does not make sense for InnoDB tables because InnoDB always
# orders table rows according to the clustered index.
#
# When used on a partitioned table, ALTER TABLE --- ORDER BY orders
# rows within each partition only.
#
# PARTITIONING OPTIONS
#
# partition_options signifies options that can be used with partitioned tables 
# for reparatitioning, to add, drop, discard, import, merge, and split partitions,
# and to perform partitioning maintenance.
#
# It is possible for an ALTER_TABLE statement to contain a PARTITION BY or 
# REMOVE PARTITIONING clause in an addition to other alter specifications,
# but the PARTITION BY or REMOVE PARTITIONING clause must be specified last
# after any other specifications.
#
# The ADD PARTITION, DROP PARTITION, DISCARD PARTITION, IMPORT PARTITION,
# COALESCE PARTITION, REORGANIZE PARTITION, EXCHANGE PARTITION, ANALYZE PARTITION,
# CHECK PARTITION and REPAIR PARTITION options cannot be combined with other
# alter specifications in a single ALTER TABLE, since the options just listed
# act on individual partitions.
#
# For more information about partition options, see SECTION 13.1.20, "CREATE TABLE SYNTAX"
# and SECTION 13.1.9.1, "ALTER TABLE PARTITION OPERATIONS"
#
# For information about and examples of ALTER TABLE --- EXCHANGE PARTITION statements,
# see SECTION 23.3.3, "EXCHANGING PARTITIONS AND SUBPARTITIONS WITH TABLES"
#
# 13.1.9.1 ALTER TABLE PARTITION OPERATIONS
#
# Partitioning-related clauses for ALTER_TABLE can be used with partitioned tables for
# reparationing, to add, drop, discard, import, merge, and split partitions,
# and to perform partitioning maintenance.
#
# 		) Simply using a partition_options clause with ALTER_TABLE on a partitioned table
# 			repartitions the table according to the partitioning scheme defined by the
# 			partition_options.
#
# 			This clause always begins with PARTITION BY, and follows the same syntax and other
# 			rules as apply to the partition_options clause for CREATE_TABLE (for more detailed information,
# 			see SECTION 13.1.20, "CREATE TABLE SYNTAX"), and can also be used to partition
# 			an existing table that is not already partitioned.
#
# 			For example, consider a (nonpartitioned) table defined as shown here:
#
# 				CREATE TABLE t1 (
# 					id INT,
# 					year_col INT
# 				);
#
# 			This table can be partitioned by HASH, using the id column as the partitioning key,
# 			into 8 partitions by means of this statement:
#
# 				ALTER TABLE t1
# 					PARTITION BY HASH(id)
# 					PARTITIONS 8;
#
# 			MySQL supports an ALGORITHM option with [SUB]PARTITION BY [LINEAR] KEY.ALGORITHM=1 causes
# 			the server to use the same key-hashing functions as MySQL 5.1 when comptuing
# 			the placement of rows in partitions;
#
# 			ALGORITHM=2 means that the server employs the key-hashing functions implemented
# 			and used by default for new KEY partitioned tables in MySQL 5.5 and later.
#
# 			(Partitioned tables created with the key-hashing functions employed in MySQL 5.5
# 			and later cannot be used by a MySQL 5.1 server)
#
# 			Not specifying the option has the same effect as using ALGORITHM=2
#
# 			This option is intended for use chiefly when upgrading or downgrading [LINEAR]
# 			KEY partitioned tables between 5.1 and later MySQL versions, or for creating
# 			tables partitioned by KEY or LINEAR KEY on a MySQL 5.5 or later server which
# 			can be used on a MySQL 5.1 server
#
# 			The table that results from using an ALTER TABLE --- PARTITION BY statement must
# 			follow the same rules as one created using CREATE TABLE --- PARTITION BY
#
# 			This includes the rules governing the relationship between any unique keys
# 			(including any primary key) that the table might have, and the column
# 			or columns used in the partitioning expression, as discussed in SECTION 23.6.1,
# 			"PARTITIONING KEYS, PRIMARY KEYS, AND UNIQUE KEYS"
#
# 			The CREATE TABLE --- PARTITION BY rules for specifying the number of partitions
# 			also apply to ALTER TABLE --- PARTITION BY
#
# 			The partition_definition clause for ALTER TABLE ADD PARTITION supports
# 			the same options as the clause of the same name for the CREATE_TABLE
# 			statement.
#
# 			(See SECTION 13.1.20, "CREATE TABLE SYNTAX", for the syntax and descriptions)
#
# 			Suppose that you have the partitioned table created as shown here:
#
# 				CREATE TABLE t1 (
# 					id INT,
# 					year_col INT
# 				)
# 				PARTITION BY RANGE (year_col) (
# 					PARTITION p0 VALUES LESS THAN (1991),
# 					PARTITION p1 VALUES LESS THAN (1995),
# 					PARTITION p2 VALUES LESS THAN (1999)
# 				);
#
# 			You can add a new partition p3 to this table for storing values
# 			less than 2002 as follows:
#
# 				ALTER TABLE t1 ADD PARTITION (PARTITION p3 VALUES LESS THAN (2002));
#
# 			DROP PARTITION can be used to drop one or more RANGE or LIST partitions.
#
# 			This statement cannot be used with HASH or KEY partitions; instead, use
# 			COALESCE PARTITION (see later in this section)
#
# 			Any data that was stored in the dropped partitions named in the
# 			partition_names list is discarded.
#
# 			For example, given the table t1 defined previously, you can drop
# 			the partitions named p0 and p1 as shown here:
#
# 				ALTER TABLE t1 DROP PARTITION p0, p1;
#
# 			NOTE:
#
#   			DROP PARTITION does not work with tables that use the NDB storage engine.
#
# 				See SECTION 23.3.1, "MANAGEMENT OF RANGE AND LIST PARTITIONS", and
# 				SECTION 22.1.7, "KNOWN LIMITATIONS OF NDB CLUSTER"
#
# 			ADD PARTITION and DROP PARTITION do not currently support IF [NOT] EXISTS
#
# 			The DISCARD_PARTITION_---_TABLESPACE and IMPORT_PARTITION_---_TABLESPACE options
# 			extend the Transportable Tablespace feature to individual InnoDB table partitions.
#
# 			Each InnoDB table partition has its own tablespace file (.idb file)
#
# 			The Transportable Tablespace feature makes it easy to copy the tablespaces
# 			from a running MySQL server instance to another running instance, or to
# 			perform a restore on the same instance.
#
# 			Both options take a comma-separated list of one or more partition names.
#
# 			For example:
#
# 				ALTER TABLE t1 DISCARD PARTITION p2, p3 TABLESPACE;
#
# 				ALTER TABLE t1 IMPORT PARTITION p2, p3 TABLESPACE;
#
# 			When running DISCARD_PARTITION_---_TABLESPACE and IMPORT_PARTITION_---_TABLESPACE
# 			on subpartitioned tables, both partition and subpartition names are
# 			allowed.
#
# 			When a partition name is specified, subpartitions of that partition are included.
#
# 			The Transportable Tablespace feature also supports copying or restoring partitioned
# 			InnoDB tables (all partitions at once)
#
# 			For additional information, see SECTION 15.6.3.7, "COPYING TABLESPACE TO ANOTHER INSTANCE",
# 			as well as, SECTION 15.6.3.7.1, "TRANSPORTABLE TABLESPACE EXAMPLES"
#
# 			Renames of partitioned tables are supported.
#
# 			You can rename individual partitions indirectly using ALTER TABLE --- REORGANIZE PARTITION;
# 			however, this operation copies the partition's data
#
# 			To delete rows from selected partitions, use the TRUNCATE PARTITION option.
#
# 			This option takes a list of one or more comma-separated partition names.
#
# 			Consider the table t1 created by this statement:
#
# 				CREATE TABLE t1 (
# 					id INT,
# 					year_col INT
# 				)
# 				PARTITION BY RANGE (year_col) (
# 					PARTITION p0 VALUES LESS THAN (1991),
# 					PARTITION p1 VALUES LESS THAN (1995),
# 					PARTITION p2 VALUES LESS THAN (1999),
# 					PARTITION p3 VALUES LESS THAN (2003),
# 					PARTITION p4 VALUES LESS THAN (2007)
# 				);
#
# 			To delete all rows from partition p0, use the following statement:
#
# 				ALTER TABLE t1 TRUNCATE PARTITION p0;
#
# 			The statement just shown has the same effect as the following DELETE statement:
#
# 				DELETE FROM t1 WHERE year_col < 1991;
#
# 			When truncating multiple partitions, the partitions do not have to be contiguous:
#
# 				This can greatly simplify delete operations on partitioned tables that would
# 				otherwise require very complex WHERE Conditions if done with DELETE statements.
#
# 				For example, this statement deletes all rows from partitions p1 and p3:
#
# 					ALTER TABLE t1 TRUNCATE PARTITION p1, p3;
#
# 				An equivalent DELETE statement is shown here:
#
# 					DELETE FROM t1 WHERE
# 						(year_col >= 1991 AND year_col < 1995)
# 						OR
# 						(year_col >= 2003 AND year_col < 2007);
#
# 				If you use the ALL keyword in place of the list of partition names,
# 				the statement acts on all table partitions.
#
# 				TRUNCATE PARTITION merely deletes rows; it does not alter the definition
# 				of the table itself, or of any of its partitions.
#
# 				To verify that the rows were dropped, check the INFORMATION_SCHEMA.PARTITIONS
# 				table, using a query such as this one:
#
# 					SELECT PARTITION_NAME, TABLE_ROWS
# 						FROM INFORMATION_SCHEMA.PARTITIONS
# 						WHERE TABLE_NAME = 't1';
#
# 				COALESCE PARTITION can be used with a table that is partitioned by HASH or KEY
# 				to reduce the number of partitions by number.
#
# 				Suppose that you have created table t2 as follows:
#
# 					CREATE TABLE t2 (
# 						name VARCHAR(30),
# 						started DATE
# 					)
# 					PARTITION BY HASH( YEAR(started) )
# 					PARTITIONS 6;
#
# 				To reduce the number of partitions used by t2 from 6 to 4, use hte following statement:
#
# 					ALTER TABLE t2 COALESCE PARTITION 2;
#
# 				The data contained in the last number partitions will be merged into the remaining partitions.
#
# 				In this case, partitions 4 and 5 will be merged into the first 4 partitions (0, 1, 2, 3)
#
# 				To change some but not all the partitions used by a partitioned table, you can use
# 				REORGANIZE PARTITIOn.
#
# 				This statement can be used in severel ways:
#
# 					) TO merge a set of partitions into a single partition.
#
# 						This is done by naming several partitions in the partition_names list
# 						and supplying a single definition for partition_definition
#
# 					) To split an existing partition into several partitions.
#
# 						Accomplish this by naming a single partition for partition_names
# 						and providing multiple partition_definitions.
#
# 					) To change the ranges for a subset of partitions defined using VALUES LESS THAN
# 						or the values lists for a subset of partitions defined using VALUES IN.
#
# 						NOTE:
#
# 							For partitions that have not been explicitly named, MySQL automatically provides
# 							the default names p0, p1, p2 and so on.
#
# 							The same is true in regards to subpartitions
#
# 						For more detailed information about and examples of ALTER TABLE --- REORGANIZE PARTITION statements,
# 						see SECTION 23.3.1, "MANAGEMENT OF RANGE AND LIST PARTITIONS"
#
# 					) To exchange a table partition or subpartition with a table, use the ALTER_TABLE_---_EXCHANGE_PARTITION
# 						statement - that is, to move any existing rows in the partition or subpartition to the nonpartitioned
# 						table, and any existing rows in the nonpartitioned table to the table partition or subpartition.
#
# 						For usage information and examples, see SECTION 23.3.3, "EXCHANGING PARTITIONS AND SUBPARTITIONS WITH TABLES"
#
# 					) Several options provide partition maintenance and repair functionality analogous to that implemented for
# 						nonpartitioned tables by statements such as CHECK_TABLE and REPAIR_TABLE
#
# 						(which are also supported for partitioned tables; for more information, see
# 						see SECTION 13.7.3, "TABLE MAINTENANCE STATEMENTS")
#
# 						These include ANALYZE PARTITION, CHECK PARTITION, OPTIMIZE PARTITION, REBUILD PARTITION
# 						and REPAIR PARTITION.
#
# 						Each of these options takes a partition_names clause consisting of one or more
# 						names of partitions, separated by commas.
#
# 						The partitions must already exist in the target table.
#
# 						You can also use the ALL keyword in place of partition_names, in which case
# 						the statement acts on all table partitions.
#
# 						For more information and examples, see SECTION 23.3.4, "MAINTENANCE OF PARTITIONS"
#
# 						InnoDB does not currently support per-partition optimization; ALTER TABLE --- OPTIMIZE PARTITION
# 						causes the entire table to rebuilt and analyzed, and an appropriate warning to be issued.
#
# 						(BUG #11751825, BUG #42822)
#
# 						To work around this problem, use ALTER TABLE --- REBUILD PARTITION and ALTER TABLE --- ANALYZE PARTITION instead
#
# 						The ANALYZE PARTITION, CHECK PARTITION, OPTIMIZE PARTITION and REPAIR PARTITION options are not
# 						supported for tables which are not partitioned.
#
# 					) REMOVE PARTITIONING enables you to remove a table's partitioning without otherwise affecting the table or its data.
#
# 						This option can be combined with other ALTER_TABLE options such as those used to add, drop or rename columns or indexes.
#
# 					) Using the ENGINE option with ALTER_TABLE changes the storage engine used by the table without affecting
# 						the partitioning.
#
# 						The target storage engine must provide its own partitioning handler.
#
# 						Only the InnoDB and NDB storage engines have native partitioning handlers;
#
# 						NDB is currently not currently supported in MySQL 8.0
#
# It is possible for an ALTER_TABLE statement to contain a PARTITION BY or REMOVE PARTITIONING
# clause in an addition to other alter specifications, but the PARTITION BY or REMOVE PARTITIONING
# clause must be specified last after any other specifications.
#
# The ADD PARTITION, DROP PARTITION, COALESCE PARTITION, REORGANIZE PARTITION, ANALYZE PARTITION,
# CHECK PARTITION and REPAIR PARTITION options cannot be combined with other alter specifications
# in a single ALTER TABLE, since the options just listed act on individual partitions.
#
# For more information, see SECTION 13.1.9.1, "ALTER TABLE PARTITION OPERATIONS"
#
# Only a single instance of any one of the following options can be used in a given ALTER_TABLE
# statement:
#
# 		PARTITION BY, ADD PARTITION, DROP PARTITION, TRUNCATE PARTITION, EXCHANGE PARTITION,
# 		REORGANIZE PARTITION, or 
#
# 		COALESCE PARTITION, ANALYZE PARTITION, CHECK PARTITION, OPTIMIZE PARTITION,
# 		REBUILD PARTITION, REMOVE PARTITIONING
#
# 		For example, the following two statements are invalid:
#
# 			ALTER TABLE t1 ANALYZE PARTITION p1, ANALYZE PARTITION p2;
#
# 			ALTER TABLE t1 ANALYZE PARTITION p1, CHECK PARTITION p2;
#
# 		In the first case, you can analyze partitions p1 and p2 of table t1 concurrently
# 		using a single statement with a single ANALYZE PARTITION option that lists both
# 		of the partitions to be analyzed, like this:
#
# 			ALTER TABLE t1 ANALYZE PARTITION p1, p2;
#
# 		In the second case, it is not possible to perform ANALYZE and CHECK operations
# 		on different partitions of the same table concurrently.
#
# 		Instead, you must issue two seperate statements, like this:
#
# 			ALTER TABLE t1 ANALYZE PARTITION p1;
# 			ALTER TABLE t1 CHECK PARTITION p2;
#
# 		REBUILD operations are currently unsupported for subpartitions.
#
# 		The REBUILD keyword is expressly disallowed with subpartitions, and causes
# 		ALTER TABLE to fail with an error if so used.
#
# 		CHECK PARTITION and REPAIR PARTITION operations fail when the partition to be checked or repaired
# 		contains any duplicate key errors.
#
# 		For more information, see SECTION 23.3.4, "MAINTENANCE OF PARTITIONS"
#
# 13.1.9.2 ALTER TABLE AND GENERATED COLUMNS
#
# ALTER TABLE operations permitted for generated columns are ADD, MODIFY and CHANGE.
#
# 		) Generated columns can be added.
#
# 			CREATE TABLE t1 (c1 INT);
# 			ALTER TABLE t1 ADD COLUMN c2 INT GENERATED ALWAYS AS (c1 + 1) STORED;
#
# 		) The data type and expression of generated columns can be modified.
#
# 			CREATE TABLE t1 (c1 INT, c2 INT GENERATED ALWAYSA AS (c1 + 1) STORED);
# 			ALTER TABLE t1 MODIFY COLUMN c2 TINYINT GENERATED ALWAYS AS (c1 + 5) STORED;
#
# 		) Generated columns can be renamed or dropped, if no other column refers to them.
#
# 			CREATE TABLE t1 (c1 INT, c2 INT GENERATED ALWAYS AS (c1 + 1) STORED);
# 			ALTER TABLE t1 CHANGE c2 c3 INT GENERATED ALWAYS AS (c1 + 1) STORED;
# 			ALTER TABLE t1 DROP COLUMN c3;
#
# 		) Virtual generated columns cannot be altered to store generated columns, or vice versa.
#
# 			To work around this, drop the column, then add it with the new definition.
#
# 				CREATE TABLE t1 (c1 INT, c2 INT GENERATED ALWAYS AS (c1 + 1) VIRTUAL);
# 				ALTER TABLE t1 DROP COLUMN c2;
# 				ALTER TABLE t1 ADD COLUMN c2 INT GENERATED ALWAYS AS (c1 + 1) STORED;
#
# 		) Nongenerated columns can be altered to store but not virtual generated columns:
#
# 			CREATE TABLE t1 (c1 INT, c2 INT);
# 			ALTER TABLE t1 MODIFY COLUMN c2 INT GENERATED ALWAYS AS (c1 + 1) STORED;
#
# 		) Stored but not virtual generated columns can be altered to nongenerated columns.
#
# 			The stored generated values become the values of the nongenerated column.
#
# 				CREATE TABLE t1 (c1 INT, c2 INT GENERATED ALWAYS AS (c1 + 1) STORED);
# 				ALTER TABLE t1 MODIFY COLUMN c2 INT;
#
# 		) ADD COLUMN is not an in-place operation for stored columns (done without using a temp table)
# 			because the expression must be evaluated by the server.
#
# 			For stored columns, indexing changes are done in place, and expression changes
# 			are not done in place.
#
# 			Changes to column comments are done in place.
#
# 		) For non-partitioned tables, ADD COLUMN and DROP COLUMN are in-place operations
# 			for virtual columns.
#
# 			However, adding or dropping a virtual column cannot be performed in place in combination
# 			with other ALTER_TABLE operations.
#
# 			For partitioned tables, ADD COLUMN and DROP COLUMN are not in-place operations
# 			for virtual columns.
#
# 		) InnoDB supports secondary indexes on virtual generated columns.
#
# 			Adding or dropping a secondary index on a virtual generated column
# 			is an in-place operation.
#
# 			For more information, see SECTION 13.1.20.9, "SECONDARY INDEXES AND GENERATED COLUMNS"
#
# 		) When a VIRTUAL generated column is added to a table or modified, it is not ensured that
# 			data being calculated by the generated column expression will not be out of range for the column.
#
# 			This can lead to inconsistent data being returned and unexpectedly failed statements.
#
# 			To permit control over whether validation occurs for such columns, ALTER TABLE
# 			supports WITHOUT VALIDATION and WITH VALIDATION clauses:
#
# 				) With WITHOUT VALIDATION (the default if neither clause is specified), an in-place operation
# 					is performed (if possible), data integrity is not checked, and the statement finishes more quickly.
#
# 					However, later reads from the table might report warnings or errors for the column if values are out of range.
#
# 				) With WITH VALIDATION, ALTER TABLE copies the table.
#
# 					If an out-of-range or any other error occurs, the statement fails.
#
# 					Because a table copy is performed, the statement takes longer.
#
# 			WITHOUT VALIDATION and WITH VALIDATION are permitted only with ADD COLUMN,
# 			CHANGE COLUMN, and MODIFY COLUMN operations.
#
# 			Otherwise, an ER_WRONG_USAGE error occurs.
#
# 		) If expression evaluation causes truncation or provides incorrect input to a function,
# 			the ALTER_TABLE statement terminates with an error and the DDL operation is rejected.
#
# 		) An ALTER_TABLE statement that changes the default value of a column col_name may also
# 			change the value of a generated column expression that refers to the column using
# 			col_name, which may change the value of a generated column expression that refers
# 			to the column using DEFAULT(col name)
#
# 			For htis reason, ALTER_TABLE operations that change the definition of a column
# 			cause a table rebuild if any generated column expression uses DEFAULT()
#
# 13.1.9.3 ALTER TABLE EXAMPLES
#
# Begin with a table t1 created as shown here:
#
# 		CREATE TABLE t1 (a INTEGER, b CHAR(10));
#
# To rename teh table from t1 to t2:
#
# 		ALTER TABLE t1 RENAME t2;
#
# To change column a from INTEGER to TINYINT NOT NULL (leaving the name the same),
# and to change column b from CHAR(10) to CHAR(20) as well as renaming it from b
# to c:
#
# 		ALTER TABLE t2 MODIFY a TINYINT NOT NULL, CHANGE b c CHAR(20);
#
# To add a new TIMESTAMP column named d:
#
# 		ALTER  TABLE t2 ADD d TIMESTAMP;
#
# To add an index on column d and a UNIQUE index on column a:
#
# 		ALTER TABLE t2 ADD INDEX (d), ADD UNIQUE (a);
#
# To remove column c:
#
# 		ALTER TABLE t2 DROP COLUMN c;
#
# To add a new AUTO_INCREMENT integer column named c:
#
# 		ALTER TABLE t2 ADD c INT UNSIGNED NOT NULL AUTO_INCREMENT,
# 			ADD PRIMARY KEY (c);
#
# We indexed (c) as a PRIMARY KEY, because AUTO_INCREMENT columns must be
# indexed, and we declare c as NOT NULL because primary key columns cannot be NULL.
#
# For NDB tables, it is also possible to change the storage type used for a 
# table or column.
#
# For example, consider an NDB table created as shown here:
#
# 		CREATE TABLE t1 (c1 INT) TABLESPACE ts_1 ENGINE NDB;
# 		Query OK, 0 rows affected (1.27 sec)
#
# To convert this table to disk-based storage, you can use the following
# ALTER_TABLE statement:
#
# 		ALTER TABLE t1 TABLESPACE ts_1 STORAGE DISK;
# 		Query OK, 0 rows affected (2.99 sec)
# 		Records: 0 Duplicates: 0 Warnings: 0
#
# 		SHOW CREATE TABLE t1\G
# 		******************************* 1. row *********************************
# 						Table: t1
# 			  Create Table: CREATE TABLE `t1` (
# 					`c1` int(11) DEFAULT NULL
# 			  ) /*!50100 TABLESPACE ts_1 STORAGE DISK */
# 			  ENGINE=ndbcluster DEFAULT CHARSET=latin1
# 			  1 row in set (0.01 sec)
#
# It is not necessary that the tablespace was referenced when the table was originally
# created;
#
# However, the tablespace must be referenced by the ALTER_TABLE:
#
# 		CREATE TABLE t2 (c1 INT) ts_1 ENGINE NDB;
# 		Query OK, 0 rows affected (1.00 sec)
#
# 		ALTER TABLE t2 STORAGE DISK;
# 		ERROR 1005 (HY000): Can't create table 'c.#sql-1750_3' (errno: 140)
# 		ALTER TABLE t2 TABLESPACE ts_1 STORAGE DISK;
# 		Query OK, 0 rows affected (3.42 sec)
# 		Records: 0 Duplicates: 0 Warnings: 0
#
#  	SHOW CREATE TABLE t2\G
# 		*********************** 1. row *******************************
#  				Table: t1
# 			Create Table: CREATE TABLE `t2` (
# 				`c1` int(11) DEFAULT NULL
# 			) /*!50100 TABLESPACE ts_1 STORAGE DISK */
# 			ENGINE=ndbcluster DEFAULT CHARSET=latin1
# 			1 row in set (0.01 sec)
#
# To change the storage type of an individual column, you can use ALTER TABLE --- MODIFY [COLUMN]
#
# For example, suppose you create an NDB Cluster Disk Data table with two columns,
# using this CREATE_TABLE statement:
#
# 		CREATE TABLE t3 (c1 INT, c2 INT)
# 			TABLESPACE ts_1 STORAGE DISK ENGINE NDB;
# 		Query OK, 0 rows affected (1.34 sec)
#
# To change column c2 from disk-based to in-memory storage, include a STORAGE MEMORY
# clause in the column definition used by the ALTER TABLE statement, as shown
# here:
#
# 		ALTER TABLE t3 MODIFY c2 INT STORAGE MEMORY;
# 		Query OK, 0 rows affected (3.14 sec)
# 		Records: 0 Duplicates: 0 Warnings: 0
#
# You can make in-memory columns into a disk-based column by using STORAGE DISK
# in a similar fashion.
#
# Column c1 uses disk-based storage, since this is hte default for the table
# (determined by the table-level STORAGE DISK clause in the CREATE_TABLE statement)
#
# However, column c2 uses in-memory storage, as can be seen here in the output
# of SHOW CREATE_TABLE:
#
# 		SHOW CREATE TABLE t3\G
# 		******************** 1. row ************************************
# 				Table: t3
# 		Create Table: CREATE TABLE `t3` (
# 			`c1` int(11) DEFAULT NULL,
# 			`c2` int(11) /*!50120 STORAGE MEMORY */ DEFAULT NULL
# 		) /*!50100 TABLESPACE ts_1 STORAGE DISK */ ENGINE=ndbcluster
# 		DEFAULT CHARSET=latin1
# 		1 row in set (0.02 sec)
#
# When you add an AUTO_INCREMENT column, column values are filled in with sequence
# numbers automatically.
#
# For MyISAM tables, you can set the first sequence number by executing SET INSERT_ID=value
# before ALTER_TABLE or by using the AUTO_INCREMENT=value table option.
#
# With MyISAM tables, if you do not change the AUTO_INCREMENT column, the sequence number
# is not affected.
#
# If you drop an AUTO_INCREMENT column and then add another AUTO_INCREMENT column
# , the numbers are resequenced beginning with 1
#
# When replication is used, adding an AUTO_INCREMENT column to a table might not produce
# the same ordering of the rows on the slave and the master.
#
# This occurs because the orders in which the rows are numbered depends on the specific
# storage engine used for the table and the order in which the rows were inserted.
#
# It is important to have the same order on the master and slave, the rows must be
# ordered before assigning an AUTO_INCREMENT number
#
# Assuming that you want to add an AUTO_INCREMENT column to the table t1, the following
# statements produce a new table t2 identical to t1 but with an AUTO_INCREMENT column:
#
# 		CREATE TABLE t2 (id INT AUTO_INCREMENT PRIMARY KEY)
# 		SELECT * FROM t1 ORDER BY col1, col2;
#
# This assumes that the table t1 has columns col1 and col2
#
# This set of statements will also produce a new table t2 identical to t1,
# with the addition of an AUTO_INCREMENT column:
#
# 		CREATE TABLE t2 LIKE t1;
# 		ALTER TABLE t2 ADD id INT AUTO_INCREMENT PRIMARY KEY;
# 		INSERT INTO t2 SELECT * FROM t1 ORDER BY col1, col2;
#
# IMPORTANT:
#
# 		To guarantee the same ordering on both master and slave, all columns
# 		of t1 must be referenced in the ORDER BY clause
#
# Regardless of the method used to create and populate the copy having the
# AUTO_INCREMENT column , the final step is to drop the original table
# and then rename the copy:
#
# 		DROP TABLE t1;
# 		ALTER TABLE t2 RENAME t1;
#
# 13.1.10 ALTER TABLESPACE SYNTAX
#
# ALTER [UNDO] TABLESPACE tablespace_name
# 		NDB only:
# 			{ADD|DROP} DATAFILE 'file_name'
# 			[INTIIAL_SIZE [=] size]
# 			[WAIT]
# 		InnoDB and NDB:
# 			[RENAME TO tablespace_name]
# 		InnoDB only:
# 			[SET {ACTIVE|INACTIVE}]
# 			[ENCRYPTION [=] {'Y' | 'N'}]
# 		InnoDB and NDB:
# 			[ENGINE [=] engine_name]
#
# This statement is used with NDB and InnoDB tablespaces.
#
# It can be used to add a new data file to, or to drop a data file from
# an NDB tablespace.
#
# It can also be used to rename an NDB Cluster Disk Data tablespace,
# rename an InnoDB general tablespace, encrypt an InnoDB general tablespace,
# or mark an InnoDB undo tablespace as active or inactive.
#
# The UNDO keyword, introduced in MySQL 8.0.14, is used with the SET {ACTIVE|INACTIVE}
# clause to mark an InnoDB undo tablespace as active or inactive.
#
# For more information, See SECTION 15.6.3.4, "UNDO TABLESPACES"
#
# The ADD DATAFILE variant enables you to specify an initial size for an NDB Disk Data
# tablespace using an INITIAL_SIZE clause, where size is measured in bytes; the default
# value is 134217728 (128 mb)
#
# You may optionally follow size with a one-letter abbreviation for an order of
# magnitude, similar to those used in my.cnf
#
# Generally, this is one of the letters M (megabytes) or G (gigabytes)
#
# On 32-bit systems, the maximum supported value for INTIIAL_SIZE is 4 GB. (Bug #29186)
#
# INITIAL_SIZE is rounded, explicitly, as for CREATE_TABLESPACE
#
# Once a data file has been created, its size cannot be changed; however, you can
# add more data files to an NDB tablespace using additional ALTER TABLESPACE --- ADD DATAFILE
# statements.
#
# When ALTER TABLESPACE --- ADD DATAFILE is used with ENGINE = NDB, a data file is created
# on each Cluster data node, but only one row is generated in the INFORMATION_SCHEMA.FILES
# table.
#
# See the description of this table, as well as SECTION 22.5.13.1, "NDB CLUSTER DISK DATA OBJECTS",
# for more information.
#
# ADD DATAFILE is not supported with InnoDB tablespaces.
#
# Using DROP DATAFILE with ALTER_TABLESPACE drops the data file 'file_name' from an NDB tablespace.
#
# You cannot drop a data file from a tablespace which is in use by any table; in other words,
# the data file must be empty (no extents used)
#
# See SECTION 22.5.13.1, "NDB CLUSTER DISK DATA OBJECTS"
#
# In addition, any data file to be dropped must previously have been added to the tablespace
# with CREATE_TABLESPACE or ALTER_TABLESPACE
#
# DROP DATAFILE is not supported with InnoDB tablespaces.
#
# WAIT is parsed but otherwise ignored. It is intended for future expansion.
#
# The ENGINE clause, which specifies the storage engine used by the tablespace, is deprecated
# and will be removed.
#
# The tablespace storage engine is known by the data dictionary, making the ENGINE
# clause obsolete.
#
# If the storage engine is specified, it must match the tablespace storage engine
# defined in the data dictionary.
#
# The only values for engine_name compatible with NDB tablespaces are NDB and NDBCLUSTER.
#
# RENAME TO operations are implicitly performed in autocommit mode, regardless of the autocommit setting.
#
# A RENAME TO operation cannot be performed while LOCK_TABLES or FLUSH_TABLES_WITH_READ_LOCK is in effect
# for tables that reside in the tablespace.
#
# Exclusive metadata locks are taken on tables that reside in a general tablespace while the tablespace
# is renamed, which prevents concurrent DDL.
#
# Concurrent DML is supported.
#
# The CREATE_TABLESPACE privilege is required to rename an InnoDB general tablespace.
#
# The ENCRYPTION option is used to enable or disable page-level data encryption for an
# InnoDB general tablespace.
#
# Option values are not case-sensitive.
#
# Encryption support for general tablespaces was introduced in MySQL 8.0.13.
#
# A keyring plugin must be installed and configured to encrypt a tablespace using
# the ENCRYPTION option.
#
# When a general tablespace is encrypted, all tables residing in the tablespace are encrypted.
# Likewise, a table created in an encrypted general tablespace is encrypted.
#
# The INPLACE algorithm is used when altering the ENCRYPTION attribute of a general
# tablespace.
#
# The INPLACE algorithm permits concurrent DML on tables that reside in the general
# tablespace.
#
# Concurrent DDL is blocked.
#
# For more information, see SECTION 15.6.3.9, "TABLESPACE ENCRYPTION"
#
# 13.1.11 ALTER VIEW SYNTAX
#
# ALTER
# 		[ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}]
# 		[DEFINER = { user | CURRENT_USER }]
# 		[SQL SECURITY { DEFINER | INVOKER }]
# 		VIEW view_name [(column_list)]
# 		AS select_statement
# 		[WITH [CASCADED | LOCAL] CHECK OPTION]
#
# This statement changes the definition of a view, which must exist.
#
# The syntax is similar to that for CREATE_VIEW (see SECTION 13.1.23, "CREATE VIEW SYNTAX")
#
# This statement requires the CREATE_VIEW and DROP privileges for the view, and some
# privilege for each column referred to in the SELECT statement.
#
# ALTER_VIEW is permitted only to the definer or users with the SET_USER_ID or SUPER privilege.
#
# 13.1.12 CREATE DATABASE SYNTAX
#
# CREATE {DATABASE | SCHEMA} [IF NOT EXISTS] db_name
# 		[create_specification] ---
#
# create_specification:
# 		[DEFAULT] CHARACTER SET [=] charset_name
# 	 | [DEFAULT] COLLATE [=] collation_name
#
# CREATE_DATABASE creates a database with the given name. 
#
# To use this statement, you need the CREATE privilege for the database.
#
# CREATE_SCHEMA is a synonym for CREATE_DATABASE
#
# An error occurs if the database exists and you did not specify IF NOT EXISTS
#
# CREATE_DATABASE is not permitted within a session that has an active LOCK_TABLES statement
#
# create_specification options specify database characteristics. Database characteristics
# are stored in the data dictionary.
#
# The CHARACTER SET clause specifies the default database character set. The COLLATE
# clause specifies the default database collation.
#
# CHAPTER 10, CHARACTER SETS, COLLATIONS, UNICODE, discusses character set and collation
# names.
#
# A database in MySQL is implemented as a directory containing files that correspond to tables
# in the database.
#
# Because there are no tables in a database when it is initially created, the CREATE_DATABASE
# statement creates only a directory under the MySQL data directory.
#
# Rules for permissible database names are given in SECTION 9.2, "SCHEMA OBJECT NAMES"
#
# If a database name contains special characters, the name for the database directory
# contains encoded versions of those characters as described in SECTION 9.2.3, "MAPPING OF IDENTIFIERS TO FILE NAMES"
#
# Creating a database directory by manually creating a directory under the data directory
# (for example, with mkdir) is temporarily unsupported in MySQL 8.0.0
#
# You can also use the mysqladmin program to create databases.
#
# See SECTION 4.5.2, "MYSQLADMIN -- CLIENT FOR ADMINISTERING A MYSQL SERVER"
#
# 13.1.13 CREATE EVENT SYNTAX
#
# CREATE
# 		[DEFINER = { user | CURRENT_USER }]
# 		EVENT
# 		[IF NOT EXISTS]
# 		event_name
# 		ON SCHEDULE schedule
# 		[ON COMPLETION [NOT] PRESERVE]
# 		[ENABLE | DISABLE | DISABLE ON SLAVE]
# 		[COMMENT 'string']
# 		DO event_body;
#
# schedule:
# 		AT timestamp [+ INTERVAL interval] ---
#   | EVERY interval
# 		[STARTS timestamp [+ INTERVAL interval] ---]
# 		[ENDS timestamp [+ INTERVAL interval] ---]
#
# interval:
# 		quantity {YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE |
# 					 WEEK | SECOND  | YEAR_MONTH | DAY_HOUR | DAY_MINUTE |
# 					 DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND}
#
# This statement creates and schedules a new event.
#
# The event will not run unless the Event Scheduler is enabled.
#
# For information about checking Event Scheduler status and enabling it if
# necessary, see SECTION 24.4.2, "EVENT SCHEDULER CONFIGURATION"
#
# CREATE_EVENT requires the EVENT privilege for the schema in which the event
# is to be created.
#
# It might also require the SET_USER_ID or SUPER privilege, depending on the
# DEFINER value, as described later in this section.
#
# The minimum requirements for a valid CREATE_EVENT statement are as follows:
#
# 		) The keywords CREATE_EVENT plus an event name, which uniquely identifies the event in a database schema.
#
# 		) An ON SCHEDULE clause, which determines when and how often the event executes
#
# 		) A DO clause, which contains the SQL statement to be executed by an event
#
# This is an example of a minimal CREATE_EVENT statement:
#
# 		CREATE EVENT myevent
# 			ON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 HOUR
# 			DO
# 				UPDATE myschema.mytable SET mycol = mycol + 1;
#
# The previous statement creates an event named myevent.
#
# This event executes once - one hour following its creation - by running an 
# SQL statement that increments the value of the myschema.mytable table's
# mycol column by 1.
#
# The event_name must be a valid MySQL identifier with a maximum length of 64 characters.
#
# Event names are not case-sensitive, so you cannot have two events named myevent
# and MyEvent in the same schema.
#
# In general, the rules governing event names are the same as those for names
# of stored routines.
#
# See SECTION 9.2, "SCHEMA OBJECT NAMES"
#
# An event is associated with a schema. If no schema is indicated as part of event_name,
# the default (current) schema is assumed.
#
# To create an event in a specific schema, qualify the event name with a schema using
# schema_name.event_name syntax
#
# The DEFINER clause specifies the MySQL account to be used when checking access privileges
# at event execution time.
#
# If a user value is given, it should be a MySQL account specified as 'user_name'@'host_name',
# CURRENT_USER or CURRENT_USER()
#
# The default DEFINER value is the user who executes the CREATE_EVENT statement.
#
# This is the same as specifying DEFINER = CURRENT_USER explicitly
#
# If you specify the DEFINER clause, these rules determine the valid DEFINER user values:
#
# 		) If you do not have the SET_USER_ID or SUPER privilege, the only permitted user value is
# 			your own account, either specified literally or by using CURRENT_USER
#
# 			You cannot set the definer to some other account
#
# 		) If you have the SET_USER_ID or SUPER privilege, you can specify any syntactically valid account name.
#
# 			If the account does not exist, a warning is generated.
#
# 		) Although it is possible to create an event with a nonexistent DEFINER account, an error occurs
# 			at event execution time if the account does not exist.
#
# For more information about event security, see SECTION 24.6, "ACCESS CONTROL FOR STORED PROGRAMS AND VIEWS"
#
# Within an event, the CURRENT_USER() function returns the account used to check privileges at event
# execution time, which is the DEFINER user.
#
# For information about user auditing within events, see SECTION 6.3.13, "SQL-BASED MYSQL ACCOUNT ACTIVITY AUDITING"
#
# IF NOT EXISTS has the same meaning for CREATE_EVENT as for CREATE_TABLE:
#
# 		If an event named event_name already exists in the same schema, no action is taken,
# 		and no error results.
#
# 		(However, a warning is generated in such cases)
#
# The ON SCHEDULE clause determines when, how often, and for how long the event_body
# defined for the event repeats.
#
# This clause takes one of two forms:
#
# 		) AT timestamp is used for a one-time event.
#
# 		It specifies that the event executes one time only at the date and time given by
# 		timestamp, which must include both the date and time, or must be an expression that
# 		resolves to a datetime value.
#
# 		You may use a value of either the DATETIME or TIMESTAMP type for this purpose.
#
# 		If the date is in the past, a warning occurs, as shown here:
#
# 			SELECT NOW();
# 			+--------------------------------+
# 			| NOW() 								   |
# 			+--------------------------------+
# 			| 2006-02-10 23:59:01 			   |
# 			+--------------------------------+
# 			1 row in set (0.04 sec) 	
#
# 			CREATE EVENT e_totals
# 				ON SCHEDULE AT '2006-02-10 23:59:00'
# 				DO INSERT INTO test.totals VALUES (NOW());
# 			Query OK, 0 rows affected, 1 warning (0.00 sec)
#
# 			SHOW WARNINGS\G
# 			********************** 1. row *******************************
# 				Level: Note
# 				 Code: 1588
# 			 Message: Event execution time is in the past and ON COMPLETION NOT
# 						 PRESERVE is set.
#
# 						 The event was dropped immediately after creation.
#
# CREATE_EVENT statements which are themselves invalid - for whatever reason - fail with an error.
#
# You may use CURRENT_TIMESTAMP to specify the current date and time. 
#
# In such a case, the event acts as soon as it is created.
#
# To create an event which occurs at some point in the future relative to the current date
# and time - such as the expressed by the phrase "three weeks from now" - you can use
# the optional clause + INTERVAL interval.
#
# The interval portion consists of two parts, a quantity and a unit of time, and follows
# the syntax rules described in Temporal Intervals, except that you cannot use any units
# keywords that involve microseconds when defining an event.
#
# With some interval types, complex time units may be used.
#
# For example, "two minutes and ten seconds" can be expressed as + INTERVAL '2:10' MINUTE_SECOND
#
# You can also combine intervals.
#
# For example, AT CURRENT_TIMESTAMP + INTERVAL 3 WEEK + INTERVAL 2 DAY is equivalent
# to "three weeks and two days from now"
#
# Each portion of such a clause must begin with + INTERVAL.
#
# ) 	To repeat actions at a regular interval, use an EVERY clause.
#
	# 	The EVERY keyword is followed by an interval as described in the previous discussion
	# of the AT keyword.
	# (+ INTERVAL is not used with EVERY)
	#
	# For example, EVERY 6 WEEK means "every six weeks"
	#
	# Although + INTERVAL clauses are not permitted in an EVERY clause, you can use
	# the same complex time units permitted in a + INTERVAL
	#
	# An EVERY clause may contain an optional STARTS clause.
	#
	# STARTS is followed by a timestamp value that indicates when the action should
	# begin repeating, and may also use + INTERVAL interval to specify an amount of time
	# "from now"
	#
	# For example, EVERY 3 MONTH STARTS CURRENT_TIMESTAMP + INTERVAL 1 WEEK means
	# "every three months, beginning one week from now"
	#
	# Similarly, you can express "every two weeks, beginning six hours and fiften
	# minutes from now" as EVERY 2 WEEK STARTS CURRENT_TIMESTAMP + INTERVAL '6:15' HOUR_MINUTE
	#
	# Not specifying STARTS is the same as using STARTS CURRENT_TIMESTAMP - that is, the action
	# specified for the event begins repeating immediately upon creation of the event.
	#
	# An EVERY clause may contain an optional ENDS clause.
	#
	# The ENDS keyword is followed by a timestamp value that tells MySQL when the event
	# should stop repeating.
	#
	# You may also use + INTERVAL interval with ENDS; for instance, EVERY 12 HOUR
	# STARTS CURRENT_TIMESTAMP + INTERVAL 30 MINUTE ENDS CURRENT_TIMESTAMP + INTERVAL 4 WEEK
	# is equivalent to "every twelve hours, beginning thirty minutes from now, and ending
	# four weeks from now".
	#
	# Not using ENDS means that the event continues executing indefinitly.
	#
	# ENDS supports the same syntax for complex time units as STARTS does.
	#
	# You may use STARTS, ENDS, both or neither in an EVERY clause.
	#
	# If a repeating event does not terminate within its scheduling interval,
	# the result may be multiple instances of the event executing simultaneously.
	#
	# If this is undesirable, you should institute a mechanism to prevent simultaneous
	# instances.
	#
	# For example, you could use the GET_LOCK() function, or row or table locking.
#
# The ON SCHEDULE clause may use expressions involving built-in MySQL functions
# and user variables to obtain any of the timestamp or interval values which it contains.
#
# You may not use stored functions or user-defined functions in such expressions, nor
# may you use any table references; however, you may use SELECT FROM DUAL.
#
# This is true for both CREATE_EVENT and ALTER_EVENT statements.
#
# References to stored functions, user-defined functions, and tables in such cases
# are specifically not permitted, and fail with an error (see Bug #22830)
#
# Times in the ON SCHEDULE clause are interpreted using the current session time_zone
# value.
#
# This becomes the event time zone; that is, the time zone that is used for event scheduling
# and is in effect within the event as it executes.
#
# These times are converted to UTC and stored along with the event time zone in the
# mysql.event table.
#
# This enables event execution to proceed as defined regardless of any subsequent
# changes to the server time zone or daylight saving time effects.
#
# For additional information about representation of event times, see SECTION 24.4.4, "EVENT METADATA"
#
# See also SECTION 13.7.6.18, "SHOW EVENTS SYNTAX", and SECTION 25.9, "THE INFORMATION_SCHEMA EVENTS TABLE"
#
# Normally, once an event has expired, it is immediately dropped.
#
# You can override this behavior by specifying ON COMPLETION PRESERVE.
#
# Using ON COMPLETION NOT PRESERVE merely makes the default nonpersistent behavior explicit.
#
# You can create an event but prevent it from being active using the DISABLE keyword.
#
# Alternatively, you can use ENABLE to make explicit the default status, which is active.
#
# This is most useful in conjunction with ALTER_EVENT (see SECTION 13.1.3, "ALTER EVENT SYNTAX")
#
# A third value may also appear in place of ENABLE or DISABLE; DISABLE ON SLAVE is set for
# the status of an event on a replication slave to indicate that the event was created on
# the master and replicated to the slave, but is not executed on the slave.
#
# See SECTION 17.4.1.16, "REPLICATION OF INVOKED FEATURES"
#
# You may supply a comment for an event using a COMMENT clause. Comment may be any string
# of up to 64 chars that you wish to use for describing the event.
#
# The comment text, being a string literal, must be surrounded by quotation marks.
#
# The DO clause specifies an action carried by the event, and consists of an SQL
# statement.
#
# Nearly any valid MySQL statement that can be used in a stored routine can also
# be used as the action statement for a scheduled event.
#
# (See SECTION C.1, "RESTRICTIONS ON STORED PROGRAMS")
#
# For example, the following event e_hourly deletes all rows from the
# sessions table once per hour, where this table is part of the site_activity schema:
#
# 		CREATE EVENT e_hourly
# 			ON SCHEDULE
# 				EVERY 1 HOUR
# 			COMMENT 'Clears out sessions table each hour.'
# 			DO
# 				DELETE FROM site_activity.sessions;
#
# MySQL stores the sql_mode system variable setting in effect when an event is created
# or altered, and always executes the event with this setting in force, regardless of the
# current server SQL mode when the event begins executing.
#
# A CREATE_EVENT statement that contains an ALTER_EVENT statement in its DO clause appears
# to succeed; however, when the server attempts to execute the resulting scheduled event,
# the execution fails with an error.
#
# 		NOTE:
#
# 			Statements such as SELECT or SHOW that merely return a result set have no effect
# 			when used in an event; the output from these is not sent to the MySQL Monitor,
# 			nor is it stored anywhere.
#
# 			However, you can use statements such as SELECT_---_INTO and INSERT_INTO_---_SELECT
# 			that store a result.
#
# 			(See the next example in this section for an instance of the latter)
#
# The schema to which an event belongs is the default schema for table references in the DO clause.
#
# Any references to tables in other schemas must be qualified with the proper schema name.
#
# As with stored routines, you can use compound-statement syntax in the DO clause by using the
# BEGIN and END keywords, as shown here:
#
# 		delimiter |
#
# 		CREATE EVENT e_daily
# 			ON SCHEDULE
# 				EVERY 1 DAY
# 			COMMENT 'Saves total number of sessions then clears the table each day'
# 			DO
# 				BEGIN
# 					INSERT INTO site_activity.totals (time, total)
# 						SELECT CURRENT_TIMESTAMP, COUNT(*)
# 							FROM site_activity.sessions;
# 					DELETE FROM site_activity.sessions;
# 				END |
#
# 		delimiter ;
#
# This example uses the delimiter command to change the statement delimiter.
#
# See SECTION 24.1, "DEFINING STORED PROGRAMS"
#
# More complex compound statements, such as those used in stored routines, are possible
# in an event.
#
# This example uses local variables, an error handler, and a flow control construct:
#
# 		delimiter |
#
# 		CREATE EVENT e
# 			ON SCHEDULE
# 				EVERY 5 SECOND
# 			DO
# 				BEGIN
# 					DECLARE v INTEGER;
# 					DECLARE CONTINUE HANDLER FOR SQLEXCEPTION BEGIN END;
#
# 					SET v = 0;
#
# 					WHILE v < 5 DO
# 						INSERT INTO t1 VALUES (0);
# 						UPDATE t2 SET s1 = s1 + 1;
# 						SET v = v + 1;
# 					END WHILE;
# 			END |
#
# 		delimiter ;
#
# There is no way to pass parameters directly to or from events; however, it is
# possible to invoke a stored routine with parameters within an event:
#
# 		CREATE EVENT e_call_myproc
# 			ON SCHEDULE
# 				AT CURRENT_TIMESTAMP + INTERVAL 1 DAY
# 			DO CALL myproc(5, 27);
#
# If an event's definer has privileges sufficient to set global system variables
# (see SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES"), the event can read and write
# global variables.
#
# As granting such privileges entails a potential for abuse, extreme care must be
# taken in doing so.
#
# Generally, any statements that are valid in stored routines may be used for action
# statements executed by events.
#
# For more information about statements permissible within stored routines, see 
# SECTION 24.2.1, "STORED ROUTINE SYNTAX"
#
# You can create an event as part of a stored routine, but an event cannot be 
# created by another event.
#
# 13.1.14 CREATE FUNCTION SYNTAX
#
# The CREATE_FUNCTION statement is used to create stored functions and user-defined
# functions (UDFs):
#
# 		) For information about creating stored functions, see SECTION 13.1.17, "CREATE PROCEDURE AND CREATE FUNCTION SYNTAX"
#
# 		) For information about creating user-defined functions, see SECTION 13.7.4.1, "CREATE FUNCTION SYNTAX FOR USER-DEFINED FUNCTIONS"
#
# 13.1.15 CREATE INDEX SYNTAX
#
# CREATE [UNIQUE | FULLTEXT | SPATIAL] INDEX index_name
# 		[index_type]
# 		ON tbl_name (key_part, ---)
# 		[index_option]
# 		[algorithm_option | lock_option] ---
#
# key_part: {col_name [(length)] | (expr)} [ASC | DESC]
#
# index_option:
# 		KEY_BLOCK_SIZE [=] value
# 	 | index_type
# 	 | WITH PARSER parser_name
#   | COMMENT 'string'
#   | {VISIBILE | INVISIBILE}
#
# index_type:
# 		USING {BTREE | HASH}
#
# algorithm_option:
# 		ALGORITHM [=] {DEFAULT | INPLACE | COPY}
#
# lock_option:
# 		LOCK [=] {DEFAULT | NONE | SHARED | EXCLUSIVE}
#
# Normally, you create all indexes on a table at the time the table itself is created
# with CREATE_TABLE.
#
# See SECTION 13.1.20, "CREATE TABLE SYNTAX"
#
# This guideline is especially important for InnoDB tables, where the primary key
# determines the physical layout of rows in the data file.
#
# CREATE_INDEX enables you to add indexes to existing tables.
#
# CREATE_INDEX is mapped to an ALTER_TABLE statement to create indexes.
#
# See SECTION 13.1.9, "ALTER TABLE SYNTAX". CREATE_INDEX cannot be used to
# create a PRIMARY KEY; use ALTER_TABLE instead.
#
# For more information about indexes, see SECTION 8.3.1, "HOW MYSQL USES INDEXES"
#
# InnoDB supports secondary indexes on virtual columns. For more information,
# see SECTION 13.1.20.9, "SECONDARY INDEXES AND GENERATED COLUMNS"
#
# When the innodb_stats_persistent setting is enabled, run the ANALYZE_TABLE
# statement for an InnoDB table after creating an index on that table.
#
# An index specification of the form (key_part1, key_part2, ---) creates an index
# with multiple key parts.
#
# Index key values are formed by concatenating the values of the
# given key parts.
#
# For example (col1, col2, col3) specifies a multiple-column index with index
# keys consisting of values from col1, col2 and col3.
#
# A key_part specificaiton can end with ASC or DESC to specify whether index values 
# are stored in ascending or descending order.
#
# The default is ascending if no order specifier is given.
#
# ASC and DESC are not permitted for HASH indexes.
#
# As of MySQL 8.0.12, ASC and DESC are not permitted for SPATIAL indexes.
#
# The following sections describe different aspects of the CREATE_INDEX statement:
#
# 		) Column Prefix Key Parts
#
# 		) Functional Key parts
#
# 		) Unique Indexes
#
# 		) Full-Text Indexes
#
# 		) Spatial indexes
#
# 		) Index Options
#
# 		) Table Copying and Locking Options
#
# COLUMN PREFIX KEY PARTS
#
# For string columns, indexes can be created that use only the leading part of column values,
# using col_name(length) syntax to specify an index prefix length:
#
# 		) Prefixes can be specified for CHAR, VARCHAR, BINARY, and VARBINARY key parts
#
# 		) Prefixes must be specified for BLOB and TEXT key parts. Additionally, BLOB and TEXT columns
# 			can be indexed only for InnoDB, MyISAM, and BLACKHOLE tables.
#
# 		) Prefix limits are measured in bytes. However, prefix lengths for index specifications
# 			in CREATE_TABLE, ALTER_TABLE and CREATE_INDEX statements are interpreted as number
# 			of characters for nonbinary string types (CHAR, VARCHAR, TEXT) and number of bytes
# 			for binary string types (BINARY, VARBINARY, BLOB)
#
# 			Take this into account when specifying a prefix length for a nonbinary string column
# 			that uses a multibyte character set.
#
# 			Prefix support and lengths of prefixes (where supported) are storage engine dependent.
#
# 			For example, a prefix can be up to 767 bytes long for InnoDB tables that use the
# 			REDUNDANT or COMPACT row format.
#
# 			The prefix length limit is 3072 bytes for InnoDB tables that use the DYNAMIC
# 			or COMPRESSED row format.
#
# 			For MyISAM tables, the prefix length is 1000 bytes.
#
# 			The NDB storage engine does not support prefixes (see SECTION 22.1.7.6, "UNSUPPORTED OR MISSING FEATURES IN NDB CLUSTER")
#
# If a specified index prefix exceeds the maximum column data type size, CREATE_INDEX handles
# the index as follows:
#
# 		) For a nonunique index, either an error occurs (if strict SQL mode is enabled), or the index length is
# 			reduced to lie within the maximum column data type size and a warning is produced
# 			(if strict SQL mode is not enabled)
#
# 		) For a unique index, an error occurs regardless of SQL mode because reducing the index length
# 			might enable insertion of nonunique entries that do not meet the specified uniqueness requirement.
#
# The statement shown here creates an index using the first 10 characters of the name column
# (assuming that name has a nonbinary string type):
#
# 		CREATE INDEX part_of_name ON customer (name(10));
#
# If names in the column usually differ in the first 10 characters, lookups performed using
# this index should not be much slower than using an index created from the entire
# name column.
#
# Also, using column prefixes for indexes can make the index file much smaller, which could
# save a lot of disk space and might also speed up INSERT operations.
#
# FUNCTIONAL KEY PARTS
#
# A "normal" index indexes column values or prefixes of column values.
#
# For example, in teh following table, the index entry for a given t1 row includes
# the full col1 value and a prefix of the col2 value consisting of its first 10 bytes:
#
# 		CREATE TABLE t1 (
# 			col1 VARCHAR(10),
# 			col2 VARCHAR(20),
# 			INDEX (col1, col2(10))
# 		);
#
# MySQL 8.0.13 and higher supports functional key parts that index expression
# values rather than column or column prefix values.
#
# Use of functional key parts enables indexing of values not stored directly
# in the table. 
#
# Examples:
#
# 		CREATE TABLE t1 (col1 INT, col2 INT, INDEX func_index ((ABS(col1))));
# 		CREATE INDEX idx1 ON t1 ((col1 + col2));
# 		CREATE INDEX idx2 ON t1 ((col1 + col2), (col1 - col2), col1);
# 		ALTER TABLE t1 ADD INDEX ((col1 * 40) DESC);
#
# An index with multiple key parts can mix nonfunctional and functional key parts.
#
# ASC and DESC are supported for functional key parts.
#
# Functional key parts must adhere to the following rules. An error occurs if a key
# part definition contains disallowed constructs.
#
# 		) In index definitions, enclose expressions within parentheses to distinguish them from 
# 			columns or column prefixes.
#
# 			For example, this is permitted; the expressions are enclosed within parentheses:
#
# 				INDEX ((col1 + col2), (col3 - col4))
#
# 			This produces an error, the expressions are not enclosed within parentheses:
#
# 				INDEX (col1 + col2, col3 - col4)
#
# 		) A functional key part cannot consist solely of a column name. 
#
# 			For example, this is not permitted:
#
# 				INDEX ((col1), (col2))
#
# 			Instead, write the key parts as nonfunctional key parts, without parentheses:
#
# 				INDEX (col1, col2)
#
# 		) A functional key part expression cannot refer to column prefixes.
#
# 			For a workaround, see the discussion of SUBSTRING() and CAST() later in this section.
#
# 		) Functional key parts are not permitted in foreign key specifications.
#
# For CREATE_TABLE_---_LIKE, the destination table preserves functional key parts from the 
# original table.
#
# Functional indexes are implemented as hidden virtual generated columns, which has these implications:
#
# 		) Each functional key part counts against the limit on total number of table columns; See SECTION C.10.4, "LIMITS ON TABLE COLUMN COUNT AND ROW SIZE"
#
# 		) Functional key parts inherit all restrictions that apply to generated columns. Examples:
#
# 			) Only functions permitted for generated columns are permitted for functional key parts
#
# 			) Subqueries, parameters, variables, stored functions, and user-defined functions are not permitted.
#
# 			For more information about applicable restrictions, see SECTION 13.1.20.8, "CREATE TABLE AND GENERATED COLUMNS"
# 			and SECTION 13.1.9.2, "ALTER TABLE AND GENERATED COLUMNS"
#
# 		) The virtual generated column itself requires no storage.
#
# 			The index itself takes up storage space as any other index.
#
# UNIQUE is supported for indexes that include functional key parts. However, primary keys cannot
# include functional key parts.
#
# A primary key requires the generated column to be stored, but functional key parts are implemented
# as virtual generated columns, not stored generated columns.
#
# SPATIAL and FULLTEXT indexes cannot have functional key parts.
#
# If a table contains no primary key, InnoDB automatically promotes the first UNIQUE NOT FULL
# index to the primary key.
#
# This is not supported for UNIQUE NOT NULL indexes that have functional key parts.
#
# Nonfunctional indexes raise a warning if there are duplicate indexes.
#
# Indexes that contain functional key parts do not have this feature.
#
# To remove a column that is referenced by a functional key part, the index
# must be removed first.
#
# Otherwise, an error occurs.
#
# Although nonfunctional key parts support a prefix length specification, this
# is not possible for functional key parts.
#
# The solution is to use SUBSTRING() (or CAST(), as described later in this section)
#
# For a functional key part containing the SUBSTRING() function to be used in a query,
# the WHERE clause must contain SUBSTRING() with the same arguments.
#
# In the following example, only the second SELECT is able to use the index because
# that is the only query in which the arguments to SUBSTRING() match the index
# specification:
#
# 		CREATE TABLE tbl (
# 			col1 LONGTEXT,
# 			INDEX idx1 ((SUBSTRING(col1, 1, 10)))
# 		);
# 		SELECT * FROM tbl WHERE SUBSTRING(col1, 1, 9) = '123456789';
# 		SELECT * FROM tbl WHERE SUBSTRING(col1, 1, 10) = '1234567890';
#
# Functional key parts enable indexing of values that cannot be indexed otherwise,
# such as JSON values.
#
# However, this must be done correctly to achieve the desired effect.
#
# For example, this syntax does not work:
#
# 		CREATE TABLE employees (
# 			data JSON,
# 			INDEX ((data->>'$.name'))
# 		);
#
# This syntax fails because:
#
# 		) The ->> operator translates into JSON_UNQUOTE(JSON_EXTRACT(---))
#
# 		) JSON_UNQUOTE() returns a value with a data type of LONGTEXT, and the hidden generated
# 			column thus is assigned the same data type.
#
# 		) MySQL cannot index LONGTEXT columns specified without a prefix length on the key part,
# 			and prefix lengths are not permitted in functional key parts.
#
# To index the JSON column, you could try using the CAST() function as follows:
#
# 		CREATE TABLE employees (
# 			data JSON,
# 			INDEX ((CAST(data->>'$.name' AS CHAR(30))))
# 		);
#
# The hidden generated column is assigned the VARCHAR(30) data type, which can be indexed.
#
# But this approach produces a new issue when trying to use theh index:
#
# 		) CAST() returns a string with the collation utf8mb4_0900_ai_ci (the server default collation)
#
# 		) JSON_UNQUOTE() returns a string with the collation utf8mb4_bin (hard coded)
#
# As a result, there is a collation mismatch between the indexed expression in the preceding
# table definition and the WHERE clause expression in the following query:
#
# 		SELECT * FROM employees WHERE data->>'$.name' = 'James';
#
# The index is not used because the expression in the query and the index differ.
#
# To support this kind of scenario for functional key parts, the optimizer automatically
# strips CAST() when looking for an index to use, but only if the collation of the indexed
# expression matches that of the query expression.
#
# For an index with a functional key part to be used, either of the following two solutions
# work (although they differ somewhat in effect):
#
# 		) Solution 1.  Assign the indexed expression the same collation as JSON_UNQUOTE():
#
	# 			CREATE TABLE employees (
	# 				data JSON,
	# 				INDEX idx ((CAST(data->>"$.name" AS CHAR(30)) COLLATE utf8mb4_bin))
	# 			);
	# 			INSERT INTO employees VALUES
	# 				('{ "name": "james", "salary": 9000 }'),
	# 				('{ "name": "James", "salary": 10000 }'),
	# 				('{ "name": "Mary", "salary": 12000 }'),
	# 				('{ "name": "Peter", "salary": 8000 }');
	# 			SELECT * FROM employees WHERE data->>'$.name' = 'James';
#
# 			The ->> operator is the same as JSON_UNQUOTE(JSON_EXTRACT(---)), and JSON_UNQUOTE()
# 			returns a string with collation utf8mb4_bin
#
# 			The comparison is thus case sensitive, and only one row matches:
#
# 				+--------------------------------------------+
# 				| data 													|
# 				+--------------------------------------------+
# 				| {"name": "James", "salary": 10000} 		   |
# 				+--------------------------------------------+
#
# 		) Solution 2. Specify the full expression in the query:
#
	# 			CREATE TABLE employees (
	# 				data JSON,
	# 				INDEX idx ((CAST(data->>"$.name" AS CHAR(30))))
	# 			);
	# 			INSERT INTO employees VALUES
	# 				('{ "name": "james", "salary": 9000 }'),
	# 				('{ "name": "James", "salary": 10000 }'),
	# 				('{ "name": "Mary", "salary": 12000 }'),
	# 				('{ "name": "Peter", "salary": 8000 }');
	# 			SELECT * FROM employees WHERE CAST(data->>'$.name' AS CHAR(30)) = 'James';
#
# 		
# 			CAST() returns a string with collation utf8mb4_0900_ai_ci, so the comparison case
# 			insensitive and two rows match:
#
# 				+----------------------------------------------+
# 				| data 													  |
# 				+----------------------------------------------+
# 				| {"name": "james", "salary": 9000} 			  |
# 				| {"name": "James", "salary": 10000} 			  |
# 				+----------------------------------------------+
#
# 			Be aware that although the optimizer supports automatically stripping CAST() with indexed
# 			generated columns, the following approach does not work because it produces a different
# 			result with and without an index (Bug#27337092):
#
# 				CREATE TABLE employees (
# 					data JSON,
# 					generated_col VARCHAR(30) AS (CAST(data->>'$.name' AS CHAR(30)))
# 				);
# 				Query OK, 0 rows affected, 1 warning (0.03 sec)
#
# 				INSERT INTO employees (data)
# 				VALUES ('{"name": "james"}'), ('{"name": "James"}');
# 				Query OK, 2 rows affected, 1 warning (0.01 sec)
# 				Records: 2 Duplicates: 0 Warnings: 1
#
# 				SELECT * FROM employees WHERE data->>'$.name' = 'James';
# 				+------------------------------+----------------------+
# 				| data 								 | generated_col 			|
# 				+------------------------------+----------------------+
# 				| {"name": "James"} 				 | James 					|
# 				+------------------------------+----------------------+
# 				1 row in set (0.00 sec)
#
# 				ALTER TABLE employees ADD INDEX idx (generated_col);
# 				Query OK, 0 rows affected, 1 warning (0.03 sec)
# 				Records: 0 Duplicates: 0 Warnings: 1
#
# 				SELECT * FROM employees WHERE data->>'$.name' = 'James';
# 				+------------------------------+----------------------+
# 				| data 								 | generated_col 		   |
# 				+------------------------------+----------------------+
# 				| {"name": "james"} 				 | james 					|
# 				| {"name": "James"} 				 | James 					|
# 				+------------------------------+----------------------+
# 				2 rows in set (0.01 sec)
#
# UNIQUE INDEXES
#
# A UNIQUE index creates a constraint such that all values in the index must be
# distinct.
#
# An error occurs if you try to add a new row with a key value that matches an
# existing row.
#
# If you specify a prefix value for a column in a UNIQUE index, the column values
# must be unique within the prefix length.
#
# A UNIQUE index permits multiple NULL values for columns that can contain NULL
#
# If a table has a PRIMARY KEY or UNIQUE NOT NULL index that consists of a single
# column that has an integer type, you can use _rowid to refer to the indexed
# column in SELECT statements, as follows:
#
# 		) _rowid refers to the PRIMARY KEY column if there is a PRIMARY KEY consisting
# 			of a single integer column.
#
# 			If there is a PRIMARY KEY but it does not consist of a single integer column,
# 			_rowid cannot be used.
#
# 		) Otherwise, _rowid refers to the column in the first UNIQUE NOT NULL index if that
# 			index consists of a single integer column.
#
# 			If the first UNIQUE NOT NULL index does not consist of a single integer column,
# 			_rowid cannot be used.
#
# FULL-TEXT INDEXES
#
# FULLTEXT indexes are supported only for InnoDB and MyISAM tables and can include only
# CHAR, VARCHAR and TEXT columns.
#
# Indexing always happens over the entire column; column prefix indexing is not
# supported and any prefix length is ignored if specified.
#
# See SECTION 12.9, "FULL-TEXT SEARCH FUNCTIONS", for details of operation.
#
# SPATIAL INDEXES
#
# The MyISAM, InnoDB, NDB and ARCHIVE storage engines support spatial columns such as
# POINT and GEOMETRY.
#
# (SECTION 11.5, "SPATIAL DATA TYPES", describes the spatial data types)
#
# However, support for spatial column indexing varies among engines.
#
# Spatial and nonspatial indexes on spatial columns are available according to
# the following rules.
#
# Spatial indexes on spatial columns have these characteristics:
#
# 		) Available only for InnoDB and MyISAM tables. Specifying SPATIAL INDEX for other storage engines results in an error.
#
# 		) As of MySQL 8.0.12, an index on a spatial column MUST be a SPATIAL index.
#
# 			The SPATIAL keyword is thus optional but implicit for creating an index on a spatial column.
#
# 		) Available for single spatial columns only. A spatial index cannot be created over multiple spatial columns.
#
# 		) Indexed columns must be NOT NULL
#
# 		) Column prefix lengths are prohibited. The full width of each column is indexed.
#
# 		) Not permitted for a primary key or unique index
#
# Nonspatial indexes on spatial columns (created with INDEX, UNIQUE, or PRIMARY KEY) have these
# characteristics:
#
# 		) Permitted for any storage engine that supports spatial columns except ARCHIVE
#
# 		) Columns can be NULL unless the index is a primary key
#
# 		) The index type for a non-SPATIAL index depends on the storage engine. Currently, B-tree is used.
#
# 		) Permitted for a column that can have NULL values only for InnoDB, MyISAM, and MEMORY tables
#
# INDEX OPTIONS
#
# Following the key part list, index options can be given.
#
# An index_option value can be any of the following:
#
# 		) KEY_BLOCK_SIZE [=] value
#
# 			For MyISAM tables, KEY_BLOCK_SIZE optionally specifies the size in bytes to use
# 			for index key blocks.
#
# 			The value is treated as a hint; a different size could be used if necessary.
#
# 			A KEY_BLOCK_SIZE value specified for an individual index definition overrides
# 			a table-level KEY_BLOCK_SIZE value.
#
# 			KEY_BLOCK_SIZE is not supported at the index level for InnoDB tables.
#
# 			See SECTION 13.1.20, "CREATE TABLE SYNTAX"
#
# 		) index_type
#
# 			Some storage engines permit you to specify an index type when creating an index.
#
# 			For example:
#
# 				CREATE TABLE lookup (id INT) ENGINE = MEMORY;
# 				CREATE INDEX id_index ON lookup (id) USING BTREE;
#
# 			TABLE 13.1, "INDEX TYPES PER STORAGE ENGINE" shows the permissible index type values
# 			supported by different storage engines.
#
# 			Where multiple index types are listed, the first one is the default when no
# 			index type specifier is given.
#
# 			Storage engines not listed in the table do not support an index_type clause
# 			in index definitions.
#
	# 			TABLE 13.1 INDEX TYPES PER STORAGE ENGINE
	#
	# 			STORAGE ENGINE 			PERMISSIBILE INDEX TYPES
	#
	# 			InnoDB 						BTREE
	#
	# 			MyISAM 						BTREE
	#
	# 			MEMORY/HEAP 				HASH, BTREE
	#
	# 			NDB 							HASH, BTREE (see note in text)
#
# 			The index_type clause cannot be used for FULLTEXT INDEX or (prior to MySQL 8.0.12)
# 			SPATIAL INDEX specifications.
#
# 			Full-text index implementation is storage engine dependent.
#
# 			Spatial indexes are implemented as R-tree indexes.
#
# 			If you specify an index type that is not valid for a given storage engine,
# 			but another index type is available that the engine can use without affecting
# 			query results, the engine uses the available type.
#
# 			The parser recognizes RTREE as a type name. AS of MySQL 8.0.12, this is permitted
# 			only for SPATIAL indexes.
#
# 			Prior to 8.0.12, RTREE cannot be specified for any storage engine.
#
# 			BTREE indexes are implemented by the NDB storage engine as T-tree indexes.
#
# 				NOTE:
#
# 					For indexes on NDB table columns, the USING option can be specified only
# 					for a unique index or primary key.
#
# 					USING HASH prevents the creation of an ordered index; otherwise,
# 					creating a unique index or primary key on an NDB table automatically
# 					results in the creation of both an ordered index and a hash index,
# 					each of which indexes the same set of columns.
#
# 					For unique indexes that include one or more NULL columns of an NDB
# 					table, the hash index can be used only to look up literal values,
# 					which means that IS [NOT] NULL conditions require a full scan of the table.
#
# 					One workaround is to make sure that a unique index using one or more NULL
# 					columns on such a table is always created in such a way that it includes
# 					the ordered index; that is, avoid employing USING HASH when creating the
# 					index.
#
# 			If you specify an index type that is not valid for a given storage engine, but another
# 			index type is available that the engine can use without affecting query results,
# 			the engine uses the available type.
#
# 			The parser recognizes RTREE as a type name, but currently this cannot be specified
# 			for any storage engine.
#
# 				NOTE:
#
# 					Use of the index_type option before the ON tbl_name clause is deprecated;
# 					support for use of the option in this position will be removed in a 
# 					future MySQL release.
#
# 					If an index_type option is given in both the earlier and later positions,
# 					the final option applies.
#
# 			TYPE type_name is recognized as a synonym for USING type_name.
#
# 			However, USING is the preferred form.
#
# 			The following tables show index characteristics for the storage engines that
# 			support the index_type option.
#
# 				TABLE 13.2 InnoDB STORAGE ENGINE INDEX CHARACTERISTICS
#
# 					INDEX CLASS 		INDEX TYPE 		STORES NULL VALUES 		PERMITS MULTIPLE NULL VALUES 	IS NULL SCAN TYPE 		IS NOT NULL SCAN TYPE
#
# 					Primary key 		BTREE 			No 							No 									N/A 							N/A
# 					Unique 				BTREE 			Yes 							Yes 									Index 						Index
# 					Key 					BTREE 			Yes 							Yes 									Index 						Index
# 					FULLTEXT 			N/A 				Yes 							Yes 									Table 						Table
# 					SPATIAL 				N/A 				No 							No 									N/A 							N/A
#
# 				Table 13.3 MyISAM Storage Engine Index Characteristics
#
# 					INDEX CLASS 		INDEX TYPE 		STORES NULL VALUES 		PERMITS MULTIPLE NULL VALUES 	IS NULL SCAN TYPE 		IS NOT NULL SCAN TYPE
#
# 					Primary key 		BTREE 			No 							No 									N/A 							N/A
# 					Unique 				BTREE 			Yes 							Yes 									Index 						Index
# 					Key 					BTREE 			Yes 							Yes 									Index 						Index
# 					FULLTEXT 			N/A 				Yes 							Yes 									Table 						Table
# 					SPATIAL 				N/A 				No 							No 									N/A 							N/A
#
# 				Table 13.4 MEMORY Storage Engine Index Characteristics
#
# 					INDEX CLASS 		INDEX TYPE 		STORES NULL VALUES 		PERMITS MULTIPLE NULL VALUES 	IS NULL SCAN TYPE 		IS NOT NULL SCAN TYPE
#
# 					Primary key 		BTREE 			No 							No 									N/A 							N/A
# 					Unique 				BTREE 			Yes 							Yes 									Index 						Index
# 					Key 					BTREE 			Yes 							Yes 									Index 						Index
# 					Primary key 		HASH 				No 							No 									N/A 							N/A
# 					Unique 				HASH 				Yes 							Yes 									Index 						Index
# 					Key 					HASH 				Yes 							Yes 									Index 						Index
#
# 				Table 13.5 NDB Storage Engine Index Characteristics
#
# 					INDEX CLASS 		INDEX TYPE 		STORES NULL VALUES 		PERMITS MULTIPLE NULL VALUES 	IS NULL SCAN TYPE 		IS NOT NULL SCAN TYPE
#
# 					Primary key 		BTREE 			No 							No 									Index 						Index
# 					Unique 				BTREE 			Yes 							Yes 									Index 						Index
# 					Key 					BTREE 			Yes 							Yes 									Index 						Index
# 					Primary key 		HASH 				No 							No 									Table (see note 1) 		Table (see note 1)
# 					Unique 				HASH 				Yes 							Yes 									Table (see note 1) 		Table (see note 1)
# 					Key 					HASH 				Yes 							Yes 									Table (see note 1) 		Table (see note 1)
#
# 			Table note:
#
# 				1. USING HASH prevents creation of an implicit ordered index
#
# 		) WITH PARSER parser_name
#
# 			This option can be used only with FULLTEXT indexes.
#
# 			It associates a parser plugin with the index if full-text indexing and searching
# 			operations need special handling.
#
# 			InnoDB and MyISAM support full-text parser plugins
#
# 			See FULL-TEXT PARSER PLUGINS and SECTION 29.2.4.4, "WRITING FULL-TEXT PARSER PLUGINS"
# 			for more information.
#
# 		) COMMENT 'string'
#
# 			Index definitions can include an optional comment of up to 1024 characters
#
# 			The MERGE_THRESHOLD for index pages can be configured for individual indexes using the
# 			index_option COMMENT clause of the CREATE_INDEX statement.
#
# 			For example:
#
# 				CREATE TABLE t1 (id INT);
# 				CREATE INDEX id_index ON t1 (id) COMMENT 'MERGE_THRESHOLD=40';
#
# 			If the page-full percentage for an index page falls below the MERGE_THRESOLD value
# 			when a row is deleted or when a row is shortened by an update operation, InnoDB
# 			attempts to merge the index page with a neighboring index page.
#
# 			The default MERGE_THRESHOLD value is 50, which is the previously hardcoded value.
#
# 			MERGE_THRESHOLD can also be defined at the index level using CREATE_TABLE and ALTER_TABLE statements.
#
# 			For more information, see SECTION 15.8.11, "CONFIGURING THE MERGE THRESHOLD FOR INDEX PAGES"
#
# 		) VISIBLE, INVISIBLE
#
# 			Specify index visibility.
#
# 			Indexes are visible by default. An invisible index is not used by the optimizer.
#
# 			Specification of index visibility applies to indexes other than primary keys
# 			(either explicit or implicit)
#
# 			For more information, see SECTION 8.3.12, "INVISIBLE INDEXES"
#
# TABLE COPYING AND LOCKING OPTIONS
#
# ALGORITHM and LOCK clauses may be given to influence the table copying method and
# level of concurrency for reading and writing the table while its indexes are being modified.
#
# They have the same meanings as for the ALTER_TABLE statement.
#
# For more information, see SECTION 13.1.9, "ALTER TABLE SYNTAX"
#
# NDB Cluster supports online operations using the same ALGORITHM=INPLACE syntax used
# with the standard MySQL server.
#
# See SECTION 22.5.14, "ONLINE OPERATIONS WITH ALTER TABLE IN NDB CLUSTER", for more information.
#
# 13.1.16 CREATE LOGFILE GROUP SYNTAX
#
# CREATE LOGFILE GROUP logfile_group
# 		ADD UNDOFILE 'undo_file'
# 		[INITIAL_SIZE [=] initial_size]
# 		[UNDO_BUFFER_SIZE [=] undo_buffer_size]
# 		[REDO_BUFFER_SIZE [=] redo_buffer_size]
# 		[NODEGROUP [=] nodegroup_id]
# 		[WAIT]
# 		[COMMENT [=] 'string']
# 		ENGINE [=] engine_name
#
# This statement creates a new log file group named logfile_group having a single
# UNDO file named 'undo_file'
#
# A CREATE_LOGFILE_GROUP statement has one and only one ADD UNDOFILE clause
#
# For rules covering the naming of log file groups, see SECTION 9.2, "SCHEMA OBJECT NAMES"
#
# NOTE:
#
# 		All NDB Cluster Disk Data objects share the same namespace.
#
# 		This means that each Disk Data Object must be uniquely named
# 		(and not merely each Disk Data object of a given type)
#
# 		For example, you cannot have a tablespace and a log file group
# 		with the same name, or a tablespace and a data file with the same name.
#
# There can be only one log file group per NDB Cluster instance at any given time.
#
# The optional INITIAL_SIZE parameter sets the UNDO file's initial size; if not specified,
# it defaults to 128M (128 megabytes)
#
# The optional UNDO_BUFFER_SIZE parameter sets the size used by the UNDO buffer
# for the log file group; The default value for UNDO_BUFFER_SIZE is 8M (eight MB);
# This value cannot exceed the amount of system memory available.
#
# Both of these parameters are specified in bytes.
#
# You may optionally follow either or both of these with a one-letter
# abbreviation for an order of magnitude, similar to those used in my.cnf
#
# Generally, this is one of the letters M (for megabytes) or G (for gigabytes)
#
# Memory used for UNDO_BUFFER_SIZE comes from the global pool whose size is
# determined by the value of the SharedGlobalMemory data node configuration
# parameter.
#
# This includes any default value implied for this option by the setting
# of the InitialLogFileGroup data node configuration parameter.
#
# The maximum permitted for UNDO_BUFFER_SIZE is 629145600 (600 MB)
#
# On 32-bit systems, the maximum supported value for INITIAL_SIZE 
# is 4294967296 (4 GB) (Bug #29186)
#
# The minimum allowed value for INITIAL_SIZE is 1048576 (1 MB)
#
# The ENGINE option determines the storage engine to be used by this log file group,
# with engine_name being the name of the storage engine.
#
# In MySQL 8.0, this must be NDB (or NDBCLUSTER)
#
# If ENGINE is not set, MySQL tries to use the engine specified by the default_storage_engine
# server system variable (formerly storage engine)
#
# In any case, if the engine is not specified as NDB or NDBCLUSTER, the CREATE LOGFILE GROUP
# statement appears to succeed but actually fails to create the log file group, as shown
# here:
#
# 		CREATE LOGFILE GROUP lg1
# 			ADD UNDOFILE 'undo.dat' INITIAL_SIZE = 10M;
# 		Query OK, 0 rows affected, 1 warning (0.00 sec)
#
# 		SHOW WARNINGS;
# 		+-----------+-----------+------------------------------------------------------------------------------------------------+
# 		| Level 		| Code 		| Message 																								 					 |
# 		+-----------+-----------+------------------------------------------------------------------------------------------------+
# 		| Error 		| 1478 	   | Table storage engine 'InnoDB' does not support the create option 'TABLESPACE or LOGFILE GROUP' |
# 		+-----------+-----------+------------------------------------------------------------------------------------------------+
# 		1 row in set (0.00 sec)
#
# 		DROP LOGFILE GROUP lg1 ENGINE = NDB;
# 		ERROR 1529 (HY000): Failed to drop LOGFILE GROUP
#
# 		CREATE LOGFILE GROUP lg1
# 			ADD UNDOFILE 'undo.dat' INITIAL_SIZE = 10M
# 			ENGINE = NDB;
# 		Query OK, 0 rows affected (2.97 sec)
#
# The fact that the CREATE LOGFILE GROUP statement does not actually return an error when a non-NDB storage
# engine is named, but rather appears to succeed, is a known issue which we hope to address in a future
# release of NDB Cluster.
#
# REDO_BUFFER_SIZE, NODEGROUP, WAIT and COMMENT are parsed but ignored, and so have no effect in MySQL 8.0
#
# These options are intended for future expansion
#
# When used with ENGINE [=] NDB, a log file group and associated UNDO log file are created on each Cluster
# data node.
#
# You can verify that the UNDO files were created and obtain information about them by querying the
# INFORMATION_SCHEMA.FILES table.
#
# For example:
#
# 		SELECT LOGFILE_GROUP_NAME, LOGFILE_GROUP_NUMBER, EXTRA
# 			FROM INFORMATION_SCHEMA.FILES
# 			WHERE FILE_NAME = 'undo_10.dat';
# 		+-------------------------+------------------------------+--------------------+
# 		| LOGFILE_GROUP_NAME 	  | LOGFILE_GROUP_NUMBER 		   | EXTRA 					|
# 		+-------------------------+------------------------------+--------------------+
# 		| lg_3 						  | 11 								   | CLUSTER_NODE=3 	   |
# 		| lg_3 						  | 11 									| CLUSTER_NODE=4 		|
# 		+-------------------------+------------------------------+--------------------+
#
# CREATE_LOGFILE_GROUP is useful only with Disk Data storage for NDB Cluster.
#
# See SECTION 22.5.13, "NDB CLUSTER DISK DATA TABLES"
#
# 13.1.17 CREATE PROCEDURE AND CREATE FUNCTION SYNTAX
#
# 		CREATE
# 			[DEFINER = { user | CURRENT_USER }]
# 			PROCEDURE sp_name ([proc_parameter[,---]])
# 			[characteristic ---] routine_body
#
# 		CREATE
# 			[DEFINER = { user | CURRENT_USER }]
# 			FUNCTION sp_name ([func_parameter[,---]])
# 			RETURNS type
# 			[characteristic ---] routine_body
#
# 		proc_parameter:
# 			[ IN | OUT | INOUT ] param_name type
#
# 		func_parameter:
# 			param_name type
#
# 		type:
# 			Any valid MySQL data type
#
# 		characteristic:
# 			COMMENT 'string'
# 		 | LANGUAGE SQL
# 		 | [NOT] DETERMINISTIC
# 		 | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }
# 		 | SQL SECURITY { DEFINER | INVOKER }
#
# 		routine_body:
# 			Valid SQL routine statement
#
# These statements create stored routines.
#
# By default, a routine is associated with the default database.
#
# To associate the routine explicitly with a given database, specify the
# name as db_name.sp_name when you create it.
#
# The CREATE_FUNCTION statement is also used in MySQL to support UDFs (user-defined functions)
#
# See SECTION 29.4, "ADDING NEW FUNCTIONS TO MYSQL"
#
# A UDF can be regarded as an external stored function. Stored functions share their namespace with
# UDFs.
#
# See SECTION 9.2.4, "FUNCTION NAME PARSING AND RESOLUTION", for the rules describing how the server
# interprets references to different kinds of functions.
#
# To invoke a stored procedure, use the CALL statement (see SECTION 13.2.1, "CALL SYNTAX")
#
# To invoke a stored function, refer to it in an expression. The function returns a
# value during expression evaluation.
#
# CREATE_PROCEDURE and CREATE_FUNCTION require the CREATE_ROUTINE privilege.
#
# They might also require the SET_USER_ID or SUPER privilege, depending on the
# DEFINER value, as described later in this section.
#
# If binary logging is enabled, CREATE_FUNCTION might require the SUPER privilege,
# as described in SECTION 24.7, "BINARY LOGGING OF STORED PROGRAMS"
#
# By default, MySQL automatically grants the ALTER_ROUTINE and EXECUTE privileges
# to the routine creator.
#
# This behavior can be changed by disabling the automatic_sp_privileges system variable.
#
# See SECTION 24.2.2, "STORED ROUTINES AND MYSQL PRIVILEGES"
#
# The DEFINER and SQL SECURITY clauses specify the security context to be used when
# checking access privileges at routine execution time, as described later in this section.
#
# If the routine name is the same as the name of a built-in SQL function, a syntax error occurs
# unless you use a space between the name and the following parenthesis when defining
# the routine or invoking it later.
#
# For this reason, avoid using the names of existing SQL functions for your own stored routines.
#
# The IGNORE_SPACE SQL mode applies to built in functions, not to stored routines.
#
# IT is always permissibile to have spaces after a stored routine name, regardless
# of whether IGNORE_SPACE is enabled.
#
# The parameter list enclosed within parentheses must always be present.
#
# If there are no parameters, an empty parameter list of () should be used.
#
# Parameter names are not case sensitive.
#
# Each parameter is an IN parameter by default. To specify otherwise for a parameter,
# use the keyword OUT or INOUT before the parameter name.
#
# NOTE:
#
# 		Specifying a parameter as IN, OUT, or INOUT is valid only for a PROCEDURE.
#
# 		For a FUNCTION, parameters are always regarded as IN parameters.
#
# An IN parameter passes a value into a procedure.
#
# The procedure might modify the value, but the modification is not visible
# to the caller when the procedure returns.
#
# An OUT parameter passes a value from the procedure back to the caller.
#
# Its initial value is NULL within the procedure, and its value is visible
# to the caller when the procedure returns.
#
# An INOUT parameter is initialized by the caller, can be modified by the
# procedure, and any change made by the procedure is visible to the caller
# when the procedure returns.
#
# For each OUT or INOUT parameter, pass a user-defined variable in the CALL
# statement that invokes the procedure so that you can obtain its value
# when the procedure returns.
#
# If you are calling the procedure from within another stored procedure or
# function, you can also pass a routine parameter or local routine variable
# as an OUT or INOUT parameter.
#
# If you are calling the procedure from within a trigger, you can also pass
# NEW.col_name as an OUT or INOUT parameter.
#
# For information about the effect of unhandled conditions on procedure params,
# see SECTION 13.6.7.8, "CONDITION HANDLING AND OUT OR INOUT PARAMETERS"
#
# Routine parameters cannot be referenced in statements prepared within the
# routine; see SECTION C.1, "RESTRICTIONS ON STORED PROGRAMS"
#
# The following example shows a simple stored procedure that uses an OUT parameter:
#
# 		delimiter //
#
# 		CREATE PROCEDURE simpleproc (OUT param1 INT)
# 		BEGIN
# 			SELECT COUNT(*) INTO param1 FROM t;
# 		END//
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		delimiter ;
#
# 		CALL simpleproc(@a);
# 		Query OK, 0 rows affected (0.00 sec)
# 
# 		SELECT @a;
# 		+---------+
# 		| @a 		 |
# 		+---------+
# 		| 3 		 |
# 		+---------+
# 		1 row in set (0.00 sec)
#
# The example uses the mysql client delimiter command to change the statement
# delimiter from ; to // while the procedure is being defined.
#
# This enables the ; delimiter used in the procedure body to be passed through
# to the server rather than being interpreted by mysql itself.
#
# See SECTION 24.1, "DEFINING STORED PROGRAMS"
#
# The RETURNS clause may be specified only for a FUNCTION, for which it is mandatory.
#
# It indicates the return type of the function, and the function body must contain
# a RETURN value statement.
#
# If the RETURN statement returns a value of a different type, the value is coerced
# to the proper type.
#
# For example, if a function specifies an ENUM or SET value in the RETURNS clause,
# but the RETURN statement returns an integer, the value returned from the function
# is the string for the corresponding ENUM member of set of SET members.
#
# The following example function takes a parameter, performs an operation using an
# SQL function, and returns the result.
#
# In this case, it is unnecessary to use delimiter because the function definition
# contains no internal ; statement delimiters:
#
# 			CREATE FUNCTION hello (s CHAR(20))
# 			RETURNS CHAR(50) DETERMINISTIC
# 			RETURN CONCAT('Hello, ',s,'!');
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		SELECT hello('world');
# 		+--------------------+
# 		| hello('world') 		|
# 		+--------------------+
# 		| Hello, world! 		|
# 		+--------------------+
# 		1 row in set (0.00 sec)
#
# Parameter types and function return types can be declared to use any valid data type.
#
# The COLLATE attribute can be used if preceded by a CHARACTER SET specification.
#
# The routine_body consists of a valid SQL routine statement.
#
# This can be a simple statement such as SELECT or INSERT, or a compound statement
# written using BEGIN and END.
#
# Compound statements can contain declarations, loops, and other control structure
# statements.
#
# The syntax for these statements is described in SECTION 13.6, "COMPOUND-STATEMENT SYNTAX"
#
# MySQL permits routines to contain DDL statements, such as CREATE and DROP.
#
# MySQL also permits stored procedures (but not stored functions) to contain
# SQL transaction statements such as COMMIT.
#
# Stored functions may not contain statements that perform explicit or implicit
# commit or rollback.
#
# Support for these statements is not required by the SQL standard, which states
# that  each DBMS vendor may decide whether to permit them.
#
# Statements that return a result can be used within a stored procedure but not
# within a stored function.
#
# This prohibition includes SELECT statements that do not have an INTO var_list clause
# and other statements such as SHOW, EXPLAIN and CHECK_TABLE
#
# FOr statements that cna be determined at function definition time to return a result
# set, a Not allowed to return a result set from a function error occurs (ER_SP_NO_RETSET)
#
# For statements that can be determined only at runtime to return a result set, a 
# PROCEDURE %s can't return a result set in the given context error occurs (ER_SP_BADSELECT)
#
# USE statements within stored routines are not permitted.
#
# When a routine is invoked, an implicit USE db_name is performed (and undone when the
# routine terminates)
#
# The causes the routine to have the given default database while it executes.
#
# References to objects in databases other than the routine default database
# should be qualified with the appropriate database name.
#
# For additional information about statements that are not permitted in stored routines,
# see SECTION C.1, "RESTRICTIONS ON STORED PROGRAMS"
#
# For information about invoking stored procedures from within programs written in a language
# that has a MySQL interface, see SECTION 13.2.1, "CALL SYNTAX"
#
# MySQL stores the sql_mode system variable setting in effect when a routine is created or
# altered, and always executes the routine with this setting in force, regardless of the
# current server SQL mode when the routine begins executing.
#
# The switch from the SQL mode of the invoker to that of the routine occurs after evaluation
# of arguments and assignment of the resulting values to routine parameters.
#
# If you define a routine in strict SQL mode but invoke it in nonstrict mode, assignment
# of arguments to routine parameters does not take place in strict mode.
#
# If you require that expressions passed to a routine be assigned in strict SQL mode, you
# should invoke the routine with strict mode in effect.
#
# The COMMENT characteristic is a MySQL extension, and may be used to describe the stored
# routine.
#
# This information is displayed by the SHOW_CREATE_PROCEDURE and SHOW_CREATE_FUNCTION statements.
#
# The LANGUAGE characteristic indicates the language in which the routine is written.
#
# The server ignores the characteristic; only SQL routines are supported.
#
# A routine is considered "determinsitic" if it always produces the same result for the
# same input params, and "not deterministic" otherwise.
#
# If neither DETERMINISTIC nor NOT DETERMINISTIC is given in the routine definition, the default
# is NOT DETERMINSITIC.
#
# To declare that a function is deterministic, you must specify DETERMINISTIC explicitly.
#
# ASsessment of the nature of a routine is based on the "honesty" of the creator:
#
# MySQL does not  check that a routien declared DETERMINSITIC is free of statements that produce
# RNG results.
#
# However, misdeclaring a routine might affect results or affect performance.
#
# Declaring a nondeterministic routine as DETERMINISTIC might lead to unexpected
# results by causing the optimizer to make incorrect execution plan choices.
#
# Declaring a deterministic routine as NONDETERMINISTIC might diminish performance
# by causing available optimizations not ot be used.
#
# If binary logging is enabled, the DETERMINISTIC characteristic affects which routine
# definitions MySQL accepts.
#
# See SECTION 24.7, "BINARY LOGGING OF STORED PROGRAMS"
#
# A routine that contains the NOW() function (or its synonyms) or RAND()
# is RNG, but it might still be replication-safe.
#
# For NOW(), the binary log includes the timestamp and replicates correctly.
#
# RAND() also replicates correctly as long as it is called only a single
# time during the execution of a routine.
#
# (You can consider the routine execution timestamp and random number seed as implicit
# inputs that are identical on the master and slave)
#
# Several characteristics provide information about the nature of data use by the routine.
#
# In MySQL, these characteristics are advisory only.
#
# THe server does not use them to constrain what kinds of statements a routine will be
# permitted to execute.
#
# 		) CONTAINS SQL indicates that the routine does not contain statements that read or write data.
#
# 			This is the default if none of these characteristics is given explicitly.
#
# 			Examples of such statements are SET @x = 1 or DO RELEASE_LOCK('abc'), which execute
# 			but neither read nor write data.
#
# 		) NO SQL indicates that the routine contains no SQL statements.
#
# 		) READS SQL DATA indicates that the routine contains statements that read data (for example, SELECT),
# 			but not statements that write data.
#
# 		) MODIFIES SQL DATA indicates that the routine contains statements that may write data (for example, INSERT or DELETE)
#
# The SQL SECURITY characteristic can be DEFINER or INVOKER to specify the security context; that is, whether the
# routine executes using the privileges of the account named in the routine DEFINER clause or the user who
# invokes it.
#
# This account must have permission to access the database with which the routine is associated.
#
# The default value is DEFINER.
#
# The user who invokes the routine must have the EXECUTE privilege for it, as must the DEFINER account
# if the routine executes in definer security context.
#
# The DEFINER clause specifies the MySQL account to be used when checking access privileges at routine
# execution time for routines that have the SQL SECURITY DEFINER characteristic.
#
# If a user value is given for the DEFINER clause, it should be a MySQL account specified as 
# 'user_name'@'host_name', CURRENT_USER or CURRENT_USER()
#
# THe default DEFINER value is the user who executes the CREATE_PROCEDURE or CREATE_FUNCTION
# statement.
#
# This is the same as specifying DEFINER = CURRENT_USER explicitly.
#
# If you specify the DEFINER clause, these rules determine the valid DEFINER user values:
#
# 		) If you do not have the SET_USER_ID or SUPER privilege, the only permitted user value is
# 			your own account, either specified literally or by using CURRENT_USER.
#
# 			You cannot set the definer to some other account.
#
# 		) If you have the SET_USER_ID or SUPER privilege, you can specify any syntactically
# 		valid account name.
#
# 		If the account does not exist, a warning is generated.
#
# 		) ALthough it is possible to create a routine with a nonexistent DEFINER account,
# 			an error occurs at routine execution time if the SQL SECURITY value is DEFINER
# 			but the definer account does not exist.
#
# For more information about stored routine security, see SECTION 24.6, "ACCESS CONTROL FOR STORED PROGRAMS AND VIEWS"
#
# Within a stored routine that is defined with the SQL SECURITY DEFINER characteristic, CURRENT_USER
# returns the routine's DEFINER value.
#
# For information about user auditing within stored routines, see SECTION 6.3.13, "SQL-BASED MYSQL ACCOUNT ACTIVITY AUDITING"
#
# Consider the following procedure, which displays a count of the number of MySQL accounts listed in
# the mysql.user system table:
#
# 		CREATE DEFINER = 'admin'@'localhost' PROCEDURE account_count()
# 		BEGIN
# 			SELECT 'Number of accounts:', COUNT(*) FROM mysql.user;
# 		END;
#
# The procedure is assigned a DEFINER account of 'admin'@'localhost' no matter which user defines it.
#
# It executes with the privileges of that account no matter which user invokes it
# (because the default security characteristic is DEFINER)
#
# THe procedure succeeds or fails depending on whether invoker has the EXECUTE
# privilege for it and 'admin'@'localhost' has the SELECT privilege for the mysql.user table
#
# Now suppose that the procedure is defined with the SQL SECURITY INVOKER characteristic:
#
# 		CREATE DEFINER = 'admin'@'localhost' PROCEDURE account_count()
# 		SQL SECURITY INVOKER
# 		BEGIN
# 			SELECT 'Number of accounts:', COUNT(*) FROM mysql.user;
# 		END;
#
# The procedure still has a DEFINER of 'admin'@'localhost', but in this case, it executes
# with the privileges of the invoking user.
#
# Thus, the procedure succeeds or fails depending on whether the invoker has the EXECUTE
# privilege for it and the SELECT privilege for the mysql.user table
#
# THe server handles the data type of a routine parameter, local routine variable created
# with DECLARE, or function return value as follows:
#
# 		) Assignments are checked for data type mismatches and overflow.
#
# 			Conversion and overflow problems result in warnings, or errors in strict SQL mode.
#
# 		) Only scalar values can be assigned. For example, a statement such as SET x = (SELECT 1, 2) is invalid
#
# 		) For character data types, if CHARACTER SET is included in the declaration, the specified character set
# 			and its default collation is used.
#
# 			IF the COLLATE attribute is also present, that collation is used rather than the default collation.
#
# 			If CHARACTER SET and COLLATE are not present, the database character set and collation in effect
# 			at routine creation time are used.
#
# 			To avoid having the server use the database character set and collation, provide an explicit
# 			CHARACTER SET and a COLLATE attribute for character data parameters.
#
# 			If you change the database default character set or collation, stored routines that use
# 			the database defaults must be dropped and recreated so that they use the new defaults.
#
# 			The database character set and collation are given by the value of the character_set_database
# 			and collation_database system variables.
#
# 			For more information, see SECTION 10.3.3, "DATABASE CHARACTER SET AND COLLATION"
#
# 13.1.18 CREATE SERVER SYNTAX
#
# CREATE SERVER server_name
# 		FOREIGN DATA WRAPPER wrapper_name
# 		OPTIONS (option [, option] ---)
#
# option:
# 		{ HOST character-literal
# 		| DATABASE character-literal
#		| USER character-literal
# 		| PASSWORD character-literal
# 		| SOCKET character-literal
# 		| OWNER character-literal
# 		| PORT numeric-literal }
#
# This statement creates the definition of a server for use with the FEDERATED storage engine.
#
# The CREATE SERVER statement creates a new row in the servers table in the mysql database.
#
# This statement requires the SUPER privilege.
#
# The server_name should be a unique reference to the server. Server definitions are global
# within the scope of the server, it is not possible to qualify the server definition to
# a specific database.
#
# server_name has a maximum length of 64 characters (names longer than 64 chars are silently
# truncated), and is case insensitive.
#
# You may specify the name as a quoted string.
#
# The wrapper_name is an identifier and may be quoted with single quotation marks.
#
# For each option you must specify either a character literal or numeric literal
#
# Character literals are UTF-8, support a max length of 64 chars and default to a
# blank (empty) string.
#
# String literals are silently truncated to 64 chars.
#
# Numeric literals must be a number between 0 and 9999, default value is 0.
#
# NOTE:
#
# 		The OWNER option is currently not applied, and has no effect on the ownership or operation
# 		of the server connection that is created.
#
# The CREATE SERVER statement creates an entry in the mysql.servers table that can later
# be used with the CREATE_TABLE statement when creating a FEDERATED table.
#
# The options that you specify will be used to populate the columns in the mysql.servers table.
#
# The table columns are Server_name, Host, Db, Username, Password, Port and Socket.
#
# For example:
#
# 		CREATE SERVER s
# 		FOREIGN DATA WRAPPER mysql
# 		OPTIONS (USER 'Remote', HOST '198.51.100.106', DATABASE 'test');
#
# Be sure to specify all options necessary to establish a connection to the server.
#
# The user name, host name, and DB name are mandatory.
#
# oTher options might be required as well, such as Password.
#
# The data stored in teh table can be used when creating a connection to a FEDERATED table:
#
# 		CREATE TABLE t (s1 INT) ENGINE=FEDERATED CONNECTION='s';
#
# For more information, see SECTION 16.8, "THE FEDERATED STORAGE ENGINE"
#
# CREATE SERVER causes an implicit commit. See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# CREATE SERVER is not written to the binary log, regardless of the logging format that is in use.
#
# 13.1.19 CREATE SPATIAL REFERENCE SYSTEM SYNTAX
#
# CREATE OR REPLACE SPATIAL REFERENCE SYSTEM
# 		srid srs_attribute
#
# CREATE SPATIAL REFERENCE SYSTEM
# 		[IF NOT EXISTS]
# 		srid srs_attribute ---
#
# srs_attribute: {
# 		NAME 'srs_name'
# 	 | DEFINITION 'definition'
# 	 | ORGANIZATION 'org_name' IDENTIFIED BY org_id
# 	 | DESCRIPTION 'description'
# }
#
# srid, org_id: 32-bit unsigned integer
#
# This statement creates a spatial reference system (SRS) definition and stores it in the data dictionary.
#
# THe definition can be inspected using the INFORMATION_SCHEMA ST_SPATIAL_REFERENCE_SYSTEMS table.
#
# This statement requires the SUPER privilege.
#
# If neither OR REPLACE nor IF NOT EXISTS is specified, an error occurs if an SRS definition with the 
# SRID value already exists.
#
# With CREATE OR REPLACE syntax, any existing SRS Definition with the same SRID value is replaced,
# unless the SRID value is used by some column in an existing table.
#
# In taht case, an error occurs.
#
# For example:
#
# 		CREATE OR REPLACE SPATIAL REFERENCE SYSTEM 4326 ---;
# 		ERROR 3716 (SR005): Can't modify SRID 4326.
#
# 		There is at least one column depending on it.
#
# To identify which column or columns use the SRID, use this query:
#
# 		SELECT * FROM INFORMATION_SCHEMA.ST_GEOMETRY_COLUMNS WHERE SRS_ID=4326;
#
# With CREATE_---_IF NOT EXISTS syntax, any existing SRS definition with the same SRID value
# causes the new definition to be ignored and a warning occurs.
#
# SRID values must be in the range of 32-bit unsigned integers, with these restrictions:
#
# 		) SRID 0 is a valid SRID but cannot be used with CREATE_SPATIAL_REFERENCE_SYSTEM
#
# 		) If the value is in a reserved SRID range, a warning occurs.
#
# 			Reserved ranges are [0, 32767] (reserved by EPSG), [60,000,000,69,999,999,999] (reserved by EPSG),
# 			and [2,000,000,000, 2,147,483,647] (reserved by MySQL)
#
# 		) Uses should not create SRSs with SRIDs in the reserved ranges.
#
# 			Doing so runs the risk that the SRIDs will conflict with future SRS
# 			definitions distributed with MySQL, with the reuslt that hte new system-provided
# 			SRS are not installed for MySQL upgrades or that hte user-defined SRSs are overwritten.
#
# Attributes for the statement must satisfy these conditions:
#
# 		) Attributes can be given in any order, but not attribute can be given more than once
#
# 		) The NAME and DEFINITION attributes are mandatory
#
# 		) The NAME srs_name attribute value must be unique. The combination of the ORGANIZATION
# 			org_name and org_id attribute values must be unique.
#
# 		) The NAME srs_name attribute value and ORGANIZATION org_name attribute value cannot be
# 			empty or begin or end with whitespace.
#
# 		) String values in attribute specifications cannot contain control chars, including newline
#
# 		) The following table shows hte max lengths for string attrib values
#
# 			TABLE 13.6 CREATE SPATIAL REFERENCE SYSTEM ATTRIBUTE LENGTHS
#
# 			Attribute 			Max length (chars)
#
# 			NAME 					80
#
# 			DEFINITION 			4096
#
# 			ORGANIZATION 		256
#
# 			DESCRIPTION 		2048
#
# Here is an example CREATE_SPATIAL_REFERENCE_SYSTEM statement.
#
# The DEFINITION value is reformatted across multiple lines for readability.
#
# (For the statement to be legal, the value actually must be given on a single line)
#
# 		CREATE SPATIAL REFERENCE SYSTEM 4120
# 		NAME 'Greek'
# 		ORGANIZATION 'EPSG' IDENTIFIED BY 4120
# 		DEFINITION
# 			'GEOGCS["Greek",DATUM["Greek",SPHEROID["Bessel 1841",
# 			<numbers>, etc.';
#
# The grammar for SRS definition is based on teh grammar defined in OpenGIS Implementation Spec: Coordinate tarnsformation services.
#
# Revision 1.00, OGC 01-009, January 12, 2001, Section 7.2
#
# This specification exists at <link>
#
# MySQL incorporates these changes to the spec:
#
# 		) Only the <horz cs> production rule is implemented (that is, geographic and projected SRSs)
#
# 		) There is an optional, nonstandard <authority> clause for <parameter>
#
# 			This makes it possible to recognize projection parameters by authority instead of name
#
# 		) SRS definitions may not contain newlines
#
# 13.1.20 CREATE TABLE SYNTAX
#
# 13.1.20.1 CREATE TABLE STATEMENT RETENTION
# 13.1.20.2 FILES CREATED BY CREATE TABLE
# 13.1.20.3 CREATE TEMPORARY TABLE SYNTAX
# 13.1.20.4 CREATE TABLE --- LIKE SYNTAX
# 13.1.20.5 CREATE TABLE --- SELECT SYNTAX
#
# 13.1.20.6 USING FOREIGN KEY CONSTRAINTS
# 13.1.20.7 SILENT COLUMN SPECIFICATION CHANGES
# 13.1.20.8 CREATE TABLE AND GENERATED COLUMNS
# 13.1.20.9 SECONDARY INDEXES AND GENERATED COLUMNS
# 
# 13.1.20.10 SETTING NDB_TABLE OPTIONS
#
# CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
# 		(create_definition, ---)
# 		[table_options]
# 		[partition_options]
#
# CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
# 		[(create_definition,---)]
# 		[table_options]
# 		[partition_options]
# 		[IGNORE | REPLACE]
# 		[AS] query_expression
#
# CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
# 		{ LIKE old_tbl_name | (LIKE old_tbl_name) }
#
# create_definition:
# 		col_name column_definition
# 	 | [CONSTRAINT [symbol]] PRIMARY KEY [index_type] (key_part,---)
# 		[index_option] ---
# 	 | {INDEX|KEY} [index_name] [index_type] (key_part,---)
# 		[index_option] ---
#
# 	 | [CONSTRAINT [symbol]] UNIQUE [INDEX|KEY]
# 			[index_name] [index_type] (key_part,---)
# 			[index_option] ---
# 	 | {FULLTEXT|SPATIAL} [INDEX|KEY] [index_name] (key_part,---)
# 			[index_option] ---
# 	 | [CONSTRAINT [symbol]] FOREIGN KEY
# 			[index_name] (col_name, ---) reference_definition
#   | CHECK (expr)
#
# column_definition:
# 		data_type [NOT NULL | NULL] [DEFAULT {literal | (expr)} ]
# 			[AUTO_INCREMENT] [UNIQUE [KEY]] [[PRIMARY] KEY]
# 			[COMMENT 'string']
# 			[COLLATE collation_name]
# 			[COLUMN_FORMAT {FIXED|DYNAMIC|DEFAULT}]
# 			[STORAGE {DISK|MEMORY|DEFAULT}]
# 			[reference_definition]
# 		| data_type
# 			[GENERATED ALWAYS] AS (expression)
# 			[VIRTUAL | STORED] [NOT NULL | NULL]
# 			[UNIQUE [KEY]] [[PRIMARY] KEY]
# 			[COMMENT 'string']
#
# data_Type:
# 		(see Chapter 11, Data types)
#
# key_part: {col_name [(length)] | (expr)} [ASC | DESC]
#
# index_type:
# 		USING {BTREE | HASH}
#
# index_option:
# 		KEY_BLOCK_SIZE [=] value
# 	 | index_type
# 	 | WITH PARSER parser_name
#   | COMMENT 'string'
# 	 | {VISIBLE | INVISIBLE}
#
# reference_definition:
# 		REFERENCES tbl_name (key_part, ---)
# 			[MATCH FULL | MATCH PARTIAL | MATCH SIMPLE]
# 			[ON DELETE reference_option]
# 			[ON UPDATE reference_option]
#
# reference_option:
# 		RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT
#
# table_options:
# 		table_option [[,] table_option] ---
#
# table_option:
# 		AUTO_INCREMENT [=] value
# 	 | AVG_ROW_LENGTH [=] value
#   | [DEFAULT] CHARACTER SET [=] charset_name
# 	 | CHECKSUM [=] {0 | 1}
#   | [DEFAULT] COLLATE [=] collation_name
# 	 | COMMENT [=] 'string'
# 	 | COMPRESSION [=] {'ZLIB'|'LZ4'|'NONE'}
# 	 | CONNECTION [=] 'connect_string'
#   | {DATA|INDEX} DIRECTORY [=] 'absolute path to directory'
# 	 | DELAY_KEY_WRITE [=] {0 | 1}
# 	 | ENCRYPTION [=] {'Y' | 'N'}
# 	 | ENGINE [=] engine_name
#   | INSERT_METHOD [=] { NO | FIRST | LAST }
#   | KEY_BLOCK_SIZE [=] value
#   | MAX_ROWS [=] value
#   | MIN_ROWS [=] value
#   | PACK_KEYS [=] {0 | 1 | DEFAULT}
#   | PASSWORD [=] 'string'
#   | ROW_FORMAT [=] {DEFAULT|DYNAMIC|FIXED|COMPRESSED|REDUNDANT|COMPACT}
#   | STATS_AUTO_RECALC [=] {DEFAULT|0|1}
#   | STATS_PERSISTENT [=] {DEFAULT|0|1}
#   | STATS_SAMPLE_PAGES [=] value
#   | TABLESPACE tablespace_name [STORAGE {DISK|MEMORY|DEFAULT}]
#   | UNION [=] (tbl_name[,tbl_name]---)
#
# partition_options:
# 		PARTITION BY
# 			{ [LINEAR] HASH(expr)
# 			| [LINEAR] KEY [ALGORITHM={1|2}] (column_list)
# 			| RANGE{(expr) | COLUMNS(column_list)}
# 			| LIST{(expr) | COLUMNS(column_list)} }
# 		[PARTITIONS num]
# 		[SUBPARTITION BY
# 			{ [LINEAR] HASH(expr)
# 			| [LINEAR] KEY [ALGORITHM={1|2}] (column_list) }
# 		 [SUBPARTITIONS num]
# 		]
# 		[(partition_definition [, partition_definition] ---)]
#
# partition_definition:
# 		PARTITION partition_name
# 			[VALUES
# 				{LESS THAN {(expr | value_list) | MAXVALUE}
# 				|
# 				IN (value_list)}]
# 			[[STORAGE] ENGINE [=] engine_name]
# 			[COMMENT [=] 'string' ]
# 			[DATA DIRECTORY [=] 'data_dir']
# 			[INDEX DIRECTORY [=] 'index_dir']
# 			[MAX_ROWS [=] max_number_of_rows]
# 			[MIN_ROWS [=] min_number_of_rows]
# 			[TABLESPACE [=] tablespace_name]
# 			[(subpartition_definition [, subpartition_definition] ---)]
#
# subpartition_definition:
# 		SUBPARTITION logical_name
# 			[[STORAGE] ENGINE [=] engine_name]
# 			[COMMENT [=] 'string' ]
# 			[DATA DIRECTORY [=] 'data_dir']
# 			[INDEX DIRECTORY [=] 'index_dir']
# 			[MAX_ROWS [=] max_number_of_rows]
# 			[MIN_ROWS [=] min_number_of_rows]
# 			[TABLESPACE [=] tablespace_name]
#
# query_expression:
# 		SELECT --- (Some valid select or union statement)
#
# CREATE_TABLE creates a table with the given name. You must have the CREATE privilege for the table.
#
# By default, tables are created in the default database, using the InnoDB storage engine.
#
# An error occurs if the table exists, if there is no default database, or if the database
# does not exist.
#
# For information about the physical representation of a table, see SECTION 13.1.20.2, "FILES CREATED BY CREATE TABLE"
#
# The original CREATE_TABLE statement, including all specifications and table options are stored
# by MySQL when the table is created.
#
# For more information, see SECTION 13.1.20.1, "CREATE TABLE STATEMENT RETENTION"
#
# There are several aspects to the CREATE_TABLE statement, described under the following topics
# in this section:
#
# 		) TABLE NAME
#
# 		) TEMPORARY TABLES
#
# 		) CLONING OR COPYING A TABLE
#
# 		) COLUMN DATA TYPE AND ATTRIBUTES
#
# 		) INDEXES AND FOREIGN KEYS
#
# 		) TABLE OPTIONS
#
# 		) CREATING PARTITIONED TABLES
#
# TABLE NAME
#
# 		) tbl_name
#
# 			The table name can be specified as db_name.tbl_name to create the table in a specific database.
#
# 			This works regardless of whether there is a default database, assuming that the database exists.
#
# 			If you use quoted identifiers, quote the database and table names separately.
#
# 			For example, write `mydb`.`mytbl`, not `mydb.mytbl`
#
# 			Rules for permissible table names are given in SECTION 9.2, "SCHEMA OBJECT NAMES"
#
# 		) IF NOT EXISTS
#
# 			Prevents an error from occurring if the table exists.
#
# 			However, there is no verification that the existing table has a structure identical
# 			to that indicated by the CREATE_TABLE statement.
#
# TEMPORARY TABLES
#
# You can use the TEMPORARY keyword when creating a table.
#
# A TEMPORARY table is visible only within the current session, and is dropped automatically
# when the session is closed.
#
# For more information, see SECTION 13.1.20.3, "CREATE TEMPORARY TABLE SYNTAX"
#
# CLONING OR COPYING A TABLE
#
# 		) LIKE
#
# 			Use CREATE TABLE --- LIKE to create an empty table based on the defininition of another
# 			table, including any column attributes and indexes defined in the original table:
#
# 				CREATE TABLE new_tbl LIKE orig_tbl;
#
# 			For more information, see SECTION 13.1.20.4, "CREATE TABLE --- LIKE SYNTAX"
#
# 		) [AS] query_expression
#
# 			To create one table from another, add a SELECT statement at the end of the
# 			CREATE TABLE statement:
#
# 				CREATE TABLE new_tbl AS SELECT * FROM orig_tbl;
#
# 			For more information, see SECTION 13.1.20.5, "CREATE TABLE --- SELECT SYNTAX"
#
# 		) IGNORE|REPLACE
#
# 			The IGNORE and REPLACE options indicate how to handle rows that duplicate unique key
# 			values when copying a table using a SELECT statement.
#
# 			For more information, see SECTION 13.1.20.5, "CREATE TABLE --- SELECT SYNTAX"
#
# COLUMN DATA TYPES AND ATTRIBUTES
#
# There is a hard limit of 4096 columns per table, but the effective maximum may be less
# for a given table and depends on the factors discussed in SECTION C.10.4, "LIMITS ON TABLE COLUMN COUNT AND ROW SIZE"
#
# 		) data_type
#
# 			data_type represents the data type in a column definition.
#
# 			For a full description of the syntax available for specifying column data types,
# 			as well as information about the properties of each type, see CHAPTER 11, DATA TYPES
#
# 				) Some attributes do not apply to all data types.
#
# 					AUTO_INCREMENT applies only to integer and floating-point types.
#
# 					Prior to MySQL 8.0.13, DEFAULT does not apply to the BLOB, TEXT, GEOMETRY
# 					and JSON types.
#
# 				) Character data types (CHAR, VARCHAR, the TEXT types, ENUM, SET and any synonyms) synonyms) can include
# 					CHARACTER SET to specify the character set for the column.
#
# 					CHARSET is a synonym for CHARACTER SET.
#
# 					A collation for the character set can be specified with the COLLATE attribute,
# 					along with any other attributes.
#
# 					For details, see CHAPTER 10, CHARACTER SETS, COLLATIONS, UNICODE. Example:
#
# 						CREATE TABLE t (c CHAR(20) CHARACTER SET utf8 COLLATE utf8_bin);
#
# 					MySQL 8.0 interprets length specifications in character column definitions in characters.
#
# 					Lengths for BINARY and VARBINARY are in bytes.
#
# 				) For CHAR, VARCHAR, BINARY and VARBINARY columns, indexes can be created that use only
# 					the leading part of column values, using col_name(length) syntax to specify an index
# 					prefix length.
#
# 					BLOB and TEXT columns also can be indexed, but a prefix length must be given.
#
# 					Prefix lengths are given in characters for nonbinary string types and in bytes
# 					for binary string types.
#
# 					That is, index entries consist of the first length characters of each column value
# 					for CHAR, VARCHAR, and TEXT columns, and the first length bytes of each column value
# 					for BINARY, VARBINARY, and BLOB columns.
#
# 					Indexing only a prefix of column values like this can make the index file much smaller.
#
# 					For additional information about index prefixes, see SECTION 13.1.15, "CREATE INDEX SYNTAX"
#
# 					Only the InnoDB and MyISAM storage engines support indexing on BLOB and TEXT columns.
#
# 					For example:
#
# 						CREATE TABLE test (blob_col BLOB, INDEX(blob_col(10)));
#
# 					If a specified index prefix exceeds the maximum column data type size, CREATE_TABLE
# 					handles the index as follows:
#
# 						) For a nonunique index, either an error occurs (if strict SQL mode is enabled),
# 							or the index length is reduced to lie within the maximum column data type size
# 							and a warning is produed (if strict SQL mode is not enabled)
#
# 						) For a unique index, an error occurs regardless of SQL mode because reducing the index
# 							length might enable insertion of nonunique entries that do not meet the specified
# 							uniqueness requirement.
#
# 			) JSON columns cannot be indexed.
#
# 				You can work around this restriction by creating an index on a generated column
# 				that extracts a scalar value from the JSON column.
#
# 				See INDEXING A GENERATED COLUMN TO PROVIDE A JSON COLUMN INDEX, for a detailed example.
#
# 		) NOT NULL | NULL
#
# 			If neither NULL nor NOT NULL is specified, the column is treated as though NULL had been specified.
#
# 			In MySQL 8.0, only the InnoDB, MyISAM, and MEMORY storage engines support indexes on columns
# 			that can have NULL values.
#
# 			In other cases, you must declare indexed columns as NOT NULL or an error results
#
# 		) DEFAULT
#
# 			Specifies a default value for a column.
#
# 			For more information about default value handling, including the case that
# 			a column definition includes no explicit DEFAULT value, see SECTION 11.7, "DATA TYPE DEFAULT VALUES"
#
# 			If the NO_ZERO_DATE or NO_ZERO_IN_DATE SQL mode is enabled and a date-valued default is not correct
# 			according to that mode, CREATE_TABLE produces a warning if strict SQL mode is not enabled and an
# 			error if strict mode is enabled.
#
# 			For example, with NO_ZERO_IN_DATE enabled, c1 DATE DEFAULT '2010-00-00' produces a warning.
#
# 		) AUTO_INCREMENT
#
# 			An integer or floating-point column can have the additional attribute AUTO_INCREMENT.
#
# 			When you insert a value of NULL (recommended) or 0 into an indexed AUTO_INCREMENT column,
# 			the column is set to the next sequence value.
#
# 			Typically this is value+1, where value is the largest value for the column currently
# 			in the table.
#
# 			AUTO_INCREMENT sequences begin with 1.
#
# 			To retrieve an AUTO_INCREMENT value after inserting a row, use the LAST_INSERT_ID()
# 			SQL function or the mysql_insert_id() C API function
#
# 			See SECTION 12.15, "INFORMATION FUNCTIONS" and SECTION 28.7.7.38, "mysql_insert_id()"
#
# 			If the NO_AUTO_VALUES_ON_ZERO SQL mode is enabled, you can store 0 in AUTO_INCREMENT
# 			columns as 0 without generating a new sequence value.
#
# 			See SECTION 5.1.11, "SERVER SQL MODES"
#
# 			There can be only one AUTO_INCREMENT column per table, it must be indexed, and it cannot
# 			have a DEFAULT value.
#
# 			An AUTO_INCREMENT column works properly only if it contains only positive values.
#
# 			Inserting a negative number is regarded as inserting a very large positive number.
#
# 			This is done to avoid precision problems when numbers "wrap" over from positive to
# 			negative and also to ensure that you do not accidentally get an AUTO_INCREMENT
# 			column that contains 0.
#
# 			For MyISAM tables, you can specify an AUTO_INCREMENT secondary column in a multiple-column
# 			key.
#
# 			See SECTION 3.6.9, "USING AUTO_INCREMENT"
#
# 			To make MySQL compatible with some ODBC applications, you can find the AUTO_INCREMENT
# 			value for the last inserted row with the following query:
#
# 				SELECT * FROM tbl_name WHERE auto_col IS NULL
#
# 			This method requires that sql_auto_is_null variable is not set to 0.
#
# 			See SECTION 5.1.8, "SERVER SYSTEM VARIABLES"
#
# 			For information about InnoDB and AUTO_INCREMENT, see SECTION 15.6.1.4,
# 			"AUTO_INCREMENT HANDLING IN INNODB"
#
# 			For information about AUTO_INCREMENT and MySQL Replication, see 
# 			SECTION 17.4.1.1, "REPLICATION AND AUTO_INCREMENT"
#
# 		) COMMENT
#
# 			A comment for a column can be specified with the COMMENT option, up to
# 			1024 characters long.
#
# 			The comment is displayed by the SHOW_CREATE_TABLE and SHOW_FULL_COLUMNS
# 			statements.
#
# 		) COLUMN_FORMAT
#
# 			In NDB Cluster, it is also possible to specify a data storage format for
# 			individual columns of NDB tables using COLUMN_FORMAT.
#
# 			Permissible column formats are FIXED, DYNAMIC and DEFAULT.
#
# 			FIXED is used to specify fixed-width storage, DYNAMIC permits the column
# 			to be variable-width, and DEFAULT causes the column to use fixed-width
# 			or variable-width storage as determined by the column's data type
# 			(possibly overridden by a ROW_FORMAT specifier)
#
# 			For NDB tables, the default value for COLUMN_FORMAT is FIXED.
#
# 			COLUMN_FORMAT currently has no effect on columns of tables using storage engines
# 			other than NDB.
#
# 			MySQL 8.0 silently ignores COLUMN_FORMAT
#
# 		) STORAGE
#
# 			For NDB tables, it is possible to specify whether the column is stored on disk or
# 			in memory by using a STORAGE clause.
#
# 			STORAGE DISK causes the column to be stored on disk, and STORAGE MEMORY causes
# 			in-memory storage to be used.
#
# 			The CREATE_TABLE statement used must still include a TABLESPACE clause:
#
# 				CREATE TABLE t1 (
# 					c1 INT STORAGE DISK,
# 					c2 INT STORAGE MEMORY
# 				) ENGINE NDB;
# 				ERROR 1005 (HY000): Can't create table 'c.t1' (errno: 140)
#
# 				CREATE TABLE t1 (
# 					c1 INT STORAGE DISK,
# 					c2 INT STORAGE MEMORY
# 				) TABLESPACE ts_1 ENGINE NDB;
# 				Query OK, 0 rows affected (1.06 sec)
#
# 			For NDB tables, STORAGE DEFAULT is equivalent to STORAGE MEMORY.
#
# 			The STORAGE clause has no effect on tables using storage engines other than
# 			NDB.
#
# 			The STORAGE keyword is supported only in the build of mysqld that is supplied
# 			with NDB Cluster; it is not recognized in any other version of MySQL, where
# 			any attempt to use the STORAGE keyword cause a syntax error.
#
# 		) GENERATED ALWAYS
#
# 			Used to specify a generated column expression.
#
# 			For information about generated columns, see SECTION 13.1.20.8, "CREATE TABLE AND GENERATED COLUMNS"
#
# 			Stored generated columns can be indexed.
#
# 			InnoDB supports secondary indexes on virtual generated columns.
# 			See SECTION 13.1.20.9, "SECONDARY INDEXES AND GENERATED COLUMNS"
#
# INDEXES AND FOREIGN KEYS
#
# Several keywords apply to creation of indexes and foreign keys.
#
# For general background in addition to the following descriptions, see
# SECTION 13.1.15, "CREATE INDEX SYNTAX" and SECTION 13.1.20.6, "USING FOREIGN KEY CONSTRAINTS"
#
# 		) CONSTRAINT <SYMBOL>
#
# 			If the CONSTRAINT symbol clause is given, the symbol value, if used, must be unique
# 			in the database.
#
# 			A duplicate symbol results in an error.
#
# 			If the clause is not given, or a symbol is not included following the CONSTRAINT keyword,
# 			a name for the constraint is created automatically.
#
# 		) PRIMARY KEY
#
# 			A unique index where all key columns must be defined as NOT NULL.
#
# 			If they are not explicitly declared as NOT NULL, MySQL declares them so
# 			implicitly (and silently)
#
# 			A table can have only one PRIMARY KEY. 
#
# 			The name of a PRIMARY KEY is always PRIMARY, which thus cannot be used
# 			as the name for any other kind of index.
#
# 			If you do not have a PRIMARY KEY and an application asks for the PRIMARY KEY
# 			in your tables, MySQL returns the first UNIQUE index that has no NULL
# 			columns as the PRIMARY KEY.
#
# 			In InnoDB tables, keep the PRIMARY KEY short to minimize storage overhead
# 			for secondary indexes.
#
# 			Each secondary index entry contains a copy of the primary key columns
# 			for the corresponding row. (See SECTION 15.6.2.1, "CLUSTERED AND SECONDARY INDEXES")
#
# 			In the created table, a PRIMARY KEY is placed first, followed by all UNIQUE indexes,
# 			and then the nonunique indexes.
#
# 			This helps the MySQL optimizer to prioritize which index to use and also
# 			more quickly to detect duplicated UNIQUE keys.
#
# 			A PRIMARY KEY can be a multiple-column index.
#
# 			However, you cannot create a multiple-column index using the PRIMARY KEY
# 			attribute in a column specification.
#
# 			Doing so only marks that single column as primary.
#
# 			You must use a separate PRIMARY KEY(key_part, ---) clause
#
# 			If a table has a PRIMARY KEY or UNIQUE NOT NULL index that consists
# 			of a single column that has an integer type, you can use _rowid
# 			to refer to the indexed column in SELECT statements, as described
# 			in Unique Indexes.
#
# 			In MySQL, the name of a PRIMARY KEY is PRIMARY.
#
# 			Foro ther indexes, if you do not assign a name, the index is assigned
# 			the same name as the first indexed column, with an optional suffix
# 			(_2,_3,---) to make it unique.
#
# 			You can see index names for a table using SHOW INDEX FROM tbl_name.
#
# 			See SECTION 13.7.6.22, "SHOW INDEX SYNTAX"
#
# 		) KEY | INDEX
#
# 			KEY is normally a synonym for INDEX.
#
# 			The key attribute PRIMARY KEY can also be specified as just KEY when given
# 			in a column definition.
#
# 			This was implemented for compatibility with other database systems.
#
# 		) UNIQUE
#
# 			A UNIQUE index creates a constraint such that all values in the index must be distinct.
#
# 			An error occurs if you try to add a new row with a key value that matches an
# 			existing row.
#
# 			For all engines, a UNIQUE index permits multiple NULL values for columns that
# 			can contain NULL.
#
# 			If you specify a prefix value for a column in a UNIQUE index, the column values
# 			must be unique within the prefix length.
#
# 			If a table has a PRIMARY KEY or UNIQUE NOT NULL index that consists of a single
# 			column that has an integer type, you can use _rowid to refer to the indexed
# 			column in SELECT statements, as described in UNIQUE INDEXES.
#
# 		) FULLTEXT
#
# 			A FULLTEXT index is a special type of index used for full-text searches.
#
# 			Only the InnoDB and MyISAM storage engines support FULLTEXT indexes.
#
# 			They can be created only from CHAR, VARCHAR, and TEXT columns.
#
# 			Indexing always happens over the entire column; column prefix indexing
# 			is not supported and any prefix length is ignored if specified.
#
# 			See SECTION 12.9, "FULL-TEXT SEARCH FUNCTIONS", for details of operation.
#
# 			A WITH PARSER clause can be specified as an index_option value to associate
# 			a parser plugin with the index if full-text indexing and searching operations
# 			need special handling.
#
# 			This clause is valid only for FULLTEXT indexes.
#
# 			InnoDB and MyISAM support full-text parser plugins.
#
# 			See FULL-TEXT PARSER PLUGINS and SECTION 29.2.4.4, "WRITING FULL-TEXT PARSER PLUGINS"
# 			for more information.
#
# 		) SPATIAL
#
# 			You can create SPATIAL indexes on spatial data types.
#
# 			Spatial types are supported only for InnoDB and MyISAM tables, and indexed
# 			columns must be declared as NOT NULL.
#
# 			See SECTION 11.5, "SPATIAL DATA TYPES"
#
# 		) FOREIGN KEY
#
# 			MySQL supports foreign keys, which let you cross-reference related data
# 			across tables, and foreign key constraints, which help keep this spread-out
# 			data consistent.
#
# 			For definition and option information, see REFERENCE_DEFINITION and REFERENCE_OPTION
#
# 			Partitioned tables employing the InnoDB storage engine do not support foreign keys.
#
# 			See SECTION 23.6, "RESTRICTIONS AND LIMITATIONS ON PARTITIONING", for more information.
#
# 		) CHECK
#
# 			The CHECK clause is parsed but ignored by all storage engines.
#
# 			See SECTION 1.8.2.3, "FOREIGN KEY DIFFERENCES"
#
# 		) key_part
#
# 			) A key_part specification can end with ASC or DESC to specify whether index values
# 				are stored in ascending or descending order.
#
# 				The default is ascending if no order specifier is given.
#
# 			) Prefixes, defined by the length attribute, can be up to 767 bytes long for InnoDB tables
# 				that use the REDUNDANT or COMPACT row format.
#
# 				The prefix length limit is 3072 bytes for InnoDB tables that use the DYNAMIC
# 				or COMPRESSED row format.
#
# 				For MyISAM tables, the prefix length limit is 1000 bytes.
#
# 				Prefix limits are measured in bytes.
#
# 				However, prefix lengths for index specifications in CREATE_TABLE,
# 				ALTER_TABLE and CREATE_INDEX statements are interpreted as number of
# 				characters for nonbinary string types (CHAR, VARCHAR, TEXT) and number of
# 				bytes for binary string types (BINARY, VARBINARY, BLOB)
#
# 				Take this into account when specifying a prefix length for a nonbinary
# 				string column that uses a multibyte character set.
#
# 		) index_type
#
# 			Some storage engine permit you to specify an index type when creating an index.
#
# 			The syntax for the index_type specifier is USING type_name
#
# 			Example:
#
# 				CREATE TABLE lookup
# 					(id INT, INDEX USING BTREE (id))
# 					ENGINE = MEMORY;
#
# 			The preferred position for USING is after the index column list.
#
# 			It can be given before the column list, but support for use of the option
# 			in that position is deprecated and will be removed in a future release.
#
# 		) index_option
#
# 			index_option values specify additional options for an index
#
# 				) KEY_BLOCK_SIZE
#
# 					For MyISAM tables, KEY_BLOCK_SIZE optionally specifies the size in bytes to use
# 					for index key blocks.
#
# 					The value is treated as a hint; a different size could be used if necessary.
#
# 					A KEY_BLOCK_SIZE value specified for an individual index definition overrides
# 					the table-level KEY_BLOCK_SIZE value.
#
# 					For information about the table-level KEY_BLOCK_SIZE attribute, see TABLE OPTIONS
#
# 				) WITH PARSER
#
# 					The WITH PARSER option can only be used with FULLTEXT indexes.
#
# 					It associates a parser plugin with the index if full-text indexing
# 					and searching operations need special handling.
#
# 					InnoDB and MyISAM support full-text parser plugins.
#
# 					If you have a MyISAM table with an associated full-text parser
# 					plugin, you can convert the table to InnoDB using ALTER TABLE.
#
# 				) COMMENT
#
# 					In MySQL 8.0, index definitions can include an optional comment
# 					of up to 1024 characters.
#
# 					You can set the InnoDB MERGE_THRESHOLD value for an individual index using
# 					the index_option COMMENT clause.
#
# 					See SECTION 15.8.11, "CONFIGURING THE MERGE THRESHOLD FOR INDEX PAGES"
#
# 			For more information about permissible index_option values, see SECTION 13.1.15, "CREATE INDEX SYNTAX"
#
# 			For more information about indexes, see SECTION 8.3.1, "HOW MYSQL USES INDEXES"
#
# 		) reference_definition
#
# 			For reference_definition syntax details and examples, see SECTION 13.1.20.6,
# 			"USING FOREIGN KEY CONSTRAINTS"
#
# 			For information specific to foreign keys in InnoDB, see SECTION 15.6.1.5, "InnoDB AND FOREIGN KEY CONSTRAINTS"
#
# 			InnoDB and NDB tables support checking of foreign key constraints.
#
# 			The  columns of the referenced table must always be explicitly named.
#
# 			Both ON DELETE and ON UPDATE actions on foreign keys are supported.
#
# 			For more detailed information and examples, see SECTION 13.1.20.6,
# 			"USING FOREIGN KEY CONSTRAINTS"
#
# 			For information specific to foreign keys in InnoDB, see SECTION 15.6.1.5,
# 			"InnoDB AND FOREIGN KEY CONSTRAINTS"
#
# 			For other storage engines, MySQL Server parses and ignores the FOREIGN KEY
# 			and REFERENCES syntax in CREATE_TABLE statements.
#
# 			See SECTION 1.8.2.3, "FOREIGN KEY DIFFERENCES"
#
# 				IMPORTANT:
#
# 					For users familiar with the ANSI/ISO SQL Standard, please note that no storage engine,
# 					includin InnoDB, recognizes or enforces the MATCH clause used in referential integrity
# 					constraint definitions.
#
# 					Use of an explicit MATCH clause will not have the specified effect, and also causes
# 					ON DELETE and ON UPDATE clauses to be ignored. For these reasons, specifying
# 					MATCH should be avoided.
#
# 					The MATCH clause in SQL standard controls how NULL values in a composite (multiple-column)
# 					foreign key are handled when comparing to a primary key.
#
# 					InnoDB essentially implements the semantics defined by MATCH SIMPLE, which permit a foreign
# 					key to be all or partially NULL.
#
# 					In that case, the (child table) row containing such a foreign key is permitted to be inserted,
# 					and does not match any row in the referenced (parent) table.
#
# 					It is possible to implement other semantics using triggers.
#
# 					Additionally, MySQL requires that the referenced columns be indexed for performance.
#
# 					However, InnoDB does not enforce any requirement that the referenced columns be declared
# 					UNIQUE or NOT NULL.
#
# 					The handling of foreign key references to nonunique keys or keys that contain NULL values
# 					is not well defined for operations such as UPDATE or DELETE CASCADE.
#
# 					You are advised to use foreign keys that reference only keys that are both UNIQUE 
# 					(or PRIMARY) and NOT NULL.
#
# 					MySQL parses but ignores "inline REFERENCES specifications" (as defined in the SQL standard)
# 					where the references are defined as part of the column specification.
#
# 					MySQL accepts REFERENCES clauses only when specified as part of a separate FOREIGN KEY 
# 					specification.
#
# 		) reference_option
#
# 			For information about the RESTRICT, CASCADE, SET NULL, NO ACTION and SET DEFAULT options, see
# 			SECTION 13.1.20.6, "USING FOREIGN KEY CONSTRAINTS"
#
# TABLE OPPTIONS
#
# Table options are used to optimize the behavior of the table.
#
# In most cases, you do not have to specify any of them. These options apply to all storage engines
# unless otherwise indicated.
#
# Options that do not apply to a given storage engine may be accepted and remembered as part of the
# table definition.
#
# Such options then apply if you later use ALTER_TABLE to convert the table to use a different storage engine.
#
# 		) ENGINE
#
# 			Specifies the storage engine for the table, using one of the names shown in the following table.
#
# 			The engine name can be unquoted or quoted. The quoted name 'DEFAULT' is recognized but ignored.
#
# 			STORAGE ENGINE 			DESC
#
# 			InnoDB 					Transaction-safe tables with row locking and foreign keys.
#
# 										The default storage engine for new tables. 
#
# 										See CHAPTER 15, The InnoDB Storage Engine, and in particular Section 15.1,
# 										"Introduction to InnoDB" if you have MySQL experience but are new to InnoDB.
#  		
# 			MyISAM 					The binary portable storage engine that is primarily used for read-only or read-mostly
# 										workloads.
#
# 										See SECTION 16.2, "THE MYISAM STORAGE ENGINE"
#
# 			MEMORY 					The data for this storage engine is stored only in memory. See SECTION 16.3, "THE MEMORY STORAGE ENGINE"
#
# 			CSV 						Tables that store rows in comma-separated values format. See SECTION 16.4, "THE CSV STORAGE ENGINE"
#
# 			ARCHIVE 					The archiving storage engine. See SECTION 16.5, "THE ARCHIVE STORAGE ENGINE"
#
# 			EXAMPLE 					An example engine. See SECTION 16.9, "THE EXAMPLE STORAGE ENGINE"
#
# 			FEDERATED 				Storage engine that accesses remote tables. See SECTION 16.8, "THE FEDERATED STORAGE ENGINE"
#
# 			HEAP 						This is a synonym for MEMORY
#
# 			MERGE 					A collection of MyISAM tables used as one table.
#
# 										Also known as MRG_MyISAM. See SECTION 16.7, "THE MERGE STORAGE ENGINE"
#
# 			NDB 						Clustered, fault-tolerant, memory-based tables, supporting transactions and foreign
# 										keys.
#
# 										Also known as NDBCLUSTER.
#
# 										See CHAPTER 22, MySQL NDB CLUSTER 8.0
#
# 			By default, if a storage engine is specified that is N/A, the statement fails with an error.
#
# 			You can override this behavior by removing NO_ENGINE_SUBSTITUTION from the server SQL mode
# 			(see SECTION 5.1.11, "SERVER SQL MODES") so that MySQL allows substitution of the specified
# 			engine with the default storage engine instead.
#
# 			Normally, in such cases, this is InnoDB, which is the default value for the default_storage_engine
# 			system variable.
#
# 			When NO_ENGINE_SUBSTIUTTION is disabled, a warning occurs if the storage engine specification is not honored.
#
# 		) AUTO_INCREMENT
#
# 			The initial AUTO_INCREMENT value for the table.
#
# 			In MySQL 8.0, this works for MyISAM, MEMORY, InnoDB and ARCHIVE tables.
#
# 			To set the first auto-increment value for engines that do not support the
# 			AUTO_INCREMENT table option, insert a "dummy" row with a value one less than the
# 			desired value after creating the table, and then delete the dummy row.
#
# 			For engines that support the AUTO_INCREMENT table option in CREATE_TABLE statements,
# 			you can also use ALTER TABLE tbl_name AUTO_INCREMENT = N to reset the AUTO_INCREMENT value.
#
# 			The value cannot be set lower than the maximum value currently in the column.
#
# 		) AVG_ROW_LENGTH
#
# 			An approximation of the average row length for your table. You need to set this only for large
# 			tables with variable-size rows.
#
# 			When you create a MyISAM table, MySQL uses the product of the MAX_ROWS and AVG_ROW_LENGTH options
# 			to decide how big the resulting table is.
#
# 			If you do not specify either option, the max size for MyISAM data and index file is 256TB by default.
#
# 			(If your OS does not support files that large, table sizes are constrained by the file size limit)
#
# 			If you want to keep down the pointer size to make the index smaller and faster and you do not really
# 			need big files, you can decrease the default pointer size by setting the myisam_data_pointer_size
# 			system variable.
#
# 			(See SECTION 5.1.8, "SERVER SYSTEM VARIABLES")
#
# 			If you want all your tables to be able to grow above the default limit and are willing to have your
# 			tables slightly slower and larger than necessary, you can increase the default point size by setting
# 			this variable.
#
# 			Setting this value to 7 permits table sizes up to 65,536TB
#
# 		) [DEFAULT] CHARACTER SET
#
# 			Specifies a default character set for the table.
#
# 			CHARSET is a synonym for CHARACTER SET.
#
# 			If the character set name is DEFAULT, the database character set is used.
#
# 		) CHECKSUM
#
# 			Set this to 1 if you want MySQL to maintain live checksum for all rows (that is,
# 			a checksum that MySQL updates automatically as the table changes)
#
# 			This makes the table a little slower to update, but also makes it easier
# 			to find corrupted tables.
#
# 			The CHECKSUM_TABLE statement reports the checksum. (MyISAM only)
#
# 		) [DEFAULT] COLLATE
#
# 			Specifies a default collation for the table.
#
# 		) COMMENT
#
# 			A comment for the table, up to 2048 characters long.
#
# 			You can set the InnoDB MERGE_THRESHOLD value for a table using the 
# 			table_option COMMENT clause.
#
# 			See SECTION 15.8.11, "CONFIGURING THE MERGE THRESHOLD FOR INDEX PAGES"
#
# 			SETTING NDB_TABLE OPTIONS.
#
# 			The table comment in a CREATE TABLE that creates an NDB table or an
# 			ALTER_TABLE statement which alters one can also be used to specify one
# 			to four of the NDB_TABLE options:
#
# 				NOLOGGING
#
# 				READ_BACKUP
#
# 				PARTITION_BALANCE
#
# 				FULLY_REPLICATED
#
# 			as a set of name-value pairs, separated by commas if need be, immediately
# 			following the string NDB_TABLE= that begins the quoted comment text.
#
# 			An example statement using this syntax is shown here (emphasized text):
#
# 				CREATE TABLE t1 (
# 					c1 INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
# 					c2 VARCHAR(100),
# 					c3 VARCHAR(100) )
# 				ENGINE=NDB
# 				COMMENT="NDB_TABLE=READ_BACKUP=0,PARTITION_BALANCE=FOR_RP_BY_NODE";
#
# 			Spaces are not permitted within the quoted string.
#
# 			The string is case-insensitive.
#
# 			The comment is displayed as part of the output of SHOW_CREATE_TABLE
#
# 			The text of the comment is also available as the TABLE_COMMENT column of the
# 			MySQL Information Schema TABLES table.
#
# 			This comment syntax is also supported with ALTER_TABLE statements for NDB tables.
#
# 			Keep in mind that a table comment used with ALTER TABLE replaces any existing
# 			comment which the table might have had previously.
#
# 			Setting the MERGE_THRESHOLD option in table comments is not supported for NDB tables
# 			(it is ignored)
#
# 			For complete syntax information and examples, see SECTION 13.1.20.10, "SETTING NDB_TABLE OPTIONS"
#
# 		) COMPRESSION
#
# 			The compression algorithm used for page level compression for InnoDB tables.
#
# 			Supported values include Zlib, LZ4, and None.
#
# 			The COMPRESSION attribute was introduced with the transparent page compression
# 			feature.
#
# 			Page compression is only supported with InnoDB tables that reside in file-per-table
# 			tablespaces, and is only available on Linux and Windows platforms that support
# 			sparse files and hole punching.
#
# 			For more information, see SECTION 15.9.2, "InnoDB PAGE COMPRESSION"
#
# 		) CONNECTION
#
# 			The connection string for a FEDERATED table
#
# 			NOTE:
#
# 				Older versions of MySQL used a COMMENT option for the connection string
#
# 		) DATA DIRECTORY, INDEX DIRECTORY
#
# 			For InnoDB, the DATA_DIRECTORY='directory' clause permits creating a file-per-table
# 			tablespace outside of the data directory.
#
# 			The tablespace data file is created in the specified directory, inside a subdirectory
# 			with the same name as the schema.
#
# 			The innodb_file_per_table variable must be enabled to use the DATA DIRECTORY
# 			clause.
#
# 			The full directory path must be specified.
#
# 			For more information, see SECTION 15.6.3.6, "CREATING A TABLESPACE OUTSIDE OF THE DATA DIRECTORY"
#
# 			When creating MyISAM tables, you can use the DATA DIRECTORY='directory' clause, the INDEX DIRECTORY='directory'
# 			clause, or both.
#
# 			They specify where to put a MyISAM table's data file and index file, respectively.
#
# 			Unlike InnoDB tables, MySQL does not create subdirectories that correspond to the 
# 			database name when creating a MyISAM table with a DATA DIRECTORY or INDEX DIRECTORY option.
#
# 			Files are created in the directory that is specified.
#
# 			You must have the FILE privilege to use the DATA DIRECTORY or INDEX DIRECTORY table option.
#
# 			IMPORTANT:
#
# 				Table-level DATA DIRECTORY and INDEX DIRECTORY options are ignored for partitioned tables.
# 				(Bug #32091)
#
# 			These options work only when you are not using the --skip-symbolic-links option.
#
# 			Your OS must also have a working, thread-safe realpath() call
#
# 			See SECTION 8.12.2.2, "USING SYMBOLIC LINKS FOR MYISAM TABLES ON UNIX", for more
# 			complete information.
#
# 			If a MyISAM table is created with no DATA DIRECTORY option, the .MYD file is created
# 			in the database directory.
#
# 			By default, if MyISAM finds an existing .MYD file in this case, it overwrites it.
#
# 			The same applies to .MYI files for tables created with no INDEX DIRECTORY option.
#
# 			To suppress this behavior, start the server with the --keep_files_on_create option,
# 			in which case MyISAM will not overwrite existing files and returns an error instead.
#
# 			If a MyISAM table is created with a DATA DIRECTORY or INDEX DIRECTORY option and
# 			an existing .MYD or .MYI file is found, MyISAM always returns an error.
#
# 			It will not overwrite a file in the specified directory.
#
# 				IMPORATNT:
#
# 					You cannot use path names that contain the MySQL data directory with
# 					DATA DIRECTORY or INDEX DIRECTORY.
#
# 					THis includes partitioned tables and individual table partitions.
# 					(See Bug #32167)
#
# 		) DELAY_KEY_WRITE
#
# 			Set this to 1 if you want to delay key updates for the table until the table is closed.
#
# 			See the description of the delay_key_write system variable in SECTION 5.1.8, "SERVER SYSTEM VARIABLES"
# 			(MyISAM only)
#
# 		) ENCRYPTION
#
# 			Set the ENCRYPTION option to 'Y' to enable page-level data encryption for an InnoDB
# 			table created in a file-per-table tablespace.
#
# 			Option values are not case-sensitive.
#
# 			The ENCRYPTION option was introduced with the InnoDB tablespace encryption feature;
# 			see SECTION 15.6.3.9, "TABLESPACE ENCRYPTION"
#
# 			A keyring plugin must be installed and configured to use the ENCRYPTION option.
#
# 		) INSERT_METHOD
#
# 			If you want to insert data into a MERGE table; you must specify with INSERT_METHOD
# 			the table into which the row should be inserted.
#
# 			INSERT_METHOD is an option useful for MERGE tables only.
#
# 			Use a value of FIRST or LAST to have inserts go to the first or last table,
# 			or a value of NO to prevent inserts.
#
# 			See SECTION 16.7, "THE MERGE STORAGE ENGINE"
#
# 		) KEY_BLOCK_SIZE
#
# 			For MyISAM tables, KEY_BLOCK_SIZE optionally specifies the size in bytes to use
# 			for index key blocks.
#
# 			The value is treated as a hint; a different size could be used if necessary.
#
# 			A KEY_BLOCK_SIZE value specified for an individual index definition overrides
# 			the table-level KEY_BLOCK_SIZE value.
#
# 			For InnoDB tables, KEY_BLOCK_SIZE specifies the page size in kilobytes to use
# 			for compressed InnoDB tables.
#
# 			The KEY_BLOCK_SIZE value is treated as a hint; a different size could be used
# 			by InnoDB if necessary.
#
# 			KEY_BLOCK_SIZE can only be less than or equal to the innodb_page_size value.
#
# 			A value of 0 represents the default compressed page size, which is half of the
# 			innodb_page_size value.
#
# 			Depending on innodb_page_size, possible KEY_BLOCK_SIZE values include
# 			0, 1, 2, 4, 8 and 16.
#
# 			See SECTION 15.9.1, "InnoDB TABLE COMPRESSION" for more information
#
# 			Oracle recommends enabling innodb_strict_mode when specifying KEY_BLOCK_SIZE
# 			for InnoDB tables.
#
# 			When innodb_strict_mode is enabled, specifying an invalid KEY_BLOCK_SIZE value
# 			returns an error.
#
# 			If innodb_strict_mode is disabled, an invalid KEY_BLOCK_SIZE value results
# 			in a warning, and the KEY_BLOCK_SIZE option is ignored.
#
# 			The Create_options column in response to SHOW_TABLE_STATUS reports the
# 			actual KEY_BLOCK_SIZE used by the table, as does SHOW_CREATE_TABLE
#
# 			InnoDB only supports KEY_BLOCK_SIZE at the table level
#
# 			KEY_BLOCK_SIZE is not supported with 32KB and 64KB innodb_page_size values.
#
# 			InnoDB table compression does not support these page sizes.
#
# 			InnoDB does not support the KEY_BLOCK_SIZE option when creating temporary tables.
#
# 		) MAX_ROWS
#
# 			The maximum number of rows you plan to store in the table.
#
# 			This is not a hard limit, but rather a hint to the storage engine that
# 			the table must be able to store at least this many rows.
#
# 				IMPORTANT:
#
# 					The use of MAX_ROWS with NDB tables to control the number of table
# 					partitions is deprecated.
#
# 					It remains supported in later versions for backward compatibility,
# 					but is subject to removal in a future release.
#
# 					Use PARTITION_BALANCE instead; see SETTING NDB_TABLE OPTIONS
#
# 			The NDB storage engine treats this value as a maximum.
#
# 			If you plan to create very large NDB Cluster tables (containing millions of rows),
# 			you should use this option to insure that NDB allocates sufficient number of index slots
# 			in the hash table used for storing hashes of the table's primary keys by setting
# 			MAX_ROWS = 2 * rows, where rows is the number of rows that you expect to insert
# 			into the table.
#
# 			The maximum MAX_ROWS value is 4294967295; larger values are truncated to this limit.
#
# 		) MIN_ROWS
#
# 			The minimum number of rows you plan to store in the table.
#
# 			The MEMORY storage engine uses this option as a hint about memory use.
#
# 		) PACK_KEYS
#
# 			Takes effect only with MyISAM tables.
#
# 			Set this option to 1 if you want to have smaller indexes.
#
# 			This usually makes updates slower and reads faster. 
# 			Setting this option to 0 disables all packing of keys.
#
# 			Setting it to DEFAULT tells the storage engine to pack only long
# 			CHAR, VARCHAR, BINARY or VARBINARY columns.
#
# 			If you do not use PACK_KEYS, the default is to pack strings, but not 
# 			numbers.
#
# 			If you use PACK_KEYS=1, numbers are packed as well.
#
# 			When packing binary number keys, MySQL uses prefix compression:
#
# 				) Every key needs one extra byte to indicate how many bytes of the previous key are the same 
# 					for the next key.
#
# 				) The pointer to the row is stored in high-byte-first order directly after the key, to improve compression.
#
# 			This means that if you have many equal keys on two consecutive rows, all following "same" keys
# 			usually only take two bytes (including the pointer ot the row)
#
# 			Compare this to the ordinary case where the following keys takes storage_size_for_key +
# 			pointer_size (where the pointer size is usually 4)
#
# 			Conversely, you get a significant benefit from prefix compression only if you have
# 			many numbers that are the same.
#
# 			If all keys are totally different, you use one byte more per key, if the key is not a key
# 			that can have NULL values.
#
# 			(In this case, the packed key length is stored in the same byte that is used to mark if a key is NULL)
#
# 		) PASSWORD
#
# 			This option is unused
#
# 		) ROW_FORMAT
#
# 			Defines the physical format in which the rows are stored.
#
# 			When executing a CREATE_TABLE statement with strict mode disabled, if you specify
# 			a row format that is not supported by the storage engine that is used
# 			for the table, the table is created using that storage engine's default row format.
#
# 			The actual row format of the table is reported in the Row_format and Create_options
# 			columns in response to SHOW_TABLE_STATUS.
#
# 			SHOW_CREATE_TABLE also reports the actual row format of the table.
#
# 			Row format choices differ depending on the storage engine used for the table.
#
# 			For InnoDB tables:
#
# 				) The default row format is defined by innodb_default_row_format, which has
# 					a default setting of DYNAMIC.
#
# 					The default row format is used when the ROW_FORMAT option is not defined
# 					or when ROW_FORMAT=DEFAULT is used.
#
# 					If the ROW_FORMAT option is not defined, or if ROW_FORMAT=DEFAULT is used,
# 					operations that rebuild a table also silently change the row format
# 					of the table to the default defined by innodb_default_row_format.
#
# 					For more information, see DEFINING THE ROW FORMAT OF A TABLE
#
# 				) For more efficient InnoDB storage of data types, especially BLOB types,
# 					use the DYNAMIC.
#
# 					See DYNAMIC ROW FORMAT for requirements associated with the DYNAMIC
# 					row format.
#
# 				) To enable compression for InnoDB tables, specify ROW_FORMAT=COMPRESSED
#
# 					The ROW_FORMAT=COMPRESSED option is not supported when creating temporary
# 					tables.
#
# 					See SECTION 15.9, "InnoDB TABLE AND PAGE COMPRESSION" for requirements
# 					associated with the COMPRESSED row format.
#
# 				) The row format used in older versions of MySQL can still be requested by specifying the REDUNDANT row format.
#
# 				) When you specify a non-default ROW_FORMAT clause, consider also enabling the innodb_strict_mode configuration option
#
# 				) ROW_FORMAT=FIXED is not supported.
#
# 					If ROW_FORMAT=FIXED is specified while innodb_strict_mode is disabled, InnoDB issues a warning
# 					and assumes ROW_FORMAT=DYNAMIC.
#
# 					If ROW_FORMAT=FIXED is specified while innodb_strict_mode is enabled,
# 					which is the default, InnoDB returns an error.
#
# 				) For additional information about InnoDB row formats, see SECTION 15.10, "InnoDB ROW FORMATS"
#
# 			For MyISAM tables, the option value can be FIXED or DYNAMIC for static or variable-length
# 			row format.
#
# 			myisampack sets the type to COMPRESSED. See SECTION 16.2.3, "MyISAM TABLE STORAGE FORMATS"
#
# 			For NDB tables, the default ROW_FORMAT is DYNAMIC
#
# 		) STATS_AUTO_RECALC
#
# 			Specifies whether to automatically recalculate persistent statistics for an InnoDB table.
#
# 			The value DEFAULT causes the persistent statistics setting for the table to be
# 			determined by the innodb_stats_auto_recalc configuration option.
#
# 			The value 1 causes statistics to be recalculated when 10% of the data in teh table
# 			has changed.
#
# 			The value 0 prevents automatic recalculation for this table; with this setting,
# 			issue an ANALYZE_TABLE statement to recalculate the statistics after making
# 			substansial changes to the table.
#
# 			For more information about the persistent statistics feature, see SECTION 15.8.10.1,
# 			"CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 		) STATS_PERSISTENT
#
# 			Specifies whether to enable persistent statistics for an InnoDB table.
#
# 			The value DEFAULT causes the persistent statistics setting for the table to
# 			be determined by the innodb_stats_persistent configuration option.
#
# 			The value 1 enables persistent statistics for the table, while the value 
# 			0 turns off this feature.
#
# 			After enabling persistent statistics through a CREATE TABLE or ALTER TABLE
# 			statement, issue an ANALYZE_TABLE statement to calculate the statistics,
# 			after loading representative data into the table.
#
# 			For more information about the persistent statistics feature,
# 			see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 		) STATS_SAMPLE_PAGES
#
# 			The number of index pages to sample when estimating cardinality and other
# 			statistics for an indexed column, such as those calculated by ANALYZE_TABLE.
#
# 			For more information, see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 		) TABLESPACE
#
# 			The TABLESPACE clause can be used to create a table in an existing general tablespace,
# 			a file-per-table tablespace, or the system tablespace.
#
# 				CREATE TABLE tbl_name --- TABLESPACE [=] tablespace_name
#
# 			The general tablespace that you specify must exist prior to using the
# 			TABLESPACE clause.
#
# 			For information about general tablespaces, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# 			The tablespace_name is a case-sensitive identifier.
#
# 			It may be quoted or unquoted. The forward slash character ("/") is not permitted.
#
# 			Names beginning with "innodb_" are reserved for special use.
#
# 			To create a table in the system tablespace, specify innodb_system as the tablespace name.
#
# 				CREATE TABLE tbl_name --- TABLESPACE [=] innodb_system
#
# 			Using TABLESPACE [=] innodb_system you can place a table of any uncompressed row
# 			format in the system tablespace regardless of the innodb_file_per_table setting.
#
# 			For example, you can add a table with ROW_FORMAT=DYNAMIC to the system tablespace
# 			using TABLESPACE [=] innodb_system
#
# 			To create a table in a file-per-table tablespace, specify innodb_file_per_table as
# 			the tablespace name.
#
# 				CREATE TABLE tbl_name --- TABLESPACE [=] innodb_file_per_table
#
# 			NOTE:
#
# 				If innodb_file_per_table is enabled, you need not specify TABLESPACE=innodb_file_per_table
# 				to create an InnoDB file-per-table tablespace.
#
# 				InnoDB tables are created in file-per-table tablespaces by default when innodb_file_per_table 
# 				is enabled.
#
# 			The DATA DIRECTORY clause is permitted with CREATE TABLE --- TABLESPACE=innodb_file_per_table but
# 			is otherwise not supported for use in combination with the TABLESPACE clause.
#
# 			NOTE:
#
# 				Support for TABLESPACE = innodb_file_per_table and TABLESPACE = innodb_temporary clauses
# 				with CREATE_TEMPORARY_TABLE is deprecated as of MySQL 8.0.13, and will be removed
# 				in a future version of MySQL.
#
# 		) UNION
#
# 			Used to access a collection of identical MyISAM tables as one.
#
# 			This works only with MERGE tables. See SECTION 16.7, "THE MERGE STORAGE ENGINE"
#
# 			You must have SELECT, UPDATE and DELETE privileges for the tables you map to a MERGE table
#
# 			NOTE:
#
# 				Formerly, all tables used had to be in the same database as the MERGE table itself.
#
# 				This restriction no longer applies.
#
# CREATING PARTITIONED TABLES
#
# partition_options can be used to control partitioning of the table created with CREATE_TABLE
#
# Not all options shown in the syntax for partition_options at the beginning of this section are
# available for all partitioning types.
#
# Please see the listings for the following individual types for information specific to each
# type,  and see CHAPTER 23, PARTITIONING, for more complete information about the workings
# of and uses for partitioning in MySQL, as well as additional examples of table creation
# and other statements relating to MySQL partitioning.
#
# Partitions can be modified, merged, added to tables, and dropped from tables.
#
# For basic information about the MySQL statements to accomplish these tasks, see
# SECTION 13.1.9, "ALTER TABLE SYNTAX"
#
# For more detailed descriptions and examples, see SECTION 23.3, "PARTITION MANAGEMENT"
#
# 		) PARTITION BY
#
# 			If used, a partition_options clause begins with PARTITION BY.
#
# 			This clause contains the function that is used to determine the partition;
# 			the function returns an integer value ranging from 1 to num, where num is
# 			the number of partitions.
#
# 			(The maximum number of user-defined partitions which a table may contain is 
# 			1024; the number of subpartitions - discussed later in this section - 
# 			is included in this maximum)
#
# 			NOTE:
#
# 				The expression (expr) used in a PARTITION BY clause cannot refer to any
# 				columns not in the table being created;
#
# 				Such references are specifically not permitted and cause the statement to fail
# 				with an error (Bug #29444)
#
# 		) HASH(expr)
#
# 			Hashes one or more columns to create a key for placing and locating rows.
#
# 			expr is an expression using one or more table columns.
#
# 			This can be any valid MySQL expression (including MySQL functions) that
# 			yields a single integer value.
#
# 			For example, these are both valid CREATE_TABLE statements using PARTITION BY HASH:
#
# 				CREATE TABLE t1 (col1 INT, col2 CHAR(5))
# 					PARTITION BY HASH(col1);
#
# 				CREATE TABLE t1 (col1 INT, col2 CHAR(5), col3 DATETIME)
# 					PARTITION BY HASH ( YEAR(col3) );
#
# 			You may not use either VALUES LESS THAN or VALUES IN clauses with PARTITION BY HASH
#
# 			PARTITION BY HASH uses the remainder of expr divided by the number of partitions (that is, modulus)
#
# 			For examples and additional information, see SECTION 23.2.4, "HASH PARTITIONING"
#
# 			The LINEAR keyword entails a somewhat different algorithm.
#
# 			In this case, the number of the partition in which a row is stored is calculated
# 			as the result of one or more logical AND operations.
#
# 			For discussion and examples of linear hashing, see SECTION 23.2.4.1, "LINEAR HASH PARTITIONING"
#
# 		) KEY(column_list)
#
# 			This is similar to HASH, except that MySQL supplies the hashing function so as to
# 			guarantee an even data distribution.
#
# 			The column_list argument is simply a list of 1 or more table columns (max: 16)
#
# 			This example shows a simple table partitioned by key, with 4 partitions:
#
# 				CREATE TABLE tk (col1 INT, col2 CHAR(5), col3 DATE)
# 					PARTITION BY KEY(col3)
# 					PARTITIONS 4;
#
# 			For tables that are partitioned by key, you can employ linear partitioning by using
# 			the LINEAR keyword.
#
# 			THis has the same effect as with tables that are partitioned by HASH.
#
# 			That is, the partition number is found using the & operator rather than
# 			the modulus (see SECTION 23.2.4.1, "LINEAR HASH PARTITIONING", and SECTION 23.2.5, "KEY PARTITIONING", for details)
#
# 			This example uses linear partitioning by key to distribute data between 5 partitions:
#
# 				CREATE TABLE tk (col1 INT, col2 CHAR(5), col3 DATE)
# 					PARTITION BY LINEAR KEY(col3)
# 					PARTITIONS 5;
#
# 			The ALGORITHM={1|2} option is supported with [SUB]PARTITION BY [LINEAR] KEY ALGORITHM=1 causes
# 			the server to use the same key-hashing functions as MySQL 5.1
#
# 			ALGORITHM=2 means that hte server employs the key-hashing functions implemented and used
# 			by default for new KEY partitioned tables in MySQL 5.5 and later.
#
# 			(Partitioned tables created with the key-hashing functions employed in MySQL 5.5 and later
# 			cannot be used by a MySQL 5.1 server)
#
# 			Not specifying the option has the same effect as using ALGORITHM=2
#
# 			This option is intended for use chiefly when upgrading or downgrading [LINEAR]
# 			KEY partitioned tables between MySQL 5.1 and later MySQL versions, or for creating
# 			tables partitioned by KEY or LINEAR KEY on a MySQL 5.5 or later server which can be used
# 			on a MySQL 5.1 server
#
# 			For more information, see SECTION 13.1.9.1, "ALTER TABLE PARTITION OPERATIONS"
#
# 			mysqldump in MySQL 5.7 (and later) writes this option encased in versioned comments like this:
#
# 				CREATE TABLE t1 (a INT)
# 				/*!50100 PARTITION BY KEY */ /*!50611 ALGORITHM = 1 */ /*!50100 ()
# 						PARTITIONS 3 */
#
# 			This causes MySQL 5.6.10 and earlier servers to ignore the option, which would otherwise
# 			cause a syntax error in those versions.
#
# 			If you plan to load a dump made on a MySQL 5.7 server where you use tables that are
# 			partitioned or subpartitioned by KEY into a MySQL 5.6 server previous to 5.6.11,
# 			be sure to consult changes in MySQL 5.6
#
# 			(The information found there also applies if you are loading a dump containing KEY
# 			partitioned or subpartitioned tables made from a MySQL 5.7 - actually, 5.6.11
# 			or later - server into a MySQL 5.5.30 or earlier server)
#
# 			Also in MySQL 5.6.11, and later, ALGORITHM=1 is shown when necessary in the output
# 			of SHOW_CREATE_TABLE using versioned comments in teh same manner as mysqldump.
#
# 			ALGORITHM=2 is always omitted from SHOW CREATE TABLE output, even if this option
# 			was speified when creating the original table.
#
# 			You may not use either VALUES LESS THAN or VALUES IN clauses with PARTITION BY KEY
#
# 		) RANGE(expr)
#
# 			In this case, expr shows a range of values using a set of VALUES LESS THAN operators.
#
# 			When using range partitioning, you must define at least one partition using
# 			VALUE LESS THAN
#
# 			You cannot use VALUES IN with range partitioning
#
# 			NOTE:
#
# 				For tables partitioned by RANGE, VALUES LESS THAN must be used with either
# 				an integer literal value or an expression that evaluates to a single integer
# 				value.
#
# 				In MySQL 8.0, you can overcome this limitation in a table that is defined
# 				using PARTITION BY RANGE COLUMNS, as described later in this section.
#
# 			Suppose that you have a table that you wish to partition on a column containing
# 			year values, according to the following scheme.
#
	# 			PARTITION NUMBER: 			YEARS RANGE:
	#
	# 			0 									1990 and earlier
	#
	# 			1 									1991 to 1994
	#
	# 			2 									1995 to 1998
	#
	# 			3 									1999 to 2002
	#
	# 			4 									2003 to 2005
	#
	# 			5 									2006 and later
#
# 			A table implementing such a partitioning scheme can be realized by the 
# 			CREATE_TABLE statement shown here:
#
# 				CREATE TABLE t1 (
# 					year_col INT,
# 					some_data INT
# 				)
# 				PARTITION BY RANGE (year_col) (
# 					PARTITION p0 VALUES LESS THAN (1991),
# 					PARTITION p1 VALUES LESS THAN (1995),
# 					PARTITION p2 VALUES LESS THAN (1999),
# 					PARTITION p3 VALUES LESS THAN (2002),
# 					PARTITION p4 VALUES LESS THAN (2006),
# 					PARTITION p5 VALUES LESS THAN MAXVALUE
# 				);
#
# 			PARTITION --- VALUES LESS THAN --- statements work in a consecutive fashion.
#
# 			VALUES LESS THAN MAXVALUE works to specify "leftover" values that are greater
# 			than the maximum value otherwise specified.
#
# 			VALUES LESS THAN clauses work sequentially in a manner similar to that of hte case
# 			portions of a switch --- case block (as found in many languages)
#
# 			THat is, the clauses must be arranged in such a way that hte upper limit specified
# 			in each successive VALUES LESS THAN is greater than that of the previous one,
# 			with the one referencing MAXVALUE coming last of all in the list.
#
# 		) RANGE COLUMNS (column_list)
#
# 			This variant of RANGE facilitates partition pruning for queries using range conditions
# 			on multiple columns (that is, having conditions such as WHERE a = 1 AND b < 10 or
# 			WHERE a = 1 AND b = 10 AND c < 10)
#
# 			It enables you to specify value ranges in multiple columns by using a list of columns
# 			in the COLUMNS clause and a set of column values in each PARTITION --- VALUES LESS THAN (value_list)
# 			partition definition clause.
#
# 			(In the simplest case, this set consists of a single column)
#
# 			The maximum number of columns that can be referenced in the column_list and
# 			value_list is 16.
#
# 			The column_list used in the COLUMNS clause may contain only names of columns;
# 			each column in the list must be one of the following MySQL data types:
#
# 				The integer types
#
# 				The string types
#
# 				´Time or date column types
#
# 			Columns using BLOB, TEXT, SET, ENUM, BIT or spatial data types
# 			are not permitted;
#
# 			Columns that use floating-point number types are also not permitted.
#
# 			You also may not use functions or arithmetic expressions in the COLUMNS clause.
#
# 			The VALUES LESS THAN clause used in a partition definition must specify a literal value
# 			for each column that appears in the COLUMNS() clause
#
# 			That is, the list of values used for each VALUES LESS THAN clause must contain the 
# 			same number of values as there are columns listed in the COLUMNS clause.
#
# 			AN attempt to use more or fewer values in a VALUES LESS THAN clause than there are
# 			in the COLUMNS clause causes the statements to fail with the:
#
# 			 error Inconsistency in usage of column lists for partitionining 	error
#
# 			You cannot use NULL for any value appearing in VALUES LESS THAN
#
# 			It is possible to use MAXVALUE more than once for a given column
# 			other than the first, as shown in this example:
#
# 				CREATE TABLE rc (
# 					a INT NOT NULL,
# 					b INT NOT NULL
# 				)
# 				PARTITION BY RANGE COLUMNS(a,b) (
# 					PARTITION p0 VALUES LESS THAN (10,5),
# 					PARTITION p1 VALUES LESS THAN (20,10),
# 					PARTITION p2 VALUES LESS THAN (50,MAXVALUE),
# 					PARTITION p3 VALUES LESS THAN (65,MAXVALUE),
# 					PARTITION p4 VALUES LESS THAN (MAXVALUE,MAVALUE)
# 				);
#
# 			Each value in a VALUES LESS THAN value list must match the type of the corresponding
# 			column exactly; no conversion is made.
#
# 			For example, you cannot use the string '1' for a value that matches a column that uses
# 			an integer type (you must use the numeral 1 instead), nor can you use hte numeral 1
# 			for a value that matches a column that uses a string type (in such a case, you must use a quoted string '1')
#
# 			For more information, see SECTION 23.2.1, "RANGE PARTITIONING" and SECTION 23.4, "PARTITION PRUNING"
#
# 		) LIST(expr)
#
# 			THis is useful when assigning partitions based on a table column with a restricted set of possible values,
# 			such as a state or country code.
#
# 			In such a case, all rows pertaining to a certain state or country can be assigned
# 			to a single partition, or a partition can be reserved for a certain set of states
# 			or countries.
#
# 			It is similar to RANGE, except that only VALUES IN may be used to specify permissible
# 			values for each partition.
#
# 			VALUES IN is used with a list of values to be matched.
#
# 			For instance, you could create a partitioning scheme such as
# 			the following:
#
# 				CREATE TABLE client_firms (
# 					id INT,
# 					name VARCHAR(35)
# 				)
# 				PARTITION BY LIST (id) (
# 					PARTITION r0 VALUES IN (1, 5, 9, 13, 17, 21),
# 					PARTITION r1 VALUES IN (2, 6, 10, 14, 18, 22),
# 					PARTITION r2 VALUES IN (3, 7, 11, 15, 19, 23),
# 					PARTITION r3 VALUES IN (4, 8, 12, 16, 20, 24)
# 				);
#
# 			When using list partitioning, you must define at least one partition using VALUES IN.
#
# 			You cannot use VALUES LESS THAN with PARTITION BY LIST.
#
# 			NOTE:
#
# 				For tables partitioned by LIST, the value list used with VALUES IN must consist
# 				of integer values only.
#
# 				In MySQL 8.0, you can overcome this limitation using partitioning by LIST COLUMNS,
# 				which is described later in this section.
#
# 		) LIST COLUMNS(column_list)
#
# 			This variant on LIST facilitates partition pruning for queries using comparison conditions
# 			on multiple columns (that is, having conditions such as WHERE a = 5 AND b = 5 or WHERE a = 1 AND b = 10 AND c = 5)
#
# 			It enables you to specify values in multiple columns by using a list of columns in the
# 			COLUMNS clause and a set of column values in each PARTITION --- VALUES IN (value_list)
# 			partition definition clause.
#
# 			The rules governing regarding data types for the column list used in LIST COLUMNS (column_list)
# 			and the value listed used in VALUES IN (value_list) are the same as those for the column
# 			list used in RANGE COLUMNS (column_list) and the value list used in VALUES LESS THAN (value_list),
# 			respectively, except that in the VALUES IN clause, MAXVALUE is not permitted, and you may use NULL.
#
# 			There is one important difference between the list of values used for VALUES IN with PARTITION BY LIST COLUMNS
# 			as opposed to when it is used with PARTITION BY LIST.
#
# 			When used with PARTITION BY LIST COLUMNS, each element in the VALUES IN clause must be a set of column values;
# 			the number of values in each set must be the same as the number of columns used in the COLUMNS clause, and the
# 			data types of these values must match those of the columns (and occur in the same order)
#
# 			In the simplest case, the set consists of a single column.
#
# 			The maximum number of columns that can be used in the column_list and in the elements
# 			making up the value_list is 16.
#
# 			The table defined by the following CREATE TABLE statement provides an example of a table
# 			using LIST COLUMNS partitioning:
#
# 				CREATE TABLE lc (
# 					a INT NULL,
# 					b INT NULL
# 				)
# 				PARTITION BY LIST COLUMNS(a,b) (
# 					PARTITION p0 VALUES IN( (0,0), (NULL,NULL) ),
# 					PARTITION p1 VALUES IN( (0,1), (0,2), (0,3), (1,1), (1,2) ),
# 					PARTITION p2 VALUES IN( (1,0), (2,0), (2,1), (3,0), (3,1) ),
# 					PARTITION p3 VALUES IN( (1,3), (2,2), (2,3), (3,2), (3,3) )
# 				);
#
# 		) PARTITIONS num
#
# 			The number of partitions may optionally be specified with a PARTITIONS num clause,
# 			where num is the number of partitions.
#
# 			If both this clause and any PARTITION clause are used, num must be equal to the
# 			total number of any partitions that are declared using PARTITION clauses.
#
# 			NOTE:
#
# 				Whether or not you use a PARTITIONS clause in creating a table that is partitioned
# 				by RANGE or LIST, you must still include at least one PARTITION VALUES clause in
# 				the table definition (see below)
#
# 		) SUBPARTITION BY
#
# 			A partition may optionally be divided into a number of subpartitions.
#
# 			This can be indicated by using the optional SUBPARTITION BY clause.
#
# 			Subpartitioning may be done by HASH or KEY.
#
# 			Either of these may be LINEAR. These work in the same way as previously described
# 			for the equivalent partitioning types.
#
# 			(It is not possible to subpartition by LIST or RANGE)
#
# 			The number of subpartitions can be indicated using the SUBPARTITIONS keyword
# 			followed by an integer value.
#
# 		) Rigorous checking of the value used in PARTITIONS or SUBPARTITIONS clauses is applied
# 			and this value must adhere to the following rules:
#
# 				) THe value must be a positive, nonzero integer
#
# 				) No leading zeros are permitted
#
# 				) The value must be an integer literal, and cannot not be an expression.
#
# 					For example, PARTITIONS 0.2E+01 is not permitted, even though 0.2E+01 evaluates
# 					to 2. (Bug #15890)
#
# 		) partition_definition
#
# 			Each partition may be individually defined using a partition_definition clause.
#
# 			The individual parts making up this clause are as follows:
#
# 				) PARTITION partition_name
#
# 					Specifies a logical name for the partition
#
# 				) VALUES
#
# 					For range partitioning, each partition must include a VALUES LESS THAN clause;
# 					for list partitioning, you must specify a VALUES IN clause for each partition.
#
# 					This is used to determine which rows are to be stored in this partition.
#
# 					See the discussion of partitioning types in CHAPTER 23, PARTITIONING, for syntax examples.
#
# 				) [STORAGE] ENGINE
#
# 					MySQL accepts a [STORAGE] ENGINE option for both PARTITION and SUBPARTITION.
#
# 					Currently, the only way in which this option can be used is to set all partitions
# 					or all subpartitions to the same storage engine, and an attempt to set different storage
# 					engines for partitions or subpartitions in the same table will give rise to the error:
#
# 						ERROR 1469 (HY000): The mix of handlers in the partitions is not permitted in this version of MySQL
#
# 				) COMMENT
#
# 					An optional COMMENT clause may be used to specify a string that describes the partition.
#
# 					Example:
#
# 						COMMENT = 'Data for the years previous to 1999'
#
# 					The maximum length for a partition comment is 1024 chars.
#
# 				) DATA DIRECTORY and INDEX DIRECTORY
#
# 					DATA DIRECTORY and INDEX DIRECTORY may be used to indicate the directory where,
# 					respectively, the data and indexes for this partition are to be stored.
#
# 					Both the data_dir and the index_dir must be absolute system path names.
#
# 					You must have the FILE privilege to use the DATA DIRECTORY or INDEX DIRECTORY partition option.
#
# 					Example:
#
# 						CREATE TABLE th (id INT, name VARCHAR(30), adate DATE)
# 						PARTITION BY LIST(YEAR(adate))
# 						(
# 							PARTITION p1999 VALUES IN (1995, 1999, 2003)
# 								DATA DIRECTORY = '/var/appdata/95/data'
# 								INDEX DIRECTORY = '/var/appdata/95/idx',
# 							PARTITION p2000 VALUES IN (1996, 2000, 2004)
# 								DATA DIRECTORY = '/var/appdata/96/data'
# 								INDEX DIRECTORY = '/var/appdata/96/idx',
# 							PARTITION p2001 VALUES IN (1997, 2001, 2005)
# 								DATA DIRECTORY = '/var/appdata/97/data'
# 								INDEX DIRECTORY = '/var/appdata/97/idx',
# 							PARTITION p2002 VALUES IN (1998, 2002, 2006)
# 								DATA DIRECTORY = '/var/appdata/98/data'
# 								INDEX DIRECTORY = '/var/appdata/98/idx'
# 						);
#
# 					DATA DIRECTORY and INDEX DIRECTORY behave in the same way as in the CREATE TABLE statements
# 					table_option clause as used for MyISAM tables.
#
# 					One data directory and one index directory may be specified per partition.
#
# 					If left unspecified, the data and the indexes are stored by default in teh table's database
# 					directory.
#
# 					The DATA DIRECTORY and INDEX DIRECTORY options are ignored for creating partitioned tables if
# 					NO_DIR_IN_CREATE is in effect.
# 				
# 				) MAX_ROWS and MIN_ROWS
#
# 					May be used to specify, respectively, the maximum and the minimum number of rows to be stored
# 					in the partition.
#
# 					The values for max_number_of_rows and min_number_of_rows must be positive integers.
#
# 					As with the table-level options with the same names, these act only as "suggestions"
# 					to the server and are not hard limits.
#
# 				) TABLESPACE
#
# 					May be used to designate an InnoDB file-per-table tablespace for the partition by specifying
# 					TABLESPACE `innodb_file_per_table
#
# 					All partitions must belong to the same storage engine
#
# 					Placing InnoDB table partitions in shared InnoDB tablespaces is not supported.
#
# 					Shared tablespaces include the InnoDB system tablesapce and general tablespaces.
#
# 		) subpartition_definition
#
# 			The partition definition may optionally contain one or more subpartition_definition clauses.
#
# 			Each of these consists at a minimum of the SUBPARTITION name, where name is an identifier
# 			for the subpartition.
#
# 			Except for the replacement of the PARTITION keyword with SUBPARTITION, the syntax for a 
# 			subpartition definintion is identical to that for a partition definition.
#
# 			Subpartitioning must be done by HASH or KEY, and can be done only on RANGE or LIST partitions.
#
# 			See SECTION 23.2.6, "SUBPARTITIONING"
#
# PARTITIONING BY GENERATED COLUMNS
#
# Partitioning by generated columns is permitted.
#
# For example:
#
# 		CREATE TABLE t1 (
# 			s1 INT,
# 			s2 INT AS (EXP(s1)) STORED
# 		)
# 		PARTITION BY LIST (s2) (
# 			PARTITION p1 VALUES IN (1)
# 		);
#
# Partitioning sees a generated column as a regular column, which enables workarounds for limitations
# on functions that are not permitted for partitioning (see SECTION 23.6.3, "PARTITIONING LIMITATIONS RELATING TO FUNCTIONS")
#
# THe preceding example demonstrates this technique:
#
# 		EXP() cannot be used directly in teh PARTITION BY clause, but a generated
# 		column defined using EXP() is permitted.
#
# 13.1.20.1 CREATE TABLE STATEMENT RETENTION
#
# The original CREATE_TABLE statement, including all specifications and table options are stored by
# MySQL when the table is created.
#
# The information is retained so that if you change storage engines, collations or other settings
# using an ALTER_TABLE statement, the original table options specified are retained.
#
# This enables you to change between InnoDB and MyISAM table types even though the row formats
# supported by the two engines are different.
#
# Because the text of the original statement is retained, but due to the way that certain values
# and options may be silently reconfigured, the active table definition (accessible through 
# DESCRIBE or with SHOW_TABLE_STATUS) and the table creation string (accessible through
# SHOW_CREATE_TABLE) may report different values.
#
# For InnoDB tables, SHOW_CREATE_TABLE and the Create_options column reported by SHOW_TABLE_STATUS
# show the actual ROW_FORMAT and KEY_BLOCK_SIZE attributes used by the table.
#
# In previous MySQL releases, the originally specified values for these attributes were reported.
#
# 13.1.20.2 FILES CREATED BY CREATE TABLE
#
# For an InnoDB table created in a file-per-table tablespace or general tablespace, table data
# and associated indexes are stored in an ibd file in the database directory.
#
# When an InnoDB table is created in the system tablespace, table data and indexes are stored
# in the ibdata* files that represent the system tablespace.
#
# The innodb_file_per_table option controls whether tables are created in file-per-table
# tablespaces or the system tablespace, by default.
#
# The TABLESPACE option can be used to place a table in a file-per-table tablespace,
# general tablespace, or the  system tablespace, regardless of the innodb_file_per_table
# setting.
#
# For MyISAM tables, the storage engine creates data and index files.
#
# Thus, for each MyISAM table tbl_name, there are two disk files.
#
# FILE 			PURPOSE
#
# tbl_name.MYD Data file
# tbl_name.MYI Index file
#
# Chapter 16, ALTERNATIVE STORAGE ENGINES describes what files each storage engine
# creates to represent tables.
#
# If a table name contains special characters, the names for the table files 
# contain encoded versions of those characters as described in SECTION 9.2.3, "MAPPING OF IDENTIFIERS TO FILE NAMES"
#
# 13.1.20.3 CREATE TEMPORARY TABLE SYNTAX
#
# You can use the TEMPORARY keyword when creating a table.
#
# A TEMPORARY table is visible only within the current session, and is dropped automatically when
# the session is closed.
#
# THis means that two different sessions can use the same temporary table name without conflicting
# with each other or with an existing non-TEMPORARY table of the same name.
#
# (The existing table is hidden until the temporary table is dropped)
#
# InnoDB does not support compressed temporary tables.
#
# When innodb_strict_mode is enabled (the default), CREATE_TEMPORARY_TABLE returns an error
# if ROW_FORMAT=COMPRESSED or KEY_BLOCK_SIZE is specified.
#
# If innodb_strict_mode is disabled, warnings are issued and the temporary table is created
# using a non-compressed row format.
#
# The innodb_file_per-table option does not affect the creation of InnoDB temporary tables.
#
# CREATE_TABLE causes an implicit commit, except when used with the TEMPORARY keyword.
#
# See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# TEMPORARY tables have a very loose relationship with databases (schemas)
#
# Dropping a database does not automatically drop any TEMPORARY tables created
# within that database.
#
# ALso, you can create a TEMPORARY table in a nonexistent database if you qualify the
# table name with the database name in the CREATE TABLE statement.
#
# In this case, all subsequent references to the table must be qualified with the database name.
#
# To create a temporary table, you must have the CREATE_TEMPORARY_TABLES privilege.
#
# After a session has created a temporary table, the server performs no further
# privilege checks on the table.
#
# The creating session can perform any operation on the table, such as DROP_TABLE,
# INSERT, UPDATE or SELECT.
#
# One implication of this behavior is that a session can manipulate its temporary table
# even if the current user has no privilege to create them.
#
# Suppose that hte current user does not have the CREATE_TEMPORARY_TABLES privilege
# but is able to execute a definer-context stored procedure that executes with the
# privileges of a user who does have CREATE_TEMPORARY_TABLES and that creates a 
# temporary table.
#
# While the procedure executes, the session uses the privileges of the defining user.
#
# After hte procedure returns, the effective privileges revert to those of the current
# user, which can still see the temporary table and perform any operation on it.
#
# YOu cannot use CREATE TEMPORARY TABLE --- LIKE to create an empty table based on the
# definition of a table that resides in teh mysql tablespace, InnoDB system tablespace
# (innodb_system), or a general tablespace.
#
# The tablespace definition for such a table includes a TABLESPACE attribute that defines
# the tablespace where the table resides, and the aformentioned tablespaces do not support
# temporary tables.
#
# To create a temporary table based on the definition of such a table, use this syntax instead:
#
# 		CREATE TEMPORARY TABLE new_tbl SELECT * FROM orig_tbl LIMIT 0;
#
# NOTE:
#
# 		Support for TABLESPACE = innodb_file_per_table and TABLESPACE = innodb_temporary clauses
# 		with CREATE_TEMPORARY_TABLE is deprecated as of MysQL 8.0.13.
#
# 13.1.20.4 CREATE TABLE --- LIKE SYNTAX
#
# Use CREATE TABLE --- LIKE to create an empty table based on the definition
# of another table, including any column attributes and indexes defined in the
# original table:
#
# 		CREATE TABLE new_tbl LIKE orig_tbl;
#
# The copy is created using the same version of the table storage format as the 
# original table.
#
# The SELECT privilege is required on the original table
#
# LIKE works only for base tables, not for views.
#
# IMPORTANT:
#
# 		You cannot execute CREATE TABLE or CREATE TABLE --- LIKE while a LOCK_TABLES statement is in effect.
#
# 		CREATE_TABLE_---_LIKE makes the same checks as CREATE_TABLE
#
# 		This means that if the current SQL mode is different from the mode in effect when
# 		the original table was created, the table definition might be considered invalid for
# 		the new mode and the statement will fail.
#
# For CREATE TABLE --- LIKE, the destination table preserves generated column information
# from the original table.
#
# For CREATE TABLE --- LIKE, the destination table expression default values from the original table
#
# CREATE TABLE --- LIKE does not preserve any DATA DIRECTORY or INDEX DIRECTORY table options
# that were specified for the original table, or any foreign key defintiions.
#
# If hte original table is a TEMPORARY table, CREATE TABLE --- LIKE does not preserve TEMPORARY
#
# To create a TEMPORARY destination table, use CREATE TEMPORARY TABLE --- LIKE
#
# Tables created in the mysql tablespace, the InnoDB system tablespace (innodb_system),
# or general tablespaces include a TABLESPACE attribute in the table definition, which
# defines the tablespace where the table resides.
#
# Due to a temporary regression, CREATE TABLE --- LIKE preserves the TABLESPACE attribute
# and creates the table in the defined tablespace regardless of the innodb_file_per_table
# setting.
#
# To avoid the TABLESPACE attribute when creating an empty table based on the definition
# of such a table, use this syntax instead:
#
# 		CREATE TABLE new_tbl SELECT * FROM orig_tbl LIMIT 0;
#
# 13.1.20.5 CREATE TABLE --- SELECT SYNTAX
#
# You can create one table from another by adding a SELECT statement at the end
# of the CREATE_TABLE statement:
#
# 		CREATE TABLE new_tbl [AS] SELECT * FROM orig_tbl;
#
# MySQL creates new columns for all elements in the SELECT. For example:
#
# 		CREATE TABLE test (a INT NOT NULL AUTO_INCREMENT,
# 					PRIMARY KEY (a), KEY(b))
# 					ENGINE=MyISAM SELECT b,c FROM test2;
#
# This creates a MyISAM table with three columns, a,b and c.
#
# The ENGINE option is part of the CREATE_TABLE statement, and should
# not be used following the SELECT; This would result in syntax error.
#
# THe same is true for other CREATE_TABLE options such as CHARSET.
#
# Notice that the columns from the SELECT statements are appended to the right
# side of the table, not overlapped onto it.
#
# Take the following example:
#
# 		SELECT * FROM foo;
# 		+----+
# 		| n  |
# 		+----+
# 		| 1  |
# 		+----+
#
# 		CREATE TABLE bar (m INT) SELECT n FROM foo;
# 		Query OK, 1 row affected (0.02 sec)
# 		Records: 1 Duplicates: 0 Warnings: 0
#
# 		SELECT * FROM bar;
# 		+--------+-------+
# 		| m 		| n 	  |
# 		+--------+-------+
# 		| NULL   | 1 	  |
# 		+--------+-------+
# 		1 row in set (0.00 sec)
#
# For each row in table foo, a row is inserted in bar with the value from foo
# and default values for the new columns.
#
# In a table resulting from CREATE_TABLE_---_SELECT, columns named only in the
# CREATE_TABLE part come first.
#
# Columns named in both parts or only in the SELECT part come after that.
#
# The data type of SELECT columns can be overridden by also specifying
# the column in the CREATE_TABLE part
#
# If any errors occur while copying the data to the table, it is automatically
# dropped and not created.
#
# You can precede the SELECT by IGNORE or REPLACE to indicate how to handle rows
# that duplicate unique key values.
#
# With IGNORE, rows that duplicate an existing row on a unqiue key value are discarded.
#
# With REPLACE, new rows replace rows that have the same unique key value.
#
# If neither IGNORE nor REPLACE is specified, duplicate unique key values result
# in an error.
#
# For more information, see COMPARISON OF THE IGNORE KEYWORD AND STRICT SQL MODE.
#
# Because the ordering of the rows in the underlying SELECT statements cannot always
# be determined, CREATE TABLE --- IGNORE SELECT and CREATE TABLE --- REPLACE SELECT
# statements are flagged as unsafe for statement-based replication.
#
# Such statements produce a warning in the error log when using statement-based mode
# and are written to the binary log using the row-based format when using MIXED mode.
#
# See also SECTION 17.2.1.1, "ADVANTAGES AND DISADAVANTAGES OF STATEMENT-BASED AND ROW-BASED REPLICATION"
#
# CREATE_TABLE_---_SELECT does not automatically create any indexes for you.
#
# This is done intentionally to make the statement as flexible as possible.
#
# If you want to have indexes in the created table, you should specify these before
# the SELECT statement:
#
# 		CREATE TABLE bar (UNIQUE (n)) SELECT n FROM foo;
#
# For CREATE TABLE --- SELECT, the destination table does not preserve information about
# whether columns in the selected-from table are generated columns.
#
# The SELECT part of the statement cannot assign values to generated columns in the destination table.
#
# For CREATE TABLE --- SELECT, the destination table does preserve expression default values
# from the original table.
#
# Some conversion of data types might occur. For example, the AUTO_INCREMENT attribute is not
# preserved, and VARCHAR columns can become CHAR columns.
#
# Retrained attributes are NULL (or NOT NULL) and, for those columns that have them,
# CHARACTER SET, COLLATION, COMMENT, and the DEFAULT clause.
#
# When creating a table with CREATE_TABLE_---_SELECT, make sure to alias any function
# calls or expressions in the query.
#
# If you do not, the CREATE statement might fail or result in undesirable column names.
#
# CREATE TABLE artists_and_works
# 		SELECT artist.name, COUNT(work.artist_id) AS number_of_works
# 		FROM artist LEFT JOIN work ON artist.id = work.artist_id
# 		GROUP BY artist.id;
#
# You can also explicitly specify the data type for a column in the created table:
#
# 		CREATE TABLE foo (a TINYINT NOT NULL) SELECT b+1 AS a FROM bar;
#
# For CREATE_TABLE_---_SELECT, if IF NOT EXISTS is given and the target table exists,
# nothing is inserted into the destination table, and the statement is not logged.
#
# To ensure that the binary log can be used to re-create the original tables, MySQL
# does not permit concurrent inserts during CREATE_TABLE_---_SELECT
#
# You cannot use FOR UPDATE as part of the SELECT in a statement such as 
# CREATE_TABLE_new_table_SELECT_---_FROM_old_table_---
#
# Attempting to do so, causes the statement to fail.
#
# 13.1.20.6 USING FOREIGN KEY CONSTRAINTS
#
# MySQL supports foreign keys, which let you cross-reference related data across
# tables, and foreign key constraints, which help keep this spread-out data consistent.
#
# The essential syntax for a foreign key constraint definition in a CREATE_TABLE or
# ALTER_TABLE statement looks like this:
#
# 		[CONSTRAINT [symbol]] FOREIGN KEY
# 			[index_name] (col_name, ---)
# 			REFERENCES tbl_name (col_name, ---)
# 			[ON DELETE reference_option]
# 			[ON UPDATE reference_option]
# 		
# 		reference_option:
# 			RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT
#
# index_name represents a foreign key ID.
#
# The index_name value is ignored if there is already an explicitly defined index
# on the child table that  can support the foreign key.
#
# Otherwise, MySQL implicitly creates a foreign key index that is named according
# to the following rules:
#
# 		) If defined, the CONSTRAINT symbol value is used. Otherwise, the FOREIGN KEY index_name value is used
#
# 		) If neither a CONSTRAINT symbol or FOREIGN KEY index_name is defined, the foreign key index name is generated
# 			using the name of the referencing foreign key column.
#
# The FOREIGN KEY index_name value must be unique in the database.
#
# Foreign keys definitions are subject to the following conditions:
#
# 		) Foreign key relationships involve a parent table that holds the central data values,
# 			and a child table with identical values pointing back to its parent.
#
# 			The FOREIGN KEY clause is specified in the child table.
#
# 			The parent and child tables must use the same storage engine.
#
# 			They must not be TEMPORARY tables.
#
# 			In MySQL 8.0, creation of a foreign key constraint requires the REFERENCES privilege
# 			for the parent table
#
# 		) Corresponding columns in the foreign key and the referenced key must have similar data types.
#
# 			The size and sign of integer types must be the same. The length of string types need
# 			not be the same.
#
# 			For nonbinary (character) string columns, the character set and collation must be the same.
#
# 		) When foreign_key_checks is enabled, which is the default setting, character set conversion is not
# 			permitted on tables that include a character string column used in a foreign key constraint.
#
# 			The workaround is described in SECTION 13.1.9, "ALTER TABLE SYNTAX"
#
# 		) MySQL requires indexes on foreign keys and referenced keys so that foreign key checks can be fast
#  		and not require a table scan.
#
# 			In the referencing table, there must be an index where the foreign key columns are listed as
# 			the first columns in the same order.
#
# 			Such an index is created on the referencing table automatically if it does not exist.
#
# 			This index might be silently dropped later, if you create another index that can be used to enforce
# 			the foreign key constraint.
#
# 			index_name, if given, is used as described previously.
#
# 		) InnoDB permits a foreign key to reference any column or group of columns.
#
# 			However, in the reference table, there must be an index where the referenced
# 			columns are listed as the first columns in the same order.
#
# 			NDB requires an explicit unique key (or primary key) on any column referenced as a foreign key.
#
# 		) Index prefixes on foreign key columns are not supported.
#
# 			One consequence of this is that BLOB and TEXT columns cannot be included in a foreign key because
# 			indexes on those columns must always include a prefix length.
#
# 		) If the CONSTRAINT symbol clause is given, the symbol value, if used, must be unique in the database.
#
# 			A duplicate symbol will result in an error similar to: 
#
# 				ERROR 1022 (2300): Can't write; duplicate key in table '#sql- 464_1'
#
# 			If the clause is not given, or a symbol is not included following the CONSTRAINT keyword,
# 			a name for the constraint is created automatically.
#
# 		) InnoDB does not currently support foreign keys for tables with user-defined partitioning.
#
# 			This includes both parent and child tables.
#
# 			This restriction does not apply for NDB tables that are partitioned by KEY or LINEAR KEY
# 			(the only user partitioning types supported by the NDB storage engine);
#
# 			these may have foreign key references or be the targets of such references.
#
# 		) For NDB tables, ON UPDATE CASCADE is not supported where the reference is to be the parent
# 			table's primary key.
#
# Additional aspects of FOREIGN KEY constraint usage are described under the following topics in this section:
#
# 		) REFERENTIAL ACTIONS
#
# 		) EXAMPLES OF FOREIGN KEY CLAUSES
#
# 		) ADDING FOREIGN KEYS
#
# 		) DROPPING FOREIGN KEYS
#
# 		) FOREIGN KEYS AND OTHER MYSQL STATEMENTS
#
# 		) FOREIGN KEYS AND THE ANSI/ISO SQL STANDARD
#
# 		) FOREIGN KEY METADATA 
#
# 		) FOREIGN KEY ERRORS
#
# REFERENTIAL ACTIONS
#
# This section describes how foreign keys help guarantee referential integrity.
#
# For storage engines supporting foreign keys, MySQL rejects any INSERT or UPDATE operation that
# attempts to create a foreign key value in a child  table if there is not a matching candidate
# key value in the parent table.
#
# When an UPDATE or DELETE operation affects a key value in the parent table that has matching
# rows in the child table, the result depends on the referential action specified using
# ON UPDATE and ON DELETE subclauses of the FOREIGN KEY clause.
#
# MySQL supports five options regarding the action to be taken, listed here:
#
# 		) CASCADE: Delete or update the row from the parent table, and automatically delete or update
# 			the matching rows in the child table.
#
# 			Both ON DELETE CASCADE and ON UPDATE CASCADE are supported.
#
# 			Between two tables, do not define several ON UPDATE CASCADE clauses that act
# 			on the same column in the parent table or in the child table.
#
# 			If a FOREIGN KEY clause is defined on both tables in a foreign key relationship,
# 			making both tables a parent and child, an ON UPDATE CASCADE or ON DELETE CASCADE
# 			subclause defined for one FOREIGN KEY clause must be defined for the other in order
# 			for cascading operations to succeed.
#
# 			If an ON UPDATE CASCADE or ON DELETE CASCADE subclause is only defined for one 
# 			FOREIGN KEY clause, cascading operations fail with an error.
#
# 			NOTE:
#
# 				Cascaded foreign key actions do not active triggers
#
# 		) SET NULL:
#
# 			Delete or update the row from the parent table, and set the foreign key column
# 			or columns in the child table to NULL.
#
# 			Both ON DELETE SET NULL and ON UPDATE SET NULL clauses are supported.
#
# 			If you specify a SET NULL action, make sure that you have not declared the columns
# 			in the child table as NOT NULL
#
# 		) RESTRICT:
#
# 			Rejects the delete or update operation for the parent table.
#
# 			Specifying RESTRICT (or NO ACTION) is the same as omitting the ON DELETE
# 			or ON UPDATE clause.
#
# 		) NO ACTION:
#
# 			A keyword from standard SQL. In MySQL, equivalent to RESTRICT.
#
# 			The MySQL Server rejects the delete or update operation for the
# 			parent table if there is a related foreign key value in the referenced table.
#
# 			Some database systems have deferred checks, and NO ACTION is a deferred check.
#
# 			In MySQL, foreign key constraints are checked immediately, so NO ACTION is the same
# 			as RESTRICT.
#
# 		) SET DEFAULT:
#
# 			This action is recognized by the MySQL parser, but both InnoDB and NDB reject table
# 			defintions containing ON DELETE SET DEFAULT or ON UPDATE SET DEFAULT clauses.
#
# For an ON DELETE or ON UPDATE that is not specified, the default action is always RESTRICT.
#
# MySQL supports foreign key references between one column and another within a table.
#
# (A column cannot have a foreign key reference to itself)
#
# In these cases, "child table records" really refers to dependent records within the
# same table.
#
# A foreign key constraint on a stored generated column cannot use ON UPDATE CASCADE,
# ON DELETE SET NULL, ON UPDATE SET NULL, ON DELETE SET DEFAULT or ON UPDATE SET DEFAULT.
#
# A foreign key constraint cannot reference a virtual generated column.
#
# For InnoDB restrictions related to foreign keys and generated columns, see
# SECTION 15.6.1.5, "InnoDB AND FOREIGN KEY CONSTRAINTS"
#
# EXAMPLES OF FOREIGN KEY CLAUSES
#
# Here is a simple example that relates parent and child tables through a single-column
# foreign key:
#
# 		CREATE TABLE parent (
# 			id INT NOT NULL,
# 			PRIMARY KEY (id)
# 		) ENGINE=INNODB;
#
# 		CREATE TABLE child (
# 			id INT,
# 			parent_id INT,
# 			INDEX par_ind (parent_id),
# 			FOREIGN KEY (parent_id)
# 				REFERENCES parent(id)
# 				ON DELETE CASCADE
# 		) ENGINE=INNODB;
#
# A more complex example in which a product_order table has foreign keys for two other tables.
#
# One foreign key references a two-column index in the product table.
#
# The other references a single-column index in the customer table:
#
# 		CREATE TABLE product (
# 			category INT NOT NULL, id INT NOT NULL,
# 			price DECIMAL,
# 			PRIMARY KEY(category, id)
# 		) ENGINE=INNODB;
#
# 		CREATE TABLE customer (
# 			id INT NOT NULL,
# 			PRIMARY KEY (id)
# 		) ENGINE=INNODB;
#
# 		CREATE TABLE product_order (
# 			no INT NOT NULL AUTO_INCREMENT,
# 			product_category INT NOT NULL,
# 			product_id INT NOT NULL,
# 			customer_id INT NOT NULL,
#
# 			PRIMARY KEY(no),
# 			INDEX (product_category, product_id),
# 			INDEX (customer_id),
#
# 			FOREIGN KEY (product_category, product_id)
# 				REFERENCES product(category, id)
# 				ON UPDATE CASCADE ON DELETE RESTRICT,
#
# 			FOREIGN KEY (customer_id)
# 				REFERENCES customer(id)
# 		) 	ENGINE=INNODB;
#
# ADDING FOREIGN KEYS
#
# You can add a new foreign key constraint to an existing table by using
# ALTER_TABLE
#
# The syntax relating to foreign keys for this statement is shown here:
#
# 		ALTER TABLE tbl_name
# 			ADD [CONSTRAINT [symbol]] FOREIGN KEY
# 			[index_name] (col_name, ---)
# 			REFERENCES tbl_name (col_name,---)
# 			[ON DELETE reference_option]
# 			[ON UPDATE reference_option]
#
# The foreign key can be self referential (referring to the same table)
#
# When you add a foreign key constraint to a table using ALTER_TABLE,
# remember to create the required indexes first.
#
# DROPPING FOREIGN KEYS
#
# You can also use ALTER_TABLE to drop foreign keys, using the syntax shown here:
#
# 		ALTER TABLE tbl_name DROP FOREIGN KEY fk_symbol;
#
# If the FOREIGN KEY clause included a CONSTRAINT name when you created the foreign key,
# you can refer to that name to drop the foreign key.
#
# Otherwise, the fk_symbol value is generated internally when the foreign key is created.
#
# To find out the symbol value when you want to drop a foreign key, use a SHOW_CREATE_TABLE
# statement, as shown here:
#
# 		SHOW CREATE TABLE ibtest11c\G
# 		************************** 1. row ******************************
# 						Table: ibtest11c
# 			Create Table  : CREATE TABLE `ibtest11c` (
# 				`A` int(11) NOT NULL auto_increment,
# 				`D` int(11) NOT NULL default '0',
# 				`B` varchar(200) NOT NULL default '',
# 				`C` varchar(175) default NULL,
# 				PRIMARY KEY (`A`, `D`, `B`),
# 				KEY `B` (`B`,`C`),
# 				KEY `C` (`C`),
# 				CONSTRAINT `0_38775` FOREIGN KEY (`A`, `D`)
# 			REFERENCES `ibtest11a` (`A`, `D`)
# 			ON DELETE CASCADE ON UPDATE CASCADE,
# 				CONSTRAINT `0_38776` FOREIGN KEY (`B`, `C`)
# 			REFERENCES `ibtest11a` (`B`, `C`)
# 			ON DELETE CASCADE ON UPDATE CASCADE
# 			) ENGINE=INNODB CHARSET=utf8mb4
# 			1 row in set (0.01 sec)
#
# 			ALTER TABLE ibtest11c DROP FOREIGN KEY `0_38775`;
#
# Adding and dropping a foreign key in the same ALTER_TABLE statement is supported for
# ALTER_TABLE_---_ALGORITHM=INPLACE but is unsupported for ALTER_TABLE_---_ALGORITHM=COPY
#
# In MySQL 8.0, the server prohibits changes to foreign key columns with the potential
# to cause loss of referential integrity.
#
# A workaround is to use ALTER_TABLE_---_DROP_FOREIGN_KEY before changing the column
# defintion and ALTER_TABLE_---_ADD_FOREIGN_KEY afteward
#
# FOREIGN KEYS AND OTHER MYSQL STATEMENTS
#
# Table and column identifiers in a FOREIGN KEY --- REFERENCES --- clause can be quoted
# within backticks (`)
#
# Alternatively, double quotation marks (") can be used if the ANSI_QUOTES SQL mode is enabled.
#
# The setting of the lower_case_table_names system variable is also taken into account.
#
# You can view a child table's foreign key definitions as part of the output of the
# SHOW_CREATE_TABLE statement:
#
# 		SHOW CREATE TABLE tbl_name;
#
# You can also obtain information about foreign keys by querying the INFORMATION_SCHEMA.KEY_COLUMN_USAGE table
#
# You can find information about foreign keys used by InnoDB tables in the INNODB_FOREIGN and
# INNODB_FOREIGN_COLS tables, also in the INFORMATION_SCHEMA database.
#
# mysqldump produces correct defintiions of tables in the dump file, including the foreign keys for child tables.
#
# TO make it easier to reload dump files that have foreign key relationships, mysqldump automatically
# includes a statement in the dump output to set foreign_key_checks to 0
#
# This avoids problems with tables having to be reloaded in a particular order when the dump is
# reloaded.
#
# It is also possible to set this variable manually:
#
# 		SET foreign_key_checks = 0;
# 		SOURCE dump_file_name;
# 		SET foreign_key_checks = 1;
#
# This enables you to import the tables in any order if the dump file contains tables
# that are not correctly ordered for foreign keys.
#
# It also speeds up the import operation.
#
# Setting foreign_key_checks to 0 can also be useful for ignoring foreign key constraints
# during LOAD_DATA and ALTER_TABLE operations.
#
# However, even if foreign_key_checks = 0, MySQL does not permit the creation of a foreign key
# constraint where a column references a nonmatching column type.
#
# ALso, if a table has foreign key constraints, ALTER_TABLE cannot be used to alter the table
# to use another storage engine.
#
# To change the storage engine, you must drop any foreign key constarints first.
#
# You cannot issue DROP_TABLE for a table that is referenced by a FOREIGN KEY constraint,
# unless you do SET foreign_key_checks = 0
#
# When you drop a table, any constraints that were defined in the statement used to create
# that table are also dropped.
#
# If you re-create a table that was dropped, it must have a definition that conforms to
# the foreign key constraints referencing it.
#
# It must have the correct column names and types, and it must have indexes on the referenced
# keys, as stated earlier.
#
# If these are not satisfied, MySQL returns Error 1005 and refers to Error 150 in the error message,
# which means that a foreign key constraint was not correctly formed.
#
# SImilarly, if an ALTER_TABLE fails due to Error 150, this means that a foreign key definition
# would be incorrectly formed for the altered table
#
# For InnoDB tables, you can obtain a detailed explanation of the most recent InnoDB foreign key
# error in the MySQL Server, by checking the output of SHOW_ENGINE_INNODB_STATUS
#
# MySQL extends metadata locks, as necessary, to tables that are related by a foreign key constraint.
#
# Extending metadata locks prevents conflicting DML and DDL operations from executing concurrently
# on related tables.
#
# This feature also enables updates to foreign key metadata when a parent table is modified.
#
# In earlier MysQl releases, foreign key metadata, which is owned by the child table, could not
# be updated safely.
#
# If a table is locked explicitly with LOCK_TABLES, any tables related by a foreign key constraint
# are opened and locked implicitly.
#
# For foreign key checks, a shared read-only lock (LOCK_TABLES_READ) is taken on related tables.
#
# For cascading updates, a shared-nothing write lock (LOCK_TABLES_WRITE) is taken on related
# tables that are involved in the operation
#
# FOREIGN KEYS AND THE ANSI/ISO SQL STANDARD
#
# For users familiar with the ANSI/ISO SQL Standard, please note htat no storage engine, including
# innoDB, recognizes or enforces the MATCH clause used in referential-integrity constraint defintiions.
#
# Use of an explicit MATCH clause will not have the specified effect, and also causes ON DELETE
# and ON UPDATE clauses to be ignored.
#
# For these reasons, specifying MATCH should be avoided
#
# The MATCH clause in the SQL standard controls how NULL values in a composite (multiple-column)
# foreign key are handled when comparing to a primary key
#
# MySQL essentialy implements the semantics defined by MATCH SIMPLE, which permit a foreign key
# to be all or partially NULL
#
# In that case, the (child table) row containing such a foreign key is permitted to be inserted,
# and does not match any row in the referenced (parent) table
#
# It is possible to implement other semantics using triggers.
#
# Additionally, MySQL requires that the referenced columns be indexed for performance reasons.
#
# However, the system does not enforce a requirement that the referenced columns be UNIQUE
# or be declared NOT NULL
#
# The handling of foreign key references to nonunique keys or keys that contain NULL values
# is not well defined for operations such as UPDATE or DELETE CASCADE
#
# You are advised to use foreign keys that reference only UNIQUE (including PRIMARY)
# and NOT NULL keys.
#
# Furthermore, MySQL parses but ignores "inline REFERENCES specifications" (as defined in the
# SQL standard) where the references are defined as part of the column specification.
#
# MySQL accepts REFERENCES clauses only when specified as part of a separate FOREIGN KEY
# specification.
#
# For storage engines that do not support foreign keys (such as MyISAM), MySQL Server
# parses and ignores foreign key specifications.
#
# FOREIGN KEY METADATA
#
# The INFORMATION_SCHEMA.KEY_COLUMN_USAGE table identifies the key columns that have constraints.
#
# Metadata specific to InnoDB foreign keys is found in the INNODB_SYS_FOREIGN and INNODB_SYS_FOREIGN_COLS
# tables
#
# FOREIGN KEY ERRORS
#
# In the event of a foreign key error involving InnoDB tables (usually Error 150 in the MySQL Server),
# information about the most recent InnoDB foreign key error can be obtained by checking
# SHOW_ENGINE_INNODB_STATUS output.
#
# 	WARNING:
#
# 		If a user has table-level privileges for all parent tables, ER_NO_REFERENCED_ROW_2 and
# 		ER_ROW_IS_REFERENCED_2 error messages for foreign key operations expose information
# 		about parent tables.
#
# 		If a user does not have table-level privileges for all parent tables, more generic
# 		eror messages are displayed instead (ER_NO_REFERENCED_ROW and ER_ROW_IS_REFERENCED)
#
# 		An exception is that, for stored programs defiend to execute with DEFINER privileges,
# 		the user against which privileges are assessed is the user in the program DEFINER clause,
# 		not the invoking user.
#
# 		If that user has table-level parent table privileges, parent table informaiton is
# 		still displayed.
#
# 		In this case, it is the responsibility of the stored program creator to hide
# 		information by including appropriate condition handlers.
#
# 13.1.20.7 SILENT COLUMN SPECIFICATION CHANGES
#
# In some cases, MySQL silently changes column specifications from those given in a 
# CREATE_TABLE or ALTER_TABLE statement.
#
# These might be changes to a data type, to attributes associated with a data type,
# or to an index specification.
#
# ALl changes are subject to the internal row-size limit of 65,535 bytes, which may
# cause some attempts at data type changes to fail.
#
# See SECTION C.10.4, "LIMITS ON TABLE COLUMN COUNT AND ROW SIZE"
#
# 		) Columns that are part of a PRIMARY KEY are made NOT NULL even if not declared that way
#
# 		) Trailing spaces are automatically deleted from ENUM and SET member values when the table is created.
#
# 		) MySQL maps certain data types used by other SQL database vendors to MySQL types.
#
# 			See SECTION 11.10, "USING DATA TYPES FROM OTHER DATABASE ENGINES"
#
# 		) If you include a USING clause to specify an index type that is not permitted for a given storage
# 			engine, but there is another index type available that the engine can use without affecting
# 			query results, the engine uses the available type.
#
# 		) If strict SQL mode is not enabled, a VARCHAR column with a length specification greater than
# 			65535 is converted to TEXT, and a VARBINARY column with a length specification greater than
# 			65535 is converted to BLOB.
#
# 			Otherwise, an error occurs in either of these cases
#
# 		) Specifying the CHARACTER SET binary attribute for a character data type causes the column to be
# 			created as the corresponding binary data type:
#
# 				CHAR becomes BINARY, VARCHAR becomes VARBINARY, and TEXT becomes BLOB
#
# 			For the ENUM and SET data types, this does not occur; they are created as declared.
#
# 			Suppose that you specify a table using this definition:
#
# 				CREATE TABLE t
# 				(
# 					c1 VARCHAR(10) CHARACTER SET binary,
# 					c2 TEXT CHARACTER SET binary,
# 					c3 ENUM('a', 'b', 'c') CHARACTER SET binary
# 				);
#
# 			The resulting table has this definition:
#
# 				CREATE TABLE t
# 				(
# 					c1 VARBINARY(10),
# 					c2 BLOB,
# 					c3 ENUM('a', 'b', 'c') CHARACTER SET binary
# 				);
#
# 			To see whether MySQL uses a data type other than the one you specified, issue a DESCRIBE
# 			or SHOW_CREATE_TABLE statement after creating or altering the table.
#
# 			Certain other data type changes can occur if you compress a table using myisampack.
#
# 			See SECTION 16.2.3.3, "COMPRESSED TABLE CHARACTERISTICS"
#
# 13.1.20.8 CREATE TABLE AND GENERATED COLUMNS
#
# CREATE_TABLE supports the specification of generated columns.
#
# Values of a generated column are computed from an expression included in the column definition.
#
# Generated columns are also supported by the NDB storage engine.
#
# The following simple example shows a table that stores the lengths of the sides of right triangles
# in the sidea and sideb columns, and computes hte length of the hypotenuse in sidec
# (the square root of the sums of the squares of the other sides):
#
# 		CREATE TABLE triangle (
# 			sidea DOUBLE,
# 			sideb DOUBLE,
# 			sidec DOUBLE AS (SQRT(sidea * sidea + sideb * sideb))
# 		);
# 		INSERT INTO triangle (sidea, sideb) VALUES(1,1), (3,4), (6,8);
#
# Selecting from the table yields this result:
#
# 		SELECT * FROM triangle;
# 		+-----------+-------------+-----------------------------+
# 		| sidea     | sideb 		  | sidec 							  |
# 		+-----------+-------------+-----------------------------+
# 		| 1 			| 1 			  | 1.4142135623730951 			  |
# 		| 3 		  	| 4 			  | 5 								  |
# 		| 6 			| 8 			  | 10 								  |
# 		+-----------+-------------+-----------------------------+
#
# ANy application that uses the triangle table has access to the hypotenuse values without
# having to specify the expression that calculates them.
#
# Generated column definitions have this syntax:
#
# 		col_name data_type [GENERATED ALWAYS] AS (expression)
# 			[VIRTUAL | STORED] [NOT NULL | NULL]
# 			[UNIQUE [KEY]] [[PRIMARY] KEY]
# 			[COMMENT 'string']
#
# AS (expression) indicates that the column is genrated and defines the expression
# used to compute column values.
#
# AS may be preceded by GENERATED ALWAYS to make the generated nature of the column
# more explicit.
#
# Constructs that are permitted or prohibited in the expression are discussed later.
#
# The VIRTUAL or STORED keyword indicates how column values are stored, which has implications
# for column use:
#
# 		) VIRTUAL: Column values are not stored, but are evaluated when rows are read, immediately
# 			after any BEFORE triggers.
#
# 			A virtual column takes no storage
#
# 			InnoDB supports secondary indexes on virtual columns.
#
# 			See SECTION 13.1.20.9, "SECONDARY INDEXES AND GENERATED COLUMNS"
#
# 		) STORED: Column values are evaluated and stored when rows are inserted or updated.
#
# 			A stored column does require storage space and can be indexed.
#
# The default is VIRTUAL if neither keyword is specified.
#
# It is permitted to mix VIRTUAL and STORED columns within a table.
#
# Other attributes may be given to indicate whether the column is indexed or can be
# NULL or provide a comment.
#
# Genrated column expressions must adhere to hte following rules.
#
# An error occurs if an expression contains disallowed constructs.
#
# 		) Literals, determinsitic built-in functions, and operators are permitted.
#
# 			A function is determinsitic, if given the same data in tables, multiple
# 			invocations produce the same result, independently of the connected user.
#
# 			Examples of functions that fail this definition:
#
# 				CONNECTION_ID(), CURRENT_USER(), NOW()
#
# 		) Subqueries, parameters, variables, stored functions, and user-defined functions are not permitted.
#
# 		) A generated column definition can refer to other generated columns, but only those occurring
# 			earlier in the table definition.
#
# 			A generated column definition can refer to any base (nongenerated) column in the table
# 			whether its definition occurs earlier or later.
#
# 		) The AUTO_INCREMENT attribute cannot be used in a generated column definition
#
# 		) An AUTO_INCREMENT column cannot be used as a base column in a generated column definition
#
# 		) If expression evaluation causes truncation or provides incorrect input to a function,
# 			the CREATE_TABLE statement terminates with an error and the DDL operation is rejected.
#
# If the expression evaluates to a data type that differs from the declared column type,
# Implicit coercion to the declared type occurs according to the usual MySQL type-conversion
# rules.
#
# See SECTION 12.2, "TYPE CONVERSION IN EXPRESSION EVALUATION"
#
# NOTE:
#
# 		If any component of the expression depends on the SQL mode, different results may
# 		occur for different uses of the table unless the SQL mode is the same during
# 		all uses.
#
# For CREATE_TABLE_---_LIKE, the destination table preserves generated column information from the original table
#
# For CREATE_TABLE_---_SELECT, the destination table does not preserve information about whether columns
# in the selected-from table are generated columns.
#
# The SELECT part of the statement cannot assign values to generated columns in teh destination table
#
# Partitioning by generated columns is permitted. See CREATING PARTITIONED TABLES
#
# A foreign key constraint on a stored generated column cannot use:
#
# 		) ON UPDATE CASCADE
#
# 		) ON DELETE SET NULL
#
# 		) ON UPDATE SET NULL
#
# 		) ON DELETE SET DEFAULT
#
# 		) ON UPDATE SET DEFAULT
#
# A foreign key constraint cannot reference a virtual generated column
#
# For InnoDB restrictions related to foreign keys and generated columns, see SECTION 15.6.1.5, "InnoDB AND FOREIGN KEY CONSTRAINTS"
#
# Triggers cannot use NEW.col_name or use OLD.col_name to refer to generated columns
#
# For INSERT, REPLACE, and UPDATE,, if a generated column is inserted into, replaced, or updated explicitly,
# the only permitted value is DEFAULT
#
# A generated column in a view is considered updatable because it is possible to assign to it
#
# However, if such a column is updated explicitly, the only permitted value is DEFAULT
#
# Generated columns have several use cases, such as these:
#
# 		) Virtual generated columns can be used as a way to simplify and unify queries.
#
# 			A complicated condition can be defined as a generated column and referred 
# 			to from multiple queries on the table to ensure that all of them use exactly
# 			the same condition.
#
# 		) Stored generated columns can be used as a materialized cache for complicated conditions that
# 			are costly to calculate on the fly.
#
# 		) Generated columns can simulate functional indexes; Use a generated column to define a functional
# 			expression and index it.
#
# 			This can be useful for working with columns of types that cannot be indexed directly,
# 			such as JSON columns;
#
# 			See INDEXING A GENERATED COLUMN TO PROVIDE A JSON COLUMN INDEX, for a detailed example 
# 
# 			For stored generated columns, the disadvantage of htis approach is that values are stored twice,
# 			once as the value of the generated column and once in the index.
#
# 		) If a generated column is indexed, the optimizer recognizes query expressions that match the column
# 			definition and uses indexes from the column as appropriate during query execution, even if a query
# 			does not refer to the column directly by  name.
#
# 			For details, see SECTION 8.3.11, "OPTIMIZER USE OF GENERATED COLUMN INDEXES"
#
# Example:
#
# 		Suppose that a table t1 contains first_name and last_name columns and that applications
# 		frequently construct the full name using an expression like this:
#
# 			SELECT CONCAT(first_name, ' ',last_name) AS full_name FROM t1;
#
# 		One way to avoid writing out the expression is to create a view v1 on t1, which simplifies
# 		applications by enabling them to select full_name directly without using an expression:
#
# 			CREATE VIEW v1 AS
# 			SELECT *, CONCAT(first_name,' ',last_name) AS full_name FROM t1;
#
# 			SELECT full_name FROM v1;
#
# 		A generated column also enables applications to select full_name directly
# 		without the need to define a view:
#
# 			CREATE TABLE t1 (
# 				first_name VARCHAR(10),
# 				last_name VARCHAR(10),
# 				full_name VARCHAR(255) AS (CONCAT(first_name,' ',last_name))
# 			);
#
# 			SELECT full_name FROM t1;
#
# 13.1.20.9 SECONDARY INDEXES AND GENERATED COLUMNS
#
# InnoDB supports secondary indexes on virtual generated columns.
#
# Other index types are not supported. A secondary index defined on a virtual column
# is sometimes referred to as a "virtual index"
#
# A secondary index may be created on one or more virtual columns or on a combination
# of virtual columns and regular columns or stored generated columns.
#
# Secondary indexes that include virtual columns may be defined as UNIQUE
#
# When a secondary index is created on a virtual generated column, generated column values
# are materialized in the records of the index.
#
# If the index is a covering index (one that includes all the columns retrieved by a query),
# generated column values are retrieved from materialized values in the index structure
# instead of computed "on the fly"
#
# There are additional write costs to consider when using a secondary index on a virtual column
# due to computation performed when materializing virtual column values in secondary
# secondary index records during INSERT and UPDATE operations.
#
# Even with additional write costs, secondary indexes on virtual columns may be preferable
# to generated stored columns, which are materialized in the clustered index, resulting
# in larger tables that require more disk space and memory.
#
# If a secondary index is not defined on a virtual column, there are additional costs for
# reads, as virtual column values must be computed each time the column's row is examined.
#
# Values of an indexed virtual column are MVCC-logged to avoid unnecessary recomputation of
# generated column values during rollback or during a purge operation.
#
# The data length of logged values is limited by the index key limit of 767 bytes for
# COMPACT and REDUNDANT row formats, and 3072 bytes for DYNAMIC and COMPRESSED row formats.
#
# Adding or dropping a secondary index on a virtual column is an in-place operation
#
# INDEXING A GENERATED COLUMN TO PROVIDE A JSON COLUMN INDEX
#
# As noted elsewhere, JSON columns cannot be indexed directly
#
# To create an index that references such a column directly, you can define a generated
# column that extracts the information that hsould be indexed, then create an index
# on the generated column, as shown in this example:
#
# 		CREATE TABLE jemp (
# 			c JSON,
# 			g INT GENERATED ALWAYS AS (c->"$.id")),
# 			INDEX i (g)
# 		);
# 		Query OK, 0 rows affected (0.28 sec)
#
# 		INSERT INTO jemp (c) VALUES
# 			('{"id": "1", "name": "Fred"}'), ('{"id": "2", "name": "Wilma"}'),
# 			('{"id": "3", "name": "Barney"}'), ('{"id": "4", "name": "Betty"}');
# 		Query OK, 4 rows affected (0.04 sec)
# 		Records: 4 Duplicates: 0 Warnings: 0
#
# 		SELECT c->>"$.name" AS name
# 			FROM jemp WHERE g > 2;
# 		+-----------+
# 		| name 		|
# 		+-----------+
# 		| Barney 	|
# 		| Betty 	   |
# 		+-----------+
# 		2 rows in set (0.00 sec)
#
# 		EXPLAIN SELECT c->>"$.name" AS name
# 			FROM jemp WHERE g > 2\G
# 		************************ 1. row *******************************
# 						id: 1
# 				select_type: SIMPLE
# 				table  : jemp
# 			partitions: NULL
# 				type   : range
# 		possible_keys: i
# 				key    : i
# 			key_len   : 5
#
# 				ref    : NULL
# 				rows   : 2
# 			filtered  : 100.00
# 			Extra     : Using where
# 	1 row in set, 1 warning (0.00 sec)
#
# SHOW WARNINGS\G
# ************************ 1. row *******************************
# 		Level: Note
# 	Code    : 1003
#  Message : /* select#1 */ select json_unquote(json_extract(`test`.`jemp`.`c`, '$.name'))
# 	AS `name` from `test`.`jemp` where (`test`.`jemp`.`g` > 2)
#  1 row in set (0.00 sec)
#
# (We have wrapped the output from the last statement in this example to fit the viewing area)
#
# When you use EXPLAIN on a SELECT or other SQL statement containing one or more expressions that use
# the -> or ->> operator, these expressions are translated into their equivalents using JSON_EXTRACT()
# and (if needed) JSON_UNQUOTE() instead, as shown here in the output from SHOW_WARNINGS immediately
# following this EXPLAIN statement:
#
# 		EXPLAIN SELECT c->>"$.name"
# 		FROM jemp WHERE g > 2 ORDER BY c->"$.name"\G
# 		*************************** 1. row ***************************
# 					id: 1
# 		select_type: SIMPLE
# 		table      : jemp
# 		partitions : NULL
# 		type       : range
# 		possible_keys: i
# 		key 		  : i
#
# 		key_len    : 5
# 		ref        : NULL
# 		rows       : 2
# 		filtered   : 100.00
# 		Extra      : Using where; Using filesort
# 		1 row in set, 1 warning (0.00 sec)
#
# 		SHOW WARNINGS\G
# 		************************* 1. row *******************************
# 		Level: Note
# 		Code : 1003
# 	  Message: /* select#1 */ select json_unquote(json_extract(`test`.`jemp`.`c`, '$.name'))
# 		AS `c->>"$.name"` from `test`.`jemp` where (`test`.`jemp`.`g` > 2) order by 
# 		json_extract(`test`.`jemp`.`c`,'$.name')
# 		1 row in set (0.00 sec)
#
# See the descriptions of the -> and ->> operators, as well as those of the JSON_EXTRACT() and
# JSON_UNQUOTE() functions, for additional information and examples.
#
# This technique also can be used to provide indexes that indirectly reference columns of other
# types that cannot be indexed directly, such as GEOMETRY columns.
#
# JSON COLUMNS AND INDIRECT INDEXING IN NDB CLUSTER
#
# It is also possible to use indirect indexing of JSON columns in MySQL NDB cluster, subject to the
# following conditions:
#
# 		1. NDB handles a JSON column value internally as a BLOB.
#
# 			This means that any NDB table having one or more JSON columns must have a primary key,
# 			else it cannot be recorded in the binary log.
#
# 		2. The NDB storage engine does not support indexing of virtual columns.
#
# 			Since the default for generated columns is VIRTUAL, you must specify explicitly
# 			the generated column to which to apply the indirect index as STORED.
#
# The CREATE TABLE statement used to create the table jempn shown here is a version of the
# jemp table shown previously, with modifications making it compatible with NDB:
#
# 		CREATE TABLE jempn (
# 			a BIGINT(20) NOT NULL AUTO_INCREMENT PRIMARY KEY,
# 			c JSON DEFAULT NULL,
# 			g INT GENERATED ALWAYS AS (c->"$.name") STORED,
# 			INDEX i (g)
# 		) ENGINE=NDB;
#
# We can populate this table using the following INSERT statement:
#
# 		INSERT INTO jempn (a, c) VALUES
# 			(NULL, '{"id": "1", "name": "Fred"}'),
# 			(NULL, '{"id": "2", "name": "Wilma"}'),
# 			(NULL, '{"id": "3", "name": "Barney"}'),
# 			(NULL, '{"id": "4", "name": "Betty"}');
#
# Now NDB can use index i, as shown here:
#
# 		EXPLAIN SELECT c->>"$.name" AS name
# 			FROM jempn WHERE g > 2\G
# 		******************** 1. row *********************************
# 						id: 1
# 				select_type: SIMPLE
# 					table: jempn
# 			partitions : p0,p1
# 					type : range
# 		possible_keys : i
# 					key  : i
# 				key_len : 5
# 					ref  : NULL
# 				   rows : 3
# 			filtered   : 100.00
# 				Extra   : Using where with pushed condition (`test`.`jempn`.`g` > 2)
# 			1 row in set, 1 warning (0.00 sec)
#
# 		SHOW WARNINGS\G
# 		*********************** 1. row ****************************
# 		 	Level: Note
# 			Code : 1003
# 		Message : /* select#1 */ select
# 		json_unquote(json_extract(`test`.`jempn`.`c`,'$.name')) AS `name` from
# 		`test`.`jempn` where (`test`.`jempn`.`g` > 2)
# 		1 row in set (0.00 sec)
#
# You should keep in mind that a stored generated column uses DataMemory,
# and that an index on such a column uses IndexMemory
#
# 13.1.20.10 SETTING NDB_TABLE OPTIONS
#
# In MySQL NDB Cluster, the table comment in a CREATE TABLE or ALTER_TABLE statement
# can also be used to specify an NDB_TABLE option, which consists of one or more name-value
# pairs, separated by commas if need be, following the string NDB_TABLE= 
#
# Complete syntax for names and values syntax is shown here:
#
# 		COMMENT="NDB_TABLE=ndb_table_option[,ndb_table_option[,---]]"
#
# 		ndb_table_option:
# 			NOLOGGING={1|0}
# 		 | READ_BACKUP={1|0}
# 		 | PARTITION_BALANCE={FOR_RP_BY_NODE|FOR_RA_BY_NODE|FOR_RP_BY_LDM
# 									|FOR_RA_BY_LDM|FOR_RA_BY_LDM_X_2
# 									|FOR_RA_BY_LDM_X_3|FOR_RA_BY_LDM_X_4}
# 		 | FULLY_REPLICATED={1|0}
#
# Spaces are not permitted within the quoted string. The string is case-insensitive.
#
# The four NDB table options that can be set as part of a comment in this way are described
# in more detail in the next few paragraphs.
#
# NOLOGGING: Using 1 corresponds to having ndb_table_no_logging enabled, but has no actual effect.
#
# Provided as a placeholder, mostly for completeness of ALTER_TABLE statements
#
# READ_BACKUP: Setting this option to 1 has the same effect as though ndb_read_backup were enabled;
# enables reading from any replica.
#
# You can set READ_BACKUP for an existing table online, using an ALTER TABLE statement similar to
# one of those shown here:
#
# 		ALTER TABLE --- ALGORITHM=INPLACE, COMMENT="NDB_TABLE=READ_BACKUP=1";
#
# 		ALTER TABLE --- ALGORITHM=INPLACE, COMMENT="NDB_TABLE=READ_BACKUP=0";
#
# For more information about the ALGORITHM option for ALTER TABLE, see
# SECTION 22.5.14, "ONLINE OPERATIONS WITH ALTER TABLE IN NDB CLUSTER"
#
# PARTITION_BALANCE: Provides additional control over assignment and placement of partitions.
#
# The following four schemes are supported:
#
# 		1. FOR_RP_BY_NODE: One partition per node
#
# 			Only one LDM on each node stores a primary partition. Each partition is stored
# 			in the same LDM (same ID) on all nodes
#
# 		2. FOR_RA_BY_NODE: One partition per node group
#
# 			Each node stores a single partition, which can be either a primary replica
# 			or a backup replica.
#
# 			Each partition is stored in the same LDM on all nodes.
#
# 		3. FOR_RP_BY_LDM: One partition for each LDM on each node; the default
#
# 			This is the same behavior as prior to MySQL NDB Cluster 7.5.2, except for a slightly
# 			different mapping of partitions to LDMs, starting with LDM 0 and placing one
# 			partition per node group, then moving on to the next LDM.
#
# 			This is the setting used if READ_BACKUP is set to 1
#
# 		4. FOR_RA_BY_LDM: One partition per LDM in each node group
#
# 			These partitions can be primary or backup partitions
#
# 		5. FOR_RA_BY_LDM_X_2: Two partitions per LDM in each node group.
#
# 			These partitions can be primary or backup partitions.
#
# 		6. FOR_RA_BY_LDM_X_3: Three partitions per LDM in each node group.
#
# 			These partitions can be primary or backup partitions
#
# 		7. FOR_RA_BY_LDM_X_4: Four partitions per LDM in each node group.
#
# 			These partitions can be primary or backup partitions
#
# PARTITION_BALANCE is the preferred interface for setting the number of partitions per table.
#
# Using MAX_ROWS to force the number of partitions is deprecated but continues to be supported
# for backwards compatibility; it is subject to removal in a future release of MySQL NDB Cluster.
# (Bug #81759, Bug #23544301)
#
# FULLY_REPLICATED controls whether the table is fully replicated, that is, whether each data node
# has a complete code of the table.
#
# To enable full replication of the table, use FULLY_REPLICATED=1
#
# This setting can also be controlled using the ndb_fully_replicated system variable.
#
# Setting it to ON enables the option by default for all new NDB tables;
# the default is OFF.
#
# The ndb_data_node_neighbour system variable is also used for fully replicated tables,
# to ensure that when a fully replicated table is accessed, we access the data node
# which is local to this MySQL server.
#
# An example of a CREATE TABLE statement using such a comment when creating an NDB table
# is shown here:
#
# 		CREATE TABLE t1 (
# 			c1 INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
# 			c2 VARCHAR(100),
# 			c3 VARCHAR(100) )
# 		ENGINE=NDB
#
# 		COMMENT="NDB_TABLE=READ_BACKUP=0,PARTITION_BALANCE=FOR_RP_BY_NODE";
#
# The comment is displayed as part of the output of SHOW_CREATE_TABLE 
#
# The text of the comment is also available from querying the MySQL Information Schema
# TABLES table, as in this example:
#
# 		SELECT TABLE_NAME, TABLE_SCHEMA, TABLE_COMMENT
# 		FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME="t1";
# 		+--------------+------------------+-----------------------------------------------------------+
# 		| TABLE_NAME   | TABLE_SCHEMA     | TABLE_COMMENT 															 |
# 		+--------------+------------------+-----------------------------------------------------------+
# 		| t1 			   | c 					 | NDB_TABLE=READ_BACKUP=0,PARTITION_BALANCE=FOR_RP_BY_NODE  |
# 		| t1 			   | d 					 | 																			 |
# 		+--------------+------------------+-----------------------------------------------------------+
# 		2 rows in set (0.00 sec)
#
# This comment syntax is also supported with ALTER_TABLE statements for NDB tables.
#
# Keep in mind that a table comment used with ALTER TABLE replaces any existing comment which
# the table might have.
#
# 		ALTER TABLE t1 COMMENT="NDB_TABLE=PARTTION_BALANCE=FOR_RA_BY_NODE";
# 		Query OK, 0 rows affected (0.40 sec)
# 		Records: 0 Duplicates: 0 Warnings: 0
#
# 		SELECT TABLE_NAME, TABLE_SCHEMA, TABLE_COMMENT
# 		FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME="t1";
# 		+--------------+-------------------------+------------------------------------------------------+
# 		| TABLE_NAME   | TABLE_SCHEMA 			  | TABLE_COMMENT 													|
# 		+--------------+-------------------------+------------------------------------------------------+
# 		| t1 				| c 							  | NDB_TABLE=PARTITION_BALANCE=FOR_RA_BY_NODE 			   |
# 		| t1 				| d 							  | 																		|
# 		+--------------+-------------------------+------------------------------------------------------+
# 		2 rows in set (0.01 sec)
#
# You can also see the value of the PARTITION_BALANCE option in the output of ndb_desc.ndb_desc also
# shows whether the READ_BACKUP and FULLY_REPLICATED options are set for the table.
#
# See the description of this program for more information.
#
# Because the READ_BACKUP value was not carried over to the new comment set by the ALTER TABLE statement,
# there is no longer a way using SQL to retrieve the value previously set for it.
#
# To keep this from happening, it is suggested that you preserve any such values from the existing comment
# string, like this:
#
# 		SELECT TABLE_NAME, TABLE_SCHEMA, TABLE_COMMENT
# 		FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME="t1";
# 		+---------------+-----------------------+----------------------------------------------------------+
# 		| TABLE_NAME 	 | TABLE_SCHEMA 			 | TABLE_COMMENT 														   |
# 		+---------------+-----------------------+----------------------------------------------------------+
# 		| t1 				 | c 							 | NDB_TABLE=READ_BACKUP=0,PARTITION_BALANCE=FOR_RP_BY_NODE |
# 		| t1 				 | d 							 | 																			|
# 		+---------------+-----------------------+----------------------------------------------------------+
# 		2 rows in set (0.00 sec)
#
# 		ALTER TABLE t1 COMMENT="NDB_TABLE=READ_BACKUP=0,PARTITION_BALANCE=FOR_RA_BY_NODE";
# 		Query OK, 0 rows affected (1.56 sec)
# 		Records: 0 Duplicates: 0 Warnings: 0
#
# 		SELECT TABLE_NAME, TABLE_SCHEMA, TABLE_COMMENT
# 		FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME="t1";
# 		+----------------+---------------------------+----------------------------------------------------------+
# 		| TABLE_NAME 	  | TABLE_SCHEMA 					| TABLE_COMMENT 														  |
# 		+----------------+---------------------------+----------------------------------------------------------+
# 		| t1 				  | c 								| NDB_TABLE=READ_BACKUP=0,PARTITION_BALANCE=FOR_RA_BY_NODE |
# 		| t1 				  | d 								| 																			  |
# 		+----------------+---------------------------+----------------------------------------------------------+
# 		2 rows in set (0.01 sec)
#
# 13.1.21 CREATE TABLESPACE SYNTAX
#
# 		CREATE [UNDO] TABLESPACE tablespace_name
#
# 		InnoDB and NDB:
# 			[ADD DATAFILE 'file_name']
#
# 		InnoDB only:
# 			[FILE_BLOCK_SIZE = value]
# 			[ENCRYPTION [=] {'Y' | 'N'}]
#
# 		NDB only:
# 			USE LOGFILE GROUP logfile_group
# 			[EXTENT_SIZE [=] extent_size]
# 			[INITIAL_SIZE [=] initial_size]
# 			[AUTOEXTEND_SIZE [=] autoextend_size]
# 			[MAX_SIZE [=] max_size]
# 			[NODEGROUP [=] nodegroup_id]
# 			[WAIT]
# 			[COMMENT [=] 'string']
#
# 		InnoDB and NDB:
# 			[ENGINE [=] engine_name]
#
# This statement is used to create a tablespace.
#
# The precise syntax and semantics depend on the storage engine used.
#
# In standard MySQL releases, this is always an InnoDB tablespace.
#
# MySQL NDB Cluster also supports tablespaces using the NDB storage engine.
#
# 		) Considerations for InnoDB
#
# 		) Considerations for NDB Cluster
#
# 		) Options
#
# 		) Notes
#
# 		) InnoDB Examples
#
# 		) NDB Example
#
# CONSIDERATIONS FOR INNODB
#
# CREATE_TABLESPACE syntax is used to create general tablespaces or undo tablespaces.
#
# The UNDO keyword, introduced in MySQL 8.0.14, must be specified to create an undo tablespace.
#
# A general tablespace is a shared tablespace.
#
# It can hold multiple tables, and supports all table row formats.
#
# General tablespaces can be created in a location relative to or independent
# of the data directory.
#
# After creating an InnoDB general tablespace, use CREATE_TABLE_tbl_name_---_TABLESPACE_[=]_tablespace_name
# or ALTER_TABLE_tbl_name_TABLESPACE_[=]_tablespace_name to add tables to the tablespace.
#
# For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# Undo tablespaces contain undo logs.
#
# Undo tablespaces can be created in a chosen location by specifying a fully qualified data file path.
#
# For more information, see SECTION 15.6.3.4, "UNDO TABLESPACES"
#
# CONSIDERATIONS FOR NDB CLUSTER
#
# This statement is used to create a tablespace, which can contain one or more data files, providing
# storage space for NDB Cluster Disk Data tables (see SECTION 22.5.13, "NDB CLUSTER DISK TABLES")
#
# One data file is created and added to the tablespace using this statement.
#
# Additional data files may be added to the tablespace by using the ALTER_TABLESPACE statement
# (see SECTION 13.1.10, "ALTER TABLESPACE SYNTAX")
#
# NOTE:
#
# 		All NDB Cluster Disk Data objects share the same namespace.
#
# 		This means that each Disk Data object must be uniquely named (and not merely each
# 		Disk Data object of a given type)
#
# 		For example, you cannot have a tablespace and a log file group with the same name,
# 		or a tablespace and a data file with the same name.
#
# A log file group of one or more UNDO log files must be assigned to the tablespace to be created
# with the USE LOGFILE GROUP clause.
#
# logfile_group must be an existing log file group created with CREATE_LOGFILE_GROUP
# (see SECTION 13.1.16, "CREATE LOGFILE GROUP SYNTAX")
#
# Multiple tablespaces may use the same log file group for UNDO logging.
#
# When setting EXTENT_SIZE or INITIAL_SIZE, you may optionally follow the number with a 
# one-letter abbreviation for an order of magnitude, similar to those used in my.cnf
#
# Generally, this is one of the letters M (for megabytes) or G (for gigabytes)
#
# INITIAL_SIZE and EXTENT_SIZE qre subject to rounding as follows:
#
# 		) EXTENT_SIZE is rounded up to the nearest whole multiple of 32K
#
# 		) INITIAL_SIZE is rounded down to the nearest whole multiple of 32K; this result is rounded up
# 			to the nearest whole multiple of EXTENT_SIZE (after any rounding)
#
# The rounding just described is done explicitly, and a warning is issued by the MySQL Server
# when any such rounding is performed.
#
# The rounded values are also used by the NDB kernel for calculating INFORMATION_SCHEMA.FILES
# column values and other purposes.
#
# However, to avoid an unexpected result, we suggest that you always use whole multiples of
# 32K in specifying these options.
#
# When CREATE_TABLESPACE is used with ENGINE [=] NDB, a tablespace and associated data file
# are created on each Cluster data node.
#
# You can verify the data files were created and obtain information about them by querying
# the INFORMATION_SCHEMA.FILES table
#
# (See the example later in this section)
#
# (See SECTION 25.10, "THE INFORMATION_SCHEMA FILES TABLE")
#
# OPTIONS
#
# 		) ADD DATAFILE:
#
# 			Defines the name of a tablespace data file.
#
# 			The ADD DATAFILE clause is required when creating undo tablespaces.
# 			Otherwise, it is optional as of MySQL 8.0.14
#
# 			An InnoDB tablespace supports only a single data file, whose name must include
# 			a .ibd extension
#
# 			An NDB Cluster tablespace supports multiple data files which can have any
# 			legal file names; More data files can be added to an NDB Cluster tablespace
# 			following its creation by using an ALTER_TABLESPACE statement.
#
# 			To place a general tablespace data file in a location outside of the data directory,
# 			include a fully qualified path or a path relative to the data directory.
#
# 			Only a fully qualified path is permitted for undo tablespaces.
#
# 			If you do not specify a path, a general tablespace is created in the
# 			data directory.
#
# 			An undo tablespace created without specifying a path is created in the directory
# 			defined by the innodb_undo_directory variable.
#
# 			If the innodb_undo_directory variable is undefined, undo tablespaces are created
# 			in the data directory.
#
# 			Creating a general tablespace in a subdirectory under the data directory is not supported
# 			to avoid conflict with implicitly created file-per-table tablespaces.
#
# 			When creating a general tablespace or undo tablespace outside of the data directory,
# 			the directory must exist and must be known to InnoDB prior to creating the tablespace.
#
# 			To make a directory known to InnoDB, add it to the innodb_directories value or to one
# 			of the variables whose values are appended to the innodb_directories value.
#
# 			innodb_directories is a read-only variable. Configuring it requires restarting the server.
#
# 			The file_name, including any specified path, must be quoted with single or double quotation marks.
#
# 			File names (not counting the file extensions) and directory names must be at least one byte
# 			in length.
#
# 			Zero length file names and dir names are not supported.
#
# 			If the ADD DATAFILE clause is not specified when creating a tablespace, a tablespace data file
# 			with a unique file name is created implicitly.
#
# 			The unique file name is a 128 bit UUID formatted into five groups of hexadecimal numbers
# 			separated by dashes (aaaaaaaa-bbbb-cccc-dddd-eeeeeeee)
#
# 			A file extension is added if required by the storage engine.
#
# 			An .ibd file extension is added for InnoDB general tablespace data files.
#
# 			In a replication environment, the data file name created on the master is not the same
# 			as the data file name created on the slave.
#
# 		) FILE_BLOCK_SIZE:
#
# 			This option - which is specific to InnoDB general tablespaces, and is ignored by NDB -
# 			defines the block size for the tablespace data file.
#
# 			Values can be specified in bytes or kilobytes.
#
# 			For example, an 8 kilobyte file block size can be specified as 8192 or 8K
#
# 			If you do not specify this option, FILE_BLOCK_SIZE defaults to the innodb_page_Size
# 			value.
#
# 			FILE_BLOCK_SIZE is required when you intend to use the tablespace for storing
# 			compressed InnoDB tables (ROW_FORMAT=COMPRESSED)
#
# 			In this case, you must define the tablespace FILE_BLOCK_SIZE when creating the tablespace.
#
#			If FILE_BLOCK_SIZE is equal to the innodb_page_size value, the tablespace can contain only
# 			tables having an uncompressed row format (COMPACT, REDUNDANT and DYNAMIC)
#
# 			Tables with a COMPRESSED row format have a different physical page size than uncompressed tables.
#
# 			Therefore, compressed tables cannot coexist in the same tablespace as uncompressed tables.
#
# 			For a general tablespace to contain compressed tables, FILE_BLOCK_SIZE must be specified,
# 			and the FILE_BLOCK_SIZE value must be a valid compressed page size in relation to the
# 			innodb_page_size value.
#
# 			Also, the physical page size of the compressed table (KEY_BLOCK_SIZE) must be equal
# 			to FILE_BLOCK_SIZE/1024
#
# 			For example, if innodb_page_size=16K, and FILE_BLOCK_SIZE=8K, the KEY_BLOCK_SIZE
# 			of the table must be 8.
#
# 			For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# 		) USE LOGFILE GROUP:
#
# 			Required for NDB, this is the name of a log file group previously created using
# 			CREATE_LOGFILE_GROUP
#
# 			Not supported for InnoDB, where it fails with an error.
#
# 		) EXTENT_SIZE:
#
# 			This option is specific to NDB, and is not supported by InnoDB, where it fails
# 			with an error.
#
# 			EXTENT_SIZE sets the size, in bytes, of the extents used by any files belonging
# 			to the tablespace.
#
# 			The default value is 1M. The minimum size is 32K, and theoretical maximum is
# 			2G, although the practical maximum size depends on a number of factors.
#
# 			In most cases, changing the extent size does not have any measurable effect
# 			on performance, and the default value is recommended for all but the most unusual situations.
#
# 			An extent is a unit of disk space allocation.
#
# 			One extent is filled with as much data as that extent can contain before another extent is used.
#
# 			In theory, up to 65,535 (64K) extents may be used per data file; however, the recommended
# 			maximum is 32,768 (32K)
#
# 			The recommended maximum size for a single data file is 32G - that is, 32K
# 			extents x 1 MB per extent.
#
# 			In addition, once an extent is allocated to a given partition, it cannot be used to store
# 			data from a different partition; an extent cannot store data from more than one partition.
#
# 			This means, for example that a tablespace having a single datafile whose INITIAL_SIZE
# 			(described in the following item) is 256 MB and whose EXTENT_SIZE is 128M has just two extents,
# 			and so can be used to store data from at most two different disk data table partitions.
#
# 			You can see how many extents remain free in a given data file by querying the INFORMATION_SCHEMA.FILES
# 			table, and so derive an estimate for how much space remains free in the file.
#
# 			For further discussion and examples, see SECTION 25.10, "THE INFORMATION_SCHEMA FILES TABLE"
#
# 		) INITIAL_SIZE:
#
# 			This option is specific to NDB, and is not supported by InnoDB, where it fails with an error.
#
# 			The INITIAL_SIZE parameter sets the total size in bytes of the data file that was specific
# 			using ADD DATAFILE.
#
# 			Once this file has been created, its size cannot be changed;
#
# 			However, you can add more data files to the tablespace using ALTER_TABLESPACE_---_ADD_DATAFILE
#
# 			INITIAL_SIZE is optional; its default value is 128MB (134217728)
#
# 			On 32-bit systems, the maximum supported value for INITIAL_SIZE is 4GB (4294967296)
#
# 		) AUTOEXTEND_SIZE:
#
# 			Currently ignored by MySQL; reserved for possible future use.
#
# 			Has no effect in any release of MySQL 8.0 or MySQL NDB Cluster 8.0, regardless
# 			of the storage engine used.
#
# 		) MAX_SIZE:
#
# 			Currently ignored by MySQL;
#
# 			reserved for possible future use. Has no effect in any release of MySQL 8.0 or
# 			MySQL NDB Cluster 8.0, regardless of the storage engine used.
#
# 		) NODEGROUP:
#
# 			Currently ignored by MySQL
#
# 			reserved for possible future use. has no effect in any release of MysQL 8.0 or
# 			MySQL NDB CLuster 8.0, regardless of the storage engine used
#
# 		) WAIT:
#
# 			Currently ignored by MySQL;
#
# 			reserved for possible future use. Has no effect in any release of MySQL 8.0 or
# 			MySQL NDB Cluster 8.0, regardless of the storage engine used.
#
# 		) COMMENT:
#
# 			Currently ignored by MySQL;
#
# 			reserved for possible future use.
#
# 			Has no effect in any release of MySQL 8.0 or MySQL NDB Cluster 8.0, regardless
# 			of the storage engine used.
#
# 		) The ENCRYPTION option is used to enable or disable page-level data encryption for an
# 			InnoDB general tablespace.
#
# 			Option values are not case-sensitive.
#
# 			Encryption support for general tablespaces was introduced in MySQL 8.0.13
#
# 			A keyring plugin must be installed and configured to use the ENCRYPTION option.
#
# 			When a general tablespace is encrypted, all tables residing in the tablespace is
# 			encrypted.
#
# 			For more information, see SECTION 15.6.3.9, "TABLESPACE ENCRYPTION"
#
# 		) ENGINE:
#
# 			Defines the storage engine which uses the tablespace, where engine_name
# 			is the name of the storage engine.
#
# 			Currently, only the InnoDB storage engine is supported by standard MySQL
# 			8.0 releases
#
# 			MySQL NDB Cluster supports both NDB and InnoDB tablespaces.
#
# 			The value of the default_storage_engine system variable is used
# 			for ENGINE if the option is not specified.
#
# NOTES
#
# 		) For the rules covering the naming of MySQL tablespaces, see SECTION 9.2, "SCHEMA OBJECT NAMES"
#
# 		In addition to these rules, the slash character ("/") is not permitted, nor can
# 		you use names beginning with innodb_, as this prefix is reserved for system use.
#
# 		) Creation of temporary general tablespace is not supported
#
# 		) General tablespaces do not support temporary tables
#
# 		) The TABLESPACE option may be used with CREATE_TABLE or ALTER_TABLE to assign
# 			an InnoDB table partition or subpartition to a file-per-table tablespace.
#
# 			All partitions must belong to the same storage engine.
#
# 			Assigning table partitions to shared InnoDB tablespaces is not supported.
#
# 			Shared tablespaces include the InnoDB system tablesapce and general tablespaces.
#
# 		) General tablespaces support the addition of tables of any row format using CREATE_TABLE_---_TABLESPACE,
# 			innodb_file_per_table does not need to be enabled.
#
# 		) innodb_strict_mode is not applicable to general tablespaces.
#
# 			Tablespace management rules are strictly enforced indepdently of
# 			innodb_strict_mode
#
# 			If CREATE TABLESPACE parameters are incorrect or incompatible,
# 			the operation fails regardless of the innodb_strict_mode setting.
#
# 			When a table is added to a general tablespace using CREATE_TABLE_---_TABLESPACE
# 			or ALTER_TABLE_---_TABLESPACE then innodb_strict_mode is ignored but the
# 			statement is evaluated as if innodb_strict_mode is enabled.
#
# 		) Use DROP TABLESPACE to remove a tablespace.
#
# 			ALl tables must be dropped from a tablespace using DROP_TABLE prior to
# 			dropping the tablespace.
#
# 			Before dropping an NDB Cluster tablespace you must also remove all
# 			its data files using one or more ALTER_TABLESPACE_---_DROP_DATAFILE
# 			statements.
#
# 			See SECTION 22.5.13.1, "NDB CLUSTER DISK DATA OBJECTS"
#
# 		) ALl parts of an InnoDB table added to an InnoDB general tablespace reside
# 			in the general tablespace, including indexes and BLOB pages.
#
# 			For an NDB table assigned to a tablespace, only those columns which are not indexed
# 			are stored on disk, and actually use the tablespace data files.
#
# 			Indexes and indexed columns for all NDB tables are always kept in memory.
#
# 		) Similar to the system tablespace, truncating or dropping tables stored in a general
# 			tablespace creates free space internally in the general tablespace
# 			.ibd data file which can only be used for new InnoDB data.
#
# 			Space is not released back to the operating system as it is for
# 			file-per-table tablespaces.
#
# 		) A general tablespace is not associated with any database or schema.
#
# 		) ALTER_TABLE_---_DISCARD_TABLESPACE and ALTER_TABLE_---_IMPORT_TABLESPACE
# 			are not supported for tables that belong to a general tablespace.
#
# 		) The several uses tablespace-level metadata locking for DDL that references
# 			general tablespaces.
#
# 			By comparison, the server uses table-level metadata locking for DDL
# 			that references file-per-table tablespaces.
#
# 		) A generated or existing tablespace cannot be changed to a general tablespace
#
# 		) There is no conflict between general tablespace names and file-per-table tablespace
# 			names.
#
# 			The "/" character, which is present in file-per-table tablespace names,
# 			is not permitted in general tablespace names.
#
# 		) mysqldump and mysqlpump do not dump InnoDB CREATE_TABLESPACE statements
#
# INNODB EXAMPLES
#
# This example demonstrates creating a general tablespace and adding three uncompressed
# tables of different row formats.
#
# 		CREATE TABLESPACE `ts1` ADD DATAFILE 'ts1.ibd' ENGINE=INNODB;
#
# 		CREATE TABLE t1 (c1 INT PRIMARY KEY) TABLESPACE ts1 ROW_FORMAT=REDUNDANT;
#
# 		CREATE TABLE t2 (c1 INT PRIMARY KEY) TABLESPACE ts1 ROW_FORMAT=COMAPCT;
#
# 		CREATE TABLE t3 (c1 INT PRIMARY KEY) TABLESPACE ts1 ROW_FORMAT=DYNAMIC;
#
# This example demonstrates creating a general tablespace and adding a compressed table.
#
# The example assumes a default innodb_page_size value of 16K
#
# The FILE_BLOCK_SIZE of 8192 requires that the compressed table have a KEY_BLOCK_SIZE
# of 8.
#
# 		CREATE TABLESPACE `ts2` ADD DATAFILE 'ts2.ibd' FILE_BLOCK_SIZE = 8192 Engine=InnoDB;
#
# 		CREATE TABLE t4 (c1 INT PRIMARY KEY) TABLESPACE ts2 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8;
#
# This example demonstrates creating a general tablespace without specifying the ADD DATAFILE
# clause, which is optional as of MySQL 8.0.14
#
# 		CREATE TABLESPACE `ts3` ENGINE=INNODB;
#
# This example demonstrates creating an undo tablespace.
#
# 		CREATE UNDO TABLESPACE undo_003 ADD DATAFILE 'undo_003.ibu';
#
# NDB EXAMPLE
#
# Suppose that you wish to create an NDB Cluster Disk Data tablespace named
# myts using a datafile named mydata-1.dat 
#
# An NDB tablespace always requires the use of a log file group consisting
# of one or more undo log files.
#
# For htis example, we first create a log file group named mylg that contains
# one undo long file named myundo-1.dat, using the CREATE_LOGFILE_GROUP statement
# shown here:
#
# 		CREATE LOGFILE GROUP myg1
# 			ADD UNDOFILE 'myundo-1.dat'
# 			ENGINE=NDB;
# 		Query OK, 0 rows affected (3.29 sec)
#
# Now you can create the tablespace previously described using the following statement:
#
# 		CREATE TABLESPACE myts
# 			ADD DATAFILE 'mydata-1.dat'
# 			USE LOGFILE GROUP mylg
# 			ENGINE=NDB;
# 		Query OK, 0 rows affected (2.98 sec)
#
# You can now create a Disk Data table using a CREATE_TABLE statement with the
# TABLESPACE and STORAGE DISK options, similar to what is shown here:
#
# 		CREATE TABLE mytable (
# 			id INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
# 			lname VARCHAR(50) NOT NULL,
# 			fname VARCHAR(50) NOT NULL,
# 			dob DATE NOT NULL,
# 			joined DATE NOT NULL,
# 			INDEX(last_name, first_name)
# 		)
# 			TABLESPACE myts STORAGE DISK
# 			ENGINE=NDB;
# 		Query OK, 0 rows affected (1.41 sec)
#
# It is important to note that only the dob and joined columns from mytable are actually
# stored on disk, due to the fact that the id, lname, and fname columns are all indexed.
#
# As mentioned previously, when CREATE TABLESPACE is used with ENGINE [=] NDB, a tablespace
# and associated data file are created on each NDB Cluster data node.
#
# You can verify that the data files were created and obtain all information about them
# by querying the INFORMATION_SCHEMA.FILES table, as shown here:
#
# 		SELECT FILE_NAME, FILE_TYPE, LOGFILE_GROUP_NAME, STATUS, EXTRA
# 			FROM INFORMATION_SCHEMA.FILES
# 			WHERE TABLESPACE_NAME = 'myts';
#  	
# 		+----------------------+-----------------+---------------------------------+--------------+--------------------+
# 		| file_name 			  | file_type 		  | logfile_group_name 					| status 		| extra 					|
# 		+----------------------+-----------------+---------------------------------+--------------+--------------------+
# 		| mydata-1.dat 		  | DATAFILE 		  | mylg 									| NORMAL 		| CLUSTER_NODE=5 	   |
# 		| mydata-1.dat 		  | DATAFILE 		  | mylg 									| NORMAL 		| CLUSTER_NODE=6 	   |
# 		| NULL 					  | TABLESPACE 	  | mylg  									| NORMAL 		| NULL 					|
# 		+----------------------+-----------------+---------------------------------+--------------+--------------------+
# 		3 rows in set (0.01 sec)
#
# For additional information and examples, see SECTION 22.5.13.1, "NDB CLUSTER DISK DATA OBJECTS"
#
# 13.1.22 CREATE TRIGGER SYNTAX
#
# 		CREATE
# 			[DEFINER = { user | CURRENT_USER }]
# 			TRIGGER trigger_name
# 			trigger_name trigger_event
# 			ON tbl_name FOR EACH ROW
# 			[trigger_order]
# 			trigger_body
#
# 		trigger_time: { BEFORE | AFTER }
#
# 		trigger_event: { INSERT | UPDATE | DELETE }
#
# 		trigger_order: { FOLLOWS | PRECEDES } other_trigger_name
#
# This statement creates a new trigger.
#
# A trigger is a named database object that is associated with a table, and that activates
# when a particular event occurs for the table.
#
# The trigger becomes associated with the table named tbl_name, which must refer to a permanent
# table.
#
# You cannot associate a trigger with a TEMPORARY table or a view.
#
# Trigger names exist in the schema namespace, meaning that all triggers must have unique names
# within a schema.
#
# Triggers in different schemas can have the same name.
#
# This section describes CREATE_TRIGGER syntax. For additional discussion, see SECTION 24.3.1, "TRIGGER SYNTAX AND EXAMPLES"
#
# CREATE_TRIGGER requires the TRIGGER privilege for the table associated with the trigger.
#
# The statement might also require the SET_USER_ID or SUPER privilege, depending on
# the DEFINER value, as described later in this section.
#
# If binary logging is enabled, CREATE_TRIGGER might require the SUPER privilege,
# as described in SECTION 24.7, "BINARY LOGGING OF STORED PROGRAMS"
#
# The DEFINER clause determines the security context to be used when checking access
# privileges at trigger activation time, as described later in this section.
#
# trigger_time is the trigger action time.
#
# It can be BEFORE or AFTER to indicate that the trigger activates before or after
# each row to be modified.
#
# Basic column value checks occur prior to trigger activation, so you cannot use BEFORE
# triggers to convert values inappropriate for the column type to valid values.
#
# trigger_event indicates the kind of operation that activates the trigger.
#
# These trigger_event values are permitted:
#
# 		) INSERT: The trigger activates whenever a new row is inserted into the table;
# 			for example, through INSERT, LOAD_DATA and REPLACE statements.
#
# 		) UPDATE: The trigger activates whenever a row is modified; for example, through UPDATE statements.
#
# 		) DELETE: The trigger activates whenever a row is deleted from the table;
#
# 		For example, through DELETE and REPLACE statements. DROP_TABLE and TRUNCATE_TABLE
# 		statements on the table do not activate this trigger, because they do not use DELETE.
#
# 		Dropping a partition does not activate DELETE triggers, either.
#
# The trigger_event does not represent a literal type of SQL statement that activates the trigger so much
# as it represents a type of table operation.
#
# For example, an INSERT trigger activates not only for INSERT statements but also LOAD_DATA statements
# because both statements insert rows into a table.
#
# A potentially confusing example of this is the INSERT INTO --- ON DUPLICATE KEY UPDATE --- syntax:
#
# 		a BEFORE INSERT trigger activates for every row, followed by either an
# 		AFTER INSERT trigger or both the BEFORE UPDATE and AFTER UPDATE triggers,
# 		depending on whether there was a duplicate key for the row.
#
# NOTE:
#
# 		Cascaded foreign key actions do not activate triggers
#
# It is possible to define multiple triggers for a given table that have the same trigger
# event and action time.
#
# For example, you can have two BEFORE UPDATE triggers for a table.
#
# By default, triggers that have the same trigger event and action name activate
# in the order htey were created.
#
# To affect trigger order, specify a trigger_order clause that indicates FOLLOWS
# or PRECEDES and the name of an existing trigger that also has the same trigger
# event and action time.
#
# With FOLLOWS, the new trigger activates after the existing trigger.
#
# With PRECEDES, the new trigger activates before the existing trigger.
#
# trigger_body is the statement to execute when the trigger activates.
#
# To execute multiple statements, use the BEGIN_---_END compound statement
# construct.
#
# This also enables you to use the same statements that are permitted within stored
# routines.
#
# See SECTION 13.6.1, "BEGIN_---_END COMPOUND-STATEMENT SYNTAX"
#
# Some statements are not permitted in triggers, see SECTION C.1, "RESTRICTIONS ON STORED PROGRAMS"
#
# Within the trigger body, you can refer to columns in the subject table (the table associated
# with the trigger) by using the aliases OLD and NEW.
#
# OLD.col_name refers to a column of an existing row before it updated or deleted
#
# NEW.col_name refers to teh column of a new row to be inserted or an existing row after
# it is updated.
#
# Triggers cannot use NEW.col_name or use OLD.col_name to refer to generated columns.
#
# FOr information about generated columns, see SECTION 13.1.20.8, "CREATE TABLE AND GENERATED COLUMNS"
#
# MySQL stores the sql_mode system variable setting in effect ´when a trigger is created, and always
# executes the trigger body with this setting in force, regardless of the current server SQL
# mode when the trigger begins executing.
#
# The DEFINER clause specifies the MySQL account to be used when checking access privileges
# at trigger activation time.
#
# If a user value is given, it should be a MySQL account specified as 'user_name'@'host_name',
# CURRENT_USER or CURRENT_USER()
#
# The default DEFINER value is the user who executes the CREATE_TRIGGER statement.
#
# THis is the same as specifying DEFINER = CURRENT_USER explicitly.
#
# If you specify the DEFINER clause, these rules determine the valid DEFINER user values:
#
# 		) If you do not have the SET_USER_ID or SUPER privilege, the only permitted user value
# 			is your own account, either specified literally or by using CURRENT_USER 
#
# 		You cannot set the definer to some other account
#
# 		) If you have the SET_USER_ID or SUPER privilege, you can specify any syntactically
# 			valid account name.
#
# 			If the account does not exist, a warning is generated.
#
# 		) Although it is possible to create a trigger with a nonexistent DEFINER account,
# 			it is not a good idea for such triggers to be activated until the account
# 			actually does exist.
#
# 			Otherwise, the behavior with respect to privilege checking is undefined.
#
# MySQL takes the DEFINER user into account when checking trigger privileges as follows:
#
# 		) At CREATE_TRIGGER time, the user who issues the statement must have the TRIGGER privilege
#
# 		) At trigger activation time, privileges are checked against the DEFINER user.
#
# 			This user must have these privileges:
#
# 				) The TRIGGER privilege for the subject table
#
# 				) The SELECT privilege for the subject table if references to table columns occur
# 				using OLD.col_name or NEW.col_name in the trigger body.
#	
# 				) The UPDATE privilege for the subject table if table columns are targets of
# 				SET NEW.col_name = value assignments in the trigger body.
#
# 				) Whatever other privileges normally are required for the statements executed by the trigger.
#
# For more information about trigger security, see SECTION 24.6, "ACCESS CONTROL FOR STORED PROGRAMS AND VIEWS"
#
# Within a trigger body, the CURRENT_USER() function returns the account used to check privileges
# at trigger activation time.
#
# This is the DEFINER user, not the user whose actions caused the trigger to be activated.
#
# For information about user auditing within triggers, see SECTION 6.3.13,
# "SQL-BASED MYSQL ACCOUNT ACTIVITY AUDITING"
#
# If you use LOCK_TABLES to lock a table that has triggers, the tables used within the trigger
# are also locked, as described in LOCK TABLES AND TRIGGERS
#
# For additional discussions of trigger use, see SECTION 24.3.1, "TRIGGER SYNTAX AND EXAMPLES"
#
# 13.1.23 CREATE VIEW SYNTAX
#
# CREATE
# 		[OR REPLACE]
# 		[ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}]
# 		[DEFINER = { user | CURRENT_USER }]
# 		[SQL SECURITY { DEFINER | INVOKER }]
# 		VIEW view_name [(column_list)]
# 		AS select_statement
# 		[WITH [CASCADED | LOCAL] CHECK OPTION]
#
# The CREATE_VIEW statement creates a new view, or replaces an existing view if hte
# OR REPLACE clause is given.
#
# If the view does not exist, CREATE_OR_REPLACE_VIEW is the same as CREATE_VIEW
# 
# If the view does exist, CREATE_OR_REPLACE_VIEW replaces it
#
# For information about restrictions on view use, see SECTION C.5, "RESTRICTIONS ON VIEWS"
#
# The select_statement is a SELECT statement that provides the definition of the view.
#
# (Selecting from the view selects, in effect, using the SELECT statement)
#
# The select_statement can select from base tables or other views.
#
# The view definition is "frozen" at creation time and is not affected by subsequent changes
# to the definitions of the underlying tables.
#
# For example, if a view is defined as SELECT * on a table, new columns added to the table
# later do not become part of the view, and columns dropped from the table will result in
# an error when selecting from teh view.
#
# the ALGORITHM clause affects how MySQL processes the view.
#
# The DEFINER and SQL SECURITY clauses specify the security context to be used when
# checking access privileges at view invocation time.
#
# The WITH CHECK OPTION clause can be given to constrain inserts or updates to rows
# in tables referenced by the view.
#
# These clauses are described later in this section.
#
# The CREATE_VIEW statement requires the CREATE_VIEW privilege for the view, and some
# privilege for each column selected by the SELECT statement.
#
# For columns used elsewhere in the SELECT statement, you must have the SELECT privilege.
#
# If the OR REPLACE clause is present, you must also have the DROP privilege for the view.
#
# CREATE VIEW might also require the SET_USER_ID or SUPER privilege, depending on the DEFINER
# value, as described later in this section.
#
# WHen a view is referenced, privilege checking occurs as described later in this section
#
# A view belongs to a database.
#
# By default, a new view is created in the default database. To create the view explicitly
# in a given database, use db_name.view_name syntax to qualify the view name with the
# database name:
#
# 		CREATE VIEW test.v AS SELECT * FROM t;
#
# Unqualified table or view names in the SELECT statement are also interpreted with respect
# to the default database.
#
# A view can refer to tables or views in other databases by qualifying the table or view
# name with the appropriate database name.
#
# Within a database, base tables and views share the same namespace, so a base table and
# a view cannot have the same name.
#
# Columns retrieved by the SELECT statement can be simple references to table columns,
# or expressions that use functions, constant values, operators, and so forth.
#
# A view must have unique columns names with no duplicates, just like a base table.
#
# By default, the names of the columns retrieved by the SELECT statement are used for
# the view column names.
#
# To define explicit names for the view columns, specify the optional column_list
# clause as a list of comma-separated identifiers.
#
# The number of names in column_list must be the same as the number of columns
# retrieved by the SELECT statement.
#
# A view can be created from many kinds of SELECT statements.
#
# It can refer to base tables or other views. It can use joins, UNION and subqueries.
#
# The SELECT need not even refer to any tables:
#
# 		CREATE VIEW v_today (today) AS SELECT CURRENT_DATE;
#
# The following example defines a view that selects two columns from another table as well
# as an expression calculated from those columns:
#
# 		CREATE TABLE t (qty INT, price INT);
# 		INSERT INTO t VALUES(3, 50);
# 		CREATE VIEW v AS SELECT qty, price, qty*price AS value FROM t;
# 		SELECT * FROM v;
# 		+-------+----------+----------+
# 		| qty   | price 	 | value 	|
# 		+-------+----------+----------+
# 		| 3 	  | 50 		 | 150 		|
# 		+-------+----------+----------+
#
# A view definition is subject to the following restrictions:
#
# 		) The SELECT statement cannot refer to system variables or user-defined variables
#
# 		) Within a stored program, the SELECT statement cannot refer to program parameters
# 			or local variables.
#
# 		) The SELECT statement cannot refer to prepared statemnt parameters.
#
# 		) Any table or view referred to in the definition must exist.
#
# 			If, after the view has been created, a table or view that the definition
# 			refers to is dropped, use of the view results in an error.
#
# 			To check a view definition for problems of this kind, use the CHECK_TABLE statement.
#
# 		) The definition cannot refer to a TEMPORARY table, and you cannot create a TEMPORARY view.
#
# 		) You cannot associate a trigger with a view
#
# 		) Aliases for column names in the SELECT statement are checked against the maximum column
# 			length of 64 characters (not the maximum alias length of 256 characters)
#
# ORDER BY is permitted in a view definition, but it is ignored if you select from a view using
# a statement that has its own ORDER BY.
#
# For other options or clauses in the definition, they are added to the options or clauses
# of the statement that references the view, but the effect is undefined.
#
# For example, if a view definition includes a LIMIT clause, and you select from the view
# using a statement that has its own LIMIT clause, it is undefined which limit applies.
#
# This same principle applies to options such as ALL, DISTINCT, or SQL_SMALL_RESULT that follow
# the SELECT keyword, and to clauses such as INTO, FOR UPDATE, FOR SHARE, LOCK IN SHARE MODE,
# and PROCEDURE.
#
# The results obtained from a view may be affected if you change the query processing
# environment by changing system variables:
#
# 		CREATE VIEW v (mycol) AS SELECT 'abc';
# 		Query OK, 0 rows affected (0.01 sec)
#
# 		SET sql_mode = '';
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		SELECT "mycol" FROM v;
# 		+----------+
# 		| mycol    |
# 		+----------+
# 		| mycol    |
# 		+----------+
# 		1 row in set (0.01 sec)
#
# 		SET sql_mode = 'ANSI_QUOTES';
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		SELECT "mycol" FROM v;
# 		+----------+
# 		| mycol    |
# 		+----------+
# 		| abc 	  |
# 		+----------+
# 		1 row in set (0.00 sec)
#
# The DEFINER and SQL SECURITY clauses determine which MySQL account to use when
# checking access privileges for the view when a statement is executed that 
# references the view.
#
# The valid SQL SECURITY characteristic values are DEFINER (the default) and INVOKER.
#
# These indicate that the required privileges must be held by the user who defined
# or invoked the view, respectively.
#
# If a user value is given for the DEFINER claus, it should be a MySQL account specified
# as 'user_name'@'host_name', CURRENT_USER or CURRENT_USER() 
#
# The default DEFINER value is the user who executes the CREATE_VIEW statement.
#
# This is the same as specifying DEFINER = CURRENT_USER explicitly.
#
# If the DEFINER clause is present, these rules determine the valid DEFINER user values:
#
# 		) If you do not have the SET_USER_ID or SUPER privilege, the only valid user value is
# 			your own account, either specified literally or by using CURRENT_USER
#
# 			You cannot set the definer to some other account
#
# 		) If you have the SET_USER_ID or SUPER privilege, you can specify any syntactically
# 			valid account name.
#
# 			If the account does not exist, a warning is generated.
#
# 		) Although it is possible to create a view with a nonexistent DEFINER account,
# 			an error occurs when the view is referenced if the SQL SECURITY value is
# 			DEFINER but the definer account does not exist.
#
# For more information about view security, see SECTION 24.6, "ACCESS CONTROL FOR STORED PROGRAMS AND VIEWS"
#
# Within a view definition, CURRENT_USER returns the view's DEFINER value by default.
#
# For views defined with the SQL SECURITY INVOKER characteristic, CURRENT_USER returns
# the account for the view's invoker.
#
# For information about user auditing within views, see SECTION 6.3.13, "SQL-BASED MYSQL
# ACCOUNT ACTIVITY AUDITING"
#
# Within a stored routine that is defined with the SQL SECURITY DEFINER characteristic,
# CURRENT_USER returns the routine's DEFINER value.
#
# THis also affects a view defined within such a routine, if hte view definition 
# contains a DEFINER value of CURRENT_USER
#
# MySQL checks view privileges like this:
#
# 		) At view definition time, the view creator must have the privileges needed to use the
# 			top-level objects accessed by the view.
#
# 			For example, if the view definitions refers to table columns, the creator must have
# 			some privilege for each column in the select list of the definition, and the SELECT
# 			privilege for each column used elsewhere in the definition.
#
# 			If the definition refers to a stored function, only the privileges needed to invoke
# 			the function can be checked.
#
# 			The privileges required at function invocation time  can be checked only as it executes:
#
# 				For different invocations, different execution paths within the function might be taken.
#
# 		) The user who references a view must have appropriate privileges to access it (SELECT to select from it,
# 			INSERT to insert into it and so forth.)
#
# 		) When a view has been referenced, privileges for objects acessed by the view are checked against the
# 			privileges held by the view DEFINER account or invoker, depending on whether the SQL SECURITY
# 			characteristic is DEFINER or INVOKER, respectively.
#
# 		) If reference to a view causes execution of a stored function, privilege checking for statements executed
# 		within the function depend on whether the function SQL SECURITY characteristic is DEFINER or INVOKER.
#
# 		If the security characteristic is DEFINER, the function runs with the privileges of the DEFINER account.
#
# 		If the characteristic is INVOKER, the function runs with the privileges determined by the view's SQL SECURITY
# 		characteristic.
#
# Example: A view might depend on a stored function, and that function might invoke other stored routines.
#
# For example, the following view invokes a stored function f():
#
# 		CREATE VIEW v AS SELECT * FROM t WHERE t.id = f(t.name);
#
# Suppose that f() contains a statement such as this:
#
# 		IF name IS NULL then
# 			CALL p1();
# 		ELSE
# 			CALL p2();
# 		END IF;
#
# The privileges required for executing statements within f() need to be checked when
# f() executes.
#
# This might mean that privileges are needed for p1() or p2(), depending on the execution
# path within f()
#
# Those privileges must be checked at runtime, and the user who must possess the privileges
# is determined by the SQL SECURITY values of the view v and the function f()
#
# The DEFINER and SQL SECURITY clauses for views are extensions to standard SQL.
#
# In standard SQL, views are handled using the rules for SQL SECURITY DEFINER
#
# The standard says that the definer of the view, which is the same as the owner
# of the view's schema, gets applicable privileges on the view (for example, SELECT)
# and may grant them.
#
# MySQL has no concept of a schema "owner", so MySQL adds a clause to identify the definer.
#
# The DEFINER clause is an extension where the intent is to have what the standard
# has; that is, a permanent record of who defined the view.
#
# This is why the default DEFINER value is the account of the view creator.
#
# The optional ALGORITHM clause is a MySQL extension to standard SQL.
#
# It affects how MySQL processes the view. ALGORITHM takes three values:
#
# 		MERGE
#
# 		TEMPTABLE
#
# 		UNDEFINED
#
# For more information, see SECTION 24.5.2, "VIEW PROCESSING ALGORITHMS", as well as
# SECTION 8.2.2.4, "OPTIMIZING DERIVED TABLES, VIEW REFERENCES, AND COMMON TABLE EXPRESSIONS
# WITH MERGING OR MATERIALIZATION"
#
# Some views are updatable.
#
# That is, you can use them in statements such as UPDATE, DELETE or INSERT to update
# the contents of the underlying table.
#
# For a view to be updatable, there must be a one-to-one relationship between the rows
# in the view and the rows in the underlying table.
#
# There are also certain other constructs that make a view nonupdatable.
#
# A generated column in a view is considered updatable because it is possible to assign to it.
#
# However, if such a column is updated explicitly, the only permitted value is
# DEFAULT.
#
# For information about generated columns, see SECTION 13.1.20.8, "CREATE TABLE AND GENERATED COLUMNS"
#
# The WITH CHECK OPTION clause can be given for an updatable view to prevent inserts or updates
# to rows except those for which the WHERE clause in the select_statement is true.
#
# In a WITH CHECK OPTION clause for an updatable view, the LOCAL and CASCADED keywords determine
# the scope of check testing when the view is defined in terms of another view.
#
# The LOCAL keyword restricts the CHECK OPTION only to the view being defined.
#
# CASCADED causes the checks for underlying views to be evaluated as well.
#
# When neither keyword is given, the default is CASCADED.
#
# For more information about updatable views and the WITH CHECK OPTION clause, see
# SECTION 24.5.3, "UPDATABLE AND INSERTABLE VIEWS", and SECTION 24.5.4, "THE VIEW WITH CHECK OPTION CLAUSE"
#
# 13.1.24 DROP DATABASE SYNTAX
#
# 		DROP {DATABASE | SCHEMA} [IF EXISTS] db_name
#
# DROP_DATABASE drops all tables in the database and deletes teh database.
#
# Be very careful with this statement. To use DROP_DATABASE, you need the DROP privilege
# on the database.
#
# DROP_SCHEMA is a synonym for DROP_DATABASE
#
# IMPORTANT:
#
# 		When a database is dropped, privileges granted specifically for the database are not automatically dropped.
#
# 		They must be dropped manually. See SECTION 13.7.1.6, "GRANT SYNTAX"
#
# IF EXISTS is used to prevent an error from occurring if the database does not exist.
#
# If the default database is dropped, the default database is unset (the DATABASE() function returns NULL)
#
# If you use DROP_DATABASE on a symbolically linked database, both the link and the original database are deleted.
#
# DROP_DATABASE returns the number of tables that were removed.
#
# The DROP_DATABASE statement removes from the given database directory those files and directories
# that MySQL itself may create during normal operation.
#
# This includes all files with the extensions shown in the following list:
#
# 		) .BAK
#
# 		) .DAT
#
# 		) .HSH
#
# 		) .MRG
#
# 		) .MYD
#
# 		) .MYI
#
# 		) .cfg
#
# 		) .db
#
# 		) .ibd
#
# 		) .ndb
#
# If other files or directories remain in the database directory after MySQL removes those just listed,
# the database directory cannot be removed.
#
# In this case, you must remove any remaining files or directories manually and issue the
# DROP_DATABASE statement again.
#
# Dropping a database does not remove any TEMPORARY tables that were created in that database.
#
# TEMPORARY tables are automatically removed when the session that created them ends.
# See SECTION 13.1.20.3, "CREATE TEMPORARY TABLE SYNTAX"
#
# You can also drop databases with mysqladmin.
#
# See SECTION 4.5.2, "MYSQLADMIN - CLIENT FOR ADMINSTERING A MYSQL SERVER"
#
# 13.1.25 DROP EVENT SYNTAX
#
# DROP EVENT [IF EXISTS] event_name
#
# This statement drops the event named event_name.
#
# The event immediately ceases being active, and is deleted completely from the server
#
# If the event does not exist, the error ERROR 1517 (HY000): Unknown event 'event_name' results.
#
# You can override this and cause the statement to generate a warning for nonexistent
# events instead using IF EXISTS
#
# This statement requires the EVENT privilege for the schema to which the event
# to be dropped belongs.
#
# 13.1.26 DROP FUNCTION SYNTAX
#
# The DROP_FUNCTION statement is used to drop stored functions and user-defined functions
# (UDFs):
#
# 		) For information about dropping stored functions, see SECTION 13.1.29, "DROP PROCEDURE AND DROP FUNCTION SYNTAX"
#
# 		) For information about dropping user-defined functions, see SECTION 13.7.4.2, "DROP FUNCTION SYNTAX"
#
# 13.1.27 DROP INDEX SYNTAX
#
# DROP INDEX index_name ON tbl_name
# 		[algorithm_option | lock_option] ---
#
# algorithm_option:
# 		ALGORITHM [=] {DEFAULT|INPLACE|COPY}
#
# lock_option:
# 		LOCK [=] {DEFAULT|NONE|SHARED|EXCLUSIVE}
#
# DROP_INDEX drops the index named index_name from the table tbl_name
#
# This statement is mapped to an ALTER_TABLE statement to drop the index.
#
# See SECTION 13.1.9, "ALTER TABLE SYNTAX"
#
# To drop a primary key, the index name is always PRIMARY, which must be specified
# as a quoted identifier because PRIMARY is a reserved word:
#
# 		DROP INDEX `PRIMARY` ON t;
#
# Indexes on variable-width columns of NDB tables are dropped online; that is, without
# any table copying.
#
# The table is not locked against access from other NDB Cluster API nodes, although
# it is locked against other operations on the same API node for the duration of the
# operation.
#
# This is done automatically by the server whenever it determines that it is possible
# to do so;
#
# you do not have to use any special SQL syntax or server options to cause it to happen.
#
# ALGORITHM and LOCK clauses may be given to influence the table copying method and level
# of concurrency for reading and writing the table while its indexes are being
# modified.
#
# They have the same meaning as for the ALTER_TABLE statement.
#
# For more information, see SECTION 13.1.9, "ALTER TABLE SYNTAX"
#
# MySQL NDB Cluster supports online operations using the same ALGORITHM=INPLACE
# syntax supported in the standard MySQL server.
#
# See SECTION 22.5.14, "ONLINE OPERATIONS WITH ALTER TABLE IN NDB CLUSTER", for more information.
#
# 13.1.28 DROP LOGFILE GROUP SYNTAX
#
# DROP LOGFILE GROUP logfile_group
# 		ENGINE [=] engine_name
#
# This statement drops the log file group named logfile_group.
#
# The log file group must already exist or an error results.
#
# (For information on creating log file groups, see SECTION 13.1.16, "CREATE LOGFILE GROUP SYNTAX")
#
# IMPORTANT:
#
# 		Before dropping a log file group, you must drop all tablespaces that use that log file
# 		group for UNDO logging.
#
# The required ENGINE clauses provides the name of the storage engine used by the log file group
# to be dropped.
#
# Currently, the only permitted values for engine_name are NDB and NDBCLUSTER
#
# DROP_LOGFILE_GROUP is useful only with Disk Data storage for NDB Cluster.
#
# See SECTION 22.5.13, "NDB CLUSTER DISK DATA TABLES"
#
# 13.1.29 DROP PROCEDURE AND DROP FUNCTION SYNTAX
#
# 	DROP {PROCEDURE | FUNCTION} [IF EXISTS] sp_name
#
# This statement is used to drop a stored procedure or function.
#
# That is, the specified routine is removed from the server. You must have the ALTER_ROUTINE
# privilege for the routine.
#
# (If the automatic_sp_privileges system variable is enabled, that privilege and EXECUTE are granted
# automatically to the routine creator when the routine is created and dropped from the creator
# when the routine is dropped.
#
# See SECTION 24.2.2, "STORED ROUTINES AND MYSQL PRIVILEGES")
#
# The IF EXISTS clause is a MySQL extension.
#
# It prevents an error from occurring if the procedure or function does not exist.
#
# A warning is produced that can be viewed with SHOW_WARNINGS.
#
# DROP_FUNCTION is also used to drop user-defined functions (see SECTION 13.7.4.2, "DROP FUNCTION SYNTAX")
#
# 13.1.30 DROP SERVER SYNTAX
#
# DROP SERVER [ IF EXISTS ] server_name
#
# Drops the server definition for the server named server_name.
#
# The corresponding row in the mysql.servers table is deleted.
#
# This statement requires the SUPER privilege.
#
# Dropping a server for table does not affect any FEDERATED tables that used
# this connection information when they were created.
#
# See SECTION 13.1.18, "CREATE SERVER SYNTAX"
#
# DROP SERVER causes an implicit commit. See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# DROP SERVER is not written to the binary log, regardless of the logging format that is in use.
#
# 13.1.31 DROP SPATIAL REFERENCE SYSTEM SYNTAX
#
# 		DROP SPATIAL REFERENCE SYSTEM
# 			[IF EXISTS]
# 			srid
#
# 		srid: 32-bit unsigned integer
#
# This statement removes a spatial reference system (SRS) definition from the data
# dictionary.
#
# It requires the SUPER privilege.
#
# Example:
#
# 		DROP SPATIAL REFERENCE SYSTEM 4120;
#
# If no SRS definition with the SRID value exists, an error occurs unless IF EXISTS
# is specified.
#
# In that case, a warning occurs rather than an error.
#
# If the SRID value is used by some column in an existing table, an error occurs.
# For example:
#
# 		DROP SPATIAL REFERENCE SYSTEM 4326;
# 		ERROR 3716 (SR005): Can't modify SRID 4326. There is at
# 		least one column depending on it.
#
# To identify which column or columns use the SRID, use this query:
#
# 		SELECT * FROM INFORMATION_SCHEMA.ST_GEOMETRY_COLUMNS WHERE SRS_ID=4326;
#
# SRID values must be in the range of 32-bit unsigned integers, with these restrictions:
#
# 		) SRID 0 is a valid SRID but cannot be used with DROP_SPATIAL_REFERENCE_SYSTEM
#
# 		) If the value is in a reserved SRID range, a warning occurs.
#
# 			Reserved ranges are [0, 32767] (reserved by EPSG), [60,000,000,000,69,999,999] (reserved by EPSG),
# 			and [2,000,000,000,2,147,483,647] (reserved by MySQL)
#
# 			EPSG stands for the European Petroleum Survey Group
#
# 		) Users should not drop SRSs with SRIDs in the reserved ranges.
#
# 			If system-installed SRSs are dropped, the SRS definitions may be recreated
# 			for MySQL upgrades.
#
# 13.1.32 DROP TABLE SYNTAX
#
# 		DROP [TEMPORARY] TABLE [IF EXISTS]
# 			tbl_name [, tbl_name] ---
# 			[RESTRICT | CASCADE]
#
# DROP_TABLE removes one or more tables. You must have the DROP privilege
# for each table.
#
# Be careful with this statement.
#
# It removes the table definition and all table data.
#
# For a partitioned table, it permanently removes the table definition,
# all its partitions and all data stored in those partitions.
#
# It also removes partition definitions associated with the dropped table.
#
# DROP_TABLE causes an implicit commit, except when used with the TEMPORARY
# keyword.
#
# See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# IMPORTANT:
#
# 		When a table is dropped, privileges granted specifically for the table are NOT
# 		automatically dropped.
#
# 		They must be dropped manually.
#
# 		See SECTION 13.7.1.6, "GRANT SYNTAX"
#
# If any tables named in the argument list do not exist, the statement fails with an
# error indicating by name which nonexisting tables it was unable to drop, and no
# changes are made.
#
# Use IF EXISTS to prevent an error from occurring for tables that do not exist.
#
# Instead of an error, a NOTE is generated for each nonexistent table; these notes
# can be displayed with SHOW_WARNINGS. See SECTION 13.7.6.40, "SHOW WARNINGS SYNTAX"
#
# IF EXISTS can also be useful for dropping tables in unusual circumstances under which
# there is an entry in the data dictionary but no table managed by the storage engine.
#
# (For example, if an abnormal server exit occurs after removal of the table from the
# storage engine but before removal of the data dictionary entry)
#
# The TEMPORARY keyword has the following effects:
#
# 		) The statement drops only TEMPORARY tables
#
# 		) The statement does not cause an implicit commit
#
# 		) No access rights are checked. A TEMPORARY table is visible only with the session
# 			that created it, so no check is necessary.
#
# Using TEMPORARY is a good way to ensure that you do not accidentally drop a 
# non-TEMPORARY table
#
# The RESTRICT and CASCADE keywords do nothing. They are permitted to make porting easier
# from other database systems.
#
# DROP_TABLE is not supported with all innodb_force_recovery settings.
#
# See SECTION 15.20.2, "FORCING INNODB RECOVERY"
#
# 13.1.33 DROP TABLESPACE SYNTAX
#
# 		DROP [UNDO] TABLESPACE tablespace_name
# 			[ENGINE [=] engine_name]
#
# This statement drops a tablespace that was previously created using CREATE_TABLESPACE.
#
# It is supported by the NDB and InnoDB storage engines.
#
# The UNDO keyword, introduced in MySQL 8.0.14, must be specified to drop an undo tablespace.
#
# Only undo tablespaces created using CREATE_UNDO_TABLESPACE syntax can be dropped.
#
# An undo tablespace must be in an empty state before it can be dropped.
# For more information, see SECTION 15.6.3.4, "UNDO TABLESPACES"
#
# ENGINE sets the storage engine that uses the tablespace, where engine_name is the name
# of the storage engine.
#
# Currently, the values InnoDB and NDB are supported.
#
# If not set, the value of default_storage_engine is used
#
# If it is not the same as the storage engine used to create the tablespace,
# the DROP TABLESPACE statement fails.
#
# tablespace_name is a case-sensitive identifier in MySQL.
#
# For an InnoDB general tablespace, all tables must be dropped from the tablespace
# prior to a DROP TABLESPACE operation.
#
# If the tablespace is not empty, DROP TABLESPACE returns an error.
#
# An NDB tablespace to be dropped must not contain any data files; in other words,
# before you can drop an NDB tablespace, you must first drop each of its data
# files using ALTER_TABLESPACE_---_DROP_DATAFILE
#
# NOTES
#
# 		) A general InnoDB tablespace is not deleted automatically when the last table in the
# 			tablespace is dropped.
#
# 			The tablespace must be dropped explicitly using DROP TABLESPACE tablespace_name
#
# 		) A DROP_DATABASE operation can drop tables that belong to a general tablespace but it cannot
# 			drop the tablespace, even if the operation drops all tables that belong to the tablespace.
#
# 			The tablespace must be dropped explicitly using DROP TABLESPACE tablespace_name
#
# 		) Similar to the system tablespace, truncating or dropping tables stored in a general tablespace
# 			creates free space internally in the general tablespace .ibd data file which can only
# 			be used for new InnoDB data.
#
# 			Space is not released back to the operating system as it is for file-per-table tablespaces
#
# InnoDB EXAMPLES
#
# This example demonstrates how to drop an InnoDB general tablespace.
#
# The general tablespace ts1 is created with a single table. 
# Before dropping the tablespace, the table must be dropped.
#
# 		CREATE TABLESPACE `ts1` ADD DATAFILE 'ts1.ibd' Engine=InnoDB;
#
# 		CREATE TABLE t1 (c1 INT PRIMARY KEY) TABLESPACE ts10 Engine=InnoDB;
#
# 		DROP TABLE t1;
#
# 		DROP TABLESPACE ts1;
#
# This example demonstrates dropping an undo tablespace.
#
# An undo tablespace must be in an empty state before it can be dropped.
#
# For more information, see SECTION 15.6.3.4, "UNDO TABLESPACES"
#
# 		DROP UNDO TABLESPACE undo_003;
#
# NDB EXAMPLE
#
# This example shows how to drop an NDB tablespace myts having a data file named mydata-1.dat
# after first creating the tablespace, and assumes the existence of a log file group named
# mylg (see SECTION 13.1.16, "CREATE LOGFILE GROUP SYNTAX")
#
# 		CREATE TABLESPACE myts
# 			ADD DATAFILE 'mydata-1.dat'
# 			USE LOGFILE GROUP mylg
# 			ENGINE=NDB;
#
# You must remove all data files from the tablespace using ALTER_TABLESPACE, as shown here
# , before it can be dropped:
#
# 		ALTER TABLESPACE 
# 			DROP DATAFILE 'mydata-1.dat'
# 			ENGINE=NDB;
#
# 		DROP TABLESPACE myts;
#
# 13.1.34 DROP TRIGGER SYNTAX
#
# 		DROP TRIGGER [IF EXISTS] [schema_name.]trigger_name
#
# This statement drops a trigger. The schema (database) name is optional.
#
# If the schema is omitted, the trigger is dropped from the default schema.
#
# DROP_TRIGGER requires the TRIGGER privilege for the table associated with the trigger.
#
# Use IF EXISTS to prevent an error from occurring for a trigger that does not exist.
#
# A NOTE is generated for a nonexistent trigger when using IF EXISTS.
#
# See SECTION 13.7.6.40, "SHOW WARNINGS SYNTAX"
#
# Triggers for a table are also dropped if you drop the table.
#
# 13.1.35 DROP VIEW SYNTAX
#
# 		DROP VIEW [IF EXISTS]
# 			view_name [, view_name] ---
# 			[RESTRICT | CASCADE]
#
# DROP_VIEW removes one or more views.
#
# You must have the  DROP privilege for each view.
#
# If any views named in the argument list do not exist, the statement fails
# with an error indicating by name which nonexisting views it was unable to
# drop, and no changes are made.
#
# 		NOTE:
#
# 			In MySQL 5.7 and earlier, DROP_VIEW returns an error if any views named in the argument
# 			list do not exist, but also drops all views in the list that do exist.
#
# 			Due to the change in behavior in MySQL 8.0, a partially completed DROP_VIEW operaiton
# 			on a MySQL 5.7 master fails when replicated on a MySQL 8.0 slave
#
# 			To avoid this failure scenario, use IF EXISTS syntax in DROP_VIEW statements to prevent
# 			an error from occurring for views that do not exist.
#
# 			For more information, see SECTION 13.1.1, "ATOMIC DATA DEFINITION STATEMENT SUPPORT"
#
# The IF EXISTS clause prevents an error from occurring for views taht do not exist.
#
# When this clause is given, a NOTE is generated for each nonexistent view.
#
# See SECTION 13.7.6.40, "SHOW WARNINGS SYNTAX"
#
# RESTRICT and CASCADE, if given, are parsed and ignored.
#
# 13.1.36 RENAME TABLE SYNTAX
#
# RENAME TABLE
# 		tbl_name TO new_tbl_name
# 		[, tbl_name2 TO new_tbl_name2] ---
#
# RENAME_TABLE renames one or more tables. You must have ALTER and DROP privileges
# for the original table, and CREATE and INSERT privileges for the new table.
#
# For example, to rename a table named old_table to new_table, use this statement:
#
# 		RENAME TABLE old_table TO new_table;
#
# That statement is equivalent to the following ALTER_TABLE statement:
#
# 		ALTER TABLE old_table RENAME new_table;
#
# RENAME TABLE, unlike ALTER_TABLE, can rename multiple tables within a single statement:
#
# 		RENAME TABLE old_table1 TO new_table1,
# 						 old_table2 TO new_table2,
# 						 old_table3 TO new_table3;
#
# Renaming operations are performed left to right.
#
# Thus, to swap two table names, do this (assuming that a table with the intermediary
# name tmp_table does not already exist):
#
# 		RENAME TABLE old_table TO tmp_table,
# 						 new_table TO old_table,
# 						 tmp_table TO new_table;
#
# Metadata locks on tables are acquired in name order, which in some cases can make a difference
# in operation outcome when multiple transactions execute concurrently.
#
# See SECTION 8.11.4, "METADATA LOCKING"
#
# As of MySQL 8.0.13, you can rename tables locked with a LOCK_TABLES statement, provided
# that they are locked with a WRITE lock or are the product of renaming WRITE-locked
# tables from earlier steps in a multiple-table rename operation.
#
# For example, this is permitted:
#
# 		LOCK TABLE old_table1 WRITE;
# 		RENAME TABLE old_table1 TO new_table1,
# 						 new_table1 TO new_table2;
#
# This is not permitted:
#
# 		LOCK TABLE old_table1 READ;
# 		RENAME TABLE old_table1 TO new_table1,
# 						 new_table1 TO new_table2;
#
# Prior to MySQL 8.0.13, to execute RENAME TABLE, there must be no tables locked with LOCK TABLES.
#
# With the transaction table locking conditions satisfied, the rename operation is done atomically;
# no other session can access any of the tables while the rename is in progress.
#
# If any error occurs during a RENAME TABLE, the statement fails and no changes are made.
#
# You can use RENAME TABLE to move a table from one database to another:
#
# 		RENAME TABLE current_db.tbl_name TO other_db.tbl_name;
#
# Using this method to move all tables from one database to a different one in effect
# renames the database (an operation for which MySQL has no single statement), except
# that the original database continue to exist, albeit with no tables.
#
# Like RENAME TABLE, ALTER TABLE --- RENAME can also be used to move a table to a different
# database.
#
# Regardless of the statement used, if the rename operation would move the table to a database
# located on a different file system, the success of the outcome is platform specific and
# depends on the underlying OS calls used to move table files.
#
# If a table has triggers, attempts to rename the table into a different database fail with a
# Trigger in wrong schema (ER_TRG_IN_WRONG_SCHEMA) error
#
# To rename TEMPORARY tables, RENAME TABLE does not work.
#
# Use ALTER_TABLE instead.
#
# RENAME TABLE works for views, except that views cannot be renamed into a different database.
#
# Any privileges granted specifically for a renamed table or view are not migrated to the new name.
#
# They must be changed manually.
#
# RENAME TABLE changes interally generated foreign key constraint names and user-defined foreign
# key constraint names that contain the string "tbl_name_ibfk_" to reflect the new table name.
#
# InnoDB interprets foreign key constraint names that contain the string "tbl_name_ibfk_"
# as internally generated names.
#
# Foreign key constraint names that point to the renamed table are automatically updated unless
# there is a conflict, in which case the statement fails with an error.
#
# A conflict occurs if the renamed constraint name already exists.
#
# In such cases, you must drop and re-create the foreign keys for them to function properly.
#
# 13.1.37 TRUNCATE TABLE SYNTAX
#
# TRUNCATE [TABLE] tbl_name
#
# TRUNCATE_TABLE empties a table completely. It requires the DROP privilege.
#
# Logically, TRUNCATE_TABLE is similar to DELETE statement that deletes all rows,
# or a sequence of DROP_TABLE and CREATE_TABLE statements.
#
# To achieve a high performance, TRUNCATE_TABLE bypasses the DML method of deleting data.
#
# Thus, it does not cause ON DELETE triggers to fire, it cannot be performed for 
# InnoDB tables with parent-child foreign key relationships, and it cannot be rolled
# back like a DML operation.
#
# However, TRUNCATE TABLE operations on tables that use an atomic DDL-supported storage
# engine are either fully committed or rolled back if the server halts during their operation.
#
# For more information, see SECTION 13.1.1, "ATOMIC DATA DEFINITION STATEMENT SUPPORT"
#
# Although TRUNCATE_TABLE is similar to DELETE, it is classified as a DDL statement rather than
# a DML statement.
#
# It differs from DELETE in the following ways:
#
# 		) Truncate operations drop and re-create the table, which is much faster than deleting rows
# 			one by one, particularly for large tables.
#
# 		) Truncate operations cause an implicit commit, and so cannot be rolled back.
#
# 			See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# 		) Truncation operations cannot be performed if the session holds an active table lock
#
# 		) TRUNCATE_TABLE fails for an InnoDB table or NDB table if there are any FOREIGN KEY constraints
# 			from other tables that reference the table.
#
# 			Foreign key constraints between columns of the same table are permitted.
#
# 		) Truncation operations do not return a meaningful value for the number of deleted rows.
#
# 			The usual result is "0 rows affected", which should be interpreted as "no information"
#
# 		) As long as the table definition is valid, the table can be re-created as an empty table with TRUNCATE_TABLE,
# 			even if the data or index files have become corrupted.
#
# 		) Any AUTO_INCREMENT value is reset to its start value. This is true even for MyISAM and InnoDB, which normally
# 			do not reuse sequence values.
#
# 		) When used with partitioned tables, TRUNCATE_TABLE preserves the partitioning; that is, the data and index
# 			files are dropped and re-created, while the partition definitions are unaffected.
#
# 		) The TRUNCATE_TABLE statement does not invoke ON DELETE triggers
#
# 		) Truncating a corrupted InnoDB table is supported.
#
# TRUNCATE_TABLE for a table closes all handlers for the table that were opened with HANDLER_OPEN
#
# TRUNCATE_TABLE is treated for purposes of binary logging and replication as DROP_TABLE followed by CREATE_TABLE -
# that is, as DDL rather than DML.
#
# This is due to the fact that, when using InnoDB and other transactional storage engines where the transaction
# isolation level does not permit statement-based logging (READ COMMITTED or READ UNCOMMITTED), the statement
# was not logged and replicated when using STATEMENT or MIXED logging mode.
#
# (Bug #36763)
#
# However, it is still applied on replication slaves using InnoDB in the manner described previously.
#
# In MySQL 5.7 and earlier, on a system with a large buffer pool and innodb_adaptive_hash_index
# enabled, a TRUNCATE TABLE operation could cause a temporary drop in system performance due to
# an LRU scan that occurred when removing the table's adaptive hash index entries (Bug #68184)
#
# The remapping of TRUNCATE_TABLE to DROP_TABLE and CREATE_TABLE in MySQL 8.0 avoids the problematic
# LRU scan.
#
# TRUNCATE_TABLE can be used with Performance Schema summary tables, but the effect is to reset the
# summary columns to 0 or NULL, not to remove rows.
#
# See SECTION 26.12.16, "PERFORMANCE SCHEMA SUMMARY TABLES"
#
# 13.2 DATA MANIPULATION STATEMENTS
#
# 13.2.1 CALL SYNTAX
# 13.2.2 DELETE SYNTAX
# 13.2.3 DO SYNTAX
#
# 13.2.4 HANDLER SYNTAX
# 13.2.5 IMPORT TABLE SYNTAX
# 13.2.6 INSERT SYNTAX
#
# 13.2.7 LOAD DATA INFILE SYNTAX
# 13.2.8 LOAD XML SYNTAX
# 13.2.9 REPLACE SYNTAX
# 
# 13.2.10 SELECT SYNTAX
# 13.2.11 SUBQUERY SYNTAX
# 13.2.12 UPDATE SYNTAX
#
# 13.2.13 WITH SYNTAX (COMMON TABLE EXPRESSIONS)
#
# 13.2.1 CALL SYNTAX
#
# CALL sp_name([parameter[,---]])
# CALL sp_name[()]
#
# The CALL statement invokes a stored procedure that was defined previously with CREATE_PROCEDURE
#
# Stored procedures that take no arguments can be invoked without parentheses.
#
# That is, CALL p() and CALL p are equivalent.
#
# CALL can pass back values to its caller using parameters that are declared as OUT or
# INOUT parameters.
#
# When the procedure returns, a client program can also obtain the number of rows
# affected for the final statement executed within the routine:
#
# 		At the SQL level, call the ROW_COUNT() function
#
# 		From the C API, call the mysqL_affected_rows() function
#
# For information about the effect of unhandled conditions on procedure parameters,
# see SECTION 13.6.7.8, "CONDITION HANDLING AND OUT OR INOUT PARAMETERS"
#
# To get back a value from a procedure using an OUT or INOUT parameter, pass the parameter
# by means of a user variable, and then check the value of the variable after the
# procedure returns.
#
# (If you are calling the procedure from within another stored procedure or function,
# you can also pass a routine parameter or local routine variable as an IN or INOUT parameter)
#
# For an INOUT parameter, initialize its value before passing it to the procedure.
#
# The following procedure has an OUT parameter that the procedure sets to
# the current server version, and an INOUT value that the procedure increments by one
# from its current value:
#
# 		CREATE PROCEDURE p (OUT ver_param VARCHAR(25), INOUT incr_param INT)
# 		BEGIN
# 			# Set value of OUT parameter
# 			SELECT VERSION() INTO ver_param;
# 			# Increment value of INOUT parameter
# 			SET incr_param = incr_param + 1;
# 		END;
#
# Before calling the procedure, initialize the variable to be passed as the INOUT parameter.
#
# After calling the procedure, the values of the two variables will have been set or modified:
#
# 		SET @increment = 10;
# 		CALL p(@version, @increment);
# 		SELECT @version, @increment;
# 		+-----------------------------+-------------+
# 		| @version 							| @increment  |
# 		+-----------------------------+-------------+
# 		| 8.0.3-rc-debug-log 			| 11 			  |
# 		+-----------------------------+-------------+
#
# IN prepared CALL statements used with PREPARE and EXECUTE, placeholders
# can be used for IN parameters, OUT, and INOUT parameters.
#
# These types of parameters can be used as follows:
#
# 		SET @increment = 10;
# 		PREPARE s FROM 'CALL p(?, ?)';
# 		EXECUTE s USING @version, @increment;
# 		SELECT @version, @increment;
# 		+----------------------+---------------+
# 		| @version 				  | @increment 	|
# 		+----------------------+---------------+
# 		| 8.0.3-rc-debug-log   | 11 			   |
# 		+----------------------+---------------+
#
# To write C programs that use the CALL SQL statements to execute stored procedures
# that produce result sets, the CLIENT_MULTI_RESULTS flag must be enabled.
#
# THis is because each CALL returns a result to indicate the call status, in addition
# to any result sets that might be returned by statements executed within the procedure.
#
# CLIENT_MULTI_RESULTS must also be enabled if CALL is used to execute any stored procedure
# that contains prepared statements.
#
# It cannot be determined when such a procedure is loaded whether those statements will produce
# result sets, so it is necessary to assume that they will.
#
# CLIENT_MULTI_RESULTS can be enabled when you call mysql_real_connect(), either explicitly
# by passing the CLIENT_MULTI_RESULTS flag itself, or implicitly by passing CLIENT_MULTI_STATEMENTS
# (which also enables CLIENT_MULTI_RESULTS) 
#
# CLIENT_MULTI_RESULTS is enabled by default
#
# To process the result of a CALL statement executed using mysql_query() or mysql_real_query(),
# use a loop that calls mysql_next_result() to determine whether there are more results.
#
# For an example, see SECTION 28.7.19, "C API MULTIPLE STATEMENT EXECUTION SUPPORT"
#
# C programs can use hte prepared-statement interface to execute CALL statements and access
# OUT and INOUT parameters.
#
# This is done by processing the result of a CALL statement using a loop that calls
# mysql_stmt_next_result() to determine whether there are more results.
#
# For an example, see SECTION 28.7.21, "C API PREPARED CALL STATEMENT SUPPORT"
#
# Languages that provide a MySQL interface can use prepared CALL statements to directly
# retrieve OUT and INOUT procedure statements.
#
# Metadata changes to objects referred to by stored programs are detected and cause 
# automatic reparsing of the affected statements when the program is next executed.
#
# For more information, see SECTION 8.10.3, "CACHING OF PREPARED STATEMENTS AND STORED PROGRAMS"
#
# 13.2.2 DELETE SYNTAX
#
# DELETE is a DML statement that removes rows from a table
#
# A DELETE statement can start with a WITH clause to define common table expressions accessible
# within the DELETE.
#
# See SECTION 13.2.13, "WITH SYNTAX (COMMON TABLE EXPRESSIONS)"
#
# SINGLE-TABLE SYNTAX
#
# 		DELETE [LOW_PRIORITY] [QUICK] [IGNORE] FROM tbl_name
# 			[PARTITION (partition_name [, partition_name] ---)]
# 			[WHERE where_condition]
# 			[ORDER BY ---]
# 			[LIMIT row_count]
#
# The DELETE statement deletes rows from tbl_name and returns the number of deleted rows.
#
# To check the number of deleted rows, call the ROW_COUNT() function described in
# SECTION 12.15, "INFORMATION FUNCTIONS"
#
# MAIN CLAUSES
#
# The conditions in the optional WHERE clause identify which rows to delete.
#
# With no WHERE clause, all rows are deleted.
#
# where_condition is an expression that evaluates to true for each row to be deleted.
# It is specified as described in SECTION 13.2.10, "SELECT SYNTAX"
#
# If the ORDER BY clause is specified, the rows are deleted in the order that is specified.
#
# The LIMIT clause places a limit on the number of rows that can be deleted.
#
# These clauses apply to single-table deletes, but not multi-table deletes
#
# MULTIPLE-TABLE SYNTAX
#
# 		DELETE [LOW_PRIORITY] [QUICK] [IGNORE]
# 			tbl_name[.*] [, tbl_name[.*]] ---
# 			FROM table_references
# 			[WHERE where_condition]
#
# 		DELETE [LOW_PRIORITY] [QUICK] [IGNORE]
# 			FROM tbl_name[.*] [, tbl_name[.*]] ---
# 			USING table_references
# 			[WHERE where_condition]
#
# PRIVILEGES
#
# You need the DELETE privilege on a table to delete rows from it.
#
# You need only the SELECT privilege for any columns that are only read,
# such as those named in the WHERE clause.
#
# PERFORMANCE
#
# When you do not need to know the number of deleted rows, the TRUNCATE_TABLE
# statement is a faster way to empty a table than a DELETE statement with no WHERE clause.
#
# Unlike DELETE, TRUNCATE_TABLE cannot be used within a transaction or if you have a lock
# on the table.
#
# See SECTION 13.1.37, "TRUNCATE TABLE SYNTAX" and SECTION 13.3.6, "LOCK TABLES AND UNLOCK TABLES SYNTAX"
#
# The speed of delete operations may also be affected by factors discussed in SECTION 8.2.5.3,
# "OPTIMIZING DELETE STATEMENTS"
#
# To ensure that a given DELETE statement does not take too much time, the MySQL-specific LIMIT
# row_count clause for DELETE specifies the maximum number of rows to be deleted.
#
# If the number of rows to delete is larger than the limit, repeat the DELETE statement
# until the number of affected rows is less than the LIMIT value.
#
# SUBQUERIES
#
# You cannot delete from a table and select from the same table in a subquery
#
# PARTITIONED TABLE SUPPORT
#
# DELETE supports explicit partition selection using the PARTITION option, which takes a list of the
# comma-separated names of one or more partitions or subpartitions (or both) from which to select
# rows to be dropped.
#
# Partitions not included in the list are ignored.
#
# Given a partitioned table t with a partition named p0, executing the statement
# DELETE FROM t PARTITION (p0) has the same effect on the table as executing ALTER_TABLE_t_TRUNCATE_PARTITION_(p0);
# in both cases, all rows in partition p0 are dropped.
#
# PARTITION can be used along with a WHERE condition, in which case the condition is tested
# only on rows in the listed partitions.
#
# For example, 
#
# 		DELETE FROM t PARTITION (p0) WHERE c < 5 
# 
# deletes rows only from partition p0 for which the condition c < 5 is true;
# rows in any other partitions are not checked and thus not affected by the DELETE
#
# The PARTITION option can also be used in multiple-table DELETE statements.
#  
# You can use up to one such option per table named in the FROM option
#
# For more information and examples, see SECTION 23.5, "PARTITION SELECTION"
#
# AUTO-INCREMENT COLUMNS
#
# If you delete the row containing the maximum value for an AUTO_INCREMENT column,
# the value is not reused for a MyISAM or InnoDB table.
#
# if you delete all rows in the table with DELETE FROM tbl_name (without a WHERE clause)
# in autocommit mode, the sequence starts over for all storage engines except InnoDB
# and MyISAM.
#
# There are some exceptions to this behavior for InnoDB tables, as discussed
# in SECTION 15.6.1.14, "AUTO_INCREMENT HANDLING IN INNODB"
#
# For MyISAM tables, you can specify an AUTO_INCREMENT secondary column in a multiple-column
# key.
#
# In this case, reuse of values deleted from the top of the sequence occurs even for
# MyISAM tables. See SECTION 3.6.9, "USING AUTO_INCREMENT"
#
# MODIFIERS
#
# The DELETE statement supports the following modifiers:
#
# 		) If you specify LOW_PRIORITY, the server delays execution of the DELETE until no other
# 			clients are reading from the table.
#
# 			This affects only storage engines that use only table-level locking
# 			(such as MyISAM, MEMORY and MERGE)
#
# 		) For MyISAM tables, if you use the QUICK modifier, the storage engine does not merge
# 			index leaves during delete, which may speed up some kinds of delete operations.
#
# 		) The IGNORE modifier causes MySQL to ignore errors during the process of deleting rows.
#
# 			(Errors encountered during the parsing stage are processed in the usual manner)
#
# 			Errors that are ignored due to the use of IGNORE are returend as warnings.
#
# 			For more information, see COMPARISON OF THE IGNORE KEYWORD AND STRICT SQL MODE
#
# ORDER OF DELETION
#
# If the DELETE statement includes an ORDER BY clause, rows are deleted in the order specified
# by the clause.
#
# This is useful primarily in conjunction with LIMIT.
#
# For example, the following statement finds rows matching the WHERE clause, sorts them
# by timestamp_column, and deletes the first (oldest) one:
#
# 		DELETE FROM somelog WHERE user = 'jcole'
# 		ORDER BY timestamp_column LIMIT 1;
#
# ORDER BY also helps to delete rows in an order required to avoid referential
# integrity violations.
#
# INNODB TABLES
#
# If you are deleting many rows from a large table, you may exceed the lock table
# size for an InnoDB table.
#
# To avoid this problem, or simply to minimize the time that the table remains locked,
# the following strategy (which does not use DELETE at all) might be helpful:
#
# 		1. Select the rows NOT to be deleted into an empty table that has the same structure as the original table:
#
# 			INSERT INTO t_copy SELECT * FROM t WHERE --- ;
#
# 		2. Use RENAME_TABLE to atomically move the original table out of the way and rename the copy to the original name:
#
# 			RENAME TABLE t TO t_old, t_copy TO t;
#
# 		3. Drop the original table:
#
# 			DROP TABLE t_old;
#
# No other sessions can access the tables involved while RENAME_TABLE executes, so the rename operation
# is not subject to concurrency problems.
#
# See SECTION 13.1.36, "RENAME TABLE SYNTAX"
#
# MYISAM TABLES
#
# In MyISAM tables, deleted rows are maintained in a linked list and subsequent INSERT operations
# reuse old row positions.
#
# To reclaim unused space and reduce file size, use the OPTIMIZE TABLE statement or the
# myisamchk utility to reorganize tables.
#
# OPTIMIZE_TABLE is easier to use, but myisamchk is faster.
#
# See SECTION 13.7.3.4, "OPTIMIZE TABLE SYNTAX", and SECTION 4.6.4, "MYISAMCHK -- MyISAM TABLE-MAINTENANCE UTILITY"
#
# The QUICK modifier affects whether index leaves are merged for delete operations.
#
# DELETE QUICK is most useful for applications where index values for deleted rows are replaced by similar
# index values from rows inserted later.
#
# In this case, the holes left by deleted values are reused.
#
# DELETE QUICK is not useful when deleted values lead to underfilled blocks spanning
# a range of index values for which new inserts occur again.
#
# In this case, use of QUICK can lead to wasted space in the index that remains unreclaimed.
#
# Here is an example of such a scenario:
#
# 		1. Create a table that contains an indexed AUTO_INCREMENT column
#
# 		2. Insert many rows into the table. Each insert results in an index value that is added
# 			to the high end of the index.
#
# 		3. Delete a block of rows at the low end of the column range using DELETE QUICK
#
# In this scenario, the index blocks associated with the deleted index values become underfilled
# but are not merged with other index blocks due to the use of QUICK.
#
# They remain underfilled when new inserts occur, because new rows do not have index values
# in the deleted range.
#
# Furthermore, they remain underfilled even if you later use DELETE without QUICK, unless some
# of the deleted index values happen to lie in index blocks within or adjacent to the
# underfilled blocks.
#
# To reclaimed unused index space under these circumstances, use OPTIMIZE_TABLE
#
# If you are going to delete many rows from a table, it might be faster to use DELETE QUICK
# followed by OPTIMIZE TABLE.
#
# This rebuilds the index rather than performing many index block merge operations.
#
# MULTI-TABLE DELETES
#
# You can specify multiple tables in a DELETE statement to delete rows from one or more
# tables depending on teh condition in teh WHERE clause.
#
# You cannot use ORDER BY or LIMIT in a multiple-table DELETE
#
# The table_references clause lists the tables involved in the join,
# as described in SECTION 13.2.10.2, "JOIN SYNTAX"
#
# For the first multiple-table syntax, only matching rows from tables listed before
# the FROM clause are deleted.
#
# For the second multiple-table syntax, only matching rows from the tables listed
# in the FROM clause (before the USING clause) are deleted.
#
# The effect is that you can delete rows from many tables at the same time and have
# additional tables that are used only for searching:
#
# 		DELETE t1, t2 FROM t1 INNER JOIN t2 INNER JOIN t3
# 		WHERE t1.id=t2.id AND t2.id=t3.id;
#
# OR
#
# 		DELETE FROM t1, t2 USING t1 INNER JOIN t2 INNER JOIN t3
# 		WHERE t1.id=t2.id AND t2.id=t3.id;
#
# These statements use all three tables when searching for rows to delete, but delete
# matching rows only from tables t1 and t2.
#
# The preceding examples use INNER JOIN, but multiple-table DELETE statements cna use
# other types of join permitted in SELECT statements, such as LEFT JOIN
#
# For example, to delete rows that exist in t1 that have no match in t2, use a LEFT JOIN:
#
# 		DELETE t1 FROM t1 LEFT JOIN t2 ON t1.id=t2.id WHERE t2.id IS NULL;
#
# The syntax permits .* after each tbl_name for compatibility with Access.
#
# If you use a multiple-table DELETE statement involving InnoDB tables for which
# there are foreign key constraints, the MySQL optimizer might process tables in
# an order that differs from that of their parent/child relationship.
#
# In this case, the statement fails and rolls back.
#
# Instead, you should delete from a single table and rely on the ON DELETE 
# capabilities that INnoDB provides to cause the other tables ot be modified accordingly.
#
# NOTE:
#
# 		if you declare an alias for a table, you must use the alias when referring to teh table:
#
# 			DELETE t1 FROM test AS t1, test2 WHERE ---
#
# Table aliases in a multiple-table DELETE should be declared only in the table_references
# part of the statement.
#
# Elsewhere, alias references are permitted but not alias declarations.
#
# CORRECT:
#
# 		DELETE a1, a2 FROM t1 AS a1 INNER JOIN t2 AS a2
# 		WHERE a1.id=a2.id;
#
# 		DELETE FROM a1, a2 USING t1 AS a1 INNER JOIN t2 AS a2
# 		WHERE a1.id=a2.id;
#
# INCORRECT:
#
# 		DELETE t1 AS a1, t2 AS a2 FROM t1 INNER JOIN t2
# 		WHERE a1.id=a2.id;
#
# 		DELETE FROM t1 AS a1, t2 AS a2 USING t1 INNER JOIN t2
# 		WHERE a1.id=a2.id;
#
# 13.2.3 DO SYNTAX
#
# 		DO expr [, expr] ---
#
# DO executes the expressions but does not return any results.
#
# In most respects, DO is shorthand for SELECT expr, ---, but has the advantage
# that it is slightly faster when you do not care about the result.
#
# DO is useful primarily with functions that have side effects, such as
# RELEASE_LOCCK()
#
# Example: This SELECT statement pauses, but also produces a result set:
#
# 		SELECT SLEEP(5);
# 		+-------------+
# 		| SLEEP(5) 	  |
# 		+-------------+
# 		| 		0 		  |
# 		+-------------+
# 		1 row in set (5.02 sec)
#
# DO, on the other hand, pauses without producing a result set:
#
# 		DO SLEEP(5);
# 		Query OK, 0 rows affected (4.99 sec)
#
# This could be useful, for example in a stored function or trigger, which prohibit
# statements that produce result sets.
#
# DO only executes expressions.
#
# It cannot be used in all cases where SELECT can be used.
#
# For example, DO id FROM t1 is invalid ebcause it references a table.
#
# 13.2.4 HANDLER SYNTAX
#
# 		HANDLER tbl_name OPEN [ [AS] alias]
#
# 		HANDLER tbl_name READ index_name { = | <= | >= | < | > } (value1, value2, ---)
# 			[ WHERE where_condition ] [LIMIT --- ]
# 		HANDLER tbl_name READ index_name { FIRST | NEXT | PREV | LAST }
# 			[ WHERE where_condition ] [LIMIT --- ]
#
# 		HANDLER tbl_name READ { FIRST | NEXT }
# 			[ WHERE where_condition ] [LIMIT --- ]
# 
# 		HANDLER tbl_name CLOSE
#
# The HANDLER statement provides direct access to table storage engine interfaces.
#
# It is available for InnoDB and MyISAM tables.
#
# The HANDLER --- OPEN statement opens a table, making it accessible using subsequent
# HANDLER --- READ statements.
#
# This table object is not shared by other sessions and is not closed until the
# session calls HANDLER --- CLOSE or the session terminates.
#
# If you open the table using an alias, further references to the open table with
# other HANDLER statements must use the alias rather than the table name.
#
# If you do not use an alias, but open the table using a table name qualified
# by the database name, further references must use the unqualified table name.
#
# For example, for a table opened using mydb.mytable - further references must use mytable
#
# The first HANDLER --- READ syntax fetches a row where the index specified statisfies
# the given values and the WHERE condition is met.
#
# If you have a multiple-column index, specify the index column values as a comma-separated
# list.
#
# Either specify values for all the columns in the index, or specify values for a leftmost
# prefix of the index columns.
#
# Suppose that an index my_idc includes three columns named col_a, col_b and col_c, in taht order.
#
# The HANDLER statement can specify values for all three columns in the index,
# or for the columns in a leftmost prefix. For example:
#
# 		HANDLER --- READ my_idx = (col_a_val, col_b_val, col_c_val) ---
# 		HANDLER --- READ my_idx = (col_a_val, col_b_val) ---
# 		HANDLER --- READ my_idx = (col_a_val) ---
#
# To employ the HANDLER interface to refer to a table's PRIMARY KEY, use the quoted
# identifier `PRIMARY`:
#
# 		HANDLER tbl_name READ `PRIMARY` ---
#
# The second HANDLER --- READ syntax fetches a row from the table in index order that
# matches the WHERE condition.
#
# The third HANDLER --- READ syntax fetches a row from the table in natural row order
# that matches the WHERE condition.
#
# It is faster than HANDLER tbl_name READ index_name when a full table scan is desired.
#
# Natural row order is the order in which rows are stored in a MyISAM table data file.
#
# This statement works for INnoDB tables as well, but there is no such concept because
# there is no separate data file.
#
# Without a LIMIT clause, all forms of HANDLER --- READ fetch a single row if one is 
# available.
#
# To return a specific number of rows, include a LIMIT clause.
#
# It has the same syntax as for the SELECT statement. See SECTION 13.2.10, "SELECT SYNTAX"
#
# HANDLER -- CLOSE closes a table that was opened with HANDLER --- OPEN
#
# There are several reasons to use the HANDLER interface instead of normal
# SELECT statements:
#
# 		) HANDLER is faster than SELECT
#
# 			) A desiganted storage engine handler object is allocated for the HANDLER --- OPEN.
#
# 				THe object is reused for subsequent HANDLER statements for that table;
# 				it need not be reinitialized for each one.
#
# 			) There is less parsing involved
#
# 			) There is no optimizer or query-checking overhead
#
# 			) The handler interface does not have to provide a consistent look of the data (for example,
# 				dirty reads are permitted), so the storage engine can use optimizations that SELECT
# 				does not normally permit.
#
# 		) HANDLER makes it easier to port to MySQL applications that use a low-level ISAM-like interface.
#
# 			(See SECTION 15.19, "INNODB MEMCACHED PLUGIN" for an alternative way to adapt applications
# 			that use the key-value store paradigm)
#
# 		) HANDLER enables you to traverse a database in a manner that is difficult (or even impossible)
# 			to accomplish with SELECT.
#
# 			The HANDLER interface is a more natural way to look at data when working with applications
# 			that provide an interactive user interface to the database.
#
# HANDLER is a somewhat low-level statement.
#
# For example, it does not provide consistency.
#
# That is, HANDLER --- OPEN does NOT take a snapshot of the table, and does NOT 
# lock the table.
#
# THis means that after a HANDLER --- OPEN statement is issued, table data can be modified 
# (by the current session or other sessions) and these modifications might be only
# partially visible to HANDLER --- NEXT or HANDLER --- PREV scans.
#
# An open handler can be closed and marked for reopen, in which case the handler loses
# its position in the table.
#
# This occurs when both of the following circumstances are true:
#
# 		) Any session executes FLUSH_TABLES or DDL statements on the handler's table
#
# 		) The session in which the handler is open executes non-HANDLER statements that use tables
#
# TRUNCATE_TABLE for a table closes all handlers for the table that were opened with HANDLER_OPEN
#
# If a table is flushed with FLUSH_TABLES_tbl_name_WITH_READ_LOCK was opened with HANDLER,
# the handler is implicitly flushed and loses its position.
#
# 13.2.5 IMPORT TABLE SYNTAX
#
# 		IMPORT TABLE FROM sdi_file [, sdi_file] ---
#
# The IMPORT_TABLE statement imports MyISAM tables based on information contained in .sdi
# (Serialized Dictionary Information) metadata files.
#
# IMPORT TABLE requires the FILE privilege to read the .sdi and table content files,
# and the CREATE privilege for the table to be created.
#
# Tables can be exported from one server using mysqldump to write a file of SQL statements
# and imported into another server using mysql to process the dump file.
#
# IMPORT TABLE provides a faster alternative using the "raw" table files
#
# Prior to import, the files that provide the table content must be placed in the 
# appropriate schema directory for the import server, and the .sdi file must be
# located in a directory accessible to the server.
#
# For example, the .sdi file can be placed in the directory named by the secure_file_priv
# system variable, or (if secure_file_priv is empty) in a directory under the server
# data directory.
#
# The following example describes how to export MyISAM tables named employees and
# managers from the hr schema of one server and import them into the hr schema of
# another server.
#
# The examples uses these assumptions (to perform a similar operaiton on your own system,
# modify the path names as called for):
#
# 		) For the export server, export_basedir represents its base directory, and its data directory
# 			is export_basedir/data
#
# 		) For the import server, import_basedir represents its base directory, and its data directory
# 			is import_basedir/data
#
# 		) Table files are exported from the export server into the /tmp/export directory and
# 			this directory is secure (not accessible to other users)
#
# 		) The import server uses /tmp/mysql-files as the directory named by its secure_file_priv
# 			system variable
#
# To export tables from the export server, use this procedure:
#
# 		1. Ensure a consistent snapshot by executing this statement to lock the tables so that they cannot
# 			be modified during export:
#
# 				FLUSH TABLES hr.employees, hr.managers WITH READ LOCK;
#
# 			While the lock is in effect, the tables can still be used, but only for read access.
#
# 		2. At the file system level, copy the .sdi and table content files from the hr schema directory
# 			to the secure export directory:
#
# 				) The .sdi file is located in the hr schema directory, but might not have exactly
# 					the same basename as the table name.
#
# 					For example, the .sdi files for the employees and managers tables might be named
# 					employees_125.sdi and managers_238.sdi
#
# 				) For a MyISAM table, the content files are its .MYD data file and .MYI index file
#
# 			Given those file names, the copy commands are:
#
# 				cd export_basedir/data/hr
# 				cp employees_125.sdi /tmp/export
# 				cp managers_238.sdi /tmp/export
# 				cp employees.{MYD,MYI} /tmp/export
# 				cp managers.{MYD,MYI} /tmp/export
#
# 		3. Unlock the tables:
#
# 			UNLOCK TABLES;
#
# To import tables into hte import server, use this procedure:
#
# 		1. The import schema must exist.
#
# 			If necessary, execute this statement to create it:
#
# 				CREATE SCHEMA hr;
#
# 		2. At the file system level, copy the .sdi files to the import server
# 			secure_file_priv directory, /tmp/mysql-files
#
# 			Also, copy the table content files to the hr schema directory:
#
# 				cd /tmp/export
# 				cp employees_125.sdi /tmp/mysql-files
# 				cp managers_238.sdi /tmp/mysql-files
# 				cp employees.{MYD,MYI} import_basedir/data/hr
# 				cp managers.{MYD,MYI} import_basedir/data/hr
#
# 		3. Import the tables by executing an IMPORT_TABLE statement that names the .sdi files:
#
# 			IMPORT TABLE FROM
# 				'/tmp/mysql-files/employees.sdi',
# 				'/tmp/mysql-files/managers.sdi';
#
# The .sdi file need not be placed in the import server directory named by the secure_file_priv
# system variable if that variable is empty; it can be in any directory accessible to the server,
# including the schema directory for the imported table.
#
# If the .sdi file is placed in that directory, however, it may be rewritten; the import
# operation creates a new .sdi file for hte table, which will overwrite the old .sdi file
# if hte operation uses the same file name for the new file.
#
# Each sdi_file value must be a string literal that names the .sdi file for a table
# or is a pattern that matches .sdi files.
#
# If the string is a pattern, any leading directory path and the .sdi file name
# suffix must be given literally.
#
# Pattern characters are permitted only in the base name part of the file name:
#
# 		) ? matches any single character
#
# 		) * matches any sequence of characters, including no characters
#
# Using a pattern, the previous IMPORT_TABLE statement could have been written like
# this (assuming that the /tmp/mysql-files directory contains no other .sdi files matching
# the pattern):
#
# 		IMPORT TABLE FROM '/tmp/mysql-files/*.sdi';
#
# To interpret the location of .sdi file path names, the server uses the same rules for
# IMPORT_TABLE as the server-side rules for LOAD_DATA (that is, the non-LOCAL rules)
#
# See SECTION 13.2.7, "LOAD DATA INFILE SYNTAX", paying particular attention to the
# rules used to interpret relative path names.
#
# IMPORT_TABLE fails if the .sdi or table files cannot be located.
#
# After importing a table, the server attempts to open it and reports
# as warnings any problems detected.
#
# To attempt a repair to correct any reported issues, use REPAIR_TABLE
#
# IMPORT_TABLE is not written to the binary log
#
# RESTRICTIONS AND LIMITATIONS
#
# IMPORT_TABLE applies only to non-TEMPORARY MyISAM tables.
#
# It does not apply to tables created with a transactional storage engine,
# tables created with CREATE_TEMPORARY_TABLE or views.
#
# The table data and index files must be placed in the schema directory for the
# import server prior to the import operation, unless the table as defined on the
# export server uses the DATA DIRECTORY or INDEX DIRECTORY table options.
#
# In that case, modify the import procedure using one of these alternatives
# before executing the IMPORT_TABLE statement:
#
# 		) Put the data and index files into the same directory on the import server
# 			host as on the export server host, and create symlinks in the import server
# 			schema directory to those files.
#
# 		) Put the data and index files into an import server host directory from that on the export
# 			server host, and create symlinks in the import server schema directory to those files.
#
# 			In addition, modify the .sdi file to reflect the different file locations
#
# 		) Put the data and index files into the schema directory on the import server host,
# 			and modify the .sdi file to remove the data and index directory table options.
#
# Any collation IDs stored in the .sdi file must refer to the same collations on the export
# and import servers.
#
# Trigger information for a table is not serialized into the table .sdi file, so triggers
# are not restored by the import operaiton
#
# Some edits to an .sdi file are permissible prior to executing the IMPORT_TABLE statement,
# whereas others are problematic or may even cause the import operation to fail:
#
# 		) Changing the data directory and index directory table options is required if the locations
# 			of the data and index files differ between the export and import servers.
#
# 		) Changing the schema name is required to import the table into a different schema on the import server
# 			tahn on the export server
#
# 		) Changing schema and table names may be required to accomodate differences between
# 			file system case-sensitivity semantics on the export and import servers or differences
# 			in lower_case_table_names settings.
#
# 			Changing the table names in the .sdi file may require renaming the tbale files as well
#
# 		) In some cases, changes to column definitions are permitted.
#
# 			Changing data types is likely to cause problems.
#
# 13.2.6 INSERT SYNTAX
#
# 13.2.6.1 INSERT --- SELECT SYNTAX
# 13.2.6.2 INSERT --- ON DUPLICATE KEY UPDATE SYNTAX
# 13.2.6.3 INSERT DELAYED SYNTAX
#
# 		INSERT [LOW_PRIORITY | DELAYED | HIGH_PRIORITY] [IGNORE]
# 			[INTO] tbl_name
# 			[PARTITION (partition_name [, partition_name] ---)]
# 			[(col_name [, col_name] ---)]
# 			{VALUES | VALUE} (value_list) [, (value_list)] ---
# 			[ON DUPLICATE KEY UPDATE assignment_list]
#
# 		INSERT [LOW_PRIORITY | DELAYED | HIGH_PRIORITY] [IGNORE]
# 			[INTO] tbl_name
# 			[PARTITION (partition_name [, partition_name] ---)]
# 			SET assignment_list
# 			[ON DUPLICATE KEY UPDATE assignment_list]
#
# 		INSERT [LOW_PRIORITY | HIGH_PRIORITY] [IGNORE]
# 			[INTO] tbl_name
# 			[PARTITION (partition_name [, partition_name] ---)]
# 			[(col_name [, col_name] ---)]
# 			SELECT ---
# 			[ON DUPLICATE KEY UPDATE assignment_list]
#
# 		value:
# 			{expr | DEFAULT}
#
# 		value_list:
# 			value [, value] ---
#
# 		assignment:
# 			col_name = value
#
# 		assignment_list:
# 			assignment [, assignment] ---
#
# INSERT inserts new rows into an existing table.
#
# The INSERT --- VALUES and INSERT --- SET forms of the statement insert rows based
# on explicitly specified values.
#
# The INSERT --- SELECT form inserts rows selected from another table or tables.
#
# INSERT with an ON DUPLICATE KEY UPDATE clause enables existing rows to be updated if
# a row to be inserted would cause a duplicate value in a UNIQUE index or PRIMARY KEY.
#
# For additional information about INSERT_---_SELECT and INSERT_---_ON_DUPLICATE_KEY_UPDATE,
# see SECTION 13.2.6.1, "INSERT --- SELECT SYNTAX", and SECTION 13.2.6.2, "INSERT --- ON DUPLICATE KEY UPDATE SYNTAX"
#
# In MySQL 8.0, the DELAYED keyword is accepted but ignored by the server.
#
# For the reasons for this, see SECTION 13.2.6.3, "INSERT DELAYED SYNTAX"
#
# Inserting into a table requires the INSERT privilege for the table. 
#
# If the ON DUPLICATE KEY UPDATE clause is used and a duplicate key causes an UPDATE to be performed instead, the statement
# requires the UPDATE privilege for the columns to be updated.
#
# For columns that are read but not modified you need only the SELECT privilege (such as for a column referenced only
# on the right hand side of an col_name=expr assignment in an ON DUPLICATE KEY UPDATE clause)
#
# When inserting into a partitioned table, you can control which partitions and subpartitions accept new rows.
#
# The PARTITION option takes a list of the comma-separated names of one or more partitions or subpartitions
# (or both) of the table.
#
# if any of hte rows to be inserted by a given INSERT statement do not match one of the partitions
# listed, the INSERT statement fails with the error:
#
# 		Found a row not matching the given partition set
#
# for more information and examples, see SECTION 23.5, "PARTITION SELECTION"
#
# You can use REPLACE instead of INSERT to overwrite old rows.
#
# REPLACE is the counterpart to INSERT_IGNORE in teh treatment of new rows that
# contain unique key values that duplicate old rows:
#
# 		The new rows replace the old rows rather than being discarded.
#
# 		see SECTION 13.2.9, "REPLACE SYNTAX"
#
# tbl_name is the table into which rows should be inserted.
#
# Specify the columns for which the statement provides values as follows:
#
# 		) Provide a parenthesized list of comma-separated column names following
# 			the table name.
#
# 			In this case, a value for each named column must be provided by the 
# 			VALUES list or the SELECT statement.
#
# 		) If you do not specify a list of column names for INSERT_---_VALUES or
# 			INSERT_---_SELECT, values for every column in the table must be provided
# 			by the VALUES list or the SELECT statement.
#
# 			If you do not know the order of hte columns in the table, use DESCRIBE
# 			tbl_name to find out.
#
# 		) A SET clause indicates columns explicitly by name, together with the value
# 			to assign each one.
#
# Column values can be given in several ways:
#
# 		) If strict SQL mode is not enabled, any column not explicitly given a value is set
# 			to its default (explicit or implicit) value.
#
# 			For example, if you specify a column list that does not name all the columns
# 			in the table, unnamed columns are set to their default values.
#
# 			Default value assignment is described in SECTION 11.7, "DATA TYPE DEFAULT 
# 			VALUES"
#
# 			See also SECTION 1.8.3.3, "CONSTRAINTS ON INVALID DATA"
#
# 			If strict SQL mode is enabled, an INSERT statement generates an error
# 			if it does not specify an explicit value for every column that has no default
# 			value.
#
# 			See SECTION 5.1.11, "SERVER SQL MODES"
#
# 		) If both the column list and the VALUES list are empty, INSERT creates a row
# 			with each column set to its default value:
#
# 				INSERT INTO tbl_name () VALUES();
#
# 			If strict mode is not enabled, MySQL uses the implicit default value for any
# 			column that has no explicitly defined default.
#
# 			If strict mode is enabled, an error occurs if any column has no default value.
#
# 		) Use the keyword DEFAULT to set a column explicitly to its default value.
#
# 			This makes it easier to write INSERT statements that assign values to all
# 			but a few columns, because it enables you to avoid writing an incomplete VALUES
# 			list taht does not include a value for each column in the table.
#
# 			Otherwise, you must provide the list of column names corresponding to each
# 			value in the VALUES list.
#
# 		) If a generated column is inserted into explicitly, the only permitted value is DEFAULT.
#
# 			For information about generated columns, see SECTION 13.1.20.8, "CREATE TABLE AND GENERATED COLUMNS"
#
# 		) In expressions, you can use DEFAULT(col_name) to produce the default value for column col_name.
#
# 		) Type conversions of an expression expr that provides a column value might occur if the 
# 			expression data type does not match the column data type.
#
# 			Conversion of a given value can result in different inserted values depending on the column
# 			type.
#
# 			For example, inserting the string '1999.0e-2' into an INT, FLOAT, DECIMAL(10,6) or
# 			YEAR column inserts the value 1999, 19.9921, 19.992100, or 1999, respectively.
#
# 			The value stored in the INT and YEAR columns is 1999 because the string-to-number
# 			conversion looks only at as much of the initial part of the string as may be 
# 			considered a valid integer or year.
#
# 			For the FLOAT and DECIMAL columns, the string-to-number conversion considers the
# 			entire string a valid numeric value.
#
# 		) An expression expr can refer to any column that was set earlier in a value list.
#
# 			For example, you can do this because the value for col2 refers to col1, which has
# 			previously been assigned:
#
# 				INSERT INTO tbl_name (col1,col2) VALUES(15,col1*2);
#
# 			But the following is not legal, because the value for col1 refers to col2,
# 			which is assigned after col1:
#
# 				INSERT INTO tbl_name (col1, col2) VALUES(col2*2,15);
#
# 			An exception occurs for columns that contain AUTO_INCREMENT values.
#
# 			Because AUTO_INCREMENT values are generated after other value assignments,
# 			any reference to an AUTO_INCREMENT column in the assignment returns a 0.
#
# INSERT statements that use VALUES syntax can insert multiple rows.
#
# To do this, include multiple lists of comma-separated column values, with lists
# enclosed within parentheses and separated by commas.
#
# Example:
#
# 		INSERT INTO tbl_name (a,b,c) VALUES(1,2,3),(4,5,6),(7,8,9);
#
# Each value list must contain exactly as many values as are to be inserted per row.
#
# The following statement is invalid because it contains one list of nine values,
# rather than three lists of three values each:
#
# 		INSERT INTO tbl_name (a,b,c) VALUES(1,2,3,4,5,6,7,8,9);
#
# VALUE is a synonym for VALUES in this context.
#
# Neither implies anything about hte number of values lists, nor about the number of
# values per list.
#
# Either may be used whether there is a single values list or multiple lists, and
# regardless of the number of values per list.
#
# The affected-rows value for an INSERT can be obtained using the ROW_COUNT() SQL
# function or the mysql_affected_rows() C API function.
#
# See SECTION 12.15, "INFORMATION FUNCTIONS" and SECTION 28.7.7.1, "MYSQL_AFFECTED_ROWS()"
#
# If you use an INSERT_---_VALUES statement with multiple value lists or INSERT_---_SELECT,
# the statement returns an information string in this format:
#
# 		Records: N1 Duplicates: N2 Warnings: N3
#
# If you are using the C API, the information string can be obtained by invoking the
# mysql_info() function.
#
# See SECTION 28.7.7.36, "MYSQL_INFO()"
#
# Records indicate the number of rows processed by the statement.
#
# (This is not necessarily the number of rows actually inserted because 
# Duplicates can be nonzero)
#
# Duplicates indicates the number of rows that could not be inserted because they would
# duplicate some existing unique index value.
#
# Warnings indicate the number of attempts to insert column values that were problematic
# in some way.
#
# Warnings can occur under any of the following conditions:
#
# 		) Inserting NULL into a column that has been declared NOT NULL.
#
# 			For multiple-row INSERT statements or INSERT_INTO_---_SELECT statements,
# 			the column is set to the implicit default value for the column data type.
#
# 			This is 0 for numeric types, the empty string ('') for string types, and the
# 			"zero" value for date and time types.
#
# 			INSERT INTO --- SELECT statements are handled the same way as multiple-row inserts
# 			because the server does not examine the result set from the SELECT to see whether it
# 			returns a single row.
#
# 			(For a single-row INSERT, no warning occurs when NULL is inserted into a NOT NULL column.
#
# 			Instead, the statement fails with an error)
#
# 		) Setting a numeric column to a value that lies outside the column's range.
#
# 			The value is clipped to the closest endpoint of the range.
#
# 		) Assigning a value such as '10.34 a' to a numeric column.
#
# 			The trailing nonnumeric text is stripped off and the remaining numeric part
# 			is inserted.
#
# 			If the string value has no leading numeric part, the column is set to 0.
#
# 		) Inserting a string into a string column (CHAR, VARCHAR, TEXT or BLOB) that exceeds
# 			the column's maximum length.
#
# 			The value is truncated to the column's maximum length
#
# 		) Inserting a value into a date or time column that is illegal for the data type.
#
# 			The column is set to the appropriate zero value for the type.
#
# 		) For INSERT examples involving AUTO_INCREMENT column values, see SECTION 3.6.9, "USING AUTO_INCREMENT"
#
# 			If INSERT inserts a row into a table that has an AUTO_INCREMENT column, you can find
# 			the value used for that column by using the LAST_INSERT_ID() SQL function or the
# 			mysql_insert_id() C API function.
#
# 				NOTE:
#
# 					These two functions do not always behave identically.
#
# 					The behavior of INSERT statements with respect to AUTO_INCREMENT columns is discussed
# 					further in SECTION 12.15, "INFORMATION FUNCTIONS", and SECTION 28.7.7.38, "MYSQL_INSERT_ID()"
#
# The INSERT statement supports the following modifiers:
#
# 		) if you use the LOW_PRIORITY modifier, execution of the INSERT is delayed until no other
# 			clients are reading from the table.
#
# 			This includes other clients that began reading while existing clients are reading;
# 			and while the INSERT LOW_PRIORITY statement is waiting.
#
# 			It is possible, therefore, for a client that issues an INSERT LOW_PRIORITY
# 			statement to wait or a very long time.
#
# 			LOW_PRIORITY affects only storage engines that use only table-level locking 
# 			(such as MyISAM, MEMORY, and MERGE)
#
# 			NOTE:
#
# 				LOW_PRIORITY should normally not be used with MyISAM tables because doing so
# 				disables concurrent inserts.
#
# 				See SECTION 8.11.3, "CONCURRENT INSERTS"
#
# 		) If  you specify HIGH_PRIORITY, it overrides the effect of the --low-priority-updates option if
# 			the server was started with that option.
#
# 			It also causes concurrent inserts not to be used.
#
# 			See SECTION 8.11.3, "CONCURRENT INSERTS"
#
# 			HIGH_PRIORITY affects only storage engines that use only table-level locking
# 			(such as MyISAM, MEMORY, and MERGE)
#
# 		) If you use the IGNORE modifier, errors that occur while executing the INSERT statement
# 			are ignored.
#
# 			For example, without IGNORE, a row that duplicates an existing UNIQUE index or PRIMARY KEY
# 			value in the table causes a duplicate-key error and the statement is aborted.
#
# 			With IGNORE, the row is discarded and no error occurs.
#
# 			Ignored errors generate warnings instead.
#
# 			IGNORE has a similar effect on inserts into partitioned tables where no partition matching
# 			a given value is found.
#
# 			Without IGNORE, such INSERT statements are aborted with an error.
#
# 			When INSERT_IGNORE is used, the insert operation fails silently for rows containing
# 			the unmatched value, but inserts rows that are matched.
#
# 			For an example, see SECTION 23.2.2, "LIST PARTITIONING"
#
# 			Data conversions that would trigger errors abort the statement if IGNORE is not specified.
#
# 			With IGNORE, invalid values are adjusted to the closest values and inserted;
# 			warnings are produced but the statement does not abort.
#
# 			You can determine with the mysql_info() C API function how many rows were actually
# 			inserted into the table.
#
# 			For more information, see COMPARISON OF THE IGNORE KEYWORD AND STRICT SQL MODE
#
# 		) If you specify ON DUPLICATE KEY UPDATE, and a row is inserted that would cause a duplicate value
# 			in a UNIQUE index or PRIMARY KEY, an UPDATE of the old row occurs.
#
# 			The affected-rows value per row is 1 if the row is inserted as a new row, 2 if an existing
# 			row is updated, and 0 if an existing row is set to its current values.
#
# 			If you specify the CLIENT_FOUND_ROWS flag to the mysql_real_connect() C API function
# 			when connecting to mysqld, the affected-rows value is 1 (not 0) if an existing row
# 			is set to its current values.
#
# 			See SECTION 13.2.6.2, "INSERT --- ON DUPLICATE KEY UPDATE SYNTAX"
#
# 		) INSERT_DELAYED was deprecated in MySQL 5.6, and is scheduled for eventual removal.
#
# 			In MySQL 8.0, the DELAYED modifier is accepted but ignored.
#
# 			Use INSERT (without DELAYED) instead. See SECTION 13.2.6.3, "INSERT DELAYED SYNTAX"
#
# An INSERT statement affecting a partitioned table using a storage engine such as MyISAM that
# employs table-level locks locks only those partitions into which rows are actually inserted.
#
# (For storage engines such as InnoDB that employ row-level locking, no locking of partitions takes place)
#
# For more information, see PARTITIONING AND LOCKING.
#
# 13.2.6.1 INSERT --- SELECT SYNTAX
#
# 		INSERT [LOW_PRIORITY | HIGH_PRIORITY] [IGNORE]
# 			[INTO] tbl_name
# 			[PARTITION (partition_name [, partition_name] ---)]
# 			[(col_name [, col_name] ---)]
# 			SELECT
# 			[ON DUPLICATE KEY UPDATE assignment_list]
#
# 		value:
# 			{expr | DEFAULT}
#
# 		assignment:
# 			col_name = value
#
# 		assignment_list:
# 			assignment [, assignment]
#
# With INSERT_---_SELECT, you can quickly insert many rows into a table from the result
# of a SELECT statement, which can select from one or many tables.
#
# For example:
#
# 		INSERT INTO tbl_temp2 (fld_id)
# 			SELECT tbl_temp1.fld_order_id
# 			FROM tbl_temp1 WHERE tbl_temp1.fld_order_id > 100;
#
# The following conditions hold for INSERT_---_SELECT statements:
#
# 		) Specify IGNORE to ignore rows that would cause duplicate-key violations
#
# 		) The target table of the INSERT statement may appear in the FROM clause of the
# 			SELECT part of the query.
#
# 			However, you cannot insert into a table and select from the same table in a
# 			subquery.
#
# 			When selecting from and inserting into the same table, MySQL creates an internal
# 			temporary table to hold the rows from the SELECT and then inserts those rows
# 			into the target table.
#
# 			However, you cannot use INSERT INTO t --- SELECT --- FROM t when t is a TEMPORARY
# 			table, because TEMPORARY tables cannot be referred to twice in the same statement.
#
# 			See SECTION 8.4.4, "INTERNAL TEMPORARY TABLE USE IN MYSQL", and SECTION B.6.6.2,
# 			"TEMPORARY TABLE PROBLEMS"
#
# 		) AUTO_INCREMENT columns work as usual
#
# 		) To ensure that the binary log can be used to re-create the original tables,
# 			MySQL does not permit concurrent inserts for INSERT_---_SELECT statements
# 			(see SECTION 8.11.3, "CONCURRENT INSERTS")
#
# 		) To avoid ambiguous column reference problems when the SELECT and the INSERT
# 			refer to the same table, provide a unique alias for each table used in the 
# 			SELECT part, and qualify column names in that part with the appropriate alias.
#
# You can explicitly select which partitions or subpartitions (or both) of the source or target
# table (or both) are to be used with a PARTITION option following the name of the table.
#
# When PARTITION is used with the name of the source table in the SELECT portion of the statement,
# rows are selected only from the partitions or subpartitions named in its partition list.
#
# When PARTITION is used with the name of the target table for the INSERT portion of the statement,
# it must be possible to insert all rows selected into the partitions or subpartitions named in the
# partition list following the option.
#
# Otherwise, the INSERT --- SELECT statement fails.
#
# For more information and examples, see SECTION 23.5, "PARTITION SELECTION"
#
# For INSERT_---_SELECT statements, see SECTION 13.2.6.2, "INSERT --- ON DUPLICATE KEY UPDATE SYNTAX"
# for conditions under which the SELECT columns can be referred to in an ON DUPLICATE KEY UPDATE clause.
#
# The order in which a SELECT statement with no ORDER BY clause returns rows is nondeterminsitic.
#
# This means that, when using replication, there is no guarantee that such a SELECT returns rows
# in the same order on the master and the slave, which can lead to inconsistencies between them.
#
# To prevent this from occurring, always write INSERT --- SELECT statements that are to be replicated
# using an ORDER BY clause that produces the same row order on the master and the slave.
#
# See also SECTION 17.4.1.18, "REPLICATION AND LIMIT"
#
# Due to this issue, INSERT_---_SELECT_ON_DUPLICATE_KEY_UPDATE and INSERT_IGNORE_---_SELECT
# statements are flagged as unsafe for statement-based replication.
#
# Such statements produce a warning in the error log when using statement-based mode
# and are written to the binary log using the row-based formats when using MIXED
# mode. (Bug #11758262, Bug #50439)
#
# See also SECTION 17.2.1.1, "ADVANTAGES AND DISADVANTAGES OF STATEMENT-BASED AND ROW-BASED REPLICATION"
#
# An INSERT_---_SELECT statement affecting partitioned tables using a storage engine such as
# MyISAM that employs table-level locks locks all partitions of the target table;
# However, only those partitions that are actually read from the source table are locked.
#
# (This does not occur with tables using storage engines such as InnoDB that employ
# row-level locking)
#
# For more information, see PARTITIONING AND LOCKING
#
# 13.2.6.2 INSERT --- ON DUPLICATE KEY UPDATE SYNTAX
#
# If you specify an ON DUPLICATE KEY UPDATE clause and a row to be inserted would cause a 
# duplicate value in a UNIQUE index or PRIMARY KEY, an UPDATE of the old row occurs.
#
# For example, if column a is declared as UNIQUE and contains the value 1, the following
# two statements have similar effect:
#
# 		INSERT INTO t1 (a,b,c) VALUES (1,2,3)
# 			ON DUPLICATE KEY UPDATE c=c+1;
#
# 		UPDATE t1 SET c=c+1 WHERE a=1;
#
# (The effects are not identical for an InnoDB table where a is an auto-increment column.
#
# With an auto-increment column, an INSERT statement increases the auto-increment
# value but UPDATE does not)
#
# If column b is also unique, the INSERT is equivalent to this UPDATE statement instead:
#
# 		UPDATE t1 SET c=c+1 WHERE a=1 OR b=2 LIMIT 1;
#
# If a=1 OR b=2 matches several rows, only one row is updated.
#
# In general, you should try to avoid using an ON DUPLICATE KEY UPDATE clause
# on tables with multiple unique indexes.
#
# With ON DUPLICATE KEY UPDATE, the affected-rows value per row is 1 if the row
# is inserted as a new row, 2 if an existing row is updated, and 0 if an existing
# row is set to its current values.
#
# If you specify the CLIENT_FOUND_ROWS flag to the mysql_real_connect() C API function
# when connecting to mysqld, the affected-rows value is 1 (not 0) if an existing
# row is set to its current values.
#
# If a table contains an AUTO_INCREMENT column and INSERT_---_ON_DUPLICATE_KEY_UPDATE inserts
# or updates a row, the LAST_INSERT_ID() function returns the AUTO_INCREMENT value.
#
# The ON DUPLICATE KEY UPDATE clause can contain multiple column assignments,
# separated by commas.
#
# In assignment value expressions in the ON DUPLICATE KEY UPDATE clause, you can use
# the VALUES(col name) function to refer to column values from the INSERT portion
# of the INSERT_---_ON_DUPLICATE_KEY_UPDATE statement.
#
# In other words, VALUES(col_name) in the ON DUPLICATE KEY UPDATE clause refers
# to the value of col_name that would be inserted, had no duplicate-key conflict
# occurred.
#
# This function is especially useful in multiple-row inserts.
#
# The VALUES() function is meaningful only in the ON DUPLICATE KEY UPDATE clause
# or INSERT statements and returns NULL otherwise. Example:
#
# 		INSERT INTO t1 (a,b,c) VALUES (1,2,3),(4,5,6)
# 			ON DUPLICATE KEY UPDATE c=VALUES(a)+VALUES(b);
#
# That statement is identical to the following two statements:
#
# 		INSERT INTO t1 (a,b,c) VALUES (1,2,3)
# 			ON DUPLICATE KEY UPDATE c=3;
#
# 		INSERT INTO t1 (a,b,c) VALUES (4,5,6)
# 			ON DUPLICATE KEY UPDATE c=9;
#
# For INSERT_---_SELECT statements, these rules apply regarding acceptable forms
# of SELECT query expressions that you can refer to in an ON DUPLICATE KEY UPDATE
# clause:
#
# 		) References to columns from queries on a single table, which may be a derived table
#
# 		) References to columns from queries on a join over multiple tables
#
# 		) References to columns from DISTINCT queries
#
# 		) References to columns in other tables, as long as the SELECT does not use GROUP BY.
#
# 			One side effect is that you must qualify references to nonunique column names.
#
# References to columns from a UNION are not supported.
#
# To work around this restriction, rewrite the UNION as a derived table so that its rows
# can be treated as a single-table result set.
#
# For example, this statement produces an error:
#
# 		INSERT INTO t1 (a, b)
# 			SELECT c, d FROM t2
# 			UNION
# 			SELECT e, f FROM t3
# 		ON DUPLICATE KEY UPDATE b = b + c;
#
# Instead, use an equivalent statement that rewrites the UNION as a derived table:
#
# 		INSERT INTO t1 (a, b)
# 		SELECT * FROM
# 			(SELECT c, d FROM t2
# 			UNION
# 			SELECT e, f FROM t3) AS dt
# 		ON DUPLICATE KEY UPDATE b = b + c;
#
# The technique of rewriting a query as a derived table also enables references
# to columns from GROUP BY queries.
#
# Because the results of INSERT_---_SELECT statements depend on the ordering of rows
# from the SELECT and this order cannot always be guaranteed, it is possible when
# logging INSERT_---_SELECT_ON_DUPLICATE_KEY_UPDATE statements for the master
# and the slave to diverge.
#
# Thus, INSERT_---_SELECT_ON_DUPLICATE_KEY_UPDATE statements are flagged as unsafe
# for statement-based replication.
#
# Such statements produce a warning in the error log when using statement-based mode
# and are written to the binary log using the row-based format when using MIXED mode.
#
# An INSERT_---_ON_DUPLICATE_KEY_UPDATE statement against a table having more than
# one unique or primary key is also marked as unsafe. (Bug #117655650, Bug #58637)
#
# See also SECTION 17.2.1.1, "ADAVANTAGES AND DISADVANTAGES OF STATEMENT-BASED AND 
# ROW-BASED REPLICATION"
#
# An INSERT --- ON DUPLICATE KEY UPDATE on a partitioned table using a storage engine
# such as MyISAM that employs table-level locks locks any partitions of the table
# in which a partitioning key column is updated.
#
# (This does not occur with tables using storage engines such as InnoDB that employ
# row-level locking)
#
# For more information, see PARTITIONING AND LOCKING
#
# 13.2.6.3 INSERT DELAYED SYNTAX
#
# INSERT DELAYED ---
#
# The DELAYED option for the INSERT statement is a MySQL extension to standard SQL
#
# In previous versions of MySQL, it can be used for certain kinds of tables (such as
# MyISAM), such that when a client uses INSERT_DELAYED, it gets an okay from the server
# at once, and the row is queued to be inserted when the table is not in use by any
# other thread.
#
# DELAYED inserts and replaces were deprecated in MySQL 5.6
#
# In MySQL 8.0, DELAYED is not supported.
#
# The server recognizes but ignores the DELAYED keyword, handles the insert
# as a nondelayed insert, and generates an ER_WARN_LEGACY_SYNTAX_CONVERTED
# warning ("INSERT DELAYED is no longer supported. The statement was converted to INSERT")
#
# The DELAYED keyword is scheduled for removal in a future release.
#
# 13.2.7 LOAD DATA INFILE SYNTAX
#
# 		LOAD DATA [LOW_PRIORITY | CONCURRENT] [LOCAL] INFILE 'file_name'
# 			[REPLACE | IGNORE]
# 			INTO TABLE tbl_name
# 			[PARTITION (partition_name [, partition_name] ---)]
# 			[CHARACTER SET charset_name]
# 			[{FIELDS | COLUMNS}
# 				[TERMINATED BY 'string']
# 				[[OPTIONALLY] ENCLOSED BY 'char']
# 				[ESCAPED BY 'char']
# 			]
# 			[LINES
# 				[STARTING BY 'string']
# 				[TERMINATED BY 'string']
# 			]
# 			[IGNORE number {LINES | ROWS}]
# 			[(col_name_or_user_var
# 				[,col_name_or_user_var] ---)]
# 			[SET col_name={expr | DEFAULT},
# 				[, col_name={expr | DEFAULT}] ---]
#
# The LOAD_DATA_INFILE statement reads rows from a text file into a table at 
# a very high speed.
#
# LOAD_DATA_INFILE is the complement of SELECT_---_INTO_OUTFILE 
#
# (See SECTION 13.2.10.1, "SELECT --- INTO SYNTAX")
#
# To write data from a table to a file, use SELECT_---_INTO_OUTFILE
#
# To read the file back into a table, use LOAD_DATA_INFILE 
#
# The syntax of the FIELDS and LINES clauses is the same for both statements
#
# You can also load data files by using the mysqlimport utility;
#
# See SECTION 4.5.5, "MYSQLIMPORT -- A DATA IMPORT PROGRAM"
#
# mysqlimport operates by sending a LOAD_DATA_INFILE statement to the server.
#
# The --local option causes mysqlimport to read data files from the client host.
#
# You can specify the --compress option to get better performance over slow
# networks if the client and server support the compressed protocol.
#
# For more information about the efficiency of INSERT versus LOAD_DATA_INFILE and
# speeding up LOAD_DATA_INFILE, see SECTION 8.2.5.1, "OPTIMIZING INSERT STATEMENTS"
#
# 		) PARTITIONED TABLE SUPPORT
#
# 		) INPUT FILE NAME, LOCATION, AND CONTENT INTERPRETATION
#
# 		) CONCURRENCY CONSIDERATIONS
#
# 		) DUPLICATE-KEY HANDLING
#
# 		) INDEX HANDLING
#
# 		) FIELD AND LINE HANDLING
#
# 		) COLUMN LIST SPECIFICATION
#
# 		) INPUT PREPROCESSING
#
# 		) STATEMENT RESULT INFORMATION
#
# 		) MISCELLANEOUS TOPICS
#
# PARTITIONED TABLE SUPPORT
#
# LOAD DATA supports explicit partition selection using the PARTITION option with
# a list of one or more comma-separated names of partitions, subpartitions, or both.
#
# When this option is used, if any rows from the file cannot be inserted into any of
# the partitions or subpartitions named in the list, the statement fails with the
# error:
#
# 		Found a row not matching the given partition set
#
# For more information and examples, see SECTION 23.5, "PARTITION SELECTION"
#
# For partitioned tables using storage engines taht employ table locks, such as
# MyISAM, LOAD DATA cannot prune any partition locks.
#
# This does not apply to tables using storage engines which employ row-level
# locking, such as InnoDB.
#
# For more information, see PARTITIONING AND LOCKING.
#
# INPUT FILE NAME, LOCATION AND CONTENT INTERPRETATION
#
# The file name must be given as a literal string.
#
# On Windows, specify backslashes in path names as forward slashes or doubled
# backslashes.
#
# The character_set_filesystem system variable controls the interpretation of
# the file name character set.
#
# The server uses the character set indicated by the character_set_database system
# variable to interpret the information in the file.
#
# SET_NAMES and the setting of character_set_client do not affect interpretation of
# input.
#
# If the contents of the input file use a character set that differs from teh default,
# it is usually preferable to specify the character set of the files by using the
# CHARACTER SET clause.
#
# A character set of binary specifies "no conversion"
#
# LOAD_DATA_INFILE interprets all fields in the file as having the same character set,
# regardless of hte data types of the columns into which field values are loaded.
#
# For proper interpretation of file contents, you must ensure that it was written
# with the correct character set.
#
# For example, if you write a data file with mysqldump -T or by issuing
# a SELECT_---_INTO_OUTFILE statement in Mysql, be sure to use a --default-character-set
# option so that output is written in the character set to be used when the file
# is loaded with LOAD_DATA_INFILE.
#
# 	NOTE:
#
# 		It is not possible to load data files that use the ucs2, utf16, utf16le, or utf32 character set.
#
# CONCURRENCY CONSIDERATIONS
#
# If you use LOW_PRIORITY, execution of the LOAD_DATA statement is delayed until no other clients
# are reading from the table.
#
# This affects only storage engines that use only table-level locking (such as MyISAM, MEMORY and MERGE)
#
# If you specify CONCURRENT with a MyISAM table that satisfies the condition for the concurrent
# inserts (that is, it contains no free blocks in the middle), other threads can retrieve
# data from the table while LOAD_DATA is executing.
#
# This option affects the performance of LOAD_DATA a bit, even if no other thread
# is using the table at the same time.
#
# With row-based replication, CONCURRENT is replicated regardless of MySQL version.
#
# With statement-based replication CONCURRENT is not replicated prior to MySQL 5.5.1
# (see Bug #34628)
#
# For more information, see SECTION 17.4.1.19,, "REPLICATION AND LOAD DATA INFILE"
#
# The LOCAL keyword affects expected location of the file and error handling, as described
# later.
#
# LOCAL works only if your server and your client both have been configured to permit it.
#
# For example, if mysqld was started with the local_infile system variable disabled,
# LOCAL does not work.
#
# See SECTION 6.1.6, "SECURITY ISSUES WITH LOAD DATA LOCAL"
#
# The LOCAL keyword affects where the file is expected to be found:
#
# 		) If LOCAL is specified, the file is read by the client program on the client
# 			host and sent to the server.
#
# 			The file can be given as a full path name to specify its exact location.
#
# 			If given as a relative path name, the name is interpreted relative to the
# 			directory in which the client program was started.
#
# 			When using LOCAL with LOAD_DATA, a copy of the file is created in the directory
# 			where the MySQL server stores temporary files.
#
# 			See SECTION B.6.3.5, "WHERE MySQL STORES TEMPORARY FILES"
#
# 			Lack of sufficient space for the copy in this directory can cause the LOAD_DATA_LOCAL
# 			statement to fail.
#
# 		) If LOCAL is not specified, the file must be located on the server host and is read directly
# 			by the server.
#
# 			The server uses the following rules to locate the file:
#
# 				) If the file name is an absolute path name, the server uses it as given
#
# 				) If the file name is a relative path name with one or more leading components,
# 					the server searches for the file relative to the server's data directory.
#
# 				) If a file name with no leading components is given, the server looks for the file
# 					in the database directory of the default database.
#
# In the non-LOCAL case, these rules mean that a file named as ./myfile.txt is read from the
# server's data directory, whereas the file named as myfile.txt is read from the database
# directory of the default database.
#
# For example, if db1 is the default database, the following LOAD_DATA statement reads the
# file data.txt from the database directory for db1, even though the statement explicitly
# loads the file into a table in the db2 database:
#
# 		LOAD DATA INFILE 'data.txt' INTO TABLE db2.my_table;
#
# NOTE:
#
# 		The server also uses the non-LOCAL rules to locate .sdi files for the IMPORT_TABLE statement
#
# Non-LOCAL load operations read text files located on the server.
#
# For security reasons, such operations require that you have the FILE privilege.
#
# See SECTION 6.2.1, "PRIVILEGES PROVIDED BY MYSQL"
#
# Also, non-LOCAL load operations are subject to the secure_file_priv system variable
# setting.
#
# If the variable value is a nonempty directory name, the file to be loaded must be located
# in that directory.
#
# If the variable value is empty (which is insecure), the file need only be readable by the server.
#
# Using LOCAL is a bit slower than letting the server access the files directly, because the contents
# of the file must be sent over the connection by the client to the server.
#
# On the other hand, you do not need the FILE privilege to load local files.
#
# LOCAL also affects error handling:
#
# 		) With LOAD_DATA_INFILE, data-interpretation and duplicate-key errors terminate the operation
#
# 		) With LOAD_DATA_LOCAL_INFILE, data-interpretation and duplicate-key errors become warnings and 
# 			the operation continues because the server has no way to stop transmission of the file
# 			in the middle of the operation.
#
# 			For duplicate-key errors, this is the same as if IGNORE is specified.
#
# 			IGNORE is explained further later in this section.
#
# DUPLICATE-KEY HANDLING
#
# The REPLACE and IGNORE keywords control handling of input rows that duplicate existing rows
# on unique key values:
#
# 		) If you specify REPLACE, input rows replace existing rows.
#
# 			In other words, rows that have the same value for a primary key or unique index
# 			as an existing row.
#
# 			See SECTION 13.2.9, "REPLACE SYNTAX"
#
# 		) If you specify IGNORE, rows that duplicate an existing row on a unique key value
# 			are discarded.
#
# 			For more information, see COMPARISON OF THE IGNORE KEYWORD AND STRICT SQL MODE
#
# 		) If you do not specify either option, the behavior depends on whether the LOCAL keyword
# 			is specified.
#
# 			Without LOCAL, an error occurs when a duplicate key value is found, and the rest of
# 			the text file is ignored.
#
# 			With LOCAL, the default behavior is the same as if IGNORE is specified;
#
# 			This is because the server has no way to stop transmission of the file in the
# 			middle of the operation.
#
# INDEX HANDLING
#
# To ignore foreign key constraints during the load operation, issue a SET foreign_key_checks = 0
# statement before executing LOAD_DATA
#
# If you use LOAD_DATA_INFILE on an empty MyISAM table, all nonunique indexes are created
# in a separate batch (as for REPAIR TABLE)
#
# Normally, this makes LOAD_DATA INFILE much faster when you have many indexes.
#
# In some extreme cases, you can create the indexes even faster by turning them
# off with ALTER TABLE --- DISABLE KEYS before loading the file into the table
# and using ALTER TABLE --- ENABLE KEYS to re-create the indexes after loading
# the file.
#
# See SECTION 8.2.5.1, "OPTIMIZING INSERT STATEMENTS"
#
# FILE AND LINE HANDLING
#
# For both the LOAD_DATA_INFILE and SELECT_---_INTO_OUTFILE statements, the syntax
# of the FIELDS and LINES clauses is the same.
#
# Both clauses are optional, but FIELDS must precede LINES if both are specified.
#
# If you specify a FIELDS clause, each of its subclauses (TERMINATED BY, [OPTIONALLY]
# ENCLOSED BY, and ESCAPED BY) is also optional, except that you must specify
# at least one of them. 
# 
# Arguments to these clauses are permitted to contain only ASCII characters.
#
# If you specify no FIELDS or LINES clause, the defaults are the same as if you had
# written this:
#
# 		FIELDS TERMINATED BY '\t' ENCLOSED BY '' ESCAPED BY '\\'
# 		LINES TERMINATED BY '\n' STARTING BY ''
#
# (Backslash is the MySQL escape character within strings in SQL statements, so to specify
# a literal backslash, you must specify two backslashes for the value to be interpreted
# as a single backslash.
#
# The escape sequences '\t' and '\n' specify tab and newline characters, respectively.)
#
# In other words, the default cause LOAD_DATA_INFILE to act as follows when reading input:
#
# 		) Look for line boundaries at newlines
#
# 		) Do not skip over any line prefix
#
# 		) Break lines into fields at tabs
#
# 		) Do not expect fields to be enclosed within any quoting characters
#
# 		) Interpret characters preceded by the escape character \ as escape sequences.
#
# 			For example, \t, \n, and \\ signify tab, newline, and backslash, respectively.
#
# 			See the discussion of FIELDS ESCAPED BY later for the full list of escape sequences.
#
# Conversely, the defaults cause SELECT_---_INTO_OUTFILE to act as follows when writing output:
#
# 		) Write tabs between fields
#
# 		) Do not enclose fields within any quoting characters
#
# 		) Use \ to escape instances of tab, newline, or \ that occur within field values.
#
# 		) Write newlines at the ends of lines.
#
# NOTE:
#
# 		If you have generated the text file on a Windows system, you might have to use LINES TERMINATED
# 		BY '\r\n' to read the file properly because Windows programs typically use two characters
# 		as a line terminator.
#
# 		Some programs, such as WordPad, might use \r as a line terminator when writing files
#
# 		To read such files, use LINES TERMINATED BY '\r'
#
# If all the lines you want to read in have a common prefix that you want to ignore, you can use
# LINES STARTING BY 'prefix_string' to skip over the prefix, and anything before it.
#
# If a line does not include the prefix, the entire line is skipped.
#
# Suppose that you issue the following statement:
#
# 		LOAD DATA INFILE '/tmp/test.txt' INTO TABLE test
# 			FIELDS TERMINATED BY ',' LINES STARTING BY 'xxx';
#
# If the data file looks like this:
#
# 		xxx"abc",1
# 		something xxx"def",2
# 		"ghi",3
#
# The resulting rows will be ("abc",1) and ("def",2)
#
# The third row in the file is skipped because it does not contain the prefix.
#
# The IGNORE number LINES option can be used to ignore lines at the start of the file.
#
# For example, you can use IGNORE 1 LINES to skip over an initial header line containing
# column names:
#
# 		LOAD DATA INFILE '/tmp/test.txt' INTO TABLE test IGNORE 1 LINES;
#
# When you use SELECT_---_INTO_OUTFILE in tandem with LOAD_DATA_INFILE to write data
# from a database into a file and then read the file back into the database later,
# the field- and line-handling options for both statements must match.
#
# Otherwise, LOAD_DATA_INFILE will not interpret the contents of the file properly.
#
# Suppose that you use SELECT_---_INTO_OUTFILE to write a file with fields delimited
# by commas:
#
# 		SELECT * INTO OUTFILE 'data.txt'
# 			FIELDS TERMINATED BY ','
# 			FROM table2;
#
# To read the comma-delimited file back in, the correct statement would be:
#
# 		LOAD DATA INFILE 'data.txt' INTO TABLE table2
# 			FIELDS TERMINATED BY ',';
#
# If instead you tried to read in the file with the statement shown following,
# it would not work because it instructs LOAD_DATA_INFILE to look for tabs
# between fields:
#
# 		LOAD DATA INFILE 'data.txt' INTO TABLE table2
# 			FIELDS TERMINATED BY '\t';
#
# The likely result is that each input line would be interpreted as a single field.
#
# LOAD_DATA_INFILE can be used to read files obtained from external sources.
#
# For example, many programs can export data in comma-separated values (CSV) format,
# such that lines have fields separated by commas and enclosed within double quotation
# marks, with an initial line of column names.
#
# If the lines in such a file are terminated by carriage return/newline pairs,
# the statement shown here illustrates the field- and line-handling options
# you would use to load the file:
#
# 		LOAD DATA INFILE 'data.txt' INTO TABLE tbl_name
# 			FIELDS TERMINATED BY ',' ENCLOSED BY '"'
# 			LINES TERMINATED BY '\r\n'
# 			IGNORE 1 LINES;
#
# If the input values are not necessarily enclosed within quotation marks,
# use OPTIONALLY before the ENCLOSED BY keywords.
#
# Any of the field- or line-handling options can specify an empty string
# ('')
#
# If not empty, the FIELDS [OPTIONALLY] ENCLOSED BY and FIELDS ESCAPED BY
# values must be a single character.
#
# The FIELDS TERMINATED BY, LINES STARTING BY, and LINES TERMINATED BY values
# can be more than one character.
#
# For example, to write lines that are terminated by carriage return/linefeed
# pairs, or to read a file containing such lines, specify a LINES TERMINATED BY '\r\n'
# clause.
#
# To read a file containing jokes that are separated by lines consisting of 
# %%, you can do this:
#
# 		CREATE TABLE jokes
# 			(a INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
# 			joke TEXT NOT NULL);
# 		LOAD DATA INFILE '/tmp/jokes.txt' INTO TABLE jokes
# 			FIELDS TERMINATED BY ''
# 			LINES TERMINATED BY '\n%%\n' (joke);
#
# FIELDS [OPTIONALLY] ENCLOSED BY controls quoting of fields.
#
# For output (SELECT --- INTO OUTFILE), if you omit the word OPTIONALLY,
# all fields are enclosed by the ENCLOSED BY character.
#
# An example of such output (using a comma as the field delimiter)
# is shown here:
#
# 		"1","a string","100.20"
# 		"2","a string containing a , comma","102.20"
# 		"3","a string containing a \" quote","102.20"
# 		"4","a string containing a \", quote and comma","102.20"
#
# If you specify OPTIONALLY, the ENCLOSED BY character is used only to enclose values
# from columns that have a string data type (such as CHAR, BINARY, TEXT or ENUM):
#
# 		1,"a string",100.20
# 		2,"a string containing a , comma",102.20
# 		3,"a string containing a \" quote",102.20
# 		4,"a string containing a \", quote and comma",102.20
#
# Occurrences of the ENCLOSED BY character within a field value are escaped by
# prefixing them with the ESCAPED BY character.
#
# Also, if you specify an empty ESCAPED BY value, it is possible to inadvertedly
# generate output that cannot be read properly by LOAD_DATA_INFILE
#
# For example, the preceding output just shown would appear as follows
# if the escape character is empty.
#
# Observe that the second field in the fourth line contains a comma following
# the quote, which (errorneously) appears to terminate the field:
#
# 		1,"a string",100.20
# 		2,"a string containing a , comma",102.20
# 		3,"a string containing a " quote",102.20
# 		4,"a string containing a ", quote and comma",102.20
#
# For input, the ENCLOSED BY character, if present, is stripped from the ends of field values.
#
# (This is true regardless of whether OPTIONALLY is specified; OPTIONALLY has no effect on
# input interpretation)
#
# Occurrences of the ENCLOSED BY character preceded by the ESCAPED BY character
# are interpreted as part of the current field value.
#
# If the field begins with the ENCLOSED BY character, instances of that character
# are recognized as terminating a field value only if followed by the field or
# line TERMINATED BY sequence.
#
# To avoid ambiguity, occurrences of the ENCLOSED BY character within a field value
# can be doubled and are interpreted as a single instance of the character.
#
# For example, if ENCLOSED BY '"' is specified, quotation marks are handled as shown here:
#
# 		"The ""BIG"" boss" 	-> The "BIG" boss
# 		
# 		The "BIG" boss 		-> The "BIG" boss
#
# 		The ""BIG"" boss 		-> The ""BIG"" boss
#
# FIELDS ESCAPED BY controls how to read or write special characters:
#
# 		) For input, if the FIELDS ESCAPED BY character is not empty, occurences
# 			of that character are stripped and the following character is taken
# 			literally as part of a field value.
#
# 			Some two-character sequences that are exceptions, where the first character
# 			is the escape character.
#
# 			These sequences are shown in the following table (using \ for the escape character)
#
# 			The rules for NULL handling are described later in this section.
#
# 				CHARACTER 	ESCAPE SEQUENCE
# 
#  			\0 			An ASCII NUL (X'00') character
#
# 				\b 			a backspace character
#
# 				\n 			A newline (linefeed) character
#
# 				\r 			A carriage return character
#
# 				\t 			A tab character
#
# 				\Z 			ASCII 26 (control+Z)
#
# 				\N 			NULL
#
# 			For more information about \-escape syntax, see SECTION 9.1.1, "STRING LITERALS"
#
# 			If the FIELDS ESCAPED BY character is empty, escape-sequence interpretation does not occur.
#
# 		) For output, if the FIELDS ESCAPED BY character is not empty, it is used to prefix the following
# 			characters on output:
#
# 			) The FIELDS ESCAPED BY character
#
# 			) The FIELDS [OPTIONALLY] ENCLOSED BY character
#
# 			) The first character of the FIELDS TERMINATED BY and LINES TERMINATED BY values,
# 				if the ENCLOSED BY character is empty or unspecified.
#
# 			) ASCII 0 (what is actually written following the escape character is ASCII 0, not a zero-valued byte)
#
# 			If the FIELDS ESCAPED BY character is empty, no characters are escaped and NULL is output as NULL,
# 			not \N
#
# 			It is probably not a good idea to specify an empty escape character, particularly if field values
# 			in your data contain any of the characters in the list just given.
#
# In certain cases, field- and line-handling options interact:
#
# 		) If LINES TERMINATED BY is an empty string and FIELDS TERMINATED BY is nonempty, lines are also
# 			terminated with FIELDS TERMINATED BY
#
# 		) If the FIELDS TERMINATED BY and FIELDS ENCLOSED BY values are both empty (''), a fixed-row
# 			(nondelimited) format is used.
#
# 			With fixed-row format, no delimiters are used between fields (but you can still have a line terminator)
#
# 			Instead, column values are read and written using a field width wide enough to hold all values
# 			in the field.
#
# 			For TINYINT, SMALLINT, MEDIUMINT, INT, and BIGINT, the field widths are 4, 6, 8, 11 and 20, respectively,
# 			no matter what the declared display width is.
#
# 			LINES TERMINATED BY is still used to separate lines.
#
# 			If a line does not contain all fields, the rest of the columns are set to their
# 			default values.
#
# 			If you do not have a line terminator, you should set this to ''
#
# 			In this case, the text file must contain all fields for each row.
#
# 			Fixed row format also affects handling of NULL values, as described later.
#
# 			NOTE:
#
# 				Fixed-size format does not work if you are using a multibyte character set
#
# Handling of NULL values varies according to the FIELDS and LINES options in use:
#
# 		) For the default FIELDS and LINES values, NULL is written as a field value of 
# 			\N for output, and a field value of \N is read as NULL for input
#
# 			(Assuming that the ESCAPED BY character is \)
#
# 		) If FIELDS ENCLOSED BY is not empty, a field containing the literal word NULL
# 			as its value is read as a NULL value.
#
# 			This differs from the word NULL enclosed within FIELDS ENCLOSED BY characters,
# 			which is read as the string 'NULL'
#
# 		) If FIELDS ESCAPED BY is empty, NULL is written as the word NULL
#
# 		) With fixed-row format (which is used when FIELDS TERMINATED BY and FIELDS ENCLOSED BY
# 			are both empty), NULL is written as a empty string.
#
# 			This causes both NULL values and empty strings in the table to be indistinguishable when
# 			written to the file because both are written as empty strings.
#
# 			If you need to be able to tell the two apart when reading the file back in,
# 			you should not use fixed-row format.
#
# An attempt to load NULL into a NOT NULL column causes assignment of the implicit default value
# for the column's data type and a warning, or an error in strict SQL mode.
#
# Implicit default values are discussed in SECTION 11.7, "DATA TYPE DEFAULT VALUES"
#
# Some cases are not supported by LOAD_DATA_INFILE:
#
# 		) Fixed-size rows (FIELDS TERMINATED BY and FIELDS ENCLOSED BY both empty) and BLOB or TEXT columns.
#
# 		) If you specify one separator that is the same as or a prefix of another, LOAD_DATA_INFILE cannot
# 			interpret the input properly.
#
# 			For example, the following FIELDS clause would cause problems:
#
# 				FIELDS TERMINATED BY '"' ENCLOSED BY '"'
#
# 		) If FIELDS ESCAPED BY is empty, a field value that contains an occurrence of FIELDS
# 			ENCLOSED BY or LINES TERMINATED BY followed by the FIELDS TERMINATED BY value
# 			causes LOAD_DATA_INFILE to stop reading a field or line too early.
#
# 			This happens because LOAD_DATA_INFILE cannot properly determine whether the field
# 			or line value ends.
#
# COLUMN LIST SPECIFICATION
#
# The following example loads all columns of the persondata table:
#
# 		LOAD DATA INFILE 'persondata.txt' INTO TABLE persondata;
#
# By default, when no column list is provided at the end of the LOAD_DATA_INFILE
# statement, input lines are expected to contain a field for each table column.
#
# If you want to load only some of a table's columns, specify a column list:
#
# 		LOAD DATA INFILE 'persondata.txt' INTO TABLE persondata
# 		(col_name_or_user_var [, col_name_or_user_var] ---);
#
# You must also specify a column list if the order of the fields in the input file
# differs from the order of the columns in the table.
#
# Otherwise, MySQL cannot tell how to match input fields with table columns.
#
# INPUT PREPROCESSING
#
# Each col_name_or_user_var value is either a column or a user variable.
#
# With user variables, the SET clause enables you to perform preprocessing
# transformations on their values before assigning the result to columns.
#
# User variables in the SET clause can be used in several ways.
#
# The following example uses the first input column directly for the value
# of t1.column1, and assigns the second input column to a user variable
# that is subjected to a division operation beore being used for the
# value of t1.column2:
#
# 		LOAD DATA INFILE 'file.txt'
# 			INTO TABLE t1
# 			(column1, @var1)
# 			SET column2 = @var1/100;
#
# The SET clause can be used to supply values not derived from the input file.
#
# The following statement sets column3 to the current date and time:
#
# 		LOAD DATA INFILE 'file.txt'
# 			INTO TABLE t1
# 			(column1, column2)
# 			SET column3 = CURRENT_TIMESTAMP;
#
# You can also discard an input value by assigning it to a user variable
# and not assigning the variable to a table column:
#
# 		LOAD DATA INFILE 'file.txt'
# 			INTO TABLE t1
# 			(column1, @dummy, column2, @dummy, column3);
#
# Use of the column/variable list and SET clause is subject to the following restrictions:
#
# 		) Assignments in the SET clause should have only column names on the left hand side of assignment operators
#
# 		) You can use subqueries in the right hand side of SET assignments.
#
# 			A subquery that returns a value to be assigned to a column may be a scalar subquery only.
#
# 			ALso, you cannot use a subquery to select from the table that is being loaded.
#
# 		) Lines ignored by an IGNORE clause are not processed for the column/variable list or SET clause
#
# 		) User variables cannot be used when loading data with fixed-row format because user variables do
# 			not have a display width
#
# When processing an input line, LOAD_DATA splits it into fields and uses the values according to
# the column/variable list and the SET clause, if they are present.
#
# Then the resulting row is inserted into the table. If there are BEFORE INSERT or AFTER INSERT
# triggers for the table, they are activated before or after inserting the row, respectively.
#
# If an input line has too many fields, the extra fields are ignored and the number of warnings
# is incremented.
#
# If an input line has too few fields, the table columns for which input fields are missing are set
# to their default values.
#
# Default value assignment is described in SECTION 11.7, "DATA TYPE DEFAULT VALUES"
#
# An empty field value is interpreted different from a missing field:
#
# 		) For string types, the column is set to the empty string
#
# 		) For numeric types, the column is set to 0
#
# 		) For date and time types, the column is set to the appropriate "zero" value for the type.
#
# 			See SECTION 11.3, "DATE AND TIME TYPES"
#
# These are the same values that result if you assign an empty string explicitly to a string,
# numeric, or date or time type explicitly in an INSERT or UPDATE statement.
#
# Treatment of empty or incorrect field values differs from that just described if the SQL
# mode is set to a restrictive value.
#
# For example, if sql_mode is set to TRADITIONAL, conversion of an empty value or a value
# such as 'x' for a numeric column results in an error, not conversion to 0.
#
# (With LOCAL or IGNORE, warnings occur rather than errors, even with a restrictive sql_mode value,
# and the row is inserted using the same closest-value behavior used for nonrestrictive
# SQL modes.
#
# This occurs because the server has no way to stop transmission of the file in the middle of the operation)
#
# TIMESTAMP columns are set to the current date and time only if there is a NULL value for the
# column (that is, \N) and the column is not declared to permit NULL values, or if the TIMESTAMP
# column's default value is the current timestamp and it is omitted from the field list when a 
# field list is specified.
#
# LOAD_DATA_INFILE regards all input as strings, so you cannot use numeric values for ENUM or
# SET columns the way you can with INSERT statements.
#
# All ENUM and SET values must be specified as strings.
#
# BIT values cannot be loaded directly using binary notation (for example, b'011010')
#
# To work around this, use the SET clause to strip off the leading b' and trailing '
# and perform a base-2 to base-10 conversion so that MySQL loads the values into the
# BIT column properly:
#
# 		cat /tmp/bit_test.txt
# 		b'10
# 		b'1111111'
# 		mysql test
# 		LOAD DATA INFILE '/tmp/bit_test.txt'
# 		INTO TABLE bit_test (@var1)
# 		SET b = CAST(CONV(MID(@var1, 3, LENGTH(@var1)-3), 2, 10) AS UNSIGNED);
# 		Query OK, 2 rows affected (0.00 sec)
# 		Records: 2 Deleted: 0 Skipped: 0 Warnings: 0
#
# 		SELECT BIN(b+0) FROM bit_test;
# 		+------------+
# 		| BIN(b+0)   |
# 		+------------+
# 		| 10 		    |
# 		| 1111111    |
# 		+------------+
# 		2 rows in set (0.00 sec)
#
# For BIT values in 0b binary notation (for example, 0b011010), use this SET
# clause instead to strip off the leading 0b:
#
# 		SET b = CAST(CONV(MID(@var1, 3, LENGTH(@var1)-2), 2, 10) AS UNSIGNED)
#
# STATEMENT RESULT INFORMATION
#
# When the LOAD_DATA_INFILE statement finishes, it returns an information string in the
# following format:
#
# 		Records: 1 Deleted: 0 Skipped: 0 Warnings: 0
#
# Warnings occur under the same circumstances as when values are inserted using the INSERT
# statement (see SECTION 13.2.6, "INSERT SYNTAX"), except that LOAD_DATA_INFILE also generates
# warnings when there are too few or too many fields in the input row.
#
# You can use SHOW_WARNINGS to get a list of the first max_error_count warnings as information
# about what went wrong.
#
# See SECTION 13.7.6.40, "SHOW WARNINGS SYNTAX"
#
# If you are using the C API, you can get information about the statement by calling the
# mysql_info() function.
#
# See SECTION 28.7.7.36, "MYSQL_INFO()"
#
# MISCELLANEOUS TOPICS
#
# On Unix, if you need LOAD_DATA to read from a pipe, you can use the following technique 
# (the example loads a listing of the / directory into the table db1.t1):
#
# 		mkfifo /mysql/data/db1/ls.dat
# 		chmod 666 /mysql/data/db1/ls.dat
# 		find / -ls > /mysql/data/db1/ls.dat &
# 		mysql -e "LOAD DATA INFILE 'ls.dat' INTO TABLE t1" db1
#
# Here you must run the command that generates the data to be loaded and the mysql commands
# either on separate terminals, or run the data generation process in the background
# (as shown in the preceding example)
#
# If you do not do this, the pipe will block until data is read by the mysql process
#
# 13.2.8 LOAD XML SYNTAX
#
# 		LOAD XML [LOW_PRIORITY | CONCURRENT] [LOCAL] INFILE 'file_name'
# 			[REPLACE | IGNORE]
# 			INTO TABLE [db_name.]tbl_name
# 			[CHARACTER SET charset_name]
# 			[ROWS IDENTIFIED BY '<tagname>']
# 			[IGNORE number {LINES | ROWS}]
# 			[(field_name_or_user_var
# 				[, field_name_or_user_var] ---)]
# 			[SET col_name={expr | DEFAULT},
# 				[, col_name={expr | DEFAULT}] ---]
#
# The LOAD_XML statement reads data from an XML file into a table.
#
# The file_name must be given as a literal string. The tagname in the optional ROWS IDENTIFIED BY 
# clause must also be given as a literal string, and must be surrounded by angle brackets (< and >)
#
# LOAD_XML acts as the complement of running the mysql client in XML output mode (that is, starting
# the client with the --xml option)
#
# To write data from a table to an XML file, you can invoke the mysql client with the --xml
# and -e options from the system shell, as shown here:
#
# 		mysql --xml -e 'SELECT * FROM mydb.mytable' > file.xml
#
# To read the file back into a table, use LOAD_XML_INFILE
#
# By default, the <row> element is considered to be the equivalent of a database
# table row; this can be changed using the ROWS IDENTIFIED BY clause.
#
# This statement supports three different XML formats:
#
# 		) Column names as attributes and column values as attribute values:
#
# 			<row column1="value1" column2="value2" ---/>
#
# 		) Column names as tags and column values as the content of these tags:
#
# 			<row>
# 				<column1>value1</column1>
# 				<column2>value2</column2>
# 			</row>
#
# 		) Column names are the name attributes of <field> tags, and values are the contents of these tags:
#
# 			<row>
# 				<field name='column1'>value1</field>
# 				<field name='column2'>value2</field>
# 			</row>
#
# 			This is the format used by other MySQL tools, such as mysqldump
#
# All three formats can be used in the same XML file; the import routine automatically
# detects the format for each row and interprets it correctly.
#
# Tags are matched based on the tag or attribute name and the column name.
#
# The following clauses work essentially the same way for LOAD_XML as they do for LOAD_DATA:
#
# 		) LOW_PRIORITY or CONCURRENT
#
# 		) LOCAL
#
# 		) REPLACE or IGNORE
#
# 		) CHARACTER SET
#
# 		) SET
#
# See SECTION 13.2.7, "LOAD DATA INFILE SYNTAX", for more information about these clauses.
#
# (field_name_or_user_var, ---) is a list of one or more comma-separated XML fields or user variables.
#
# The name of a user variable used for this purpose must match the name of a field from the XML file,
# prefixed with @.
#
# You can use field names to select only desired fields.
#
# User variables can be employed to store the corresponding field values for subsequent re-use.
#
# The IGNORE number LINES or IGNORE number ROWS clause causes the first number rows in the
# XML file to be skipped.
#
# It is analogous to the LOAD_DATA statement's IGNORE --- LINES clause.
#
# Suppose that we have a table named person, created as shown here:
#
# 		USE test;
#
# 		CREATE TABLE person (
# 			person_id INT NOT NULL PRIMARY KEY,
# 			fname VARCHAR(40) NULL,
# 			lname VARCHAR(40) NULL,
# 			created TIMESTAMP
# 		);
#
# Suppose further that this table is initially empty.
#
# Now suppose that we have a simple XML file person.xml, whose contents
# are as shown here:
#
# 		<list>
# 			<person person_id="1" fname="Kapek" lname="Sainnouine"/>
# 			<person person_id="2" fname="Sajon" lname="Rondela"/>
# 			<person person_id="3"><fname>Likame</fname><lname>Örrtmons</lname></person>
# 			<person person_id="4"><fname>Slar</fname><lname>Manlanth</lname></person>
# 			<person><field name="person_id">5</field><field name="fname">Stoma</field>
# 				<field name="lname">Milu</field></person>
# 			<person><field name="person_id">6</field><field name="fname">Nirtam</field>
# 				<field name="lname">Sklöd</field></person>
# 			<person person_id="7"><fname>Sungam</fname><lname>Dulbåd</lname></person>
# 			<person person_id="8" fname="Srafef" lname="Encmelt"/>
# 		</list>
#
# Each of the permissible XML formats discussed previously is represented in this example file.
#
# To import the data in person.xml into the person table, you can use this statement:
#
# 		LOAD XML LOCAL INFILE 'person.xml'
# 			INTO TABLE person
# 			ROWS IDENTIFIED BY '<person>';
# 		Query OK, 8 rows affected (0.00 sec)
# 		Records: 8 Deleted: 0 Skipped: 0 Warnings: 0
#
# Here, we assume that person.xml is located in the MySQL data directory.
#
# If the file cannot be found, the following error results:
#
# 		ERROR 2 (HY000): File '/person.xml' not found (Errcode: 2)
#
# The ROWS IDENTIFIED BY '<person>' clause means that each <person> element in the XML
# file is considered equivalent to a row in the table into which the data is to be
# imported.
#
# In this case, this is the person table in the test database.
#
# As can be seen by the response from the server, 8 rows were imported into the test.person
# table.
#
# This can be verified by a simple SELECT statement:
#
# 		SELECT * FROM person;
# 		+-----------------+---------------+----------------+----------------------+
# 		| person_id 		| fname 			 | lname 			| created 				  |
# 		+-----------------+---------------+----------------+----------------------+
# 		| 1 				   | Kapek 			 | Sainnouine 	   | 2007-07-13 16:18:47  |
# 		| 2 					| Sajon 			 | Rondela 			| 2007-07-13 16:18:47  |
# 		| 3 					| Likame 		 | Örrtmons 		| 2007-07-13 16:18:47  |
# 		| 4 					| etc.
# 		etc.
#
# This shows, as stated earlier in this section, that any or all of hte 3 permitted XML
# formats may appear in a single file and be read in using LOAD_XML
#
# The inverse of the import operation just shown - that is, dumping MySQL table data
# into an XML file - can be accomplished using the mysql client from the system shell,
# as shown here:
#
# 		mysql --xml -e "SELECT * FROM test.person" > person-dump.xml
# 		cat person-dump.xml
# 		<?xml version="1.0"?>
#
# 		<resultset statement="SELECT * FROM test.person" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
# 			<row>
# 				<field name="person_id">1</field>
# 				<field name="fname">Kapek</field>
# 				<field name="lname">Sainnouine</field>
# 			</row>
#
# 			<row>
# 				<field name="person_id">2</field>
# 				etc.
# 			etc.
# 		</resultset>
#
# NOTE:
#
# 		The --xml option causes the mysql client to use XML formatting for its output;
#
# 		The -e option causes the client to execute the SQL statement immediately following
# 		the option. See SECTION 4.5.1, "MYSQL -- THE MYSQL COMMAND-LINE CLIENT"
#
# YOu can verify that hte dump is valid by creating a copy of the person table
# and importing the dump file into the new table, like this:
#
# 		USE test;
# 		CREATE TABLE person2 LIKE person;
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		LOAD XML LOCAL INFILE 'person-dump.xml'
# 			INTO TABLE person2;
# 		Query OK, 8 rows affected (0.01 sec)
# 		Records: 8 Deleted: 0 Skipped: 0 Warnings: 0
#
# 		SELECT * FROM person2;
# 		+---------------------+---------------+-----------------------+---------------------+
# 		| person_id 			 | fname 		  | lname 					  | created 				|
# 		+---------------------+---------------+-----------------------+---------------------+
# 		| etc.
# 		8 rows in set (0.00 sec)
#
# There is no requirement that every field in the XML file be matched with a column in the 
# corresponding table.
#
# Fields which have no corresponding columns are skipped.
#
# You can see this by first emptying the person2 table and dropping the created column,
# then using the same LOAD XML statement we just employed previously, like this:
#
# 		TRUNCATE person2;
# 		Query OK, 8 rows affected (0.26 sec)
#
# 		ALTER TABLE person2 DROP COLUMN created;
# 		Query OK, 0 rows affected (0.52 sec)
# 		Records: 0 Duplicates: 0 Warnings: 0
#
# 		SHOW CREATE TABLE person2\G
# 		************************** 1. row *********************************
# 				Table: person2
# 		Create table: CREATE TABLE `person2` (
# 			`person_id` int(11) NOT NULL,
# 			`fname` varchar(40) DEFAULT NULL,
# 			`lname` varchar(40) DEFAULT NULL,
# 			PRIMARY KEY (`person_id`)
# 		) ENGINE=InnoDB DEFAULT CHARSET=utf8
# 		1 row in set (0.00 sec)
#
# 		LOAD XML LOCAL INFILE 'person-dump.xml'
# 			INTO TABLE person2;
# 		Query OK, 8 rows affected (0.01 sec)
# 		Records: 8 Deleted: 0 Skipped: 0 Warnings: 0
#
# 		SELECT * FROM person2;
# 		+--------------+--------------+--------------+
# 		| person_id    | fname 		   | lname 			|
# 		+--------------+--------------+--------------+
# 		| 		etc.
# 		8 rows in set (0.00 sec)
#
# The order in which the fields are given within each row of the XML file does not
# affect the operation of LOAD XML; the field order can vary from row to row,
# and is not required to be in the same order as the corresponding columns in the table.
#
# As mentioned previously, you can use a (field_name_or_user_var, ---) list of one or more
# XML fields (to select desired fields only) or user variables (to store the corresponding
# field values for later use)
#
# User variables can be especially useful when you want to insert data from an XML file
# into table columns whose names do not match those of the XML fields.
#
# To see how this works, we first create a table named individual whose structure
# matches that of the person table, but whose columns are named differently:
#
# 		CREATE TABLE individual (
# 			individual_id INT NOT NULL PRIMARY KEY,
# 			name1 VARCHAR(40) NULL,
# 			name2 VARCHAR(40) NULL,
# 			made TIMESTAMP
# 		);
# 		Query OK, 0 rows affected (0.42 sec)
#
# In this case, you cannot simply load the XML file directly into the table,
# because the field and column names do not match:
#
# 		LOAD XML INFILE '../bin/person-dump.xml' INTO TABLE test.individual;
# 		ERROR 1263 (22004): Column set to default value; NULL supplied to NOT NULL column
# 		'individual_id' at row 1
#
# This happens because the MySQL server looks for field names matching the column names
# of hte target table.
#
# You can work around this problem by selecting the field values into user variables,
# then setting the target table's columns equal to the values of those variables
# using SET.
#
# You can perform both of these operations in a single statement, as shown here:
#
# 		LOAD XML INFILE '../bin/person-dump.xml'
# 				INTO TABLE test.individual (@person_id, @fname, @lname, @created)
# 				SET individual_id=@person_id, name1=@fname, name2=@lname, made=@created;
# 		Query OK, 8 rows affected (0.05 sec)
# 		Records: 8 Deleted: 0 Skipped: 0 Warnings: 0
#
# 		SELECT * FROM individual;;
# 		+-----------------+--------------+-------------------+------------------------+
# 		| individual_id 	| name1 			| name2 				  | made 					   |
# 		+-----------------+--------------+-------------------+------------------------+
# 		| etc.
#
# 		8 rows in set (0.00 sec)
#
# The names of the user variables must match those of the corresponding fields from the
# XML file, with the addition of hte required @ prefix to indicate that they are variables.
#
# THe user variables need not be listed or assigned in the same order as the corresponding fields.
#
# Using a ROWS IDENTIFIED BY '<tagname>' clause, it is possible to import data from the same XML
# file into database tables with different definitions.
#
# For this example, suppose that you have a file named address.xml which contains the following XML:
#
# 		<?xml version="1.0"?>
#
# 		<list>
# 			<person person_id="1">
# 				<fname>Robert</fname>
# 				<lname>Jones</lname>
# 				<address address_id="1" street="Mill Creek Road" zip="45365" city="Sidney"/>
#  			<address address_id="2" street="Main Street" zip="28681" city="Taylorsville"/>
# 			</person>
#
# 			etc.
#
# 		</list>
#
# You can again use the test.person table as defined previously in this section,
# after clearing all the existing records from the table and then showing its
# structure as shown here:
#
# 		mysql< TRUNCATE person;
# 		Query OK, 0 rows affected (0.04 sec)
#
# 		mysql< SHOW CREATE TABLE person\G
# 		******************* 1. row *************************
# 				Table: person
# 		Create Table: CREATE TABLE `person` (
# 			`person_id` int(11) NOT NULL,
# 			`fname` varchar(40) DEFAULT NULL,
# 			`lname` varchar(40) DEFAULT NULL,
# 			`created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
# 		  PRIMARY KEY (`person_id`)
# 		) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4
# 		1 row in set (0.00 sec)
#
# Now create an address table in the test database using the following CREATE_TABLE statement:
#
# 		CREATE TABLE address (
# 			address_id INT NOT NULL PRIMARY KEY,
# 			person_id INT NULL,
# 			street VARCHAR(40) NULL,
# 			zip INT NULL,
# 			city VARCHAR(40) NULL,
# 			created TIMESTAMP
# 		);
#
# To import the data from the XML file into the person table, execute the following
# LOAD_XML statement, which specifies that rows are to be specified by the
# <person> element, as shown here:
#
# 		LOAD XML LOCAL INFILE 'address.xml'
# 			INTO TABLE person
# 			ROWS IDENTIFIED BY '<person>';
# 		Query OK, 2 rows affected (0.00 sec)
# 		Records: 2 Deleted: 0 Skipped: 0 Warnings: 0
#
# You can verify that hte records were imported using a SELECT statement:
#
# 		SELECT * FROM person;
# 		+------------------+------------------+-----------+----------------------+
# 		| person_id 		 | fname 			  | lname 	  | created 			    |
# 		+------------------+------------------+-----------+----------------------+
# 		| 1 					 | Robert 			  | Jones 	  | 2007-07-24 17:37:06  |
# 		| 2 					 | Mary 				  | Smith 	  | 2007-07-24 17:37:06  |
# 		+------------------+------------------+-----------+----------------------+
# 		2 rows in set (0.00 sec)
#
# Since the <address> elements in the XML file have no corresponding columns in the
# person table, they are skipped.
#
# To import the data from the <address> elements into the address table, using the
# LOAD_XML statement shown here:
#
# 		LOAD XML LOCAL INFILE 'address.xml'
# 			INTO TABLE address
# 			ROWS IDENTIFIED BY '<address>';
# 		Query OK, 3 rows affected (0.00 sec)
# 		Records: 3 Deleted: 0 Skipped: 0 Warnings: 0
#
# You can see that the data was imported using a SELECT statement such as this one:
#
# 		SELECT * FROM address;
# 		+-------------+---------------+--------------------+---------+---------------+---------------------+
# 		| address_id  | person_id 		| street 				| zip     | city 			  | created 		      |
# 		+-------------+---------------+--------------------+---------+---------------+---------------------+
# 		| 1 			  | 1 				| Mill Creek Road 	| 45365   | Sidney 		  | 2007-07-24 17:37:37 |
# 		 etc.
# 		
# 		3 rows in set (0.00 sec)
#
# The data from the <address> element that is enclosed in XML comments is not imported.
#
# However, since there is a person_id column in the address table, the value of the 
# person_id attribute from the parent <person> element for each <address> is imported
# into the address table.
#
# SECURITY CONSIDERATIONS
#
# AS with the LOAD_DATA statement, the transfer of the XML file from the client
# host to the server host is initiated by the MySQL server.
#
# In theory, a patched server could be built that would tell the client program to
# transfer a file of the server's choosing rather than the file named by the client
# in the LOAD_XML statement.
#
# Such a server could access any file on the client host to which the client
# user has read access.
#
# In a Web environment, clients usually connect to MySQL from a Web server.
#
# A user that can run any command against the MySQL server can use LOAD_XML_LOCAL
# to read any files ot which the Web server process has read access.
#
# In this environment, the client with respect tot the MySQL server is actually
# the Web server, not the remote program being run by the user who connects
# to the Web server.
#
# You can disable loading of XML files from clients by starting the server with
# --local-infile=0 or --local-infile=OFF
#
# This option can also be used when starting the mysql client to disable
# LOAD_XML for the duration of the client session.
#
# To prevent a client from loading XML files from the server, do not grant
# the FILE privilege to the corresponding MySQL user account, to revoke
# this privilege if the client user account already has it.
#
# IMPORTANT:
#
# 		Revoking the FILE privilege (or not granting it in the first place) keeps
# 		hte user only from executing the LOAD_XML_INFILE statement
#
# 		(as well as the LOAD_FILE() function; it does not prevent the user
# 		from executing LOAD_XML_LOCAL_INFILE 
#
# 		To disallow this statement, you must start the server or the client
# 		with --local-infile=OFF
#
# 		In other words, the FILE privilege affects only whether the client
# 		can read files on the server;
#
# 		it has no bearing on whether the client can read files on the local file system.
#
# For partitioned tables using storage engines that employ table locks, such as MyISAM;
# any locks caused by LOAD XML performs locks on all partitions of the table.
#
# This does not apply to tables using storage engines which employ row-level locking,
# such as InnoDB.
#
# For more information, see PARTITIONING AND LOCKING
#
# 13.2.9 REPLACE SYNTAX
#
# 	REPLACE [LOW_PRIORITY | DELAYED]
# 		[INTO] tbl_name
# 		[PARTITION (partition_name [, partition_name] ---)]
# 		[(col_name [, col_name] ---)]
# 		{VALUES | VALUE} (value_list) [, (value_list)] ---
#
# 	REPLACE [LOW_PRIORITY | DELAYED]
# 		[INTO] tbl_name
# 		[PARTITION (partition_name [, partition_name] ---)]
# 		SET assignment_list
#
# 	REPLACE [LOW_PRIORITY | DELAYED]
# 		[INTO] tbl_name
# 		[PARTITION (partition_name [, partition_name] ---)]
# 		[(col_name [, col_name] ---)]
# 		SELECT ---
#
# 	value:
# 		{expr | DEFAULT}
#
# 	value_list:
# 		value [, value] ---
#
# 	assignment:
# 		col_name = value
#
# 	assignment_list:
# 		assignment [, assignment] ---
#
# REPLACE works exactly like INSERT, except that if an old row in the table has the same value as a new row
# for a PRIMARY KEY or a UNIQUE index, the old row is deleted before hte new row is isnerted.
#
# See SECTION 13.2.6, "INSERT SYNTAX"
#
# REPLACE is a MySQL extension to the SQL standard.
#
# It either inserts, or deletes and inserts.
#
# For another MySQL extension to standard SQL - that either inserts
# or updates - see SECTION 13.2.6.2, "INSERT --- ON DUPLICATE KEY UPDATE SYNTAX"
#
# DELAYED inserts and replaces were deprecated in MySQL 5.6
#
# In MySQL 8.0, DELAYED is not supported. THe server recognizes but ignores the DELAYED
# keyword, handles the replace as a nondelayed replace, and generates an ER_WARN_LEGACY_SYNTAX_CONVERTED
# warning.
#
# ("REPLACE DELAYED is no longer supported. The statement was converted to REPLACE")
#
# The DELAYED keyword will be removed in a future release.
#
# NOTE:
#
# 		REPLACE makes sense only if a table has a PRIMARY KEY or UNIQUE index.
#
# 		Otherwise, it becomes equivalent to INSERT, because there is no index to be
# 		used to determine whether a new row duplicates another.
#
# Values for all columns are taken from the values specified in the REPLACE statement.
#
# Any missing columns are set to their default values, just as happens for INSERT.
#
# You cannot refer to values from the current row and use them in the new row.
#
# If you use an assignment such as SET col_name = col_name + 1, the reference
# to the column name on the right hand side is treated as DEFAULT(col_name),
# so the assignment is equivalent to SET col_name = DEFAULT(col_name) + 1
#
# To use REPLACE, you must have both the INSERT and DELETE privileges for the table.
#
# If a generated column is replaced explicitly, the only permitted value is DEFAULT.
#
# For information about generated columns, see SECTION 13.1.20.8,, "CREATE TABLE AND GENERATED COLUMNS"
#
# REPLACE supports explicit partition selection using the PARTITION keyword with a list
# of comma-separated names of partitions, subpartitions, or both.
#
# As with INSERT, if it is not possible to insert the new row into any of these
# partitions or subpartitions, the REPLACE statement fails with the error:
#
# 		Found a row not matching the given partition set
#
# For more information, and examples - see SECTION 23.5, "PARTITION SELECTION"
#
# The REPLACE statement returns a count to indicate the number of rows affected.
#
# This is the sum of hte rows deleted and inserted. If the count is 1 for a single-row
# REPLACE, a row was inserted and no rows were deleted.
#
# If the count is greater than 1, one or more old rows were deleted before the new row
# was inserted.
#
# It is possible for a single row to replace more than one old row if the table contains
# multiple unique indexes and the new row duplicates values for different old rows
# in different unique indexes.
#
# THe affected-rows count makes it easy to determine whether REPLACE only added a row
# or whether it also replaced any rows:
#
# 		Check whether the count is 1 (added) or greater (replaced)
#
# If you are using the C API, the affected-rows count cna be obtained using the
# mysql_affected_rows() function.
#
# You cannot replace into a table and select from the same table in a subquery.
#
# MySQL uses the following algorithm for REPLACE (and LOAD DATA --- REPLACE):
#
# 		1. Try to insert the new row into the table
#
# 		2. While the insertion fails because a duplicate-key error occurs for a primary key or unique index:
#
# 			a. Delete from the table the conflicting row that has the duplicate key value
#
# 			b. Try again to insert the new row into the table
#
# It is possible that in the case of a duplicate-key error, a storage engine may perform
# the REPLACE as an update rather than a delete plus insert, but the semantics are the
# same.
#
# There are no user-visible effects other than a possible difference in how the storage
# engine increments Handler_xxx status variables.
#
# Because the results of REPLACE --- SELECT statements depend on the ordering of rows
# from the SELECT and this order cannot always be guaranteed, it is possible when
# logging these statements for the master and the slave to diverge.
#
# For this reason, REPLACE --- SELECT statements are flagged as unsafe for statement-based
# replication.
#
# Such statements produce a warning in the error log when using statement-based mode and are
# written to the binary log using the row-based format when using MIXED mode.
#
# See also SECTION 17.2.1.1, "ADVANTAGES AND DISADVANTAGES OF STATEMENT-BASED AND ROW-BASED REPLICATION"
#
# When modifying an existing table that is not partitioned to accomodate partitioning, or, when modifying
# the partitioning of an already partitioned table, you may consider altering the table's primary key
# (see SECTION 23.6.1, "PARTITIONING KEYS, PRIMARY KEYS, AND UNIQUE KEYS")
#
# You should be aware that, if you do this, the results of REPLACE statements may be affected,
# just as they would be if you modified the primary key of a nonpartitioned table.
#
# Consider the table created by the following CREATE_TABLE statement:
#
# 		CREATE TABLE test (
# 			id INT UNSIGNED NOT NULL AUTO_INCREMENT,
# 			data VARCHAR(64) DEFAULT NULL,
# 			ts TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
# 			PRIMARY KEY (id)
# 		);
#
# When we create this table and run the statements shown in the mysql client, the result
# is as follows:
#
# 		REPLACE INTO test VALUES (1, 'Old', '2014-08-20 18:47:00');
# 		Query OK, 1 row affected (0.04 sec)
#
# 		REPLACE INTO test VALUES (1, 'New', '2014-08-20 18:47:42');
# 		Query OK, 2 rows affected (0.04 sec)
#
# 		SELECT * FROM test;
# 		+----+---------+-----------------------+
# 		| id | data    | ts 							|
# 		+----+---------+-----------------------+
# 		| 1  | New 		| 2014-08-20 18:47:42   |
# 		+----+---------+-----------------------+
# 		1 row in set (0.00 sec)
#
# Now we create a second table almost identical to the first, except that hte primary key
# now covers 2 columns, as shown here (emphasized text):
#
# 		CREATE TABLE test2 (
# 			id INT UNSIGNED NOT NULL AUTO_INCREMENT,
# 			data VARCHAR(64) DEFAULT NULL,
# 			ts TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
# 			PRIMARY KEY (id, ts)
# 		);
#
# When we run on test2 the same two REPLACE statements, as we did on the original test table,
# we obtain a different result:
#
# 		REPLACE INTO test2 VALUES (1, 'Old', '2014-08-20 18:47:00');
# 		Query OK, 1 row affected (0.05 sec)
#
# 		REPLACE INTO test2 VALUES (1, 'New', '2014-08-20 18:47:42');
# 		Query OK, 1 row affected (0.06 sec)
#
# 		SELECT * FROM test2;
# 		+----+----------+--------------------+
# 		| id | data     | ts 					 |
# 		+----+----------+--------------------+
# 		| 1  | Old 		 | 2014-08-20 18:47:00|
# 		| 1  | New 		 | 2014-08-20 18:47:42|
# 		+----+----------+--------------------+
# 		2 rows in set (0.00 sec)
#
# This is due to the fact that, when run on test2, both the id and ts column values must match
# those of an existing row for hte row to be replaced; otehrwise, a row is inserted.
#
# A REPLACE statement affecting a partitioned table using a storage engine such as MyISAM that
# employs table-level locks locks only those partitions containing rows that match the
# REPLACE statement WHERE clause, as long as none of the table partitioning columns are updated;
# otherwise the entire table is locked.
#
# (For storage engines such as InnoDB that employ row-level locking, no locking of partitions
# takes place)
#
# For more information, see PARTITIONING AND LOCKING
#
# 13.2.10 SELECT SYNTAX
#
# 13.2.10 SELECT --- INTO SYNTAX
# 13.2.10.2 JOIN SYNTAX
# 13.2.10.3 UNION SYNTAX
#
# 		SELECT
# 			[ALL | DISTINCT | DISTINCTROW ]
# 				[HIGH_PRIORITY]
# 				[STRAIGHT_JOIN]
# 				[SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT]
# 				SQL_NO_CACHE [SQL_CALC_FOUND_ROWS]
# 			select_expr [, select_expr ---]
# 			[FROM table_references
# 				[PARTITION partition_list]
# 			[WHERE where_condition]
# 			[GROUP BY {col_name | expr | position}, --- [WITH ROLLUP]]
# 			[HAVING where_condition]
# 			[WINDOW window_name AS (window_spec)
# 				[, window_name AS (window_spec)] ---]
# 			[ORDER BY {col_name | expr | position}
# 				[ASC | DESC], --- [WITH ROLLUP]]
# 			[LIMIT {[offset,] row_count | row_count OFFSET offset}]
# 			[INTO OUTFILE 'file_name'
# 				[CHARACTER SET charset_name]
# 				export_options
# 			| INTO DUMPFILE 'file_name'
# 			| INTO var_name [, var_name]]
# 		 [FOR {UPDATE | SHARE} [OF tbl_name [, tbl_name] ---] [NOWAIT | SKIP lOCKED]
# 			| LOCK IN SHARE MODE]]
#
# SELECT is used to retrieve rows selected from one or more tables, and can include
# UNION statements and subqueries.
#
# See SECTION 13.2.10.3, "UNION SYNTAX" and SECTION 13.2.11, "SUBQUERY 	SYNTAX"
#
# A SELECT statement can start with a WITH clause to define common table expressions
# accessible within the SELECT.
#
# See SECTION 13.2.13, "WITH SYNTAX (COMMON TABLE EXPRESSIONS)"
#
# The most commonly used clauses of SELECT statements are these:
#
# 		) Each select_expr indicates a column that you want to retrieve.
#
# 			There must be at least one select_expr
#
# 		) table_references indicates the tables or tables from which to retrieve rows.
#
# 			Its syntax is described in SECTION 13.2.10.2, "JOIN SYNTAX"
#
# 		) SELECT supports explicit partition selection using the PARTITION with a list
# 			of partitions or subpartitions (or both) following the name of hte table
# 			in a table_refrence (see SECTION 13.2.10.2, "JOIN SYNTAX")
#
# 			In this case, rows are selected only from the partitions listed, and any
# 			other partitions of the table are ignored.
#
# 			For more information and examples, see SECTION 23.5, "PARTITION SELECTION"
#
# 			SELECT --- PARTITION from tables using storage engines such as MyISAM that
# 			perform table-level locks (and thus partition locks) lock only the partitions
# 			or subpartitions named by the PARTITION option.
#
# 			For more information, see PARTITIONING AND LOCKING
#
# 		) The WHERE clause, if given, indicates the condition or conditions that rows
# 			must satisfy to be selected.
#
# 			where_condition is an expression that evaluates to true for each row to be selected.
#
# 			The statement selects all rows if there is no WHERE clause.
#
# 			In the WHERE expression, you can use any of the functions and operators taht MySQL
# 			supports, except for aggregate (summary) functions.
#
# 			See SECTION 9.5, "EXPRESSIONS" and CHAPTER 12, FUNCTIONS AND OPERATORS.
#
# SELECT can also be used to retrieve rows computed without reference to any table.
#
# For example:
#
# 		SELECT 1+1;
# 			2
#
# You are permitted to specify DUAL as a dummy table name in situations where
# no tables are referneced:
#
# 		SELECT 1 +1 FROM DUAL;
# 			2
#
# DUAL is purely for the convenience of people who require that all SELECT
# statements should have FROM and possibly other clauses.
#
# MySQL may ignore the clauses.
#
# MySQL does not require FROM DUAL if no tables are referenced.
#
# In general, clauses used must be given in exactly the order shown in the 
# syntax description.
#
# For example, a HAVING clause must come after any GROUP BY clause and
# before any ORDER BY clause.
#
# The exception is that the INTO clause can appear either as shown in the syntax
# description or immediately following the select_expr list.
#
# For more information about INFO, see SECTION 13.2.10.1, "SELECT --- INTO SYNTAX"
#
# The list of select_expr terms comprises the select list that indicates which columns
# to retrieve.
#
# Terms specify a column or expression or can use *-shorthand:
#
# 		) A select list consisting only of a single unqualified * can be used
# 			as shorthand to select all columns from all tables.
#
# 				SELECT * FROM t1 INNER JOIN t2 ---
#
# 		) tbl_name.* can be used as qualified shorthand to select all columns
# 			from the named table:
#
# 				SELECT t1.*, t2.* FROM t1 INNER JOIN t2 ---
#
# 		) Use of an unqualified * with other items in the select list may produce a parse error.
#
# 			To avoid this problem, use a qualified tbl_name.* reference
#
# 				SELECT AVG(score), t1.* FROM t1 ---
#
# The following list provides additional information about other SELECT clauses:
#
# 		) A select_expr can be given an alias using AS alias_name.
#
# 			The alias is used as the expression's column name and can be used in
# 			GROUP BY, ORDER BY, or HAVING clauses.
#
# 			For example:
#
# 				SELECT CONCAT(last_name, ', ', first_name) AS full_name
# 					FROM mytable ORDER BY full_name;
#
# 			The AS keyword is optional when aliasing a select_expr with an identifier.
#
# 			The preceeding example could have been written like this:
#
# 				SELECT CONCAT(last_name, ', ',first_name) full_name
# 					FROM mytable ORDER BY full_name;
#
# 			However, because the AS is optional, a subtle problem can occur if you
# 			forget the comma between two select_expr expressions:
#
# 				MySQL interprets the second as an alias name.
#
# 				For example, in the following statement, columnb is treated
# 				as an alias name:
#
# 					SELECT columna columnb FROM mytable;
#
# 			For this reason, it is good practice to be in the habit of using AS explicitly
# 			when specifying column aliases.
#
# 			It is not permissible to refer to a column alias in a WHERE clause, because the
# 			column value might not yet be determined when the WHERE clause is executed.
#
# 			See SECTION B.6.4.4, "PROBLEMS WITH COLUMN ALIASES"
#
# 		) The FROM table_references caluse indicates the table or tables from which to
# 			retrieve rows.
#
# 			If you name more than one table, you are performing a join.
#
# 			For information on join syntax, see SECTION 13.2.10.2, "JOIN SYNTAX"
#
# 			For each table specified, you can optionally specify an alias.
#
# 				tbl_name [[AS] alias] [index_hint]
#
# 			The use of index hints provides the optimizer with information about how
# 			to choose indexes during query processing.
#
# 			For a description of the syntax for specifying these hints, see SECTION 8.9.4, "INDEX HINTS"
#
# 			You can use SET max_seeks_for_key=value as an alternative way to force MySQL
# 			to prefer key scans instead of table scans.
#
# 			See SECTION 5.1.8, "SERVER SYSTEM VARIABLES"
#
# 		) You can refer to a table within the default database as tbl_name, or as db_name.tbl_name
# 			to specify a database explicitly.
#
# 			You can refer to a column as col_name, tbl_name.col_name or db_name.tbl_name.col_name
#
# 			You need not specify a tbl_name or db_name.tbl_name prefix for a column reference
# 			unless the reference would be ambiguous.
#
# 			See SECTION 9.2.1, "IDENTIFIER QUALIFIERS", for examples of ambiguity that require
# 			the more explicit column reference forms.
#
# 		) A table reference can be aliased using tbl_name AS alias_name or tbl_name alias_name:
#
# 			SELECT t1.name, t2.salary FROM employee AS t1, info AS t2
# 				WHERE t1.name = t2.name;
#
# 			SELECT t1.name, t2.salary FROM employee t1, info t2
# 				WHERE t1.name = t2.name;
#
# 		) Columns selected for output can be referred to in ORDER BY and GROUP BY clauses
# 			using column names, column aliases or column positions.
#
# 			Column positions are integers and begin with 1:
#
# 				SELECT college, region, seed FROM tournament
# 					ORDER BY region, seed;
#
# 				SELECT college, region AS r, seed AS s FROM tournament
# 					ORDER BY r, s;
#
# 				SELECT college, region, seed FROM tournament
# 					ORDER BY 2, 3;
#
# 			To sort in reverse order, add the DESC (descending) keyword to the name of the column
# 			in the ORDER BY clause that you are sorting by.
#
# 			The default is ascending order; This can be specified explicitly using the ASC keyword.
#
# 			If ORDER BY occurs within a subquery and also is applied in the outer query, the outermost
# 			ORDER BY takes precedence.
#
# 			For example, results for the following statement are sorted in descending order,
# 			not ascending order:
#
# 				(SELECT --- ORDER BY a) ORDER BY a DESC;
#
# 			Use of column positions is deprecated because the syntax has been removed from the SQL standard.
#
# 		) Prior to MySQL 8.0.13, MySQL supported a nonstandard syntax extension that permitted explicit
# 			ASC or DESC designators for GROUP BY columns.
#
# 			MySQL 8.0.12 and later supports ORDER BY with grouping functions so that use of this extension
# 			is no longer necessary.
#
# 			(Bug #86312, Bug #26073525)
#
# 			This also means you can sort on an arbitrary column or columns when using GROUP BY, like this:
#
# 				SELECT a, b, COUNT(c) AS t FROM test_table GROUP BY a,b ORDER BY a,t DESC;
#
# 			As of MySQL 8.0.13, the GROUP BY extension is no longer supported:
#
# 				ASC or DESC designators for GROUP BY columns are not permitted.
#
# 		) When you use ORDER BY or GROUP BY to sort a column in a SELECT, the server sorts values
# 			using only the initial number of bytes indicated by the max_sort_length system variable.
#
# 		) MySQL extends the use of GROUP BY to permit selecting fields that are not mentioned in the
# 			GROUP BY clause.
#
# 			If you are not getting the results that you expect from your query, please read the 
# 			description of GROUP BY found in SECTION 12.20, "AGGREGATE (GROUP BY) FUNCTIONS"
#
# 		) GROUP BY permits a WITH ROLLUP modifier. See SECTION 12.20.2, "GROUP BY MODIFIERS"
#
# 			Previously, it was not permitted to use ORDER BY in a query having a WITH ROLLUP modifier.
#
# 			This restriction is lifted as of MySQL 8.0.12
#
# 			See SECTION 12.20.2, "GROUP BY MODIFIERS"
#
# 		) The HAVING clause is applied nearly last, just before items are sent to the client,
# 			with no optimization
#
# 			(LIMIT is applied after HAVING)
#
# 			The SQL standard requires that HAVING must reference only columns in the GROUP BY clause
# 			or columns used in aggregate functions.
#
# 			However, MySQL supports an extension to this behavior, and permits HAVING to refer
# 			to columns in the SELECT list and columns in outer subqueries as well.
#
# 			If the HAVING clause refers to a column that is ambiguous, a warning occurs.
#
# 			In the following statement, col2 is ambiguous because it is used as both an alias
# 			and a column name:
#
# 				SELECT COUNT(col1) AS col2 FROM t GROUP BY col2 HAVING col2 = 2;
#
# 			Preference is given to standard SQL behavior, so if a HAVING column name
# 			is used both in GROUP BY and as an aliased column in the output column list,
# 			preference is given to the column in the GROUP BY column.
#
# 		) Do not use HAVING for items that should be in the WHERE clause.
#
# 			For example, do not write the following:
#
# 				SELECT col_name FROM tbl_name HAVING col_name > 0;
#
# 			Write this instead:
#
# 				SELECT col_name FROM tbl_name WHERE col_name > 0;
#
# 		) The HAVING clause can refer to aggregate functions, which the WHERE clause cannot:
#
# 			SELECT user, MAX(salary) FROM users
# 				GROUP BY users HAVING MAX(salary) > 10;
#
# 			(This did not work in some older versions of MySQL)
#
# 		) MySQL permits duplicate column names.
#
# 			That is, there can be more than one select_expr with the same name.
#
# 			This is an extension to standard SQL. Because MySQL also permits
# 			GROUP BY and HAVING to refer to select_expr values, this can result
# 			in an ambiguity:
#
# 				SELECT 12 AS a, a FROM t GROUP BY a;
#
# 			In that statement, both columns have the name a.
#
# 			To ensure that the correct column is used for grouping, use different
# 			names for each select_expr
#
# 		) The WINDOW clause, if present, defines named windows that can be referred to by window functions.
#
# 			For details, see SECTION 12.21.4, "NAMED WINDOWS"
#
# 		) MySQL resolves unqualified column or alias references in ORDER BY clauses by searching
# 			in the select_expr values, then in the columns of the tables in the FROM clause.
#
# 			For GROUP BY or HAVING clauses, it searches the FROM clause before searching in the
# 			select_expr values.
#
# 			(For GROUP BY and HAVING, this differs from the pre-MySQL 5.0 behavior that used the same
# 			rules as for ORDER BY)
#
# 		) The LIMIT clause can be used to constrain the number of rows returned by the SELECT statement.
#
# 			LIMIT takes one or two numeric arguments, which must both be nonnegative integer constants,
# 			with these exceptions:
#
# 				) Within prepared statements, LIMIT parameters can be specified using ? placeholder markers.
#
# 				) Within stored programs, LIMIT parameters can be specified using integer-valued routine parameters
# 					or local variables.
#
# 			With two arguments, the first argument specifies the offset of the first row to return,
# 			and the second specifies the maximum number of rows to return.
#
# 			The offset of the initial row is 0 (not 1):
#
# 				SELECT * FROM tbl LIMIT 5,10; # Retrieve rows 6-15
#
# 			To retrieve all rows from a certain offset up to the end of the result set,
# 			you can use some large number for the second parameter.
#
# 			This statement retrieves all rows from the 96th row to the last:
#
# 				SELECT * FROM tbl LIMIT 95,<a lot>;
#
# 			With one argument, the value specifies the number of rows to return from the
# 			beginning of the result set:
#
# 				SELECT * FROM tbl LIMIT 5; #Retrieve first 5 rows
#
# 			In other words, LIMIT row_count is equivalent to LIMIT 0, row_count
#
# 			For prepared statements, you can use placeholders. The following statements will return
# 			one row from the tbl table:
#
# 				SET @a=1;
# 				PREPARE STMT FROM 'SELECT * FROM tbl LIMIT ?';
# 				EXECUTE STMT USING @a;
#
# 			The following statements will return the second to sixth row from the tbl table:
#
# 				SET @skip=1; SET @numrows=5;;
# 				PREPARE STMT FROM 'SELECT * FROM tbl LIMIT ?, ?';
# 				EXECUTE STMT USING @skip, @numrows;
#
# 			For compatibility with PostgreSQL, MySQL also supports the LIMIT row_count OFFSET offset syntax
#
# 			If LIMIT occurs within a subquery and also is applied in the outer query, the outermost
# 			LIMIT takes precedence.
#
# 			For example, the following statement produces two rows, not one:
#
# 				(SELECT --- LIMIT 1) LIMIT 2;
#
# 		) The SELECT_---_INTO form of SELECT enables the query result to be written to a file
# 			or stored in variables.
#
# 			For more information, see SECTION 13.2.10.1, "SELECT --- INTO SYNTAX"
#
# 		) If you use FOR UPDATE with a storage engine that uses page or row locks, rows examined
# 			by the query are write-locked until the end of the current transaction.
#
# 			You cannot use FOR UPDATE as part of the SELECT in a statement such as:
#
# 				CREATE_TABLE_new_table_SELECT_---_FROM_old_table_---
#
# 			(If you attempt to do so, the statement is rejected with the error Can't
# 			update table 'old_table' while 'new_table' is being created)
#
# 			FOR SHARE and LOCK IN SHARE MODE set shared locks that permit other transactions
# 			to read the examined rows but not to update or delete them.
#
# 			FOR SHARE and LOCK IN SHARE MODE are equivalent.
#
# 			However, FOR SHARE, like FOR UPDATE, supports NOWAIT, SKIP LOCKED,
# 			and OF tbl_name options.
#
# 			FOR SHARE is a replacement for LOCK IN SHARE MODE, but LOCK IN SHARE MODE
# 			remains available for backward compatibility.
#
# 			NOWAIT causes a FOR UPDATE or FOR SHARE query to execute immediately,
# 			returning an error if a row lock cannot be obtained due to a lock
# 			held by another transaction.
#
# 			SKIP LOCKED causes a FOR UPDATE or FOR SHARE query to execute immediately,
# 			excluding rows from the result set that are locked by another transaction.
#
# 			NOWAIT and SKIP LOCKED options are unsafe for statement-based replication.
#
# 				NOTE:
#
# 					Queries that skip locked rows return an inconsistent view of the data.
#
# 					SKIP LOCKED is therefore not suitable for general transactional work.
#
# 					However, it may be used to avoid lock contention when multiple sessions
# 					access the same queue-like table.
#
# 			OF tbl_name applies FOR UPDATE and FOR SHARE queries to named tables.
#
# 			For example:
#
# 				SELECT * FROM t1, t2 FOR SHARE OF t1 FOR UPDATE OF t2;
#
# 			All tables referenced by the query block are locked when OF tbl_name
# 			is omitted.
#
# 			Consequently, using a locking clause without OF tbl_name in combination
# 			with another locking clause returns an error.
#
# 			Specifying the same table in multiple locking clauses returns an error.
#
# 			If an alias is specified as the table name in the SELECT statement,
# 			a locking clause may only use the alias.
#
# 			If the SELECT statement does not specify an alias explicitly, the locking
# 			clause may only specify the actual table name.
#
# 			For more information about FOR UPDATE and FOR SHARE, see SECTION 15.7.2.4,
# 			"LOCKING READS"
#
# 			For additional information about NOWAIT and SKIP LOCKED options, see
# 			LOCKING READ CONCURRENCY WITH NOWAIT AND SKIP LOCKED.
#
# Following the SELECT keyword, you can use a number of modifiers that affect the
# operation of the statement.
#
# HIGH_PRIORITY, STRAIGHT_JOIN and modifiers beginning with SQL_ are MySQL extensions
# to standard SQL.
#
# 		) The ALL and DISTINCT modifiers specify whether duplicate rows should be returned.
#
# 			ALL (the default) specifies that all matching rows should be returned, including
# 			duplicates.
#
# 			DISTINCT specifies removal of duplicate rows from the result set.
#
# 			It is an error to specify both modifiers. DISTINCTROW is a synonym for
# 			DISTINCT.
#
# 			In MySQL 8.0.12 and later, DISTINCT can be used with a query that also uses
# 			WITH ROLLUP. (Bug #87450, Bug #26640100)
#
# 		) HIGH_PRIORITY gives the SELECT higher priority than a statement that updates a table.
#
# 			You should use this only for queries that are very fast and must be done at once.
#
# 			A SELECT HIGH_PRIORITY query that is issued while the table is locked for reading
# 			runs even if there is an update statement waiting for the table to be free.
#
# 			This affects only storage engines that use only table-level locking (such as MyISAM, MEMORY, and MERGE)
#
# 			HIGH_PRIORITY cannot be used with SELECT statements that are part of a UNION.
#
# 		) STRAIGHT_JOIN forces the optimizer to join the tables in the order in which they are listed
# 			in the FROM clause.
#
# 			You can use this to speed up a query if the optimizer joins the tables in nonoptimal
# 			order.
#
# 			STRAIGHT_JOIN also can be used in the table_references list. See SECTION 13.2.10.2, "JOIN SYNTAX"
#
# 			STRAIGHT_JOIN does not apply to any table that the optimizer treats as a const or system table.
#
# 			Such a table produces a single row, is read during the optimization phase of query execution,
# 			and references to its columns are replaced with the appropriate column values before query
# 			execution proceeds.
#
# 			These tables will appear first in the query plan displayed by EXPLAIN.
#
# 			See SECTION 8.8.1, "OPTIMIZING QUERIES WITH EXPLAIN"
#
# 			This exception may not apply to const or system tables that are
# 			used on the NULL-complemented side of an outer join (that is, the right-side table
# 			of a LEFT JOIN or the left-side table of a RIGHT JOIN)
#
# 		) SQL_BIG_RESULT or SQL_SMALL_RESULT can be used with GROUP BY or DISTINCT to tell
# 			the optimizer that the result set has many rows or is small, respectively.
#
# 			For SQL_BIG_RESULT, MySQL directly uses disk-based temporary tables if they are created,
# 			and prefers sorting to using a temporary table with a key on the GROUP BY elements.
#
# 			For SQL_SMALL_RESULT, MySQL uses in-memory temporary tables to store the resulting table
# 			instead of using sorting.
#
# 			This should not normally be needed.
#
# 		) SQL_BUFFER_RESULT forces the result to be put into a temporary table.
#
# 			This helps MySQL free the table locks early and helps in cases where
# 			it takes a long time to send the result set to the client.
#
# 			This modifier can be used only for top-level SELECT statements,
# 			not for subqueries or following UNION.
#
# 		) SQL_CALC_FOUND_ROWS tells MySQL to calculate how many rows there would be
# 			in the result set, disregarding any LIMIT clause.
#
# 			The number of rows can then be retrieved with SELECT FOUND_ROWS()
#
# 			See SECTION 12.15, "INFORMATION FUNCTIONS"
#
# 		) The SQL_CACHE and SQL_NO_CACHE modifiers were used with the query cache prior to MySQL 8.0
#
# 			The query cache was removed in MySQL 8.0
#
# 			The SQL_CACHE modifier was removed as well.
#
# 			SQL_NO_CACHE is deprecated, has no effect, and will be removed in a future MySQL release.
#
# A SELECT from a partitioned table using a storage engine such as MyISAM that employs
# table-level locks locks only those partitions containing rows that match the SELECT
# statement WHERE clause.
#
# (This does not occur with storage engines such as InnoDB that employ row-level locking)
#
# For more information, see PARTITIONING AND LOCKING
#
# 13.2.10.1 SELECT --- INTO SYNTAX
#
# The SELECT_---_INTO form of SELECT enables a query result to be stored in variables or 
# written to a file:
#
# 		) SELECT --- INTO var_list selects column values and stores them into variables
#
# 		) SELECT --- INTO OUTFILE writes the selected rows to a file. Column and line terminators can be
# 			specified to produce a specific output format.
#
# 		) SELECT --- INTO DUMPFILE writes a single row to a file without any formatting.
#
# The SELECT syntax description (see SECTION 13.2.10, "SELECT SYNTAX") shows the INTO clause
# near the end of the statement.
#
# It is also possible to use INTO immediately following the select_expr list
#
# An INTO clause should not be used in a nested SELECT because such a SELECT
# must return its result to the outer context.
#
# The INTO clause can name a list of one or more variables, which can be user-defined
# variables, stored procedure or function parameters, or stored program local variables.
#
# (Within a prepared SELECT --- INTO OUTFILE statement, only user-defined variables
# are permitted; see SECTION 13.6.4.2, "LOCAL VARIABLE SCOPE AND RESOLUTION")
#
# The selected values are assigned to the variables.
#
# The number of variables must match the number of columns.
# The query should return a single row.
#
# If the query returns no rows, a warning with error code 1329 occurs
# (No data), and the variable values remain unchanged.
#
# If the query returns multiple rows, error 1172 occurs (Result consisted of more than one row)
#
# If it is possible that the statement may retrieve multiple rows, you can use LIMIT 1 to limit
# the result set to a single row.
#
# 		SELECT id, data INTO @x, @y FROM test.t1 LIMIT 1;
#
# User variable names are not case-sensitive. See SECTION 9.4, "USER-DEFINED VARIABLES"
#
# The SELECT_---_INTO_OUTFILE 'file_name' form of SELECT writes the selected rows to a file.
#
# The file is created on the server host, so you must have the FILE privilege to use this
# syntax.
#
# file_name cannot be an existing file, which among other things prevents files such as
# /etc/passwd and database tables from being destroyed.
#
# The character_set_filesystem system variable controls the interpretation of the file name.
#
# The SELECT_---_INTO_OUTFILE statement is intended primarily to let you very quickly
# dump a table to a text file on the server machine.
#
# If you want to create the resulting file on some other host than the server host,
# you normally cannot use SELECT_---_INTO_OUTFILE since there is no way to write a path
# to the file relative to the server host's file system.
#
# However, if the MySQL client software is installed on the remote machine, you can
# instead use a client command such as mysql -e "SELECT ---" > file_name to generate
# the file on the client host.
#
# It is also possible to create the resulting file on a different host other than the
# server host, if the location of the file on the remote host can be accessed using
# a network-mapped path on the server's file system.
#
# In this case, the presence of mysql (or some other MySQL client program) is not
# required on the target host.
#
# SELECT_---_INTO_OUTFILE is the complement of LOAD_DATA_INFILE.
#
# Columns values are written converted to the character set specified in the
# CHARACTER SET clause.
#
# If no such clause is present, values are dumped using the binary character set.
#
# In effect, there is no character set conversion. 
#
# If a result set contains columns in several character sets, 
# the output data file will as well and you may not be able to reload the file correctly.
#
# The syntax for the export_options part of the statement consists of the same FIELDS
# and LINES clauses that are used with the LOAD_DATA_INFILE statement.
#
# See SECTION 13.2.7, "LOAD DATA INFILE SYNTAX", for information about the FIELDS
# and LINES clauses, including their default values and permissible values.
#
# FIELDS ESCAPED BY controls how to write special characters.
#
# If the FIELDS ESCAPED BY character is not empty, it is used when necessary to avoid
# ambiguity as a prefix that precedes following characters on output:
#
# 		) The FIELDS ESCAPED BY character
#
# 		) The FIELDS [OPTIONALLY] ENCLOSED BY character
#
# 		) The first character of the FIELDS TERMINATED BY and LINES TERMINATED BY values
#
# 		) ASCII NUL (the zero-valued byte; what is actually written following the escape character is ASCII 0,
# 			not a zero-valuted byte)
#
# The FIELDS TERMINATED BY, ENCLOSED BY, ESCAPED BY, or LINES TERMINATED BY characters must be
# escaped so that you can read the file back in reliably.
#
# ASCII NUL is escaped to make it easier to view with some pagers.
#
# The resulting file does not have to conform to SQL syntax, so nothing else need be escaped.
#
# If the FIELDS ESCAPED BY character is empty, no characters are escaped and NULL is output as
# NULL, not \N
#
# It is probably not a good idea to specify an empty escape character, particularly if field values
# in your data contain any of the characters in the list just given.
#
# Here is an example that produces a file in the comma-separated values (CSV) format used
# by many programs:
#
# 		SELECT a,b,a+b INTO OUTFILE '/tmp/result.txt'
# 			FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"'
# 			LINES TERMINATED BY '\n'
# 			FROM test_table;
#
# If you use INTO DUMPFILE instead of INTO OUTFILE, MySQL writes only one row into the file,
# without any column or line termination and without performing any escape processing.
#
# This is useful if you want to store a BLOB value in a file.
#
# 		NOTE:
#
# 			Any file created by INTO OUTFILE or INTO DUMPFILE is writable by all users on the
# 			server host.
#
# 			The reason for this is that the MySQL server cannot create a file that is owned
# 			by anyone other than the user under whose account it is running.
#
# 			(You should never run mysqld as root for this and other reasons)
#
# 			The file thus must be world-writable so that you can manipulate its contents.
#
# 			If the secure_file_priv system variable is set to a nonempty directory name,
# 			the file to be written must be located in that directory.
#
# In the context of SELECT_---_INTO statements that occur as part of events executed by the
# Event Scheduler, diagnostics messages (not only errors, but also warnings) are written
# to the error log, and, on Windows, to the application event log.
#
# For additional information, see SECTION 24.4.5, "EVENT SCHEDULER STATUS"
#
# 13.2.10.2 JOIN SYNTAX
#
# MySQL supports the following JOIN syntax for the table_references part of SELECT
# statements and multiple-table DELETE and UPDATE statements:
#
# 		table_references:
# 			escaped_table_reference [, escaped_table_reference] ---
#
# 		escaped_table_reference:
# 			table_reference
# 		 | { OJ table_reference }
#
# 		table_reference:
# 			table_factor
# 		 | join_table
#
# 		table_factor:
# 				tbl_name [PARTITION (partition_names)]
# 					[[AS] alias] [index_hint_list]
# 			| table_subquery [AS] alias [(col_list)]
# 			| ( table_references )
#
# 		join_table:
# 			table_reference [INNER | CROSS] JOIN table_factor [join_condition]
# 		 | table_reference STRAIGHT_JOIN table_factor
# 		 | table_reference STRAIGHT_JOIN table_factor ON conditional_expr
# 		 | table_reference {LEFT|RIGHT} [OUTER] JOIN table_reference join_condition
# 		 | table_reference NATURAL [INNER | {LEFT|RIGHT} [OUTER]] JOIN table_factor
#
# 		join_condition:
# 			ON conditional_expr
# 		 | USING (column_list)
#
# 		index_hint_list:
# 			index_hint [, index_hint] ---
#
# 		index_hint:
# 			USE {INDEX|KEY}
# 				[FOR {JOIN|ORDER BY|GROUP BY}] ([index_list])
# 		 | IGNORE {INDEX|KEY}
# 				[FOR {JOIN|ORDER BY|GROUP BY}] (index_list)
# 		 | FORCE {INDEX|KEY}
# 				[FOR {JOIN|ORDER BY|GROUP BY}] (index_list)
#
# 		index_list:
# 			index_name [, index_name] ---
#
# A table reference is also known as a join expression
#
# A table reference (when it refers to a partitioned table) may contain a PARTITION option,
# including a list of comma-separated partitions, subpartitions, or both.
#
# This option follows the name of the table and precedes any alias declaration.
#
# The effect of this option is that rows are selected only from the listed partitions
# or subpartitions.
#
# Any partitions or subpartitions not named in the list are ignored.
#
# For more information and examples, see SECTION 23.5, "PARTITION SELECTION"
#
# The syntax of table_factor is extended in MySQL in comparison with standard SQL.
#
# The standard accepts only table_reference, not a list of them inside a pair of parentheses.
#
# This is a conservative extension if each comma in a list of table_reference items is considered
# as equivalent to an inner join.
#
# For example:
#
# 			SELECT * FROM t1 LEFT JOIN (t2, t3, t4)
# 									ON (t2.a = t1.a AND t3.b = t1.b AND t4.c = t1.c)
#
# 		is equivalent to:
#
# 			SELECT * FROM t1 LEFT JOIN (t2 CROSS JOIN t3 CROSS JOIN t4)
# 									ON (t2.a = t1.a AND t3.b = t1.b AND t4.c = t1.c)
#
# In MySQL, JOIN, CROSS JOIN and INNER JOIN are syntactic equivalents (they can replace each other)
#
# In standard SQL, they are not equivalent. INNER JOIN is used with an ON clause, CROSS JOIN
# is used otherwise.
#
# In general, parentheses can be ignored in join expressions containing only inner join operations.
#
# MySQL also supports nested joins. See SECTION 8.2.1.7, "NESTED JOIN OPTIMIZATION"
#
# Index hints can be specified to affect how the MySQL optimizer makes use of indexes.
# For more information, see SECTION 8.9.4, "INDEX HINTS"
#
# Optimizer hints and the optimizer_switch system variable are other ways to influence
# optimizer use of indexes.
#
# See SECTION 8.9.2, "OPTIMIZER HINTS", and SECTION 8.9.3, "SWITCHABLE OPTIMIZATIONS"
#
# The following list describes general factors to take into account when writing joins:
#
# 		) A table reference can be aliased using tbl_name AS alias_name or tbl_name alias_name:
#
# 			SELECT t1.name, t2.salary
# 				FROM employee AS t1 INNER JOIN info AS t2 ON t1.name = t2.name;
#
# 			SELECT t1.name, t2.salary
# 				FROM employee t1 INNER JOIN info t2 ON t1.name = t2.name;
#
# 		) A table_subquery is also known as a derived table or subquery in the FROM clause.
#
# 			See SECTION 13.2.11.8, "DERIVED TABLES"
#
# 			Such subqueries must include an alias to give the subquery result a table name,
# 			and may optionally include a list of table column names in parentheses.
#
# 			A trivial example follows:
#
# 				SELECT * FROM (SELECT 1, 2, 3) AS t1;
#
# 		) INNER JOIN and , (comma) are semantically equivalent in the absence of a join condition:
#
# 			Both produce a Cartesian product between the specified tables (that is, each and every row
# 			in the first table is joined to each and every row in the second table)
#
# 			However, the precedence of the comma operator is less than that of INNER JOIN, CROSS JOIN,
# 			LEFT JOIN, and so on.
#
# 			If you mix comma joins with the other join types when there is a join condition, an error
# 			of the form:
#
# 				Unknown column 'col_name' in 'on clause' may occur
#
# 			Information about dealing with this problem is given later in this section.
#
# 		) The conditional_expr used with ON is any conditional expression of the form that can be used
# 			in a WHERE clause.
#
# 			Generally, the ON clause serves for conditions that specify how to join tables,
# 			and the WHERE clause restricts which rows to include in the result set
#
# 		) If there is no matching row for the right table in the ON or USING part in a LEFT JOIN,
# 			a row with all columns set to NULL is used for the right table.
#
# 			You can use this fact to find rows in a table that have no counterpart in another table:
#
# 				SELECT left_tbl.*
# 					FROM left_tbl LEFT JOIN right_tbl ON left_tbl.id = right_tbl.id
# 					WHERE right_tbl.id IS NULL;
#
# 			This example finds all rows in left_tbl with an id value that is not present in
# 			right_tbl (that is, all rows in left_tbl with no corresponding row in right_tbl)
#
# 			See SECTION 8.2.1.8, "OUTER JOIN OPTIMIZATION"
#
# 		) The USING(column_list) clause names a list of columns that must exist in both tables.
#
# 			If tables a and b both contain columns c1, c2, and c3, the following join compares
# 			corresponding columns from the two tables:
#
# 				a LEFT JOIN b USING (c1, c2, c3)
#
# 		) The NATURAL [LEFT] JOIN of two tables is defined to be semantically equivalent to an
# 			INNER JOIN or a LEFT JOIN with a USING clause that names all columns that
# 			exist in both tables.
#
# 		) RIGHT JOIN works analogously to LEFT JOIN. To keep code portable across databases,
# 			it is recommended that you use LEFT JOIN instead of RIGHT JOIN.
#
# 		) The { OJ --- } syntax shown in the join syntax description exists only for compatibility
# 			with ODBC.
#
# 			The curly braces in the syntax should be written literally, they are not metasyntax
# 			as used elsewhere in syntax desc.
#
# 				SELECT left_tbl.*
# 					FROM { OJ left_tbl LEFT OUTER JOIN right_tbl ON left_tbl.id = right_tbl.id }
# 					WHERE right_tbl.id IS NULL;
#
# 			You can use other types of joins within { OJ --- }, such as INNER JOIN or RIGHT OUTER JOIN.
#
# 			This helps with compatibility with some third-party apps, but is not official ODBC syntax
#
# 		) STRAIGHT_JOIN is similar to JOIN, except that the left table is always read before the right table.
#
# 			This can be used for those (few) cases for which the join optimizer processes the tables
# 			in a suboptimal order.
#
# 			Some join examples:
#
# 				SELECT * FROM table1, table2;
#
# 				SELECT * FROM table1 INNER JOIN table2 ON table1.id = table2.id;
#
# 				SELECT * FROM table1 LEFT JOIN table2 ON table1.id = table2.id;
#
# 				SELECT * FROM table1 LEFT JOIN table2 USING (id);
#
# 				SELECT * FROM table1 LEFT JOIN table2 ON table1.id = table2.id
# 					LEFT JOIN table3 ON table2.id = table3.id;
#
# Natural joins and joins with USING, including outer join variants, are processed
# according to the SQL:2003 standard:
#
# 		) Redundant columns of a NATURAL join do not appear.
#
# 			Consider this set of statements:
#
# 				CREATE TABLE t1 (i INT, j INT);
# 				CREATE TABLE t2 (k INT, j INT);
#
# 				INSERT INTO t1 VALUES(1, 1);
# 				INSERT INTO t2 VALUES(1, 1);
#
# 				SELECT * FROM t1 NATURAL JOIN t2;
# 				SELECT * FROM t1 JOIN t2 USING (j);
#
# 			In the first SELECT statement, column j appears in both tables and thus becomes
# 			a join column, so, according to standard SQL, it should appear only once in the output,
# 			not twice.
#
# 			Similarly, in the second SELECT statement, column j is named in the USING clause and
# 			should appear only once in the output, not twice.
#
# 			Thus, the statements produce this output:
#
# 				+--------+--------+----------+
# 				| j 	   | i      | k 		  |
# 				+--------+--------+----------+
# 				| 1 		| 1 		| 1 		  |
# 				+--------+--------+----------+
#
# 				+--------+--------+----------+
# 				| j 		| i 		| k 		  |
# 				+--------+--------+----------+
# 				| 1 		| 1 		| 1 		  |
# 				+--------+--------+----------+
#
# 			Redundant column elimination and column ordering occurs according to standard SQL,
# 			producing this display order:
#
# 				) First, coalesced common columns of the two joined tables, in the order in which they occur in the first table
#
# 				) Second, columns unique to the first table, in order in which they occur in that table
#
# 				) Third, columns unique to the second table, in order in which they occur in that table
#
# 			The single result column that replaces two common columns is defined using the coalesce operation.
#
# 			That is, for two t1.a and t2.a the resulting single join column a is defined as
# 			a = COALESCE(t1.a, t2.a), where:
#
# 				COALESCE(x, y) = (CASE WHEN x IS NOT NULL THEN x ELSE y END)
#
# 			If the join operation is any other join, the result columns of the join consist of the
# 			concatenation of all columns of the joined tables.
#
# 			A consequence of the definition of coalesced columns is that, for outer joins, the coalesced column
# 			contains the value of the non-NULL column if one of the two columns is always NULL.
#
# 			If neither or both columns are NULL, both common columns have the same value, so it does not
# 			matter which one is chosen at the value of the coalesced column.
#
# 			A simple way to interpret this is to consider that a coalesced column of an outer join
# 			is represented by the common column of the inner table of a JOIN.
#
# 			Suppose that the tables t1(a, b) and t2(a, c) have the following contents:
#
# 				t1 	t2
# 				----  ----
# 				1 x   2 z
# 				2 y   3 w
#
# 			Then, for this join, column a contains the values of t1.a
#
# 				SELECT * FROM t1 NATURAL LEFT JOIN t2;
# 				+---------+--------+-----------+
# 				| a       | b 		 | c 			 |
# 				+---------+--------+-----------+
# 				| 1 		 | x 		 | NULL   	 |
# 				| 2 		 | y 		 | z 			 |
# 				+---------+--------+-----------+
#
# 			By contrast, for this join, column a contains the values of t2.a
#
# 				SELECT * FROM t1 NATURAL RIGHT JOIN t2;
# 				+---------+---------+---------+
# 				| a 		 | c 		  | b 		|
# 				+---------+---------+---------+
# 				| 2 		 | z 		  | y 		|
# 				| 3 		 | w 		  | NULL 	|
# 				+---------+---------+---------+
#
# 			Compare those results to the otherwise equivalent queries with JOIN --- ON:
#
# 				SELECT * FROM t1 LEFT JOIN t2 ON (t1.a = t2.a);
# 				+--------+---------+---------+---------+
# 				| a      | b 		 | a 		  | c 		|
# 				+--------+---------+---------+---------+
# 				| 1 		| x 		 | NULL 	  | NULL 	|
# 				| 2 		| y 		 | 2 		  | z 		|
# 				+--------+---------+---------+---------+
#
# 				SELECT * FROM t1 RIGHT JOIN t2 ON (t1.a = t2.a);
# 				+--------+---------+---------+---------+
# 				| a 		| b 		 | a 		  | c 		|
# 				+--------+---------+---------+---------+
# 				| 2 		| y 		 | 2 		  | z 		|
# 				| NULL   | NULL 	 | 3 		  | w 		|
# 				+--------+---------+---------+---------+
#
# 		) A USING clause can be rewritten as an ON clause that compares corresponding columns.
#
# 			However, although USING and ON are similar, they are not quite the same.
#
# 			Consider the following two queries:
#
# 				a LEFT JOIN b USING (c1, c2, c3)
# 				a LEFT JOIN b ON a.c1 = b.c1 AND a.c2 = b.c2 AND a.c3 = b.c3
#
# 			With respect to determining which rows satisfy the join condition, both joins
# 			are semantically identical.
#
# 			With respect to determining which columns to display for SELECT * expansion,
# 			the two joins are not semantically identical.
#
# 			The USING join selects the coalesced value of corresponding columns,
# 			whereas the ON join selects all columns from all tables.
#
# 			For the USING join, SELECT * selects these values:
#
# 				COALESCE(a.c1, b.c1), COALESCE(a.c2, b.c2), COALESCE(a.c3, b.c3)
#
# 			For the ON join, SELECT * selects these values:
#
# 				a.c1, a.c2, a.c3, b.c1, b.c2, b.c3
#
# 			With an inner join, COALESCE(a.c1, b.c1) is the same as either a.c1 or
# 			b.c1 because both columns will have teh same value.
#
# 			With an outer join (such as LEFT JOIN), one of the two columns can be NULL
#
# 			That column is omitted from the result.
#
# 		) An ON clause can refer only to its operands.
#
# 			Example:
#
# 				CREATE TABLE t1 (i1 INT);
# 				CREATE TABLE t2 (i2 INT);
# 				CREATE TABLE t3 (i3 INT);
# 				SELECT * FROM t1 JOIN t2 ON (i1 = i3) JOIN t3;
#
# 			The statement fails with an Unknown column 'i3' in 'on clause' error because
# 			i3 is a column in t3, which is not an operand of the ON clause.
#
# 			To enable the join to be processed, rewrite the statement as follows:
#
# 				SELECT * FROM t1 JOIN t2 JOIN t3 ON (i1 = i3);
#
# 		) JOIN has higher precedence than the comma operator (,), so the join expression
# 			t1, t2 JOIN t3 is interpreted as (t1, (t2 JOIN t3)), not as ((t1, t2) JOIN t3)
#
# 			This affects statements that use an ON clause because that clause can refer only
# 			to columns in the operands of the join, and the precedence affects interpretation
# 			of what those operands are.
#
# 			EXAMPLE:
#
# 				CREATE TABLE t1 (i1 INT, j1 INT);
# 				CREATE TABLE t2 (i2 INT, j2 INT);
# 				CREATE TABLE t3 (i3 INT, j3 INT);
#
# 				INSERT INTO t1 VALUES(1, 1);
# 				INSERT INTO t2 VALUES(1, 1);
# 				INSERT INTO t3 VALUES(1, 1);
#
# 				SELECT * FROM t1, t2 JOIN t3 ON (t1.i1 = t3.i3);
#
# 			The JOIN takes precedence over the comma operator, so the operands for the ON clause
# 			are t2 and t3.
#
# 			Because t1.i1 is not a column in either of the operands, the result is an
# 			Unknown column 't1.i1' in 'on clause' error
#
# 			To enable the join to be processed, use either of these strategies:
#
# 				) Group the first two tables explicitly with parentheses so that the operands
# 					for the ON clause are (t1, t2) and t3:
#
# 					SELECT * FROM (t1, t2) JOIN t3 ON (t1.i1 = t3.i3);
#
# 				) Avoid the use of the comma operator and use JOIN instead:
#
# 					SELECT * FROM t1 JOIN t2 JOIN t3 ON (t1.i1 = t3.i3);
#
# 			The same precedence interpretation also applies to statements that mix
# 			the comma operator with INNER JOIN, CROSS JOIN, LEFT JOIN, and RIGHT JOIN,
# 			all of which have higher precedence than the comma operator.
#
# 		) A MySQL extension compared to the SQL:2003 standard is that MySQL permits you to
# 			qualify the common (coalesced) columns of NATURAL or USING joins, whereas 
# 			the standard disallows that.
#
# 13.2.10.3 UNION SYNTAX
#
# 		SELECT ---
# 		UNION [ALL | DISTINCT] SELECT ---
# 		[UNION [ALL | DISTINCT] SELECT ---]
#
# UNION is used to combine the result from multiple SELECT statements into a single result set.
#
# The column names from the first SELECT statement are used as the column names for the
# results returned.
#
# Selected columns listed in corresponding positions of each SELECT statement should have
# the same data type.
#
# (For example, the first column selected by the first statement should have the same type
# as the first column selected by the other statements)
#
# If the data types of corresponding SELECT columns do not match, the types and lengths of
# the columns in the UNION result take into account the values retrieved by all of
# the SELECT statements.
#
# For example, consider the following:
#
# 		SELECT REPEAT('a',1) UNION SELECT REPEAT('b',10);
# 		+----------------------+
# 		| REPEAT('a',1) 		  |
# 		+----------------------+
# 		| a 						  |
# 		| bbbbbbbbbb 			  |
# 		+----------------------+
#
# The SELECT statements are normal select statements, but with the following restrictions:
#
# 		) Only the last SELECT statement can use INTO OUTFILE (However, the entire UNION result is written to the file)
#
# 		) HIGH_PRIORITY cannot be used with SELECT statements that are part of a UNION.
#
# 			If you specify it for the first SELECT, it has no effect.
#
# 			If you specify it for any subsequent SELECT statements, a syntax error results.
#
# The default behavior for UNION is that duplicate rows are removed from the result.
#
# The optional DISTINCT keyword has no effect other than the default because it also
# specifies duplicate-row removal.
#
# With the optional ALL keyword, duplicate-row removal does not occur and the result includes
# all matching rows from all the SELECT statements.
#
# You can mix UNION_ALL and UNION_DISTINCT in the same query.
#
# Mixed UNION types are treated such that a DISTINCT union overrides any ALL union
# to its left.
#
# A DISTINCT union can be produced explicitly by using UNION_DISTINCT or implicitly
# by using UNION with no following DISTINCT or ALL keyword.
#
# To apply ORDER BY or LIMIT to an individual SELECT, place the clause inside the
# parentheses that enclose the SELECT:
#
# 		(SELECT a FROM t1 WHERE a=10 AND B=1 ORDER BY a LIMIT 10)
# 		UNION
# 		(SELECT a FROM t2 WHERE a=11 AND B=2 ORDER BY a LIMIT 10);
#
# However, use of ORDER BY for individual SELECT statements implies nothing about
# the order in which the rows appear in the final result because UNION
# by default produces an unordered set of rows.
#
# Therefore, the use of ORDER BY in this context is typically in conjunction
# with LIMIT, so that it is used to determine the subset of the selected
# rows to retrieve for the SELECT, even though it does not necessarily affect
# the order of those rows in the final UNION result.
#
# If ORDER BY appears without LIMIT in a SELECT, it is optimized away
# because it will have no effect anyway.
#
# To use an ORDER BY or LIMIT clause to sort or limit the entire UNION result,
# parenthesize the individual SELECT statements and place the ORDER BY or
# LIMIT after the last one.
#
# The following example uses both clauses:
#
# 		(SELECT a FROM t1 WHERE a=10 AND B=1)
# 		UNION
# 		(SELECT a FROM t2 WHERE a=11 AND B=2)
# 		ORDER BY a LIMIT 10;
#
# A statement without parentheses is equivalent ot one parenthesized
# as just shown.
#
# This kind of ORDER BY cannot use column references that include a table
# name (that is, names in tbl_name.col_name format)
#
# Instead, provide a column alias in the first SELECT statement and refer
# to the alias in the ORDER BY.
#
# (Alternatively, refer to the column in the ORDER BY using its column position.
#
# However, use of column positions is deprecated)
#
# Also, if a column to be sorted is aliased, the ORDER BY clause must refer to the
# alias, not the column name.
#
# The first of the following statements will work, but the second will fail
# with an:
#
# 		Unknown column 'a' in 'order clause' error
#
# 		(SELECT a AS b FROM t) UNION (SELECT ---) ORDER BY b;
# 		(SELECT a AS b FROM t) UNION (SELECT ---) ORDER BY a;
#
# To cause rows in a UNION result to consist of the sets of rows retrieved by each
# SELECT one after the other, select an additional column in each SELECT to use
# as a sort column and add an ORDER BY following the last SELECT:
#
# 		(SELECT 1 AS sort_col, col1a, col1b, --- FROM t1)
# 		UNION
# 		(SELECT 2, col2a, col2b, --- FROM t2) ORDER BY sort_col;
#
# To additionally maintain sort order within individual SELECT results,
# add a secondary column to the ORDER BY clause:
#
# 		(SELECT 1 AS sort_col, col1a, col1b, --- FROM t1)
# 		UNION
# 		(SELECT 2, col2a, col2b, --- FROM t2) ORDER BY sort_col, col1a;
#
# Use of an additional column also enables you to determine which SELECT
# each row comes from.
#
# Extra columns can provide other identifying information as well, such as
# a string that indicates a table name.
#
# UNION queries with an aggregate function in an ORDER BY clause are rejected
# with an ER_AGGREGATE_ORDER_FOR_UNION error.
#
# Example:
#
# 		SELECT 1 AS foo UNION SELECT 2 ORDER BY MAX(1);
#
# 13.2.11 SUBQUERY SYNTAX
#
# 13.2.11.1 THE SUBQUERY AS SCALAR OPERAND
# 13.2.11.2 COMPARISONS USING SUBQUERIES
# 13.2.11.3 SUBQUERIES WITH ANY, IN, or SOME
#
# 13.2.11.4 SUBQUERIES WITH ALL
# 13.2.11.5 ROW SUBQUERIES
# 13.2.11.6 SUBQUERIES WITH EXISTS OR NOT EXISTS
#
# 13.2.11.7 CORRELATED SUBQUERIES
# 13.2.11.8 DERIVED TABLES
# 13.2.11.9 SUBQUERY ERRORS
#
# 13.2.11.10 OPTIMIZING SUBQUERIES
# 13.2.11.11 REWRITING SUBQUERIES AS JOINS
#
# A subquery is a SELECT statement within another statement.
#
# All subquery forms and operations that the SQL standard requires are supported,
# as well as a few features that are MySQL-specific.
#
# Here is an example of a subquery:
#
# 		SELECT * FROM t1 WHERE column1 = (SELECT column1 FROM t2);
#
# In this example, SELECT * FROM t1 --- is the outer query (or outer statement), and
# (SELECT column1 FROM t2) is the subquery.
#
# We say that the subquery is nested within the outer query, and in fact it is possible
# to nest subqueries within other subqueries, to a considerable depth.
#
# A subquery must always appear within parentheses.
#
# The main advantages of subqueries are:
#
# 		) They allow queries that are structured so that it is possible to isolate each part of a statement
#
# 		) They provide alternative ways to perform operations that would otherwise reuqire complex joins and unions
#
# 		) Many people find subqueries more readable than complex joins or unions.
#
# Here is an example statement that shows the major points about subquery syntax as specified
# by the SQL standard and supported in MySQL:
#
# 		DELETE FROM t1
# 		WHERE s11 > ANY
# 			(SELECT COUNT(*) /* no hint */ FROM t2
# 				WHERE NOT EXISTS
# 					(SELECT * FROM t3
# 						WHERE ROW(5*t2.s1,77)=
# 						 	(SELECT 50,11*s1 FROM t4 UNION SELECT 50,77 FROM
# 								(SELECT * FROM t5) AS t5)));
#
# A subquery can return a scalar (a single value), a single row, a single column or a table
# (one or more rows of one or more columns)
#
# These are called scalar, column, row, and table subqueries.
#
# Subqueries that return a particular kind of result often can be used only in certain
# contexts, as described in teh following sections.
#
# There are few restrictions on the type of statements in which subqueries can be used.
#
# A subquery can contain many of the keywords or clauses that an ordinary SELECT
# can contain:
#
# 		DISTINCT
#
# 		GROUP BY
#
# 		ORDER BY
#
# 		LIMIT
#
# 		joins
#
# 		index hints
#
# 		UNION constructs
#
# 		comments
#
# 		functions
#
# 		etc.
#
# A subquery's outer statement can be any one of:
#
# 		SELECT
#
# 		INSERT
#
# 		UPDATE
#
# 		DELETE
#
# 		SET
#
# 		DO
#
# In MySQL, you cannot modify a table and select from the same table in a subquery.
#
# This applies to statements such as DELETE, INSERT, REPLACE, UPDATE and LOAD DATA INFILE (because subqueries can be used in the SET clause)
#
# For information about how the optimizer handles subqueries, see SECTION 8.2.2,
# "OPTIMIZING SUBQUERIES, DERIVED TABLES, VIEW REFERENCES, AND COMMON TABLE EXPRESSIONS"
#
# For a discussion of restrictions on subquery use, including performance issues for certain
# forms of subquery syntax, see SECTION C.4, "RESTRICTIONS ON SUBQUERIES"
#
# 13.2.11.1 THE SUBQUERY AS SCALAR OPERAND
#
# In its simplest form, a subquery is a scalar subquery that returns a single value.
#
# A scalar subquery is a simple operand, and you can use it almost anywhere a single
# column value or literal is legal, and you can expect it to have those characteristics
# that all operands have:
#
# 		a data type
#
# 		a length
#
# 		indication taht it can be NULL
#
# 		etc.
#
# For example:
#
# 		CREATE TABLE t1 (s1 INT, s2 CHAR(5) NOT NULL);
# 		INSERT INTO t1 VALUES(100, 'abcde');
# 		SELECT (SELECT s2 FROM t1);
#
# The subquery in this SELECT returns a single value ('abcde') that has a data type
# of CHAR, a length of 5, a character set and collation equal to the defaults in effect
# at CREATE_TABLE time, and an indication that the value in the column can be NULL.
#
# Nullability of the value selected by a scalar subquery is not copied because if the
# subquery result is empty,, the result is NULL.
#
# For the subquery just shown, if t1 were empty, the result would be NULL even though
# s2 is NOT NULL
#
# There are a few contexts in which a scalar subquery cannot be used.
#
# If a statement permits only a literal value, you cannot use a subquery.
#
# For example, LIMIT requires literal integer arguments, and LOAD_DATA_INFILE
# requires a literal string file name.
#
# You cannot use subqueries to supply these values.
#
# When you see examples in the following sections that contain the rather 
# spartan construct (SELECT column1 FROM t1), imagine that your own code contains
# much more diverse and complex stuff.
##
# Suppose that we make two tables:
#
# 		CREATE TABLE t1 (s1 INT);
# 		INSERT INTO t1 VALUES(1);
#
# 		CREATE TABLE t2 (s1 INT);
# 		INSERT INTO t2 VALUES(2);
#
# Then perform a SELECT:
#
# 		SELECT (SELECT s1 FROM t2) FROM t1;
#
# The result is 2 because there is a row in t2 containing a column s1 that
# has a value of 2.
#
# A scalar subquery can be part of an expression, but remember the parentheses,
# even if the subquery is an operand that provides an argument for a function.
#
# For example:
#
# 		SELECT UPPER((SELECT s1 FROM t1)) FROM t2;
#
# 13.2.11.2 COMPARISONS USING SUBQUERIES
#
# The most common use of a subquery is in the form:
#
# 		non_subquery_operand comparison_operator (subquery)
#
# Where comparison_operator is one of these operators:
#
# 		= > < >= <= <> != <=>
#
# For example:
#
# 		--- WHERE 'a' = (SELECT column1 FROM t1)
#
# MySQL also permits this construct:
#
# 		non_subquery_operand LIKE (subquery)
#
# At one time the only legal place for a subquery was on the right side of a comparison,
# and you might still find some old DBMSs that insist on this.
#
# Here is an example of a common-form subquery comparison that you cannot do with a join.
#
# It finds all the rows in table t1 for which the column1 value is equal to
# a maximum value in table t2:
#
# 		SELECT * FROM t1
# 			WHERE column1 = (SELECT MAX(column2) FROM t2);
#
# Here is another example, which again is impossible with a join because it
# involves aggregating for one of the tables.
#
# It finds all rows in table t1 containing a value that occurs twice
# in a given column:
#
# 		SELECT * FROM t1 AS t
# 			WHERE 2 = (SELECT COUNT(*) FROM t1 WHERE t1.id = t.id);
#
# For a comparison of the subquery to a scalar, the subquery must return a scalar.
#
# For a comparison of the subquery to a row constructor, the subquery must be a row
# subquery that returns a row with the same number of values as the row constructor.
#
# See SECTION 13.2.11.5, "ROW SUBQUERIES"
#
# 13.2.11.3 SUBQUERIES WITH ANY, IN, OR SOME
#
# Syntax:
#
# 		operand comparison_operator ANY (subquery)
# 		operand IN (subquery)
# 		operand comparison_operator SOME (subquery)
#
# Where comparison_operator is one of these operators:
#
# 		= > < >= <= <> !=
#
# The ANY keyword, which must follow a comparison operator, means
# "return TRUE if the comparison is TRUE for ANY of the values in teh column that the subquery returns"
#
# For example:
#
# 		SELECT s1 FROM t1 WHERE s1 > ANY (SELECT s1 FROM t2);
#
# Suppose that there is a row in table t1 containing (10)
#
# The expression is TRUE if table t2 contains (21, 14, 7) because there is a value
# 7 in t2 that is less than 10.
#
# The expression is FALSE if table t2 contains (20,10), or if table  t2 is empty
#
# The expression is unknown (that is, NULL) if table t2 contains (NULL, NULL, NULL)
#
# When used with a subquery, the word IN is an alias for = ANY
#
# Thus, these two statements are the same:
#
# 		SELECT s1 FROM t1 WHERE s1 = ANY (SELECT s1 FROM t2);
# 		SELECT s1 FROM t1 WHERE s1 IN 	(SELECT s1 FROM t2);
#
# IN and = ANY are not synonyms when used with an expression list.
#
# IN can take an expression list, but = ANY cannot.
#
# See SECTION 12.3.2, "COMPARISON FUNCTIONS AND OPERATORS"
#
# NOT IN is not an alias for <> ANY, but for <> ALL. See SECTION 13.2.11.4,, "SUBQUERIES WITH ALL"
#
# The word SOME is an alias for ANY. Thus, these two statements are the same:
#
# 		SELECT s1 FROM t1 WHERE s1 <> ANY 	(SELECT s1 FROM t2);
# 		SELECT s1 FROM t1 WHERE s1 <> SOME  (SELECT s1 FROM t2);
#
# Use of the word SOME is rare, but this example shows why it might be useful.
#
# To most people, the enlgish phrase "a is not equal to b" means "there is no b which is equal
# to a", but that is not what is meant by the SQL syntax.
#
# The <> syntax means, not equal to or !=
#
# The syntax means "there is some b to which a is not equal"
#
# Using <> SOME instead helps ensure the meaning
#
# 13.2.11.4 SUBQUERIES WITH ALL
#
# Syntax:
#
# 		operand comparison_operator ALL (subquery)
#
# The word ALL which must follow a comparison operator, means "return TRUE if the comparison is TRUE for ALL of the
# values in the column that the subquery returns"
#
# For example:
#
# 		SELECT s1 FROM t1 WHERE s1 > ALL (SELECT s1 FROM t2);
#
# Suppose that there is a row in table t1 containing (10)
#
# The expression is TRUE if table t2 contains (-5,0,+5) because 10 is greater than all
# three values in t2.
#
# The expression is FALSE if table t2 contains (12,6,NULL,-100) beause there is a single
# value 12 in table t2 that is greater than 10.
#
# The expression is unknown (that is, NULL) if table t2 contains (0,NULL,1)
#
# Finally, the expression is TRUE if table t2 is empty.
#
# So, the following expression is TRUE when table t2 is empty:
#
# 		SELECT * FROM t1 WHERE 1 > ALL (SELECT s1 FROM t2);
#
# But this expression is NULL when table t2 is empty:
#
# 		SELECT * FROM t1 WHERE 1 > (SELECT s1 FROM t2);
#
# In addition, the following expression is NULL when table t2 is empty:
#
# 		SELECT * FROM t1 WHERE 1 > ALL (SELECT MAX(s1) FROM t2);
#
# In general, tables containing NULL values and empty tables are "edge cases"
#
# When writing subqueries, always consider whether you have taken those two
# possibilities in account.
#
# NOT IN is an alias for <> ALL. Thus, these two statements are teh same:
#
# 		SELECT s1 FROM t1 WHERE s1 <> ALL (SELECT s1 FROM t2);
# 		SELECT s1 FROM t1 WHERE s1 NOT IN (SELECT s1 FROM t2);
#
# 13.2.11.5 ROW SUBQUERIES
#
# Scalar or column subqueries return a single value or a column of values.
#
# A row subquery is a subquery variant that returns a single row an can thus
# return more than one column value.
#
# Legal operators for row subquery comparisons are:
#
# 		= > < >= <= <> != <=>
#
# Here are two examples:
#
# 		SELECT * FROM t1
# 			WHERE (col1,col2) = (SELECT col3, col4 FROM t2 WHERE id = 10);
# 		SELECT * FROM t1
# 			WHERE ROW(col1,col2) = (SELECT col3, col4 FROM t2 WHERE id = 10);
#
# For both queries, if the table t2 contains a single row with id = 10, the subquery
# returns a single row.
#
# If this row has col3 and col4 values equal to the col1 and col2 values of any rows
# in t1, the WHERE expression is TRUE and each query returns those t1 rows.
#
# If the t2 row col3 and col4 values are not equal the col1 and col2 values
# of any t1 row, the expression is FALSE and the query returns an empty result set.
#
# The expression is unknown(that is,NULL) if the subquery produces no rows.
#
# An error occurs if the subquery produces multiple rows because a row subquery
# can return at most one row.
#
# For information about how each operator work for row comparisons, see SECTION 12.3.2,
# "COMPARISON FUNCTIONS AND OPERATORS"
#
# The expressions (1,2) and ROW(1,2) are sometimes called row constructors.
#
# The two are equivalent.
#
# The row constructor and the row returned by the subquery must contain the same number of values
#
# A row constructor is used for comparisons with subqueries that return two or more columns.
#
# When a subquery returns a single column, this is regarded as a scalar value and not as
# a row, so a row constructor cannot be used with a subquery that does not return at least
# two columns.
#
# Thus, the following query fails with a syntax error:
#
# 		SELECT * FROM t1 WHERE ROW(1) = (SELECT column1 FROM t2)
#
# Row constructors are legal in other contexts.
#
# For example, the following two statements are semantically equivalent 
# (and are handled in the same way by the optimizer):
#
# 		SELECT * FROM t1 WHERE (column1, column2) = (1,1);
# 		SELECT * FROM t1 WHERE column1 = 1 AND column2 = 1;
#
# The following query answers the request, "find all rows in table t1 that also exist in table t2":
#
# 		SELECT column1, column2, column3
# 			FROM t1
# 			WHERE (column1, column2, column3) IN
# 					 (SELECT column1, column2, column3 FROM t2);
#
# For more information about the optimizer and row constructors, see SECTION 8.2.1.20, "ROW CONSTRUCTOR EXPRESSION OPTIMIZATION"
#
# 13.2.11.6 SUBQUERIES WITH EXISTS OR NOT EXISTS
#
# If a subquery returns any rows at all, EXISTS subquery is TRUE, and NOT EXISTS subquery is FALSE.
#
# For example:
#
# 		SELECT column1 FROM t1 WHERE EXISTS (SELECT * FROM t2);
#
# Traditionally, an EXISTS subquery starts with SELECT *, but it could begin with SELECT 5 or SELECT column1
# or anything at all.
#
# MySQL ignores the SELECT list in such a subquery, so it makes no difference.
#
# For the preceding example, if t2 contains any rows, even rows with nothing but NULL values,
# the EXISTS condition is TRUE.
#
# This is actually an unlikely example because a [NOT] EXISTS subquery almost always
# contains correlations.
#
# Here are some more realistic examples:
#
# 		) What kind of store is present in one or more cities
#
# 			SELECT DISTINCT store_type FROM stores
# 				WHERE EXISTS (SELECT * FROM cities_stores
# 								  WHERE cities_stores.store_type = stores.store_type);
#
# 		) What kind of store is present in no cities?
#
# 			SELECT DISTINCT store_type FROM stores
# 				WHERE NOT EXISTS (SELECT * FROM cities_stores
# 										WHERE cities_stores.store_type = stores.store_type);
#
# 		) What kind of store is present in all cities?
#
# 			SELECT DISTINCT store_type FROM stores s1
# 				WHERE NOT EXISTS (
# 					SELECT * FROM cities WHERE NOT EXISTS (
# 						SELECT * FROM cities_stores
# 							WHERE cities_stores.city = cities.city
# 							AND cities_stores.store_type = stores.store_type));
#
# The last example is a double-nested NOT EXISTS query.
#
# That is, it has a NOT EXISTS clause within a NOT EXISTS clause.
#
# Formally, it answers the question "does a city exist with a store that is not in Stores"?
#
# But it is easier to say that a nested NOT EXISTS answers the question "is x TRUE for all y?"
#
# 13.2.11.7 CORRELATED SUBQUERIES
#
# A correlated subquery is a subquery that contains a reference to a table that also appears
# in the outer query.
#
# For example:
#
# 		SELECT * FROM t1
# 			WHERE column1 = ANY (SELECT column1 FROM t2
# 										WHERE t2.column2 = t1.column2);
#
# Notice that the subquery contains a reference to a column of t1, even though
# the subquery's FROM clause does not mention a table t1
#
# So, MySQL looks outside the subquery, and finds t1 in the outer query
#
# Suppose that table t1 contains a row where column1 = 5 and column2 = 6; meanwhile,
# table t2 contains a row where column1 = 5 and column2 = 7
#
# The simple expression --- WHERE column1 = ANY (SELECT column1 FROM t2) would be TRUE,
# but in this example, the WHERE clause within the subquery is FALSE (because (5,6) is
# not equal to (5,7)), so the expression as a whole is FALSE
#
# SCOPING RULE: MySQL evaluates from inside to outside.
#
# For example:
#
# 		SELECT column1 FROM t1 AS x
# 			WHERE x.column1 = (SELECT column1 FROM t2 AS x
# 				WHERE x.column1 = (SELECT column1 FROM t3
# 					WHERE x.column2 = t3.column1));
#
# In this statement, x.column2 must be a column in table t2 because SELECT column1 FROM t2 AS x ---
# renames t2 
#
# It is not a column in table t1 because SELECT column1 FROM t1 --- is an outer query that is farther out
#
# For subqueries in HAVING or ORDER BY clauses, MySQL also looks for column names
# in the outer select list.
#
# For certain cases, a correlated subquery is optimized.
#
# For example:
#
# 		val IN (SELECT key_val FROM tbl_name WHERE correlated_condition)
#
# Otherwise, they are inefficient and likely to be slow.
#
# Rewriting the query as a join might improve performance.
#
# Aggregate functions in correlated subqueries may contain outer references,
# provided the function contains nothing but outer references, and provided the
# function is not contained in another function or expression.
#
# 13.2.11.8 DERIVED TABLES
#
# A derived table is an expression that generates a table within the scope of a query
# FROM clause.
#
# For example, a subquery in a SELECT statement FROM clause is a derived table:
#
# 		SELECT --- FROM (subquery) [AS] tbl_name ---
#
# The JSON_TABLE() function generates a table and provides another way to create a derived table:
#
# 		SELECT * FROM JSON_TABLE(arg_list) [AS] tbl_name ---
#
# The [AS] tbl_name clause is mandatory because every table in a FROM clause must have
# a name.
#
# Any columns in the derived table must have unique names.
#
# Alternatively, tbl_name may be followed by a parenthesized list of names
# for the derived table columns:
#
# 		SELECT --- FROM (subquery) [AS] tbl_name (col_list) ---
#
# The number of column names must be the same as the number of table columns.
#
# For the sake of illustration, assume that you have this table:
#
# 		CREATE TABLE t1 (s1 INT, s2 CHAR(5), s3 FLOAT);
#
# Here is how to use a subquery in the FROM clause, using the example table:
#
# 		INSERT INTO t1 VALUES (1, '1', 1.0);
# 		INSERT INTO t1 VALUES (2, '2', 2.0);
# 		SELECT sb1,sb2,sb3
# 			FROM (SELECT s1 AS sb1, s2 AS sb2, s3*2 AS sb3 FROM t1) AS sb
# 			WHERE sb1 > 1;
#
# Result:
#
# 		+-------+---------+-----------+
# 		| sb1   | sb2 	   | sb3 		|
# 		+-------+---------+-----------+
# 		| 2 	  | 2 		| 4 			|
# 		+-------+---------+-----------+
#
# Here is another example: Suppose that you want to know the average of a set
# of sums for a grouped table.
#
# This does not work:
#
# 		SELECT AVG(SUM(column1)) FROM t1 GROUP BY column1;
#
# However, this query provides the desired information:
#
# 		SELECT AVG(sum_column1)
# 			FROM (SELECT SUM(column1) AS sum_column1
# 					FROM t1 GROUP BY column1) AS t1;
#
# Notice that the column name used within the subquery (sum_column1) is recognized
# in the outer query.
#
# The column names for a derived table come from its select list:
#
# 		SELECT * FROM (SELECT 1, 2, 3, 4) AS dt;
# 		+----+-----+------+------+
# 		| 1  | 2   | 3    | 4    |
# 		+----+-----+------+------+
# 		| 1  | 2   | 3    | 4 	 |
# 		+----+-----+------+------+
#
# To provide column names explicitly, follow the derived table
# name with a parenthesized list of column names:
#
# 		SELECT * FROM (SELECT 1, 2, 3, 4) AS dt (a, b, c, d);
# 		+----+------+------+---------+
# 		| a  | b    | c 	 | d 		  |
# 		+----+------+------+---------+
# 		| 1  | 2    | 3    | 4 		  |
# 		+----+------+------+---------+
#
# A derived table can return a scalar, column, row or table
#
# Derived tables are subject to these restrictions:
#
# 		) A derived table cannot be a correlated subquery
#
# 		) A derived table cannot contain references to other tables of the same SELECT
#
# 		) Prior to MySQL 8.0.14, a derived table cannot contain outer references.
#
# 			This is a MySQL restriction that is lifted in MySQL 8.0.14, not a restriction
# 			of the SQL standard.
#
# 			For example, the derived table dt in the following query contains a reference
# 			t1.b to the table t1 in the outer query:
#
# 				WHERE t1.d > (SELECT AVG(dt.a)
# 									FROM (SELECT SUM(t2.a) AS a FROM
# 											t2 WHERE t2.b = t1.b GROUP BY t2.c) dt
# 								WHERE dt.a > 10);
#
# 			The query is valid in MySQL 8.0.14 and higher.
#
# 			Before 8.0.14, it produces an error: 
#
# 				Unknown column 't1.b' in 'where clause'
#
# The optimizer determines information about derived tables in such a way that
# EXPLAIN does not need to materialize them.
#
# See SECTION 8.2.2.4, "OPTIMIZING DERIVED TABLES, VIEW REFERENCES, AND COMMON TABLE
# EXPRESSIONS WITH MERGING OR MATERIALIZATION"
#
# It is possible under certain circumstances that using EXPLAIN_SELECT will modify table
# data.
#
# This can occur if the outer query accesses any tables and an inner query invokes
# a stored function that changes one or more rows of a table.
#
# Suppose that there are two tables t1 and t2 in database d1, and a stored function
# f1 that modifies t2, created as shown here:
#
# 		CREATE DATABASE d1;
# 		USE d1;
# 		CREATE TABLE t1 (c1 INT);
# 		CREATE TABLE t2 (c1 INT);
# 		CREATE FUNCTION f1(p1 INT) RETURNS INT
# 			BEGIN
# 				INSERT INTO t2 VALUES (p1);
# 				RETURN p1;
# 			END;
#
# Referencing the function directly in an EXPLAIN_SELECT has no effect on t2,
# as shown here:
#
# 		SELECT * FROM t2;
# 		Empty set (0.02 sec)
#
# 		EXPLAIN SELECT f1(5)\G
# 		******************************* 1. row ***********************************
# 
# 								id: 1
# 					select_type: SIMPLE
# 							table: NULL
# 					partitions : NULL
# 							type : NULL
# 				possible_keys : NULL
# 				key 			  : NULL
# 						key_len : NULL
# 							ref  : NULL
# 							rows : NULL
# 						filtered: NULL
# 							Extra: No tables used
# 				1 row in set (0.01 sec)
#
# 		SELECT * FROM t2;
# 		Empty set (0.01 sec)
#
# This is because the SELECT statement did not reference any tables, as can be seen in the 
# table and Extra columns of the output.
#
# This is also true of the following nested SELECT:
#
# 		EXPLAIN SELECT NOW() AS a1, (SELECT f1(5)) AS a2\G
# 		************************* 1. row *****************************
# 
# 								id: 1
# 				select_type   : PRIMARY
# 							table: NULL
# 						   type : NULL
# 				possible_keys : NULL
# 							  key: NULL
# 						key_len : NULL
# 							  ref: NULL
# 							rows : NULL
# 						filtered: NULL
# 							Extra: No tables used
# 		1 row in set, 1 warning (0.00 sec)
#
# 		SHOW WARNINGS;
# 		+--------+----------+--------------------------------------------+
# 		| Level  | Code 	  | Message 											  |
# 		+--------+----------+--------------------------------------------+
# 		| Note   | 1249     | Select 2 was reduced during optimization   |
# 		+--------+----------+--------------------------------------------+
# 		1 row in set (0.00 sec)
#
# 		SELECT * FROM t2;
# 		Empty set (0.00 se)
#
# However, if the outer SELECT references any tables, the optimizer executes the
# statement in the subquery as well, with the result that t2 is modified:
#
# 		EXPLAIN SELECT * FROM t1 AS a1, (SELECT f1(5)) AS a2\G
# 		**************************** 1. row *****************************
# 						id: 1
# 			select_type: PRIMARY
# 					table: <derived2>
# 			partitions : NULL
# 					type : system
# 		possible_keys : NULL
# 					  key: NULL
# 				key_len : NULL
# 					  ref: NULL
# 					 rows: 1
# 				filtered: 100.00
# 					Extra: NULL
# 		*************************** 2. row *******************************
# 						id: 1
# 			select_type: PRIMARY
# 					table: a1
# 			partitions : NULL
# 					type : ALL
# 		possible_keys : NULL
# 					key  : NULL
# 				key_len : NULL
# 					ref  : NULL
# 					rows : 1
# 			filtered   : 100.00
# 					Extra: NULL
# 		************************* 3. row *********************************
# 						id: 2
# 			select_type: DERIVED
# 					table: NULL
# 			partitions : NULL
# 					type : NULL
# 		possible_keys : NULL
# 					key  : NULL
# 				key_len : NULL
# 					ref  : NULL
# 					rows : NULL
# 				filtered: NULL
# 					Extra: No tables used
# 		3 rows in set (0.00 sec)
#
# 		SELECT * FROM t2;
# 		+--------+
# 		| c1 	   |
# 		+--------+
# 		| 5 	   |
# 		+--------+
# 		1 row in set (0.00 sec)
#
# This also means that an EXPLAIN_SELECT statement such as the one shown
# here may take a long time to execute because the BENCHMARK() function
# is executed once for each row in t1:
#
# 		EXPLAIN SELECT * FROM t1 AS a1, (SELECT BENCHMARK(1000000, MD5(NOW())));
#
# 13.2.11.9 SUBQUERY ERRORS
#
# There are some errors that apply only to subqueries. This section describes them.
#
# 		) Unsupported subquery syntax:
#
	# 			ERROR 1235 (ER_NOT_SUPPORTED_YET)
	# 			SQLSTATE = 42000
	# 			Message = "This version of MySQL doesn't yet support
	# 			'LIMIT & IN/ALL/ANY/SOME subquery'"
	#
# 			This means that MySQL does not support statements of the following form:
#
# 				SELECT * FROM t1 WHERE s1 IN (SELECT s2 FROM t2 ORDER BY s1 LIMIT 1)
#
# 		) Incorrect number of columns from subquery:
#
# 				ERROR 1241 (ER_OPERAND_COL)
# 				SQLSTATE = 21000
# 				Message = "Operand should contain 1 column(s)"
#
# 			This error occurs in cases like this:
#
# 				SELECT (SELECT column1, column2 FROM t2) FROM t1;
#
# 			You may use a subquery that returns multiple columns, if the purpose is row
# 			comparison.
#
# 			In other contexts, the subquery must be a scalar operand.
#
# 			See SECTION 13.2.11.5, "ROW SUBQUERIES"
#
# 		) Incorrect number of rows from subquery:
#
# 				ERROR 1242 (ER_SUBSELECT_NO_1_ROW)
# 				SQLSTATE = 21000
# 				Message = "Subquery returns more than 1 row"
#
# 		 	This error occurs for statements where the subquery must return at most one row
# 			but returns multiple rows.
#
# 			Consider the following example:
#
# 				SELECT * FROM t1 WHERE column1 = (SELECT column1 FROM t2);
#
# 			If SELECT column1 FROM t2 returns just one row,, the previous query will work.
#
# 			If the subquery returns more thhan one row, error 1242 will occur.
#
# 			In that case, the query should be rewritten as:
#
# 				SELECT * FROM t1 WHERE column1 = ANY (SELECT column1 FROM t2);
#
# 		) Incorrectly used table in subquery:
#
# 				Error 1093 (ER_UPDATE_TABLE_USED)
# 				SQLSTATE = HY000
# 				Message = "You can't specify target table 'x'
# 				for update in FROM clause"
#
# 			This error occurs in cases such as the following, which attempts to modify a table
# 			and select from the same table in the subquery:
#
# 				UPDATE t1 SET column2 = (SELECT MAX(column1) FROM t1);
#
# 			You can use a subquery for assignment within an UPDATE statement because subqueries
# 			are legal in UPDATE and DELETE statements as well as in SELECT statements.
#
# 			However, you cannot use the same table (in this case, table t1) for both the subquery
# 			FROM clause and the update target.
#
# For transactional storage engines, the failure of a subquery causes the entire statement to fail.
#
# For nontransactional storage engines, data modifications made before the error was
# encountered are preserved.
#
# 13.2.11.10 OPTIMIZING SUBQUERIES
#
# Development is ongoing, so no optimization tip is reliable for the long term.
#
# The following list provides some interesting tricks that you might want to paly
# with.
#
# See also SECTION 8.2.2, "OPTIMIZING SUBQUERIES, DERIVED TABLES, VIEW REFERENCES, AND COMMON TABLE EXPRESSIONS"
#
# 		) Use subquery clauses that affect the number or order of the rows in the subquery.
#
# 			For example:
#
# 				SELECT * FROM t1 WHERE t1.column1 IN
# 					(SELECT column1 FROM t2 ORDER BY column1);
#
# 				SELECT * FROM t1 WHERE t1.column1 IN
# 					(SELECT DISTINCT column1 FROM t2);
#
# 				SELECT * FROM t1 WHERE EXISTS 
# 					(SELECT * FROM t2 LIMIT 1);
#
# 		) Replace a join with a subquery. For example, try this:
#
# 				SELECT DISTINCT column1 FROM t1 WHERE t1.column1 IN (
# 					SELECT column1 FROM t2);
#
# 			Instead of this:
#
# 				SELECT DISTINCT t1.column1 FROM t1, t2
# 					WHERE t1.column1 = t2.column1;
#
# 		) Some subqueries can be transformed to joins for compatibility with older versions
# 			of MySQL that do not support subqueries.
#
# 			However, in some cases, converting a subquery to a join may improve performance.
#
# 			See SECTION 13.2.11.11, "REWRITING SUBQUERIES AS JOINS"
#
# 		) Move clauses from outside to inside the subquery. For example, use  this query:
#
# 				SELECT * FROM t1
# 					WHERE s1 IN (SELECT s1 FROM t1 UNION ALL SELECT s1 FROM t2);
#
# 			Instead of this query:
#
# 				SELECT * FROM t1
# 					WHERE s1 IN (SELECT s1 FROM t1) OR s1 IN (SELECT s1 FROM t2);
#
# 			For another example, use this query:
#
# 				SELECT (SELECT column1 + 5 FROM t1) FROM t2;
#
# 			Instead of this query:
#
# 				SELECT (SELECT column1 FROM t1) + 5 FROM t2;
#
# 		) Use a row subquery instead of a correlated subquery. For example, use this query:
#
# 				SELECT * FROM t1
# 					WHERE (column1,column2) IN (SELECT column1,column2 FROM t2);
#
# 			Instead of this query:
#
# 				SELECT * FROM t1
# 					WHERE EXISTS (SELECT * FROM t2 WHERE t2.column1=t1.column1
# 									  AND t2.column2=t1.column2);
#
# 		) Use NOT (a = ANY (---)) rather than a <> ALL (---)
#
# 		) Use x = ANY (table containing (1,2)) rather than x=1 OR x=2
#
# 		) Use = ANY rather than exists
#
# 		) For uncorrelated subqueries that always return one row, IN is always slower than =
#
# 			For example, use this query:
#
# 				SELECT * FROM t1
# 					WHERE t1.col_name = (SELECT a FROM t2 WHERE b = some_const);
#
# 			Instead of this query:
#
# 				SELECT * FROM t1
# 					WHERE t1.col_name IN (SELECT a FROM t2 WHERE b = some_const);
#
# These tricks might cause programs to go faster or slower.
#
# Using MySQL facilities like the BENCHMARK() function, you can get an idea
# about what helps in your own situation.
#
# See SECTION 12.15, "INFORMATION FUNCTIONS"
#
# Some optimizations that MySQL itself makes are:
#
# 		) MySQL executes uncorrelated subqueries only once.
#
# 			Use EXPLAIN to make sure that a given subquery really is uncorrelated.
#
# 		) MySQL rewrites IN, ALL, ANY and SOME subqueries in an attempt to take advantage
# 			of the possibility that the select-list columns in the subquery are indexed.
#
# 		) MySQL replaces subqueries of the following form with an index-lookup function,
# 			which EXPLAIN describes as a special join type (unique_subquery or index_subquery):
#
# 			--- IN (SELECT indexed_column FROM single_table ---)
#
# 		) MySQL enhances expressions of the following form with an expression involving MIN() or MAX(),
# 			unless NULL values or empty sets are involved:
#
# 				value {ALL|ANY|SOME} {> | < | >= | <=} (uncorrelated subquery)
#
# 			For example, this WHERE clause:
#
# 				WHERE 5 > ALL (SELECT x FROM t)
#
# 			might be treated by the optimizer like this:
#
# 				WHERE 5 > (SELECT MAX(x) FROM t)
#
# See also MYSQL INTERALS: HOW MYSQL TRANSFORMS SUBQUERIES
#
# 13.2.11.11 REWRITING SUBQUERIES AS JOINS
#
# Sometimes there are other ways to test membership in a set of values than by using a subquery.
#
# Also, on some ocassions - it is not only possible to rewrite a query without a subquery,
# but it can be more efficient to make use of some of these techniques rather than to use
# subqueries.
#
# One of these is the IN() construct:
#
# For example, this query:
#
# 			SELECT * FROM t1 WHERE id IN (SELECT id FROM t2);
#
# Can be rewritten as:
#
# 			SELECT DISTINCT t1.* FROM t1, t2 WHERE t1.id=t2.id;
#
# The queries:
#
# 		SELECT * FROM t1 WHERE id NOT IN (SELECT id FROM t2);
# 		SELECT * FROM t1 WHERE NOT EXISTS (SELECT id FROM t2 WHERE t1.id=t2.id);
#
# can be rewritten as:
#
# 		SELECT table1.*
# 			FROM table1 LEFT JOIN table2 ON table1.id=table2.id
# 			WHERE table2.id IS NULL;
#
# A LEFT [OUTER] JOIN can be faster than an equivalent subquery because the server might
# be able to optimizer it better - a fact that is not specific to MySQL Server alone.
#
# Prior to SQL-92, outer joins did not exist, so subqueries were the only way to do
# certain things.
#
# Today, MySQL Server and many other modern database systems offer a wide range
# of outer join types.
#
# MySQL Server supports multiple-table DELETE statements that can be used to efficiently
# delete rows based on information from one table or even from many tables at the
# same time.
#
# Multiple-table UPDATE statements are also supported.
#
# See SECTION 13.2.2, "DELETE SYNTAX" and SECTION 13.2.12, "UPDATE SYNTAX"
#
# 13.2.12 UPDATE SYNTAX
#
# UPDATE is a DML statement that modifies rows in a table.
#
# An UPDATE statement can start with a WITH clause to define common table
# expressions accessible within the UPDATE.
#
# See SECTION 13.2.13, "WITH SYNTAX (COMMON TABLE EXPRESSIONS)"
#
# Single-table syntax:
#
# 		UPDATE [LOW_PRIORITY] [IGNORE] table_reference
# 			SET assignment_list
# 			[WHERE where_condition]
# 			[ORDER BY ---]
# 			[LIMIT row_count]
#
# 		value:
# 			{expr | DEFAULT}
#
# 		assignment:
# 			col_name = value
#
# 		assignment_list:
# 			assignment [, assignment] ---
#
# Multiple-table syntax:
#
# 		UPDATE [LOW_PRIORITY] [IGNORE] table_references
# 			SET assignment_list
# 			[WHERE where_condition]
#
# For the single-table syntax, the UPDATE statement updates columns of existing
# rows in the named table with new values.
#
# The SET clause indicates which columns to modify and the values they should be given.
#
# Each value can be given as an expression, or the keyword DEFAULT to set a column
# explicitly to its default value.
#
# The WHERE clause, if given, specifies the conditions that identify which rows to update.
#
# With no WHERE clause, all rows are updated.
#
# If the ORDER BY clause is specified, the rows are updated in the order that is specified.
# The LIMIT clause places a limit on the number of rows that can be updated.
#
# For the multiple-table syntax, UPDATE updates rows in each table named in table_references
# that satisfy the conditions.
#
# Each matching row is updated once, even if it matches the conditions multiple times.
#
# For multiple-table syntax, ORDER BY and LIMIT cannot be used.
#
# For partitioned tables, both the single-single and multiple-table forms of this statement
# support the use of a PARTITION option as part of a table reference.
#
# This option takes a list of one or more partitions or subpartitions (or both)
#
# Only the partitions (or subpartitions) listed are checked for matches, and a row that
# is not in any of these partitions or subpartitions is not updated, whether it satisfies
# the where_condition or not.
#
# NOTE:
#
# 		Unlike the case when using PARTITION with an INSERT or REPLACE statement, an otherwise
# 		valid UPDATE --- PARTITION statement is considered successful even if no rows in the
# 		listed partitions (or subpartitions) match the where_condition
#
# For more information and examples, see SECTION 23.5, "PARTITION SELECTION"
#
# where_condition is an expression that evaluates to true for each row to be updated:
# For expression syntax, see SECTION 9.5, "EXPRESSIONS"
#
# table_references and where_condition are specified as described in SECTION 13.2.10, "SELECT SYNTAX"
#
# You need the UPDATE privilege only for columns referenced in an UPDATE that are actually updated.
#
# You need only the SELECT privilege for any columns that are read but not modified.
#
# The UPDATE statement supports the following modifiers:
#
# 		) With the LOW_PRIORITY modifier, execution of the UPDATE is delayed until no other clients
# 			are reading from the table.
#
# 			This affects only storage engines that use only table-level locking (such as MyISAM, MEMORY, and MERGE)
#
# 		) With the IGNORE modifier, the update statement does not abort even if errors occur during the update.
#
# 			Rows for which duplicate-key conflicts occur on a unique key value are not updated.
#
# 			Rows updated to values that would cause data conversion errors are updated to the closest
# 			valid values instead.
#
# 			For more information, see COMPARISON OF THE IGNORE KEYWORD AND STRICT SQL MODE
#
# UPDATE_IGNORE statements, including those having an ORDER BY clause, are flagged as unsafe
# for statement-based replication.
#
# (This is because the order in which the rows are updated determines which rows are ignored)
#
# Such statements produce a warning in the error log when using statement-based mode and are 
# written to the binary log using the row-based format when using MIXED mode.
#
# (Bug #11758262, Bug #50439)
#
# See SECTION 17.2.1.3, "DETERMINATION OF SAFE AND UNSAFE STATEMENTS IN BINARY LOGGING"
# for more information.
#
# If  you access a column from the table to be updated in an expression, UPDATE uses the current
# value of the column.
#
# For example, the following statement sets col1 to one more than its current value:
#
# 		UPDATE t1 SET col1 = col1 + 1;
#
# The second assignment in the following statement sets col2 to the current (updated)
# col1 value, not the original col1 value.
#
# The result is that col1 and col2 have the same value.
#
# This behavior differs from standard SQL.
#
# 		UPDATE t1 SET col1 = col1 + 1, col2 = col1;
#
# Single-table UPDATE assignments are generally evaluated from left to right.
#
# For multiple-table updates, there is no guarantee that assignments are carried
# out in any particular order.
#
# If you set a column to the value it currently has, MySQL notices this and does
# not update it.
#
# If you update a column that has been declared NOT NULL by setting to NULL, an error
# occurs if strict SQL mode is enabled; otherwise, the column is set to the implicit
# default value for the column data type and the warning count is incremented.
#
# The implicit default value is 0 for numeric types, the empty string('') for string
# types, and the "zero" value for date and time types.
#
# See SECTION 11.7, "DATA TYPE DEFAULT VALUES"
#
# If a generated column is updated explicitly, the only permitted value is DEFAULT.
#
# For information about generated columns, see SECTION 13.1.20.8, "CREATE TABLE AND GENERATED COLUMNS"
#
# UPDATE returns the number of rows that were actually changed.
#
# The mysql_info() C API function returns the number of rows that were matched and updated
# and the number of warnings that occurred during the UPDATE.
#
# You can use LIMIT row_count to restrict the scope of the UPDATE 
#
# A LIMIT clause is a rows-matched restriction. The statement stops as soon as it has found
# row_count rows that satisfy the WHERE clause, whether or not they actually were changed.
#
# If an UPDATE statement includes an ORDER BY clause, the rows are updated in the order specified
# by the clause.
#
# This can be useful in certain situations that might otherwise result in an error.
#
# Suppose that a table t contains a column id that has a unique index.
#
# The following statement could fail with a duplicate-key error, depending
# on the order in which rows are updated:
#
# 		UPDATE t SET id = id + 1;
#
# For example, if the table contains 1 and 2 in the id column and 1 is updated
# to 2 before 2 is updated to 3, an error occurs.
#
# To avoid this problem, add an ORDER BY clause to cause the rows with larger
# id values to be updated before those with smaller values:
#
# 		UPDATE t SET id = id + 1 ORDER BY id DESC;
#
# You can also perform UPDATE operations covering multiple tables.
#
# However, you cannot use ORDER BY or LIMIT with a multiple-table UPDATE.
#
# The table_references clause lists the tables involved in teh join.
#
# Its syntax is described in SECTION 13.2.10.2, "JOIN SYNTAX"
#
# Here is an example:
#
# 		UPDATE items,month SET items.price=month.price
# 		WHERE items.id=month.id;
#
# The preceding example shows an inner join that uses the comma operator, but multiple-table
# UPDATE statements can use any type of join permitted in SELECT statements, such as LEFT JOIN.
#
# If you use a multiple-table UPDATE statement involving InnoDB tables for which there are
# foreign key constraints, the MySQL optimizer might process tables in an order that
# differs from that of their parent/child relationship.
#
# In this case, the statement fails and rolls back.
#
# Instead, update a single table and rely on the ON UPDATE capabilities that
# InnoDB provides to cause the other tables to be modified accordingly.
#
# See SECTION 15.6.1.5, "INNODB AND FOREIGN KEY CONSTRAINTS"
#
# You cannot update a table and select from the same table in a subquery.
#
# An UPDATE on a partitioned table using a storage engine such as MyISAM that employs
# table-level locks locks only those partitions containing rows that match the
# UPDATE statement WHERE clause, as long as none of the table partitioning columns
# are updated.
#
# (For storage engines such as InnoDB that employ row-level locking, no locking of
# partitions takes place)
#
# For more information, see PARTITIONING AND LOCKING
#
# 13.2.13 WITH SYNTAX (COMMON TABLE EXPRESSIONS)
#
# A common table expression (CTE) is a named temporary result set that exists
# within the scope of a single statement and that can be referred to later
# within that statement, possibly multiple times.
#
# The following discussion describes how to write statements that use CTEs.
#
# 		) COMMON TABLE EXPRESISON SYNTAX
#
# 		) RECURSIVE COMMON TABLE EXPRESSIONS
#
# 		) LIMITING COMMON TABLE EXPRESSION RECURSION
#
# 		) RECURSIVE COMMON TABLE EXPRESSION EXAMPLES
#
# 		) COMMON TABLE EXPRESSIONS COMPARED TO SIMILAR CONSTRUCTS
#
# For informaiton about CTE optimization, see SECTION 8.2.2.4, "OPTIMIZING
# DERIVED TABLES, VIEW REFERENCES, AND COMMON TABLE EXPRESSIONS WITH MERGING OR MATERIALIZATION"
#
# ADDITIONAL RESOURCES
#
# These articles contain additional information about using CTEs in MySQL, including many examples:
#
# 		<LINKS, RETURN TO AFTER COMPLETED SECTION>
#
# COMMON TABLE EXPRESSION SYNTAX
#
# To specify common table expressions, use a WITH clause that has one or more comma-separated
# subclauses.
#
# Each subclause provides a subquery that produces a result set and associates a name with the
# subquery.
#
# The following example defines CTEs named cte1 and cte2 in the WITH clause, and refers to them
# in the top-level SELECT that follows the WITH clause:
#
# 		WITH
# 			cte1 AS (SELECT a, b FROM table1),
# 			cte2 AS (SELECT c, d FROM table2)
# 		SELECT b, d FROM cte1 JOIN cte2
# 		WHERE cte1.a = cte2.c;
#
# In the statement containing the WITH clause, each CTE name can be referenced
# to access the corresponding CTE result set.
#
# A CTE name can be referenced in other CTEs, enabling CTEs to be defined based
# on other CTEs.
#
# A CTE can refer to itself to define a recursive CTE
#
# Common applications of recursive CTEs include series generation and traversal
# of hierarchial or tree-structured data.
#
# Common table expressions are an optional part of the syntax for DML statements.
#
# They are defined using a WITH clause:
#
# 		with_clause:
# 			WITH [RECURSIVE]
# 				cte_name [(col_name [, col_name] ---)] AS (subquery)
# 				[, cte_name [(col_name [, col_name] ---)] AS (subquery)] ---
#
# cte_name names a single common table expression and can be used as a table
# reference in the statement containing the WITH clause.
#
# The subquery part of AS (subquery) is called the "subquery of the CTE" and is what
# produces the CTE result set.
#
# The parentheses following AS are required.
#
# A common table expression is recursive if its subquery refers to its own name.
#
# The RECURSIVE keyword must be included if any CTE in the WITH clause is recursive.
#
# For more information, see RECURSIVE COMMON TABLE EXPRESSIONS
#
# Determination of column names for a given CTE occurs as follows:
#
# 		) If a parenthesized list of names follows the CTE name, those names are the column names:
#
# 			WITH cte (col1, col2) AS
# 			(
# 				SELECT 1, 2
# 				UNION ALL
# 				SELECT 3, 4
# 			)
# 			SELECT col1, col2 FROM cte;
#
# 			The number of names in the list must be the same as the number of columns in the result set.
#
# 		) Otherwise, the column names come from the select list of the first SELECT within the AS (subquery) part:
#
# 			WITH cte AS
# 			(
# 				SELECT 1 AS col1, 2 AS col2
# 				UNION ALL
# 				SELECT 3, 4
# 			)
# 			SELECT col1, col2 FROM cte;
#
# A WITH clause is permitted in these contexts:
#
# 		) At the beginning of SELECT, UPDATE and DELETE statements.
#
# 			WITH --- SELECT ---
# 			WITH --- UPDATE ---
# 			WITH --- DELETE ---
#
# 		) At the beginning of subqueries (including derived table subqueries):
#
# 			SELECT --- WHERE id IN (WITH --- SELECT ---) ---
# 			SELECT * FROM (WITH --- SELECT ---) AS dt ---
#
# 		) Immediately preceding SELECT for statements that include a SELECT statement:
#
# 			INSERT --- WITH --- SELECT ---
# 			REPLACE --- WITH --- SELECT ---
# 		
# 			CREATE TABLE --- WITH --- SELECT ---
# 			CREATE VIEW --- WITH --- SELECT ---
#
# 			DECLARE CURSOR --- WITH --- SELECT ---
# 			EXPLAIN --- WITH --- SELECT ---
#
# Only one WITH clause is permitted at the same level.
#
# WITH followed by WITH at the same level is not permitted, so this is illegal:
#
# 		WITH cte1 AS (---) WITH cte2 AS (---) SELECT ---
#
# To make the statement legal, use a single WITH clause that separates
# the subclauses by a comma:
#
# 		WITH cte1 AS (---), cte2 AS (---) SELECT ---
#
# However, a statement can contain multiple WITH clauses if they occur at different
# levels:
#
# 		WITH cte1 AS (SELECT 1)
# 		SELECT * FROM (WITH cte2 AS (SELECT 2) SELECT * FROM cte2 JOIN cte1) AS dt;
#
# A WITH clause can define one or more common table expressions, but each CTE name
# must be unique to the clause. This is illegal:
#
# 		WITH cte1 AS (---), cte1 AS (---) SELECT ---
#
# To make the statement legal, define the CTEs with unique names:
#
# 		WITH cte1 AS (---), cte2 AS (---) SELECT ---
#
# A CTE can refer to itself or to other CTEs:
#
# 		) A self-referencing CTE is recursive
#
# 		) A CTE can refer to CTEs defined earlier in the same WITH clause, but not those defined later.
#
# 		  This constraint rules out mutually-recursive CTEs, where cte1 references cte2 and cte2
# 			references cte1.
#
# 			One of those references must be to a CTE defined later, which is not permitted.
#
# 		) A CTE in a given query block can refer to CTEs defined in query blocks at a more outer level,
# 			but not CTEs defined in query blocks at a more inner level.
#
# For resolving references to objects with the same names, derived tables hide CTEs; and CTEs
# hide base tables, TEMPORARY tables, and views.
#
# Name resolution occurs by seaching for objects in the same query block, then proceeding
# to outer blocks in turn while no object with the name is found.
#
# Like derived tables, a CTE cannot contain outer references prior to MySQL 8.0.14
#
# This is a MySQL restriction that is lifted in MySQL 8.0.14, not a restriction of the
# SQL standard.
#
# For additional syntax considerations specific to recursive CTEs, see RECURSIVE COMMON TABLE EXPRESSIONS
#
# RECURSIVE COMMON TABLE EXPRESSIONS
#
# A recursive common table expression is one having a subquery that refers to its own name.
#
# For example:
#
# 		WITH RECURSIVE cte (n) AS
# 		(
# 			SELECT 1
# 			UNION ALL
# 			SELECT n + 1 FROM cte WHERE n < 5
# 		)
# 		SELECT * FROM cte;
#
# When executed, the statement produces this result, a single column containing a simple
# linear sequence:
#
# 		+--------+
# 		| n 		|
# 		+--------+
# 		| 1 	   |
# 		| 2 		|
# 		| 3 	   |
# 		| 4 	   |
# 		| 5 		|
# 		+--------+
#
# A recursive CTE has this structure:
#
# 		) The WITH clause must begin with WITH RECURSIVE if any CTE in the WITH clause refers to itself.
#
# 			(If no CTE refers to itself, RECURSIVE is permitted but not required)
#
# 			If you forget RECURSIVE for a recursive CTE, this error is a likely result:
#
# 				ERROR 1146 (42S02): Table 'cte_name' doesn't exist
#
# 		) The recursive CTE subquery has two parts, separated by UNION_[ALL] or UNION_DISTINCT:
#
# 			SELECT --- 			--- return initial row set
# 			UNION ALL
# 			SELECT --- 			--- return additional row sets
#
# 			The first SELECT produces the initial row or rows for the CTE and does not refer to the
# 			CTE name.
#
# 			The second SELECT produces additional rows and recurses by referring to the CTE name
# 			in its FROM clause.
#
# 			Recursion ends when this part produces no new rows.
#
# 			Thus, a recursive CTE consists of a nonrecursive SELECT part followed by a recursive SELECT part.
#
# 			Each SELECT part can itself be a union of multiple SELECT statements.
#
# 		) The types of the CTE result columns are inferred from the column types of the nonrecursive
# 			SELECT part only, and the columns are all nullable.
#
# 			For type determination, the recursive SELECT part is ignored.
#
# 		) If the nonrecursive and recursive parts are separated by UNION_DISTINCT, duplicate rows
# 			are eliminated.
#
# 			This is useful for queries that perform transitive closures, to avoid infinite loops.
#
# 		) Each iteration of the recursive part operates only on the rows produced by the previous iteration.
#
# 			If the recursive part has multiple query blocks, iterations of each query block are
# 			scheduled in unspecified order, and each query block operates on rows that have been
# 			produced either by its previous iteration or by other query blocks since that previous
# 			iteration's end.
#
# The recursive CTE subquery shown earlier has this nonrecursive part that retrieves a single row
# to produce the initial row set:
#
# 		SELECT 1
#
# The CTE subquery also has this recursive part:
#
# 		SELECT n + 1 FROM cte WHERE n < 5
#
# At each iteration, that SELECT produces a row within a new value one greater than the
# value of n from the previous row set.
#
# The first iteration operates on the initial row set (1) and produces 1+1=2; 
#
# The second iteration operates on the first iteration's row set (2) and produces
# 2+1=3; and so forth.
#
# This continues until recursion ends, which occurs when n is no longer less than 5
#
# If the recursive part of a CTE produces wider values for a column than the nonrecursive
# part, it may be necessary to widen the column in the nonrecursive part to avoid
# data truncation.
#
# Consider this statement:
#
# 		WITH RECURSIVE cte AS
# 		(
# 			SELECT 1 AS n, 'abc' AS str
# 			UNION ALL
# 			SELECT n + 1, CONCAT(str, str) FROM cte WHERE n < 3
# 		)
# 		SELECT * FROM cte;
#
# In nonstrict SQL mode, the statement produces this output:
#
# 		+--------+----------+
# 		| n 		| str 	  |
# 		+--------+----------+
# 		| 1 		| abc 	  |
# 		| 2 		| abc 	  |
# 		| 3 		| abc 	  |
# 		+--------+----------+
#
# The str column values are all 'abc' because the nonrecursive SELECT determines
# the column widths.
#
# Consequently, the wider str values produced by the recursive SELECT are truncated.
#
# In strict SQL mode, the statement produces an error:
#
# 		ERROR 1406 (22001): Data too long for column 'str' at row 1
#
# To address this issue, so that the statement does not produce truncation or errors,
# use CAST() in the nonrecursive SELECT to make the str column wider:
#
# 		WITH RECURSIVE cte AS
# 		(
# 			SELECT 1 AS n, CAST('abc' AS CHAR(20)) AS str
# 			UNION ALL
# 			SELECT n + 1, CONCAT(str, str) FROM cte WHERE n < 3
# 		)
# 		SELECT * FROM cte;
#
# Now the statement produces this result, without truncation:
#
# 		+-----+-----------------+
# 		| n   | str 				|
# 		+-----+-----------------+
# 		| 1   | abc 				|
# 		| 2   | abcabc 			|
# 		| 3   | abcabcabcabc 	|
# 		+-----+-----------------+
#
# Columns are accessed by name, not position, which means that columns in the recursive
# part can access columns in the nonrecursive part that have a different position, as
# this CTE illustrates:
#
# 		WITH RECURSIVE cte AS
# 		(
# 			SELECT 1 AS n, 1 AS p, -1 AS q
# 			UNION ALL
# 			SELECT n + 1, q * 2, p * 2 FROM cte WHERE n < 5
# 		)
# 		SELECT * FROM cte;
#
# Because p in one row is derived from q in the previous row, and vice versa,
# the positive and negative values values swap positions in each successive
# row of the output:
#
# 		+--------+-----------+------------+
# 		| n 	   | p 		   | q 			 |
# 		+--------+-----------+------------+
# 		| 1 		| 1 			| -1 			 |
# 		| 2 		| -2 			| 2 			 |
# 		| 3 		| 4 			| -4 			 |
# 		| 4 		| -8 			| 8 			 |
# 		| 5 		| 16 			| -16 		 |
# 		+--------+-----------+------------+
#
# Some syntax constraints apply within recursive CTE subqueries:
#
# 		) The recursive SELECT part must not contain these constructs:
#
# 			) Aggregate functions such as SUM()
#
# 			) Window functions
#
# 			) GROUP BY
#
# 			) ORDER BY
#
# 			) LIMIT
#
# 			) DISTINCT
#
# 			This constraint does not apply to nonrecursive SELECT part of a recursive CTE.
#
# 			The prohibition on DISTINCT applies only to UNION members; UNION DISTINCT is 
# 			permitted.
#
# 		) The recursive SELECT part must reference the CTE only once and only in its FROM clause,
# 			not in any subquery.
#
# 			It can reference tables other than the CTE and join them with the CTE:
#
# 			If used in a join like this, the CTE must not be on the right side of a LEFT JOIN
#
# These constraints come from the SQL standard, other than the MySQL-specific exclusions
# of ORDER BY, LIMIT, and DISTINCT.
#
# For recursive CTEs, EXPLAIN output rows for recursive SELECT parts display Recursive
# in the Extra column.
#
# Cost estimates displayed by EXPLAIN represents cost per iteration, which might differ
# considerably from total cost.
#
# The optimizer cannot predict the number of iterations because it cannot predict
# when the WHERE clause will become false.
#
# CTE actual cost may also be affected by result set size.
#
# A CTE that produces many rows may require an internal temporary table large enough
# to be converted from in-memory to on-disk format and may suffer a performance
# penalty.
#
# If so, increasing the permitted in-memory temporary table-size may improve performance;
# see SECTION 8.4.4, "INTERNAL TEMPORARY TABLE USE IN MYSQL"
#
# LIMITING COMMON TABLE EXPRESSION RECURSION
#
# It is important for recursive CTEs that the recursive SELECT part include a condition
# to terminate recursion.
#
# As a development technique to guard against a runaway recursive CTE; you can
# force termination by placing a limit on execution time:
#
# 		) The cte_max_recursion_depth system variable enforces a limit on the number
# 			of recursion levels for CTEs.
#
# 			The server terminates execution of any CTE that recurses more levels
# 			than the value of this variable.
#
# 		) The max_execution_time system variable enforces an execution timeout for SELECT
# 			statements executed within the current session.
#
# 		) The MAX_EXECUTION_TIME optimizer hint enforces a per-query execution timeout for the
# 			SELECT statement in which it appears.
#
# Suppose that a recursive CTE is mistakenly written with no recursion execution termination condition:
#
# 		WITH RECURSIVE cte (n) AS
# 		(
# 			SELECT 1
# 			UNION ALL
# 			SELECT n + 1 FROM cte
# 		)
# 		SELECT * FROM cte;
#
# By default, cte_max_recursion_depth has a value of 1000, causing the CTE to terminate when
# it recurses past 1000 levels.
#
# Applications can change the session value to adjust for their requirements:
#
# 		SET SESSION cte_max_recursion_depth = 10; 		-- Permit only shallow recursion
# 		SET SESSION cte_max_recursion_depth = 1000000; 	-- permit deeper recursion
#
# You can also set the global cte_max_recursion_depth value to affect all sessions
# that begin subsequently.
#
# For queries that execute and thus recurse slowly or in contexts for which there is
# reason to set the cte_max_recursion_depth value very high, another way to guard
# against deep recursion is to set a per-session timeout.
#
# To do so, execute a statement like this prior to executing the CTE statement:
#
# 		SET max_execution_time = 1000; -- impose one second timeout
#
# Alternatively, include an optimizer hint within the CTE statement itself:
#
# 		WITH RECURSIVE cte (n) AS
# 		(
# 			SELECT 1
# 			UNION ALL
# 			SELECT n + 1 FROM cte
# 		)
# 		SELECT /*+ MAX_EXECUTION_TIME(1000) */ * FROM cte;
#
# If a recursive query without an execution time limit enters an infinite loop, you can
# terminate it from another session using KILL_QUERY
#
# Within the session itself, the client program used to run the query might provide
# a way to kill the query.
#
# For example, in mysql, doing CTRL+C interuppts the current statement
#
# RECURSIVE COMMON TABLE EXPRESSION EXAMPLES
#
# As mentioned previously, recursive common table expressions (CTEs)
# are frequently used for series generation and traversing hierarchial
# or tree-structured data.
#
# This section shows some simple examples of these techniques.
#
# 		) FIBONACCI SERIES GENERATION
#
# 		) DATE SERIES GENERATION
#
# 		) HIERARCHIAL DATA TRAVERSAL
#
# FIBONACCI SERIES GENERATION
#
# A Fibonacci series begins with the two numbers 0 and 1 (or 1 and 1) and each number
# after that is the sum of the previous two numbers.
#
# A recursive common table expression can generate a Fibonacci series if each row
# produced by the recursive SELECT has access to the two previous numbers
# from the series.
#
# The following CTE generates a 10-number series using 0 and 1 as the first two numbers:
#
# 		WITH RECURSIVE fibonacci (n, fib_n, next_fib_n) AS
# 		(
# 			SELECT 1, 0, 1
# 			UNION ALL
# 			SELECT n + 1, next_fib_n, fib_n + next_fib_n
# 				FROM fibonacci WHERE n < 10
# 		)
# 		SELECT * FROM fibonacci;
#
# The CTE produces this result:
#
# 		+---------+------------+---------------------+
# 		| n 		 | fib_n 	  | next_fib_n 		   |
# 		+---------+------------+---------------------+
# 		| 1 		 | 0 			  | 1 						|
# 		| 2 		 | 1 			  | 1 						|
# 		| 3 		 | 1 			  | 2 						|
# 		| 4 		 | 2 			  | 3 					   |
# 		| 5 		 | 3 			  | 5 						|
# 		| 6 		 | 5 			  | 8 						|
# 		| 7 		 | 8 			  | 13 						|
# 		| 8 		 | 13 		  | 21 					   |
# 		| 9 		 | 21 		  | 34 						|
# 		| 10 		 | 34 		  | 55 						|
# 		+---------+------------+---------------------+
#
# How the CTE works:
#
# 		) n is a display column to indicate that the row contains the n-th Fibonacci number.
#
# 			For example, the 8th Fibonacci number is 13.
#
# 		) The fib_n column displays Fibonacci number n
#
# 		) The next_fib_n column displays the next Fibonaci number after number n.
#
# 			This column provides the next series value to the next row, so that row can
# 			produce the sum of the two previous series values in its fib_n column
#
# 		) Recursion ends when n reaches 10. This is an arbitrary choice, to limit output to
# 			a small set of rows.
#
# The preceding output shows the entire CTE result.
#
# To select just part of it, add an appropriate WHERE clause to the top-level SELECT
#
# For example, to select the 8th Fibonacci number, do this:
#
# 		WITH RECURSIVE fibonacci ---
# 		---
# 		SELECT fib_n FROM fibonacci WHERE n = 8;
# 		+-----------+
# 		| fib_n 		|
# 		+-----------+
# 		| 13 			|
# 		+-----------+
#
# DATE SERIES GENERATION
#
# A common table expression can generate a series of successive dates, which is useful
# for generating summaries that include a row for all dates in the series, including
# dates not represented in the summarized data.
#
# Suppose that a table of sales numbers contains these rows:
#
# 		SELECT * FROM sales ORDER BY date, price;
# 		+----------------+----------+
# 		| date 			  | price 	 |
# 		+----------------+----------+
# 		| 2017-01-03 	  | 100.00   |
# 		| 2017-01-03 	  | 200.00   |
# 		| 2017-01-06 	  | 50.00 	 |
# 		| 2017-01-08 	  | 10.00 	 |
# 		| 2017-01-08 	  | 20.00    |
# 		| 2017-01-08 	  | 150.00   |
# 		| 2017-01-10 	  | 5.00 	 |
# 		+----------------+----------+
#
# This query summarizes the sales per day:
#
# 		SELECT date, SUM(price) AS sum_price
# 		FROM sales
# 		GROUP BY date
# 		ORDER BY date;
# 		+----------------+------------------+
# 		| date 			  | sum_price 		   |
# 		+----------------+------------------+
# 		| 2017-01-03 	  | 300.00 				|
# 		| 2017-01-06 	  | 50.00 				|
# 		| 2017-01-08 	  | 180.00 				|
# 		| 2017-01-10 	  | 5.00 				|
# 		+----------------+------------------+
#
# However, that result contains "holes" for dates not represented in the range
# of dates spanned by the table.
#
# A result that represents all dates in the range can be produced using a recursive
# CTE to generate that set of dates, joined with a LEFT JOIN to the sales data.
#
# Here is the CTE to generate the date range series:
#
# 		WITH RECURSIVE dates (date) AS
# 		(
# 			SELECT MIN(date) FROM sales
# 			UNION ALL
# 			SELECT date + INTERVAL 1 DAY FROM dates
# 			WHERE date + INTERVAL 1 DAY <= (SELECT MAX(date) FROM sales)
# 		)
# 		SELECT * FROM dates;
#
# The CTE produces this result:
#
# 		+-------------------+
# 		| date 				  |
# 		+-------------------+
# 		| 2017-01-03 		  |
# 		| 2017-01-04 		  |
# 		| 2017-01-05 		  |
# 		| 2017-01-06 		  |
# 		| 2017-01-07 		  |
# 		| 2017-01-08 		  |
# 		| 2017-01-09 		  |
# 		| 2017-01-10 		  |
# 		+-------------------+
#
# How the CTE works:
#
# 		) The nonrecursive SELECT produces the lowest date in the date range spanned by the sales table.
#
# 		) Each row produced by the recursive SELECT adds one day to the date produced by the previous row.
#
# 		) Recursion ends after the dates reach the highest date in the date range spanned by the sales table.
#
# Joining the CTE with a LEFT JOIN against the sales table produces the sales summary with a row
# for each date in the range:
#
# 		WITH RECURSIVE dates (date) AS
# 		(
# 			SELECT MIN(date) FROM sales
# 			UNION ALL
# 			SELECT date + INTERVAL 1 DAY FROM dates
# 			WHERE date + INTERVAL 1 DAY <= (SELECT MAX(date) FROM sales)
# 		)
# 		SELECT dates.date, COALESCE(SUM(price), 0) AS sum_price
# 		FROM dates LEFT JOIN sales ON dates.date = sales.date
# 		GROUP BY dates.date
# 		ORDER BY dates.date;
#
# The output looks like this:
#
# 		+--------------------+--------------+
# 		| date 					| sum_price 	|
# 		+--------------------+--------------+
# 		| 2017-01-03 			| 300.00 		|
# 		| 2017-01-04 			| 0.00 			|
# 		| 2017-01-05 			| 0.00 			|
# 		| 2017-01-06 			| 50.00 			|
# 		| 2017-01-07 			| 0.00 			|
# 		| 2017-01-08 			| 180.00 		|
# 		| 2017-01-09 			| 0.00 			|
# 		| 2017-01-10 			| 5.00 			|
# 		+--------------------+--------------+
#
# Some points to note:
#
# 		) Are the queries inefficient, particularly the one with the MAX() subquery executed
# 			for each row in the recursive SELECT?
#
# 			Checking with EXPLAIN shows that the subqueries are optimized away for efficiency.
#
# 		) The use of COALESCE() avoids displaying NULL in the sum_price column on days for which
# 			no sales data occur in the sales table.
#
# HIERARCHIAL DATA TRAVERSAL
#
# Recursive common table expressions are useful for traversing data that forms a hierarchy.
#
# Consider these statements that create a small data set that shows, for each employee
# in a company, the employee name and ID number, and the ID of the employee's manager.
#
# The top-level employee (the CEO), has a manager ID of NULL (no manager)
#
# 		CREATE TABLE employees (
# 			id 			INT PRIMARY KEY NOT NULL,
# 			name 			VARCHAR(100) NOT NULL,
# 			manager_id 	INT NULL,
# 			INDEX (manager_id),
# 		FOREIGN KEY (manager_id) REFERENCES EMPLOYEES (id)
# 		);
# 		INSERT INTO employees VALUES
# 		(333, "Yasmina", NULL), #CEO, manager_id is NULL
# 		(198, "John", 333),
# 		(692, "Tarek", 333),
# 		(29, "Pedro", 198),
# 		(4610, "Sarah", 29),
# 		(72, "Pierre", 29),
# 		(123, "Adil", 692);
#
# The resulting data set looks like this:
#
# 		SELECT * FROM employees ORDER BY id;
# 		+-----+------------+-------------+
# 		| id  | name 		 | manager_id  |
# 		+-----+------------+-------------+
# 		| 29  | Pedro 		 | 198 		   |
# 		etc.
#
# To produce the organizational chart with the management chain for each
# employee (that is, the path from CEO to employee), use a recursive CTE:
#
# 		WITH RECURSIVE employee_paths (id, name, path) AS
# 		(
# 			SELECT id, name, CAST(id AS CHAR(200))
# 				FROM employees
# 				WHERE manager_id IS NULL
# 			UNION ALL
# 			SELECT e.id, e.name, CONCAT(ep.path, ',', e.id)
# 				FROM employee_paths AS ep JOIN employees AS e
# 					ON ep.id = e.manager_id
#		)
# 		SELECT * FROM employee_paths ORDER BY path;
#
# The CTE produces this output:
#
# 		+-------+-----------+-------------------+
# 		| id 	  | name 	  | path 				 |
# 		+-------+-----------+-------------------+
# 		| 333   | Yasmina   | 333 					 |
# 		| 198   | John 	  | 333, 198 			 |
# 		| 29 	  | Pedro 	  | 333, 198, 29 	    |
# 		| 4610  | Sarah 	  | 333, 198, 29, 4610|
# 		| 72 	  | Pierre 	  | 333, 198, 29, 72  |
# 		| 692   | Tarek 	  | 333, 692 			 |
# 		| 123   | Adil 	  | 333, 692, 123 	 |
# 		+-------+-----------+-------------------+
#
# How the CTE works:
#
# 		) The nonrecursive SELECT produces the row for the CEO (the row with a NULL-manager ID)
#
# 			The path column is widened to CHAR(200) to ensure that there is room for the longer path
# 			values produced by the recursive SELECT.
#
# 		) Each row produced by the recursive SELECT finds all employees who report directly to
# 			an employee produced by a previous row.
#
# 			For each such employee, the row includes the employee ID and name, and the employee
# 			management chain.
#
# 			The chain is the manager's chain, with the employee ID added ot the end.
#
# 		) Recursion ends when employees have no others who report to them
#
# To find the path for a specific employee or employees, add a WHERE clause
# to the top-level SELECT.
#
# For example, to display the results for Tarek and Sarah, modify that SELECT
# like this:
#
# 		WITH RECURSIVE ---
# 		---
# 		SELECT * FROM employees_extended
# 		WHERE id IN (692, 4610)
# 		ORDER BY path;
# 		+------+-------+-----------------------+
# 		| id   | name  | path 					   |
# 		+------+-------+-----------------------+
# 		| 4610 | Sarah | 333, 198, 29, 4610    |
# 		| 692  | Tarek | 333, 692 					|
# 		+------+-------+-----------------------+
#
# COMMON TABLE EXPRESSIONS COMPARED TO SIMILAR CONSTRUCTS
#
# Common table expressions (CTEs) are similar to derived tables in some ways:
#
# 		) Both constructs are named
#
# 		) Both constructs exist for the scope of a single statement
#
# Because of these similarities, CTEs and derived tables often can be used
# interchangably.
#
# As a trivial example, these statements are equivalent:
#
# 		WITH cte AS (SELECT 1) SELECT * FROM cte;
# 		SELECT * FROM (SELECT 1) AS dt;
#
# However, CTEs have some advantages over derived tables:
#
# 		) A derived table can be referenced only a single time within a query.
#
# 			A CTE can be referenced multiple times.
#
# 			To use multiple instances of a derived table result, you must derive
# 			the result multiple times.
#
# 		) A CTE can be self-referencing (recursive)
#
# 		) One CTE can refer to another
#
# 		) A CTE may be easier to read when its definition appears at the beginning
# 			of the statement rather than embedded within it.
#
# CTEs are similar to tables created with CREATE [TEMPORARY] TABLE but need not be defined
# or dropped explicitly.
#
# For a CTE, you need no privileges to create tables.
#
# 13.3 TRANSACTIONAL AND LOCKING STATEMENTS
#
# 13.3.1 START TRANSACTION, COMMIT, AND ROLLBACK SYNTAX
# 13.3.2 STATEMENTS THAT CANNOT BE ROLLED BACK
#
# 13.3.3 STATEMENTS THAT CAUSE AN IMPLICIT COMMIT
# 13.3.4 SAVEPOINT, ROLLBACK TO SAVEPOINT, AND RELEASE SAVEPOINT SYNTAX
#
# 13.3.5 LOCK INSTANCE FOR BACKUP AND UNLOCK INSTANCE SYNTAX
# 13.3.6 LOCK TABLES AND UNLOCK TABLES SYNTAX
#
# 13.3.7 SET TRANSACTION SYNTAX
# 13.3.8 XA TRANSACTIONS
#
# MySQL supports local transactions (within a given client session) through statements,
# such as SET_autocommit, START_TRANSACTION, COMMIT, and ROLLBACK.
#
# See SECTION 13.3.1, "START TRANSACTION, COMMIT, AND ROLLBACK SYNTAX"
#
# XA transaction support enables MySQL to participate in distributed transactions
# as well.
#
# See SECTION 13.3.8, "XA TRANSACTIONS"
#
# 13.3.1 START TRANSACTION, COMMIT AND ROLLBACK SYNTAX
#
# START TRANSACTION 
# 		[transaction_characteristic [, transaction_characteristic] ---]
#
# transaction_characteristic: {
# 		WITH CONSISTENT SNAPSHOT
# 	 | READ WRITE
# 	 | READ ONLY
# }
#
# BEGIN [WORK]
# COMMIT [WORK] [AND [NO] CHAIN] [[NO] RELEASE]
# ROLLBACK [WORK] [AND [NO] CHAIN] [[NO] RELEASE]
# SET autocommit = {0 | 1}
#
# These statements provide control over use of transactions:
#
# 	) START TRANSACTION or BEGIN start a new transaction
#
# 	) COMMIT commits the current transaction, making its changes permanent
#
# 	) ROLLBACK rolls back the current transaction, canceling its changes
#
# 	) SET autocommit disables or enables the default autocommit mode for the current session
#
# By default, MySQL runs with autocommit mode enabled.
#
# This means that as soon as you execute a statement that updates (modifies) table,
# MySQL stores the update on disk to make it permanent.
#
# The change cannot be rolled back.
#
# To disable autocommit mode implicitly for a single series of statements, use the
# START TRANSACTION statement:
#
# 		START TRANSACTION;
# 		SELECT @A:=SUM(salary) FROM table1 WHERE type=1;
# 		UPDATE table2 SET summary=@A WHERE type=1;
# 		COMMIT;
#
# With START TRANSACTION, autocommit remains disabled until you end the transaction
# with COMMIT or ROLLBACK.
#
# The autocommit mode then reverts to its previous state.
#
# START TRANSACTION permits several modifiers that control transaction characteristics.
#
# To specify multiple modifiers, separate them by commas.
#
# 	) The WITH CONSISTENT SNAPSHOT modifier starts a consistent read for storage engines that
# 		are capable of it.
#
# 		This applies only to InnoDB
#
# 		The effect is the same as issuing a START TRANSACTION followed by a SELECT
# 		from any InnoDB table.
#
# 		See SECTION 15.7.2.3, "CONSISTENT NONLOCKING READS"
#
# 		The WITH CONSISTENT SNAPSHOT modifier does not change the current transaction
# 		isolation level, so it provides a consistent snapshot only if the current
# 		isolation level is one that permits a consistent read.
#
# 		The only isolation level that permits a consistent read is REPEATABLE_READ
#
# 		For all other isolation levels, the WITH CONSISTENT SNAPSHOT clause is ignored.
#
# 		A warning is generated when the WITH CONSISTENT SNAPSHOT clause is ignored.
#
# 	) The READ WRITE and READ ONLY modifiers set the transaction access mode.
#
# 		They permit or prohibit changes to tables used in the transaction.
#
# 		The READ ONLY restriction prevents the transaction from modifying or locking
# 		both transactional and nontransactional tables that are visible to other
# 		transactions;
#
# 		The transaction can still modify or lock temporary tables
#
# 		MySQL enables extra optimizations for queries on InnoDB tables when
# 		the transaction is known to be read-only.
#
# 		Specifying READ ONLY ensures these optimizations are applied in cases
# 		where the read-only status cannot be determined automatically.
#
# 		See SECTION 8.5.3, "OPTIMIZING INNODB READ-ONLY TRANSACTIONS" for more information.
#
# 		If no access mode is specified, the default mode applies.
#
# 		Unless the default has been changed, it is read/write.
#
# 		it is not permitted to specify both READ WRITE and READ ONLY in the same
# 		statement.
#
# 		In read-only mode, it remains possible to change tables created with the
# 		TEMPORARY keyword using DML statements.
#
# 		Changes made with DDL statements are not permitted, just as with permanent
# 		tables.
#
# 		For additional information about transaction access mode, including ways to change
# 		the default mode, see SECTION 13.3.7, "SET TRANSACTION SYNTAX"
#
# 		If the read_only system variable is enabled, explicitly starting a transaction
# 		with START TRANSACTION READ WRITE requires the CONNECTION_ADMIN or SUPER privilege.
#
# 			IMPORTANT:
#
# 				Many APIS used for writing MySQL client applications (such as JDBC) provide
# 				their own methods for starting transactions that can (and sometimes should)
# 				be used instead of sending a START TRANSACTION statement from the client.
#
# 				See CHAPTER 28, CONNECTORS AND APIS, or the documentation for your API
# 				for more information.
#
# 		To disable autocommit mode explicitly, use the following statement:
#
# 			SET autocommit=0;
#
# 		After disabling autocommit mode by setting the autocommit variable to zero, changes to
# 		transaction-safe tables (such as those for InnoDB or NDB) are not made permanent
# 		immediately.
#
# 		You must use COMMIT to store your changes to disk or ROLLBACK to ignore the changes.
#
# 		autocommit is a session variable and must be set for each session.
#
# 		To disable autocommit mode for each new connection, see the description
# 		of the autocommit system variable at SECTION 5.1.8, "SERVER SYSTEM VARIABLES"
#
# 		BEGIN and BEGIN WORK are supported as aliases of START TRANSACTION for initiating
# 		a transaction.
#
# 		START TRANSACTION is standard SQL syntax, is the recommended way to start an
# 		ad-hoc transaction, and permits modifiers that BEGIN does not.
#
# 		The BEGIN statement differs from the use of the BEGIN keyword that starts a BEGIN_---_END
# 		compound statement.
#
# 		The latter does not begin a transaction. See SECTION 13.6.1, "BEGIN --- END COMPOUND-STATEMENT SYNTAX"
#
# 		NOTE:
#
# 			Within all stored programs (stored procedures and functions, triggers, and events), the parser
# 			treats BEGIN [WORK] as the beginning of a BEGIN_---_END block.
#
# 			Begin a transaction in this context with START_TRANSACTION instead.
#
# 		The optional WORK keyword is supported for COMMIT and ROLLBACK, as are teh CHAIN and RELEASE
# 		clauses.
#
# 		CHAIN and RELEASE can be used for additional control over transaction completion.
#
# 		The value of the completion_type system variable determines the default completion
# 		behavior.
#
# 		See SECTION 5.1.8, "SERVER SYSTEM VARIABLES"
#
# 		The AND CHAIN clause causes a new transaction to begin as soon as the current one ends,
# 		and the new transaction has the same isolation level as the just-terminated transaction.
#
# 		The new transaction also uses the same access mode (READ WRITE or READ ONLY) as the just-terminated
# 		transaction.
#
# 		The RELEASE clause causes the server to disconnect the current client session after terminating
# 		the current transaction.
#
# 		Including the NO keyword suppresses CHAIN or RELEASE completion, which can be useful if
# 		the completion_type system variable is set to cause chaining or release completion by default.
#
# 		Beginning a transaction causes any pending transaction to be committed.
#
# 		See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT", for more information.
#
# 		Beginning a transaction also causes table locks acquired with LOCK_TABLES to be released,
# 		as though you had executed UNLOCK_TABLES
#
# 		Beginning a transaction does not release a global read lock acquired with FLUSH_TABLES_WITH_READ_LOCK
#
# 		For best results, transactions should be performed using only tables managed by a single
# 		transaction-safe storage engine.
#
# 		Otherwise, the following problems can occur:
#
# 			) If you use tables from more than one transaction-safe storage engine (such as InnoDB),
# 				and the transaction isolation level is not SERIALIZABLE, it is possible that when
# 				one transaction commits, another ongoing transaction that uses the same tables will
# 				see only some of the changes made by the first transaction.
#
# 				That is, the atomicity of transactions is not guaranteed with mixed engines and
# 				inconsistencies can result.
#
# 				(If mixed-engine transactions are infrequent, you can use SET_TRANSACTION_ISOLATION_lEVEL
# 				to set the isolation level to SERIALIZABLE on a per-transaction basis as necessary.)
#
# 			) If you use tables that are not transaction-safe within a transaction, changes to those tables
# 				are stored at once, regardless of the status of autocommit mode.
#
# 			) If you issue a ROLLBACK statement after updating a nontransactional table within a transaction,
# 				an ER_WARNING_NOT_COMPLETE_ROLLBACK warning occurs.
#
# 				Changes to transaction-safe tables are rolled back, but not changes to nontransaction-safe tables.
#
# 		Each transaction is stored in the binary log in one chunk, upon COMMIT.
#
# 		Transactions that are rolled back are not logged.
#
# 		(Exception:
#
# 			Modifications to nontransactional tables cannot be rolled back.
#
# 			If a transaction that is rolled back includes modifications to nontransactional
# 			tables, the entire transaction is logged with a ROLLBACK statement at the
# 			end to ensure that modifications to the nontransactional tables are replicated)
#
# 			See SECTION 5.4.4, "THE BINARY lOG"
#
# 			You can change the isolation level or access mode for transactions with the
# 			SET_TRANSACTION statement.
#
# 			See SECTION 13.3.7, "SET TRANSACTION SYNTAX"
#
# 			Rolling back can be a slow operation that may occur implicitly without the user
# 			having explicitly asked for it (for example, when an error occurs)
#
# 			Because of this, SHOW_PROCESSLIST displays Rolling back in the State column
# 			for the session, not only for explicit rollbacks performed with the ROLLBACK
# 			statement but also for implicit rollbacks.
#
# 				NOTE:
#
# 					In MySQL 8.0, BEGIN, COMMIT and ROLLBACK are not affected by
# 					--replicate-do-db or --replicate-ignore-db rules
#
# 13.3.2 STATEMENTS TAHT CANNOT BE ROLLED BACK
#
# Some statements cannot be rolled back.
#
# In general, these include data definition language (DDL) statements, such as those
# that create or drop databases,, those that create, drop or alter tables
# or stored routines.
#
# You should design your transactions not to include such statements.
#
# If you issue a statement early in a transaction that cannot be rolled back,
# and then another statement later fails, the full effect of the transaction
# cannot be rolled back in such cases by issuing a ROLLBACK statement.
#
# 13.3.3 STATEMENTS THAT CAUSE AN IMPLICIT COMMIT
#
# The statements listed in this section (and any synonyms for them) implicitly
# end any transaction active in the current session, as if you had done a COMMIT
# before executing the statement.
#
# Most of these statements also cause an implicit commit after executing.
#
# The intent is to handle each such statement in its own special transaction.
#
# Transaction-control and locking statements are exceptions:
#
# If an implicit commit occurs before execution, another does not occur after.
#
# 		) DATA DEFINITION LANGUAGE (DDL) STATEMENTS THAT DEFINE OR MODIFY DATABASE OBJECTS.
#
# 			ALTER_EVENT
#
# 			ALTER_FUNCTION
#
# 			ALTER_PROCEDURE
#
# 			ALTER_SERVER
#
# 			ALTER_TABLE
#
# 			ALTER_VIEW
#
# 			CREATE_DATABASE 
#
# 			CREATE_EVENT
#
# 			CREATE_FUNCTION
#
# 			CREATE_INDEX
#
# 			CREATE_PROCEDURE
#
# 			CREATE_ROLE
#
# 			CREATE_SERVER
#
# 			CREATE_SPATIAL_REFERENCE_SYSTEM
#
# 			CREATE_TABLE
#
# 			CREATE_TRIGGER
#
# 			CREATE_VIEW
#
# 			DROP_DATABASE 
#
# 			DROP_EVENT
#
# 			DROP_FUNCTION
#
# 			DROP_INDEX
#
# 			DROP_PROCEDURE 
#
# 			DROP_ROLE
#
# 			DROP_SERVER
#
# 			DROP_SPATIAL_REFERENCE_SYSTEM
#
# 			DROP_TABLE
#
# 			DROP_TRIGGER
#
# 			DROP_VIEW
#
# 			INSTALL_PLUGIN
#
# 			RENAME_TABLE
#
# 			TRUNCATE_TABLE
#
# 			UNINSTALL_PLUGIN
#
# 			CREATE_TABLE and DROP_TABLE statements do not commit a transaction
# 			if the TEMPORARY keyword is used.
#
# 			(This does not apply to other operations on temporary tables such as
# 			ALTER_TABLE and CREATE_INDEX, which do cause a commit)
#
# 			However, although no implicit commit occurs, neither can the statement
# 			be rolled back, which means that the use of such statements cause transactional
# 			atomicity to be violated.
#
# 			For example, if you use CREATE_TEMPORARY_TABLE and then roll back the
# 			transaction, the table remains in existence.
#
# 			The CREATE_TABLE statement in InnoDB is processed as a single transaction.
#
# 			This means that a ROLLBACK from the user does not undo CREATE_TABLE statements
# 			the user made during that transaction.
#
# 			CREATE_TABLE_---_SELECT causes an implicit commit before and after the statement
# 			is executed when you are creating nontemporary tables.
#
# 			(No commit occurs for CREATE TEMPORARY TABLE --- SELECT)
#
# 		) STATEMENTS THAT IMPLICITLY USE OR MODIFY TABLES IN THE mysql DATABASE.
#
# 			ALTER_USER
#
# 			CREATE_USER
#
# 			DROP_USER
#
# 			GRANT
#
# 			RENAME_USER
#
# 			REVOKE
#
# 			SET_PASSWORD
#
# 		) TRANSACTION-CONTROL AND LOCKING STATEMENTS.
#
# 			BEGIN
#
# 			LOCK_TABLES 
#
# 			SET autocommit = 1 (if the value is not already 1)
#
# 			START TRANSACTION
#
# 			UNLOCK TABLES
#
# 			UNLOCK_TABLES commits a transaction only if any tables currently
# 			have been locked with LOCK_TABLES to acquire nontransactional table
# 			locks.
#
# 			A commit does not occur for UNLOCK_TABLES following FLUSH_TABLES_WITH_READ_LOCK
# 			because the latter statement does not acquire table-level locks.
#
# 			Transactions cannot be nested.
#
# 			This is a consequence of the IMPLICIT commit performed for any current transaction
# 			when you issue a START_TRANSACTION statement or one of its synonyms.
#
# 			Statements that cause an implicit commit cannot be used in an XA transaction
# 			while the transaction is in an ACTIVE state.
#
# 			The BEGIN statement differs from the use of the BEGIN keyword that starts
# 			a BEGIN_---_END compound statement.
#
# 			The latter does not cause an implicit commit.
#
# 			See SECTION 13.6.1, "BEGIN -- END COMPOUND-STATEMENT SYNTAX"
#
# 		) DATA LOADING STATEMENTS
#
# 			LOAD_DATA_INFILE
#
# 			LOAD_DATA_INFILE causes an implicit commit only for tables using the NDB storage engine.
#
# 		) ADMINISTRATIVE STATEMENTS
#
# 			ANALYZE_TABLE
#
# 			CACHE_INDEX
#
# 			CHECK_TABLE
#
# 			FLUSH
#
# 			LOAD_INDEX_INTO_CACHE
#
# 			OPTIMIZE_TABLE
#
# 			REPAIR_TABLE
#
# 			RESET (but not RESET PERSIST)
#
# 		) REPLICATION CONTROL STATEMENTS
#
# 			START SLAVE
#
# 			STOP SLAVE
#
# 			RESET SLAVE
#
# 			CHANGE MASTER TO
#
# 13.3.4 SAVEPOINT, ROLLBACK TO SAVEPOINT, AND RELEASE SAVEPOINT SYNTAX
#
# 		SAVEPOINT identifier
# 		ROLLBACK [WORK] TO [SAVEPOINT] identifier
# 		RELEASE SAVEPOINT identifier
#
# InnoDB supports the SQL statements SAVEPOINT, ROLLBACK_TO_SAVEPOINT, RELEASE_SAVEPOINT
# and the optional WORK keyword for ROLLBACK
#
# The SAVEPOINT statement sets a named transaction savepoint with a name of identifier.
#
# If the current transaction has a savepoint with the same name, the old savepoint is deleted
# and a new one is set.
#
# The ROLLBACK_TO_SAVEPOINT statement rolls back a transaction to the named savepoint
# without terminating the transaction.
#
# Modifications that the current transaction made to rows after the savepoint was set
# are undone in the rollback, but InnoDB does not release the row locks that were stored
# in memory after the savepoint.
#
# (For a new inserted row, the lock information is carried by the transaction ID stored in the
# row; the lock is not separately stored in memory.
#
# In this case, the row lock is released in the undo)
#
# Savepoints that were set at a later time than the named savepoint are deleted.
#
# If the ROLLBACK_TO_SAVEPOINT statement returns the following error, it means that
# no savepoint with the specified name exists:
#
# 		ERROR 1305 (42000): SAVEPOINT identifier does not exist
#
# The RELEASE_SAVEPOINT statement removes the named savepoint from the set of savepoints
# of the current transaction.
#
# No commit or rollback occurs.
#
# It is an error if the savepoint does not exist.
#
# All savepoints of the current transaction are deleted if you execute a COMMIT,
# or a ROLLBACK that does not name a savepoint.
#
# A new savepoint level is created when a stored function is invoked or a trigger is activated.
#
# The savepoints on previous levels become unavailable and thus do not conflict
# with savepoints on the new level.
#
# When the function or trigger terminates, any savepoints it created are released
# and the previous savepoint level is restored.
#
# 13.3.5 LOCK INSTANCE FOR BACKUP AND UNLOCK INSTANCE SYNTAX
#
# 		LOCK INSTANCE FOR BACKUP
#
# 		UNLOCK INSTANCE
#
# LOCK INSTANCE FOR BACKUP acquires an instance-level backup lock that permits DML during
# an online backup while preventing operations that could result in an inconsistent snapshot.
#
# Executing the LOCK INSTANCE FOR BACKUP statement requires the BACKUP_ADMIN privilege.
#
# The BACKUP_ADMIN privilege is automatically granted to users with the RELOAD 
# privilege when performing an in-place upgrade to MySQL 8.0 from an earlier version.
#
# Multiple sessions can hold a backup lock simultaneously.
#
# UNLOCK INSTANCE releases a backup lock held by the current session.
#
# A backup lock held by a session is also released if the session is terminated.
#
# LOCK INSTANCE FOR BACKUP prevents files from being created, renamed or removed.
#
# REPAIR_TABLE
#
# TRUNCATE TABLE
#
# OPTIMIZE TABLE
#
# and account management statements are blocked.
#
# See SECTION 13.7.1, "ACCOUNT MANAGEMENT STATEMENTS"
#
# Operations that modify InnoDB files that are not recorded in the InnoDB
# redo log are also blocked.
#
# LOCK INSTANCE FOR BACKUP permits DDL operations that only affect user-created
# temporary tables
#
# In effect, files that belong to user-created temporary tables can be created,
# renamed or removed while a backup lock is held.
#
# Creation of binary log files is also permitted.
#
# A backup lock acquired by LOCK INSTANCE FOR BACKUP is independent of transactional
# locks and locks taken by FLUSH_TABLES_tbl_name [, tbl_name] --- WITH READ LOCK,
# and the following sequences of statements are permitted:
#
# 		LOCK INSTANCE FOR BACKUP;
# 		FLUSH TABLES tbl_name [, tbl_name] --- WITH READ LOCK;
# 		UNLOCK TABLES;
# 		UNLOCK INSTANCE;
#
# 		FLUSH TABLES tbl_name [, tbl_name] --- WITH READ LOCK;
# 		LOCK INSTANCE FOR BACKUP;
# 		UNLOCK INSTANCE;
# 		UNLOCK TABLES;
#
# The lock_wait_timeout setting defines the amount of time that a LOCK INSTANCE FOR BACKUP
# statement waits to acquire a lock before giving up.
#
# 13.3.6 LOCK TABLES AND UNLOCK TABLES SYNTAX
#
# LOCK TABLES
# 		tbl_name [[AS] alias] lock_type
# 		[, tbl_name [[AS] alias] lock_type] ---
#
# lock_type: {
# 		READ [LOCAL]
# 	 | [LOW_PRIORITY] WRITE
# }
#
# UNLOCK TABLES
#
# MySQL enables client sessions to acquire table locks explicitly for the purpose
# of cooperating with other sessions for access to tables, or to prevent other
# sessions from modifying tables during periods when a session requires exclusive
# access to them.
#
# A session can acquire or release locks only for itself
#
# One session cannot acquire locks for another session or release
# locks held by another session
#
# Locks may be used to emulate transactions or to get more speed when
# updating tables.
#
# This is explained in more detail in TABLE-LOCKING RESTRICTIONS AND CONDITIONS
#
# LOCK_TABLES explicitly acquires table locks for the current client session.
#
# Table locks can be acquired for base tables or views.
#
# You must have the LOCK_TABLES privilege, and the SELECT privilege
# for each object to be locked.
#
# For view locking, LOCK_TABLES adds all base tables used in the view to the
# set of tables to be locked and locks them automatically.
#
# If you lock a table explicitly with LOCK TABLES, any tables used in triggers
# are also locked implicitly, as described in LOCK TABLES AND TRIGGERS
#
# If you lock a table explicitly with LOCK_TABLES, any tables related by a foreign
# key constraint are opened and locked implicitly.
#
# For foreign key checks, a shared read-only lock (LOCK_TABLES_READ) is taken
# on related tables.
#
# For cascading updates, a shared-nothing write lock (LOCK_TABLES_WRITE) is taken
# on related tables that are involved in the operation.
#
# UNLOCK_TABLES explicitly releases any table locks held by the current session.
#
# LOCK_TABLES implicitly releases any table locks held by the current session before
# acquiring new locks.
#
# Another use for UNLOCK_TABLES is to release the global read lock acquired with 
# the FLUSH_TABLES_WITH_READ_LOCK statement, which enables you to lock all tables
# in all databases.
#
# See SECTION 13.7.7.3, "FLUSH SYNTAX" (This is a very convenient way to get backups
# if you  have a file system such as Veritas that can take snapshots in time)
#
# A table lock protects only against inappropriate reads or writes by other sessions.
#
# A session holding a WRITE lock can perform table-level operations such as DROP_TABLE
# or TRUNCATE_TABLE
#
# For sessions holding a READ lock, DROP_TABLE and TRUNCATE_TABLE operations are not permitted.
#
# The following discussion applies only to non-TEMPORARY tables.
#
# LOCK_TABLES is permitted (but ignored) for a TEMPORARY table.
#
# The table can be accessed freely by the session within which it was created,
# regardless of what other locking may be in effect.
#
# No lock is necessary because no other session can see the table.
#
# 		) TABLE LOCK ACQUISITION
#
# 		) TABLE LOCK RELEASE
#
# 		) INTERACTION OF TABLE LOCKING AND TRANSACTIONS
#
# 		) LOCK TABLES AND TRIGGERS
#
# 		) TABLE-LOCKING RESTRICTIONS AND CONDITIONS
#
# TABLE LOCK ACQUISITION
#
# To acquire table locks within the current session, use the LOCK_TABLES statement,
# which acquires metadata locks (see SECTION 8.11.4, "METADATA LOCKING")
#
# The following lock types are available:
#
# READ [LOCAL] lock:
#
# 		) The session that holds the lock can read the table (but not write it)
#
# 		) Multiple sessions can acquire a READ lock for the table at the same time
#
# 		) Other sessions can read the table without explicitly acquiring a READ lock
#
# 		) The LOCAL modifier enables nonconflicting INSERT statements (concurrent inserts)
# 			by other sessions to execute while the lock is held.
#
# 			(See SECTION 8.11.3, "CONCURRENT INSERTS")
#
# 			However, READ LOCAL cannot be used if you are going to manipulate the database
# 			using processes external to the server while you hold the lock.
#
# 			For InnoDB tables, READ LOCAL is the same as READ
#
# [LOW_PRIORITY] WRITE lock:
#
# 		) The session that holds the lock can read and write the table
#
# 		) Only the session that holds the lock can access the table.
#
# 			No other session can access it until the lock is released.
#
# 		) Lock requests for the table by other sessions block while the WRITE lock is held
#
# 		) The LOW_PRIORITY modifier has no effect.
#
# 			In previous versions of MySQL, it affected locking behavior, but this
# 			is no longer true.
#
# 			It is now deprecated and its use produces a warning. 
#
# 			Use WRITE without LOW_PRIORITY instead
#
# WRITE locks normally have higher priority than READ locks to ensure that updates
# are processed as soon as possible.
#
# This means that if one session obtains a READ lock and then another session
# requests a WRITE lock, subsequent READ lock requests wait until the session
# that requested the WRITE lock has obtained the lock and released it.
#
# (An exception to this policy can occur for small values of the max_write_lock_count
# 	system variable; See SECTION 8.11.4, "METADATA LOCKING")
#
# If the LOCK_TABLES statement must wait due to locks held by other sessions on any
# of the tables, it blocks until all locks can be acquired.
#
# A session taht requries locks must acquire all the locks that it needs in a single
# LOCK_TABLES statement.
#
# While the locks thus obtained are held, the session can access only the locked
# tables.
#
# For example, in the following sequence of statements, an error occurs for the attempt
# to access t2 because it was not locked in the LOCK_TABLES statement:
#
# 		LOCK TABLES t1 READ;
# 		SELECT COUNT(*) FROM t1;
# 		+---------------+
# 		| COUNT(*) 		 |
# 		+---------------+
# 		| 			3 		 |
# 		+---------------+
# 		SELECT COUNT(*) FROM t2;
# 		ERROR 1100 (HY000): Table 't2' was not locked with LOCK TABLES
#
# Tables in the INFORMATION_SCHEMA database are an exception.
#
# they can be accessed without being locked explicitly even while a session
# holds table locks obtained with LOCK_TABLES
#
# You cannot refer to a locked table multiple times in a single query using the
# same name.
#
# Use aliases instead, and obtain a separate lock for the table and each alias:
#
# 		LOCK TABLE t WRITE, t AS t1 READ;
# 		INSERT INTO t SELECT * FROM t;
# 		ERROR 1100: Table 't' was not locked with LOCK TABLES
# 		INSERT INTO t SELECT * FROM t AS t1;
#
# The error occurs for the first INSERT because there are two references to the same
# name for a locked table.
#
# The second INSERT succeeds because the references to the table use different names.
#
# If your statement refer to a table by means of an alias, you must lock the table
# using that same alias.
#
# It does not work to lock the table without specifying the alias:
#
# 		LOCK TABLE t READ;
# 		SELECT * FROM t AS myalias;
# 		ERROR 1100: Table 'myalias' was not locked with LOCK TABLES
#
# Conversely, if you lock a table using an alias, you must refer to it
# in your statements using that alias:
#
# 		LOCK TABLE t AS myalias READ;
# 		SELECT * FROM t;
# 		ERROR 1100: Table 't' was not locked with LOCK TABLES
# 		SELECT * FROM t AS myalias;
#
# NOTE:
#
# 		LOCK TABLES or UNLOCK TABLES, when applied to a partitioned table,
# 		always locks or unlocks the entire table;
#
# 		These statements do not support partition lock pruning. See PARTITIONING AND LOCKING.
#
# TABLE lOCK RELEASE
#
# When the table locks held by a session are released, they are all released at the same time.
#
# A session can release its locks explicitly, or locks may be released implicitly
# under certain conditions:
#
# 		) A session can release its locks explicitly with UNLOCK_TABLES
#
# 		) If a session issues a LOCK_TABLES statement to acquire a lock while already holding locks,
# 			its existing locks are released implicitly before the new locks are granted.
#
# 		) If a session begins a transaction (for example, with START_TRANSACTION), an implicit
# 			UNLOCK_TABLES is performed, which causes existing locks to be released.
#
# 			(For additional information about the interaction between table locking
# 			and transactions, see INTERACTION OF TABLE LOCKING AND TRANSACTIONS)
#
# If the connection for a client session terminates, whether normally or abnormally, the server
# implicitly releases all table locks held by the session (transactional and nontransactional)
#
# If the client reconnects, the locks will no longer be in effect.
#
# In addition, if the client had an active transaction, the server rolls back the transaction
# upon disconnect, and if reconnect occurs, the new session begins with autocommit enabled.
#
# For this reason, clients may wish to disable auto-reconnect
#
# With auto-reconnect in effect, the client is not notified if reconnect occurs but any table
# locks or current transaction will have been lost.
#
# With auto-reconnect disabled, if the connection drops, an error occurs for the next
# statement issued.
#
# The client can detect the error and take appropriate action such as reacquiring the locks
# or redoing the transaction.
#
# See SECTION 28.7.24, "C API AUTOMATIC RECONNECTION CONTROL"
#
# NOTE:
#
# 		If you use ALTER_TABLE on a locked table, it may become unlocked.
#
# 		For example, if you attempt a second ALTER_TABLE operation, the result
# 		may be an error Table 'tbl_name' was not locked with LOCK TABLES
#
# 		To handle this, lock the table again prior to the second alteration
#
# 		See also SECTION B.6.6.1, "PROBLEMS WITH ALTER TABLE"
#
# INTERACTION OF TABLE LOCKING AND TRANSACTIONS
#
# LOCK_TABLES and UNLOCK_TABLES interact with the use of transactions as follows:
#
# 		) LOCK_TABLES is not transaction-safe and implicitly commits any active transaction
# 			before attempting to lock the tables.
#
# 		) UNLOCK_TABLES implicitly commits any active transaction, but only if LOCK_TABLES
# 			has been used to acquire table locks.
#
# 			For example, in the following set of statements, UNLOCK_TABLE releases
# 			the global read lock but does not commit the transaction because no table
# 			locks are in effect:
#
# 				FLUSH TABLES WITH READ LOCK;
# 				START TRANSACTION;
# 				SELECT ---;
# 				UNLOCK TABLES;
#
# 		) Beginning a transaction (for example, with START_TRANSACTION) implicitly commits
# 			any current transaction and releases existing table locks.
#
# 		) FLUSH_TABLES_WITH_READ_LOCK acquires a global read lock and not table locks,
# 			so it is not subject to the same behavior as LOCK_TABLES and UNLOCK_TABLES
# 			with respect to table locking and implicit commits.
#
# 			For example, START_TRANSACTION does not release the global read lock
#
# 			See SECTION 13.7.7.3, "FLUSH SYNTAX"
#
# 		) Other statements that implicitly cause transactions to be committed do not
# 			release existing table locks.
#
# 			For a list of such statements, see SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# 		) The correct way to use LOCK_TABLES and UNLOCK_TABLES with transactional tables, such as
# 			InnoDB tables, is to begin a transaction with SET autocommit = 0 (not START TRANSACTION)
# 			followed by LOCK_TABLES, and to not call UNLOCK_TABLES until you commit the
# 			transaction explicitly.
#
# 			For example, if you need to write to table t1 and read from table t2, you can do this:
#
# 				SET autocommit=0;
# 				LOCK TABLES t1 WRITE, t2 READ, ---;
# 				--- Do something with tables t1 and t2 here ---
# 				COMMIT;
# 				UNLOCK TABLES;
#
# 			When you call LOCK_TABLES, InnoDB internally takes its own table lock, and MySQL
# 			takes its own table lock.
#
# 			InnoDB releases its internal table lock at the next commit, but for MySQL
# 			to release its table lock, you have to call UNLOCK_TABLES.
#
# 			You should not have autocommit = 1, because then InnoDB releases its internal
# 			table lock immediately afer the call of LOCK_TABLES, and deadlocks can very easily happen.
#
# 			InnoDB does not acquire the internal table lock at all if autocommit = 1, to help
# 			old applications avoid unnecessary deadlocks.
#
# 		) ROLLBACK does not release table locks
#
# LOCK TABLES AND TRIGGERS
#
# If you lock a table explicitly with LOCK_TABLES, any tables used in triggers are also
# locked implicitly:
#
# 		) The locks are taken as the same time as those acquired explicitly with the LOCK_TABLES statement
#
# 		) The lock on a table used in a trigger depends on whether the table is used only for reading.
#
# 			If so, a read lock suffices
#
# 			Otherwise, a write lock is used
#
# 		) If a table is locked explicitly for reading with LOCK_TABLES, but needs to be locked for
# 			writing because it might be modified within a trigger, a write lock is taken rather
# 			than a read lock.
#
# 			(That is, an implicit write lock needed due to the table's appearance within a trigger
# 				causes an explicit read lock request for the table to be converted to a write lock request)
#
# Suppose that you lock two tables, t1 and t2, using this statement:
#
# 		LOCK TABLES t1 WRITE, t2 READ;
#
# If t1 or t2 have any triggers, tables used within the triggers will also be locked.
#
# Suppose that t1 has a trigger defined like this:
#
# 		CREATE TRIGGER t1_a_ins AFTER INSERT ON t1 FOR EACH ROW
# 		BEGIN
# 			UPDATE t4 SET count = count+1
# 				WHERE id = NEW.id AND EXISTS (SELECT a FROM t3);
# 			INSERT INTO t2 VALUES(1, 2);
# 		END;
#
# The result of the LOCK_TABLES statement is that t1 and t2 are locked because they appear
# in the statement, and t3 and t4 are locked because they are used within the trigger:
#
# 		) t1 is locked for writing per the WRITE lock request
#
# 		) t2 is locked for writing, even though the request is for a READ lock.
#
# 			This occurs because t2 is inserted into within the trigger, so the READ
# 			request is converted to a WRITE request.
#
# 		) t3 is locked for reading because it is only read from within the trigger
#
# 		) t4 is locked for writing because it might be updated within the trigger
#
# TABLE-LOCKING RESTRICTIONS AND CONDITIONS
#
# You can safely use KILL to terminate a session that is waiting for a table lock.
#
# See SECTION 13.7.7.4, "KILL SYNTAX"
#
# LOCK_TABLES and UNLOCK_TABLES cannot be used within stored programs
#
# Tables in the performance_schema database cannot be locked with LOCK_TABLES,
# except the setup_xxx tables
#
# The following statements are prohibited while a LOCK_TABLES statement is in effect:
#
# 		CREATE_TABLE
#
# 		CREATE_TABLE_---_LIKE
#
# 		CREATE_VIEW
#
# 		DROP_VIEW
#
# and DDL statements on stored functions and procedures and events.
#
# For some operations, system tables in the mysql database must be accessed.
#
# For example, the HELP statement requires the contents of the server-side
# help tables, and CONVERT_TZ() might need to read the time zone tables.
#
# The server implicitly locks the system tables for reading as necessary
# so that you need not lock them explicitly.
#
# These tables are treated as just described:
#
# 		mysql.help_category
# 		mysql.help_keyword
#
# 		mysql.help_relation
# 		mysql.help_topic
#
# 		mysql.time_zone
# 		mysql.time_zone_leap_second
#
# 		mysql.time_zone_name
# 		mysql.time_zone_transition
#
# 		mysql.time_zone_transition_type
#
# If you want to explicitly place a WRITE lock on any of those tables with a LOCK_TABLES
# statement, the table must be the only one locked; no other table can be locked with
# the same statement.
#
# Normally, you do not need to lock tables, because all single UPDATE statements are atomic;
# no other session can interfere with any other currently executing SQL statement.
#
# However, there are a few cases when locking tables may provide an advantage:
#
# 		) If you are going to run many operations on a set of MyISAM tables, it is much faster
# 			to lock the tables you are going to use.
#
# 			Locking MyISAM tables speeds up inserting, updating or deleting on them
# 			because MySQL does not flush the key cache for the locked tables until
# 			UNLOCK_TABLES is called.
#
# 			Normally, the key cache is flushed after each SQL statement.
#
# 			The downside to locking the tables is that no session can update a READ-locked
# 			table (including the one holding the lock) and no session can access
# 			a WRITE-locked table other than the one holding the lock.
#
# 		) If you are using tables for a nontransactional storage engine, you must use
# 			LOCK_TABLES if you want to ensure that no other session modifies the tables
# 			between a SELECT and an UPDATE.
#
# 			The example shown here requires LOCK_TABLES to execute safely:
#
# 				LOCK TABLES trans READ, customer WRITE;
# 				SELECT SUM(value) FROM trans WHERE customer_id=some_id;
# 				UPDATE customer
# 					SET total_value=sum_from_previous_statement
# 					WHERE customer_id=some_id;
# 				UNLOCK TABLES;
#
# 			Without LOCK_TABLES, it is possible that another session might insert
# 			a new row in the trans table between execution of the SELECT and UPDATE statements.
#
# You can avoid using LOCK_TABLES in many cases by using relative updates (UPDATE customer SET value=value+new_value)
# or the LAST_INSERT_ID() function
#
# You can also avoid locking tables in some cases by using the user-level advisory lock functions
# GET_LOCK() and RELEASE_LOCK()
#
# These locks are saved in a hash table in the server and implemented with pthread_mutex_lock()
# and pthread_mutex_unlock() for high speed.
#
# See SECTION 12.14, "LOCKING FUNCTIONS"
#
# See SECTION 8.11.1, "INTERNAL LOCKING METHODS", for more information on locking policy.
#
# 13.3.7 SET TRANSACTION SYNTAX
#
# 		SET [GLOBAL | SESSION] TRANSACTION
# 			transaction_characteristic [, transaction_characteristic] ---
#
# 		transaction_characteristic: {
# 			ISOLATION LEVEL level
# 		 | access_mode
# 		}
#
# 		level: {
# 			REPEATABLE READ
# 		 | READ COMMITTED
# 		 | READ UNCOMMITTED
# 		 | SERIALIZABLE
# 		}
# 		
# 		access_mode: {
# 			READ WRITE
# 		 | READ ONLY
# 		}
#
# This statement specifies transaction characteristics.
#
# It takes a list of one or more characteristic values separated by commas.
#
# Each characteristic value sets the transaction isolation level or access mode.
#
# The isolation level is used for operations on InnoDB tables.
#
# The access mode specifies whether transactions operate in read/write or 
# read-only mode
#
# In addition, SET_TRANSACTION can include an optional GLOBAL or SESSION keyword
# to indicate the scope of the statement.
#
# 		) TRANSACTION ISOLATION LEVELS
#
# 		) TRANSACTION ACCESS MODE
#
# 		) TRANSACTION CHARACTERISTIC SCOPE
#
# TRANSACTION ISOLATION LEVELS
#
# To set the transaction isolation level, use an ISOLATION LEVEL level clause.
#
# It is not permitted to specify multiple ISOLATION LEVEL clauses in the same
# SET_TRANSACTION statement.
#
# The default isolation level is REPEATABLE_READ.
#
# Other permitted values are READ_COMMITTED, READ_UNCOMMITTED, and SERIALIZABLE
#
# For information about these isolation levels, see SECTION 15.7.2.1, "TRANSACTION ISOLATION LEVELS"
#
# TRANSACTION ACCESS MODE
#
# To set the transaction access mode, use a READ WRITE or READ ONLY clause.
#
# It is not permitted to specify multiple access-mode clauses in the same SET_TRANSACTION
# statement.
#
# By default, a transaction takes place in read/write mode, with both reads and writes permitted
# to tables used in the transaction.
#
# This mode may be specified explicitly using SET_TRANSACTION with an access mode of READ WRITE
#
# If the transaction access mode is set to READ ONLY, changes to tables are prohibited.
#
# This may enable storage engines to make performance improvements that are possible
# when writes are not permitted.
#
# In read-only mode, it remains possible to change tables created with the TEMPORARY keyword
# using DML statements.
#
# Changes made with DDL statements are not permitted, just as with permanent tables.
#
# The READ WRITE and READ ONLY access modes also may be specified for an individual transaction
# using the START_TRANSACTION statement.
#
# TRANSACTION CHARACTERISTIC SCOPE
#
# You can set transaction characteristic globally, for the current session, or for the
# next transaction only:
#
# 		) With the GLOBAL keyword:
#
# 			) The statement applies globally for all subsequent sessions
#
# 			) Existing sessions are unaffected
#
# 		) With the SESSION keyword:
#
# 			) The statement applies to all subsequent transactions performed within the current session.
#
# 			) The statement is permitted within transactions, but does not affect the current ongoing transaction
#
# 			) If executed between transactions, the statement overrides any preceding statement that sets the
# 				next-transaction value of the named characteristics
#
# 		) Without any SESSION or GLOBAL keyword:
#
# 			) The statement applies only to the next single transaction performed within the session
#
# 			) Subsequent transactions revert to using the session value of the named characteristics
#
# 			) The statement is not permitted within transactions:
#
# 				START TRANSACTION;
# 				Query OK, 0 rows affected (0.02 sec)
#
# 				SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;
# 				ERROR 1568 (25001): Transaction characteristics can't be changed
# 				while a transaction is in progress
#
# A change to global transaction characteristics requires the CONNECTION_ADMIN or SUPER privilege.
#
# Any session is free to change its session characteristics (even in the middle of a transaction),
# or the characteristics for its next transaction (prior to the start of that transaction)
#
# To set the global isolation level at server startup, use the --transaction-isolation=level option
# on the command line or in an option file.
#
# Values of level for this option uses dashes rather than spaces, so the permissible values are
# READ-UNCOMMITTED, READ-COMMITTED, REPEATABLE-READ, or SERIALIZABLE
#
# Similarly, to set the global transaction access mode at server startup, use the --transaction-read-only
# option
#
# The default is OFF (read/write mode) but the value can be set to ON for a mode of read only.
#
# For example, to set the isolation level to REPEATABLE READ and the access mode to READ WRITE,
# use these lines in the [mysqld] section of an option file:
#
# 		[mysqld]
# 		transaction-isolation = REPEATABLE-READ
# 		transaction-read-only = OFF
#
# At runtime, characteristics at the global, session, and next-transaction scope levels
# can be set indirectly using the SET_TRANSACTION statement, as described previously.
#
# They can also be set directly using the SET statement to assign values to the 
# transaction_isolation and transaction_read_only system variables:
#
# 		) SET_TRANSACTION permits optional GLOBAL and SESSION keywords for setting transaction
# 			characteristics at different scope levels.
#
# 		) The SET statement for assigning values to the transaction_isolation and transaction_read_only
# 			system variables has syntaxes for setting these variables at different scope levels.
#
# The following tables show the characteristic scope level set by each SET_TRANSACTION
# and variable-assignment syntax.
#
# TABLE 13.7 SET TRANSACTION SYNTAX FOR TRANSACTION CHARACTERISTICS
#
# 		SYNTAX 															AFFECTED CHARACTERISTIC SCOPE
#
# 	SET GLOBAL TRANSACTION transaction_characteristic 		Global
#
# 	SET SESSION TRANSACTION transaction_characteristic 	Session
#
# 	SET TRANSACTION transaction_characteristic 				Next transaction only
#
# TABLE 13.8 SET SYNTAX FOR TRANSACTION CHARACTERISTICS
#
# 		SYNTAX 															AFFECTED CHARACTERISTIC SCOPE
#
# 	SET GLOBAL var_name = value 									Global
#
# 	SET @@GLOBAL.var_name = value 								Global
#
# 	SET PERSIST var_name = value 									Global
#
# 	SET @@PERSIST.var_name = value 								Global
#
# 	SET PERSIST_ONLY var_name = value 							No runtime effect
#
# 	SET @@PERSIST_ONLY var_name = value 						No runtime effect
#
# 	SET SESSION var_name = value 									Session
#
# 	SET @@SESSION.var_name = value 								Session
#
# 	SET var_name = value 											Session
#
# 	SET @@var_name = value 											Next transaction only
#
# It is possible to check the global and sesion values of transaction characteristics at runtime:
#
# 		SELECT @@GLOBAL.transaction_isolation, @@GLOBAL.transaction_read_only;
# 		SELECT @@SESSION.transaction_isolation, @@SESSION.transaction_read_only;
#
# 13.3.8 XA TRANSACTIONS
#
# 13.3.8.1 XA TRANSACTION SQL SYNTAX
# 13.3.8.2 XA TRANSACTION STATES
#
# Support for XA transactions is available for the InnoDB storage engine.
#
# The MySQL XA implementation is based on the X/Open CAE document Distributed Transaction Processing: The XA specfication
#
# This document is published by The Open Group and available at <link>
#
# Limitations of the current XA implementation are described in SECTION C.6, "RESTRICTIONS ON XA TRANSACTIONS"
#
# On the client side, there are no special requirements. The XA interface to a MySQL server consists
# of SQL statements that begin with the XA keyword.
#
# MySQL client programs must be able to send SQL statements and to understand the semantics
# of the XA statement interface.
#
# They do not need be linked against a recent client library.
#
# Older client libraries also will work.
#
# Among the MySQL Connectors, MySQL Connector/J 5.0.0 and higher supports XA directly,
# by means of a class interface that handles the XA SQL statement interface for you.
#
# XA supports distributed transactions, that is, the ability to permit multiple separate
# transactional resources to participate in a global transaction.
#
# Transactional resources often are RDBMSs but may be other kinds of resources.
#
# A global transaction involves several actions that are transactional in themselves,
# but that all must either complete successfully as a group, or all be rolled back
# as a group.
#
# In essence, this extends ACID properties "up a level" so that multiple ACID transactions
# can be executed in concert as components of a global operation that also has ACID
# properties.
#
# (As with nondistributed transactions, SERIALIZABLE may be preferred if your applications
# are sensitive to read phenomena.
#
# REPEATABLE_READ may not be sufficient for distributed transactions)
#
# Some examples of distributed transactions:
#
# 		) An application may act as an integration tool that combines a messaging service
# 			with an RDBMS.
#
# 			The application makes sure that transactions dealing with message sending, retrieval
# 			and processing that also involve a transactional database all happen in a global transaction.
#
# 			You can think of this as "transactional email"
#
# 		) An application performs actions that involve different database servers, such as a MySQL
# 			server and an Oracle server (or multiple MySQL servers), where actions that involve
# 			multiple servers must happen as part of a global transaction, rather than as separate
# 			transactions local to each server.
#
# 		) A bank keeps account information in an RDBMS and distributes and receives money through
# 			automated teller machines (ATMs) 
#
# 			It is necessary to ensure that ATM actions are correctly reflected in the accounts,
# 			but this cannot be done with the RDBMS alone.
#
# 			A global transaction manager integrates the ATM and database resources to ensure
# 			overall consistency of financial transactions.
#
# Applications that use global transactions involve one or more Resource Managers and a Transaction Manager:
#
# 		) A Resource Manager (RM) provides access to transactional resources.
#
# 			A database server is one kind of resource manager. It must be possible to either commit or roll
# 			back transactions managed by the RM
#
# 		) A Transaction Manager (TM) coordinates the transactions that are part of a global transaction.
#
# 			It communicates with the RMs that handle each of these transactions.
#
# 			The individual transactions within a global transaction are "branches" of the global transaction.
#
# 			Global transactions and their branches are identified by a naming scheme described later.
#
# The MySQL implementation of XA enables a MySQL server to act as a Resource Manager that handles
# XA transactions within a global transaction.
#
# A client program that connects to the MySQL server acts as the Transaction Manager
#
# To carry out a global transaction, it is necessary to know which components are involved,
# and bring each component to a point when it can be committed or rolled back.
#
# Depending on what each component reports about its ability to succeed, they must all commit
# or roll back as an atomic group.
#
# That is, either all components must commit, or all components must roll back.
#
# To manage a global transaction, it is necessary to take into account that any
# component or the connecting network might ffail.
#
# The process for executing a global transaction uses two-phase commit (2PC)
#
# This takes place after the actions performed by the branches of the global transaction
# have been executed.
#
# 		1. In the first phase, all branches are prepared.
#
# 			That is, they are told by the TM to get ready to commit.
#
# 			Typically, this means each RM that manages a branch records the
# 			actions for the branch in stable storage.
#
# 			The branches indicate whether they are able to do this, and these
# 			results are used for the second phase.
#
# 		2. In the second phase, the TM tells the RMs whether to commit or roll back.
#
# 			If all branches indicated when they were prepared that they will be able
# 			to commit, all branches are told to commit.
#
# 			If any branch indicated when it was prepared that it will not be able to commit,
# 			all branches are told to roll back.
#
# In some cases, a global transaction might use one-phase commit (1PC)
#
# For example, when a Transaction Manager finds a global transaction consists of only
# one transactional resource (that is, a single branch), that resource can be told
# to prepare and commit at the same time.
#
# 13.3.8.1 XA TRANSACTION SQL SYNTAX
#
# To perform XA transactions in MySQL, use the following statements:
#
# 		XA {START|BEGIN} xid [JOIN|RESUME]
#
# 		XA END xid [SUSPEND [FOR MIGRATE]]
#
# 		XA PREPARE xid
#
# 		XA COMMIT xid [ONE PHASE]
#
# 		XA ROLLBACK xid
#
# 		XA RECOVER [CONVERT XID]
#
# For XA_START, the JOIN and RESUME clauses are not supported.
#
# For XA_END the SUSPEND [FOR MIGRATE] clause is not supported.
#
# Each XA statement begins with the XA keyword, and most of them require an
# xid value.
#
# An xid is an XA transaction identifier.
#
# It indicates which transaction the statement applies to.
# xid values are supplied by the client, or generated by the MySQL server.
#
# An xid value has from one to three parts:
#
# 		xid: gtrid [, bqual [, formatID ]]
#
# gtrid is a global transaction identifier, bqual is a branch qualifier, and
# formatID is a number that identifies the format used by the gtrid and bqual
# values.
#
# As indicated by the syntax, bqual and formatID are optional.
#
# The default bqual value is '' if not given.
#
# The default formatID value is 1 if not given.
#
# gtrid and bqual must be string literals, each up to 64 bytes (not characters)
# long
#
# gtrid and bqual can be specified in several ways
#
# You can use a quoted string ('ab'), hex string(X'6162', 0x6162) or bit value (b'nnnn')
#
# formatID is an unsigned integer
#
# The gtrid and bqual values are interpreted in bytes by the MySQL server's underlying
# XA support routines.
#
# However, while an SQL statement containing an XA statement is being parsed,
# the server works with some specific character set.
#
# To be safe, write gtrid and bqual as hex strings.
#
# xid values typically are generated by the Transaction Manager.
#
# Values generated by one TM must be different from values generated by other
# TMs.
#
# A given TM must be able to recognize its own xid values in a list of values
# returned by the XA_RECOVER statement.
#
# XA_START_xd starts an XA transaction with the given xid value.
#
# Each XA transaction must have a unique xid value, so the value must not 
# currently be used by another XA transaction.
#
# Uniqueness is assessed using the gtrid and bqual values. All following XA statements
# for the XA transaction must be specified using the same xid value as that
# given in the XA_START statement.
#
# If you use any of those statements but specify an xid value that does not
# correspond to some existing XA transaction, an error occurs.
#
# One or more XA transactions can be part of the same global transaction.
#
# All XA transactions within a given global transaction must use the same
# gtrid value in the xid value´.
#
# For this reason, gtrid values must be globally unique so that there is
# no ambiguity about which global transaction a given XA transaction is
# part of.
#
# The bqual part of the xid value must be different for each XA transaction
# within a global transaction.
#
# (The requirement that bqual values be different is a limitation of the
# current MySQL XA implementation.
#
# It is not part of the XA specification)
#
# The XA_RECOVER statement returns information for those XA transactions
# on the MySQL server that are in the PREPARED state.
#
# (See SECTION 13.3.8.2, "XA TRANSACTION STATES")
#
# The output includes a row for each such XA transaction on the server,
# regardless of which client started it.
#
# XA_RECOVER requires the XA_RECOVER_ADMIN privilege.
#
# This privilege requirement prevents users from discovering the XID values
# for oustanding prepared XA transactions other than their own.
#
# It does not affect normal commit or rollback of an XA transaction
# because the user who started it knows its XID
#
# XA_RECOVER output rows look like this (for an example xid value consisting of the parts 'abc', 'def', and 7):
#
# 		XA RECOVER;
# 		+----------+------------------+--------------------+-------------------+
# 		| formatID | gtrid_length 		| bqual_length 		| data 				  |
# 		+----------+------------------+--------------------+-------------------+
# 		| 		7 	  | 			3 		 	| 		3 					| 	abcdef 			  |
# 		+----------+------------------+--------------------+-------------------+
#
# The output columns have the following meanings:
#
# 		) formatID is the formatID part of the transaction xid
#
# 		) gtrid_length is the length in bytes of the gtrid part of the xid
#
# 		) bqual_length is the length in bytes of the bqual part of the xid
#
# 		) data is the concatenation of the gtrid and bqual parts of the xid
#
# XID values may contain nonprintable characters 
#
# XA_RECOVER permits an optional CONVERT XID clause so that clients can request
# XID values in hexadecimal.
#
# 13.3.8.2 XA TRANSACTION STATES
#
# An XA transaction progresses through the following states:
#
# 		1. Use XA_START to start an XA transaction and put it in the ACTIVE state
#
# 		2. For an ACTIVE XA transaction, issue the SQL statements that make up the transaction,
# 			and then issue an XA_END statement.
#
# 			XA_END puts the transaction in the IDLE state.
#
# 		3. For an IDLE XA transaction, you can issue either an XA_PREPARE statement or an
# 			XA COMMIT --- ONE phase statement:
#
# 			) XA_PREPARE puts the transaction in the PREPARED state.
#
# 				An XA_RECOVER statement at this point will include the transaction's xid value
# 				in its output, because XA_RECOVER lists all XA transactions that are in the 
# 				PREPARED state
#
# 			) XA COMMIT --- ONE PHASE prepares and commits the transaction.
#
# 				The xid value will not be listed by XA_RECOVER because the transaction terminates.
#
# 		4. For a PREPARED XA transaction, you can issue an XA_COMMIT statement to commit and
# 			terminate the transaction, or XA_ROLLBACK to roll back and terminate the transaction.
#
# Here is a simple XA transaction that inserts a row into a table as part of a global transaction:
#
# 		XA START 'xatest';
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		INSERT INTO mytable (i) VALUES(10);
# 		Query OK, 1 row affected (0.04 sec)
#
# 		XA END 'xatest';
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		XA PREPARE 'xatest';
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		XA COMMIT 'xatest';
# 		Query OK, 0 rows affected (0.00 sec)
#
# Within the context of a given client connection, XA transactions and local (non-XA) transactions
# are mutually exclusive.
#
# For example, if XA_START has been issued to begin an XA transaction, a local transaction
# cannot be started until the XA transaction has been committed or rolled back.
#
# Conversely, if a local transaction has been started with START_TRANSACTION,
# no XA statements can be used until the transaction has been committed or rolled back.
#
# If an XA transaction is in the ACTIVE state, you cannot issue any statements that cause
# an implicit commit.
#
# That would violate the XA contract because you could not roll back the XA transaction.
#
# You will receive the following error if you try to execute such a statement:
#
# 		ERROR 1399 (XAE07): XAER_RMFAIL: The command cannot be executed
# 		when global transaction is in the ACTIVE state
#
# Statements to which the preceding remark applies are listed at 
# SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# 13.4 REPLICATION STATEMENTS
#
# 13.4.1 SQL STATEMENTS FOR CONTROLLING MASTER SERVERS
# 13.4.2 SQL STATEMENTS FOR CONTROLLING SLAVE SERVERS
# 13.4.3 SQL STATEMENTS FOR CONTROLLING GROUP REPLICATION
#
# Replication can be controlled through the SQL interface using the statements
# described in this section.
#
# Statements are split into a group which controls master servers, a group
# which controls slave servers, and a group which can be applied to any replication
# servers.
#
# 13.4.1 SQL STATEMENTS FOR CONTROLLING MASTER SERVERS
#
# 13.4.1.1 PURGE BINARY LOGS SYNTAX
# 13.4.1.2 RESET MASTER SYNTAX
# 13.4.1.3 SET SQL_LOG_BIN SYNTAX
#
# This section discusses statements for managing master replication servers.
#
# SECTION 13.4.2, "SQL STATEMENTS FOR CONTROLLING SLAVE SERVERS", discusses
# statements for managing slave servers.
#
# In addition to the statements described here, the following SHOW statements
# are used with master servers in replication.
#
# For information about these statements, see SECTION 13.7.6, "SHOW SYNTAX"
#
# 		) SHOW BINARY LOGS
#
# 		) SHOW BINLOG EVENTS
#
# 		) SHOW MASTER STATUS
#
# 		) SHOW SLAVE HOSTS
#
# 13.4.1.1 PURGE BINARY LOGS SYNTAX
#
# 	PURGE { BINARY | MASTER } LOGS
# 		{ TO 'log_name' | BEFORE datetime_expr }
#
# The binary log is a set of files that contain information about data modifications
# made by the MySQL server.
#
# The log consists of a set of binary log files, plus an index file 
# (See SECTION 5.4.4, "THE BINARY LOG")
#
# The PURGE_BINARY_LOGS statement deletes all the binary log files listed in the
# log index file prior to the specified log file name or date.
#
# BINARY and MASTER are synonyms
#
# Deleted log files also are removed from the list recorded in the index file, so that
# the given log file becomes the first in the list.
#
# This statement has no effect if the server was not started with the --log-bin
# option to enable binary logging.
#
# Examples:
#
# 		PURGE BINARY LOGS TO 'mysql-bin.010';
# 		PURGE BINARY LOGS BEFORE '2008-04-02 22:46:26';
#
# The BEFORE variant's datetime_expr argument should evaluate to a DATETIME value
# (a value in 'YYYY-MM-DD hh:mm:ss' format)
#
# This statement is safe to run while slaves are replicating.
#
# You need not stop them. If you have an active slave that currently is reading
# one of the log files you are trying to delete, this statement does not delete
# the log file that is in use or any log files later than that one, but it deletes
# any earlier log files.
#
# A warning message is issued in this situation.
#
# However, if a slave is not connected and you happen to purge one of the log files
# it has yet to read, the slave will be unable to replicate after it reconnects.
#
# To safely purge binary log files, follow this procedure:
#
# 		1. On each slave server, use SHOW_SLAVE_STATUS to check which log file it is reading.
#
# 		2. Obtain a listing of the binary log files on the master server with SHOW_BINARY_LOGS
#
# 		3. Determine the earliest log file among all the slaves.
#
# 			This is the target file. If all the slaves are up to date, this is the last log file
# 			on the list.
#
# 		4. Make a backup of all the log files you are about to delete (optional)
#
# 		5. Purge all log files up to but not including the target file
#
# PURGE BINARY LOGS TO and PURGE BINARY LOGS BEFORE both fail with an error when binary
# log files listed in the .index file had been removed from the system by some other means
# (such as using rm on Linux)
#
# (Bug #18199, Bug #18453) 
#
# To handle such errors, edit the .index file (which is a simple text file) manually
# to ensure that it lists only the binary log files that are actually present, then
# run again the PURGE_BINARY_LOGS statement that failed.
#
# Binary log files are automatically removed after the server's binary log expiration
# period.
#
# Removal of the files can take place at startup and when the binary log is flushed.
#
# The default binary log expiration period is 30 days.
#
# You can specify an alternative expiration period using the binlog_expire_logs_seconds
# system variable.
#
# If you are using replication, you should specify an expiration period that is no lower
# than the maximum amount of time your slaves might lag behind the master.
#
# 13.4.1.2 RESET MASTER SYNTAX
#
# 		RESET MASTER [TO binary_log_file_number]
#
# RESET MASTER enables you to delete any binary log files and their related binary log
# index file, returning the master to its state before binary logging was started.
#
# WARNING:
#
# 		Use this statement with caution to ensure you do not lose binary log file data
#
# Issuing RESET MASTER without the optional TO clause deletes all binary log files listed
# in the index file, resets the binary log index file to be empty, and creates a new
# binary log file starting at 1.
#
# Use the optional TO clause to start the binary log file index from a number other than
# 1 after the reset.
#
# Issuing RESET MASTER also clears the values of the gtid_purged system variable and the
# gtid_executed system variable; that is, issuing this statement sets each of these
# values to an empty string ('')
#
# This statement also clears the mysql.gtid_executed table (see MYSQL.GTID_EXECUTED TABLE)
#
# Using RESET MASTER with the TO clause to specify a binary log file index number to start
# from simplifies failover by providing a single statement alternative to the FLUSH_BINARY_LOGS
# and PURGE_BINARY_LOGS_TO statements.
#
# The following example demonstrates TO clause usage:
#
# 		RESET MASTER TO 1234;
#
# 		SHOW BINARY LOGS;
# 		+-----------------------------+---------------+
# 		| Log_name 							| File_size 	 |
# 		+-----------------------------+---------------+
# 		| master-bin.001234 			   | 154 			 |
# 		+-----------------------------+---------------+
#
# IMPORTANT:
#
# 		The effects of RESET_MASTER without the TO clause differ from those of
# 		PURGE_BINARY_LOGS in 2 key ways:
#
# 			1. RESET_MASTER removes all binary log files that are listed in the index file,
# 				leaving only a single, empty binary log file with a numeric suffix of
# 				.000001, whereas the numbering is not reset by PURGE_BINARY_LOGS
#
# 			2. RESET_MASTER is not intended to be used while any replication slaves
# 				are running.
#
# 				The behavior of RESET_MASTER when used while slaves are running
# 				is undefined (and thus unsupported), whereas PURGE_BINARY_LOGS
# 				may be safely used while replication slaves are running.
#
# 		See also SECTION 13.4.1.1, "PURGE BINARY LOGS SYNTAX"
#
# RESET_MASTER without the TO clause can prove useful when you first set up
# the master and the slave, so that you can verify the setup as follows:
#
# 		1. Start the master and slave, and start replication (see SECTION 17.1.2, "SETTING UP BINARY LOG FILE POSITION BASED REPLICATION")
#
# 		2. Execute a few test queries on the master
#
# 		3. Check that the queries were replicated to the slave
#
# 		4. When replication is running correctly, issue STOP_SLAVE followed by
# 			RESET_SLAVE on the slave, then verify that no unwanted data from the
# 			test queries exists on the slave.
#
# 		5. Issue RESET_MASTER on the master to clean up the test queries
#
# After verifying the setup, resetting the master and slave and ensuring that no unwanted
# data or binary log files generated by testing remain on the master or slave, you can
# start the slave and begin replicating.
#
# 13.4.1.3 SET SQL_LOG_BIN_SYNTAX
#
# 		SET sql_log_bin = {OFF|ON}
#
# The sql_log_bin variable controls whether logging to the binary log is enabled
# for the current session (assuming that the binary log itself is enabled)
#
# The default value is ON
#
# To disable or enable binary logging for the current session, set the session
# sql_log_bin variable to OFF or ON
#
# Set this variable to OFF for a session to temporarily disable binary logging
# while making changes to the master you do not want replicated to the slave.
#
# Setting the session value of this system variable is a restricted operation
#
# The session user must have privileges sufficient to set restricted session
# variables.
#
# See SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES"
#
# It is not possible to set the session value of sql_log_bin within
# a transaction or subquery
#
# Setting this variable to OFF prevents GTIDs from being assigned to transactions
# in the binary log.
#
# If you are using GTIDs for replication, this means that even when binary logging
# is later enabled again, the GTIDs written into the log from this point do not
# account for any transactions that occurred in the meantime, so in effect those
# transactions are lost.
#
# 13.4.2 SQL STATEMENTS FOR CONTROLLING SLAVE SERVERS
#
# 13.4.2.1 CHANGE MASTER TO SYNTAX
# 13.4.2.2 CHANGE REPLICATION FILTER SYNTAX
#
# 13.4.2.3 MASTER_POS_WAIT() SYNTAX
# 13.4.2.4 RESET SLAVE SYNTAX
#
# 13.4.2.5 SET GLOBAL SQL_SLAVE_SKIP_COUNTER SYNTAX
# 13.4.2.6 START SLAVE SYNTAX
#
# 13.4.2.7 STOP SLAVE SYNTAX
#
# This section discusses statements for managing slave replication servers.
#
# SECTION 13.4.1, "SQL STATEMENTS FOR CONTROLLING MASTER SERVERS", discusses statements
# for managing master servers.
#
# In addition to the statements described here, SHOW_SLAVE_STATUS and SHOW_RELAYLOG_EVENTS
# are also used with replicaiton slaves.
#
# For information about these statements, see SECTION 13.7.6.34, "SHOW SLAVE STATUS SYNTAX"
# and SECTION 13.7.6.32, "SHOW RELAYLOG EVENTS SYNTAX"
#
# 13.4.2.1 CHANGE MASTER TO SYNTAX
#
# 	CHANGE MASTER TO option [, option] --- [ channel_option ]
#
# 	option:
# 		MASTER_BIND = 'interface_name'
#   | MASTER_HOST = 'host_name'
#   | MASTER_USER = 'user_name'
#   | MASTER_PASSWORD = 'password'
#   | MASTER_PORT = port_num
#   | MASTER_CONNECT_RETRY = interval
#   | MASTER_RETRY_COUNT = count
#   | MASTER_DELAY = interval
#
#   | MASTER_HEARTBEAT_PERIOD = interval
# 	 | MASTER_LOG_FILE = 'master_log_name'
#   | MASTER_LOG_POS = master_log_pos
#   | MASTER_AUTO_POSITION = {0|1}
#   | RELAY_LOG_FILE = 'relay_log_name'
# 	 | RELAY_LOG_POS = relay_log_pos
#
# 	 | MASTER_SSL = {0|1}
# 	 | MASTER_SSL_CA = 'ca_file_name'
#   | MASTER_SSL_CAPATH = 'ca_directory_name'
# 	 | MASTER_SSL_CERT = 'cert_file_name'
# 	 | MASTER_SSL_CRT = 'crl_file_name'
# 	 | MASTER_SSL_CRLPATH = 'crl_directory_name'
#   | MASTER_SSL_KEY = 'key_file_name'
#   | MASTER_SSL_CIPHER = 'cipher_list'
#   | MASTER_SSL_VERIFY_SERVER_CERT = {0|1}
#   | MASTER_TLS_VERSION = 'protocol_list'
#   | MASTER_PUBLIC_KEY_PATH = 'key_file_name'
#   | GET_MASTER_PUBLIC_KEY = {0|1}
#   | IGNORE_SERVER_IDS = (server_id_list)
#
# 	channel_option:
# 		FOR CHANNEL channel
#
# 	server_id_list:
# 		[server_id [, server_id] --- ]
#
# CHANGE_MASTER_TO changes the parameters that the slave server uses for
# connecting to the master server, for reading the master binary log and
# reading the slave relay log.
#
# It also updates the contents of the master info and relay log info repositories
# (see SECTION 17.2.4, "REPLICATION RELAY AND STATUS LOGS")
#
# CHANGE_MASTER_TO requires the REPLICATION_SLAVE_ADMIN or SUPER privilege
#
# You can issue CHANGE MASTER TO statements on a running slave without first
# stopping it, depending on the states of the slave SQL thread and slave I/O
# thread.
#
# The rules governing such use are provided later in this section
#
# When using a multithreaded slave (in other words slave_parallel_workers is greater
# than 0), stopping the slave can cause "gaps" in the sequence of transactions that
# have been executed from the relay log, regardless of whether the slave was stopped
# intentionally or otherwise.
#
# When such gaps exist, issuing CHANGE_MASTER_TO fails
#
# The solution in this situation is to issue START_SLAVE_UNTIL_SQL_AFTER_MTS_GAPS
# which ensures that the gaps are closed.
#
# The optional FOR CHANNEL channel clause enables you to name which replication
# channel the statement applies to.
#
# Providing a FOR CHANNEL channel clause applies the CHANGE MASTER to statement
# to a specific replication channel, and is used to add a new channel or modify
# an existing channel.
#
# For example, to add a new channel called channel2:
#
# 		CHANGE MASTER TO MASTER_HOST=host1, MASTER_PORT=3002 FOR CHANNEL 'channel2'
#
# If no clause is named and no extra channels exist, the statement applies to the
# default channel.
#
# When using multiple replication channels, if a CHANGE MASTER TO statement does not
# name a channel using a FOR CHANNEL channel clause, an error occurs.
#
# See SECTION 17.2.3, "REPLICATION CHANNELS" for more information.
#
# Options not specified retain their value, except as indicated in the following discussion.
#
# Thus, in most cases, there is no need to specify options that do not change.
#
# MASTER_HOST, MASTER_USER, MASTER_PASSWORD and MASTER_PORT provide information
# to the slave about how to connect to its master:
#
# 		) MASTER_HOST and MASTER_PORT are the host name (or IP address) of the master host and its TCP/IP port
#
# 			NOTE:
#
# 				Replication channel use UNIX socket files.
#
# 				You must be able to connect to the master MySQL server using TCP/IP
#
# 			If you specify the MASTER_HOST or MASTER_PORT option, the slave assumes that the master
# 			server is different from before (even if the option value is the same as its current value)
#
# 			In this case, the old values for the master binary log file name and position are considered
# 			no longer applicable, so if you do not specify MASTER_LOG_FILE and MASTER_lOG_POS in the
# 			statement, MASTER_LOG_FILE='' and MASTER_LOG_POS=4 are silently appended to it.
#
# 			Setting MASTER_HOST='' (that is, setting its value explicitly to an empty string) is not
# 			the same as not setting MASTER_HOST at all.
#
# 			Trying to set MASTER_HOT to an empty string fails with an error.
#
# 			Values used for MASTER_HOST and other CHANGE MASTER TO options are checked for
# 			linefeed (\n or 0x0A) characters; the presence of such characters in these values
# 			causes the statement to fail with ER_MASTER_INFO.
#
# 			(Bug #11758581, Bug #50801)
#
# 		) MASTER_USER and MASTER_PASSWORD are the user name and password of the account to use
# 			for connecting to the master.
#
# 			MASTER_USER cannot be made empty; setting MASTER_USER = '' or leaving it unset when
# 			setting a value for MASTER_PASSWORD causes an error (Bug #13427949)
#
# 			The password used for a MySQL Replication slave account in a CHANGE MASTER TO
# 			statement is limited to 32 characters in length;
#
# 			Trying to use a password of more than 32 characters causes CHANGE MASTER TO to fail
#
# 			The text of a running CHANGE_MASTER_TO statement, including values for MASTER_USER and
# 			MASTER_PASSWORD, can be seen in the output of a concurrent SHOW_PROCESSLIST statement.
#
# 			(The complete text of a START_SLAVE statement is also visible to SHOW_PROCESSLIST)
#
# The MASTER_SSL_xxx options, and the MASTER_TLS_VERSION option, specify how the slave uses
# encryption and ciphers to secure the replication connection.
#
# These options can be changed even on slaves that are compiled without SSL support.
#
# They are saved to the master info repository, but are ignored if the slave does not
# have SSL support enabled.
#
# The MASTER_SSL_xxx options perform the same functions as the --ssl-xxx options described
# in SECTION 6.4.2, "COMMAND OPTIONS FOR ENCRYPTED CONNECTIONS"
#
# The correspondence between the two sets of options, and the use of the MASTER_SSL_xxx
# and MASTER_TLS_VERSION options to set up a secure connection, is explained in
# SECTION 17.3.9, "SETTING UP REPLICATION TO USE ENCRYPTED CONNECTIONS"
#
# 		IMPORTANT:
#
# 			To connect to the replication master using a user account that authenticates
# 			with the caching_sha2_password plugin, you must either set up a secure
# 			connection as described in SECTION 17.3.9, "SETTING UP REPLICATION TO USE ENCRYPTED CONNECTIONS",
# 			or enable the unencrypted connection to support password exchange using an
# 			RSA key pair.
#
# 			The caching_sha2_password authentication plugin is the default for new users
# 			created from MySQL 8.0 (for details, see SECTION 6.5.1.3, "CACHING SHA-2 PLUGGABLE AUTHENTICATION")
#
# 			If the user account that you create or use for replication (as specified by the
# 			MASTER_USER option) uses this authentication plugin, and you are not using a
# 			secure connection, you must enable RSA key pair-based password exchange for a 
# 			successful connection.
#
# To enable RSA key pair-based password exchange, specify either the MASTER_PUBLIC_KEY_PATH
# or the GET_MASTER_PUBLIC_KEY=1 option
#
# Either of these options provides the RSA public key to the slave:
#
# 		) MASTER_PUBLIC_KEY_PATH indicates the path name to a file containing a slave-side
# 			copy of the public key required by the master for RSA key pair-based password
# 			exchange.
#
# 			The file must be in PEM format.
#
# 			This option applies to slaves that authenticate with the sha256_password
# 			or caching_sha2_password authentication plugin.
#
# 			(For sha256_password, MASTER_PUBLIC_KEY_PATH can be used only if MySQL
# 			was built using OpenSSL)
#
# 		) GET_MASTER_PUBLIC_KEY indicates whether to request from the master the public key
# 			required for RSA key pair-based password exchange.
#
# 			This option applies to slaves that authenticate with the caching_sha2_password
# 			authentication plugin.
#
# 			For connections by accounts that authenticate using this plugin, the master
# 			does not send the public key unless requested, so it must be requested or
# 			specified in the client.
#
# 			If MASTER_PUBLIC_KEY_PATH is given and specifies a valid public key file,
# 			it takes precedence over GET_MASTER_PUBLIC_KEY
#
# The MASTER_HEARTBEAT_PERIOD, MASTER_CONNECT_RETRY and MASTER_RETRY_COUNT options
# control how the slave recognizes that the connection to the master has been
# lost and makes attempts to reconnect.
#
# 		) The slave_net_timeout system variable specifies the number of seconds that
# 			 the slave waits for either more data or a heartbeat signal from the master,
# 			before the slave considers the connection broken, aborts the read, and tries
# 			to reconnect.
#
# 			The default value is 60 seconds (one minute)
#
# 		) The heartbeat interval, which stops the connection timeout occurring in the
# 			absence of data if the connection is still good, is controlled by the 
# 			MASTER_HEARTBEAT_PERIOD option.
#
# 			A heartbeat signal is sent to the slave after that number of seconds, and the
# 			waiting period is reset whenever the master's binary log is updated
# 			with an event.
#
# 			Heartbeats are therefore sent by the master only if there are no unset events
# 			in the binary log file for a period longer than this.
#
# 			The heartbeat interval interval is a decimal value having the range 0 to 4294967
# 			seconds and a resolution in milliseconds; the smallest nonzero value is 0.001
#
# 			Setting interval to 0 disables heartbeats altogether.
#
# 			The heartbeat interval defaults to half the value of the slave_net_timeout
# 			system variable.
#
# 			It is recorded in the master info log and shown in the replication_connection_configuration
# 			Performance Schema table.
#
# 			Issuing RESET_SLAVE resets the heartbeat interval to the default value.
#
# 			Note that a change to the value or default setting of slave_net_timeout does not
# 			automatically change the heartbeat interval, whether that has been set
# 			explicitly or is using a previously calculated default.
#
# 			A warning is issued if you set @@GLOBAL.slave_net_timeout to a value less than
# 			that of the current heartbeat interval.
#
# 			If slave_net_timeout is changed, you must also issue CHANGE_MASTER_TO to adjust
# 			the heartbeat interval to an appropriate value so that the heartbeat signal occurs before
# 			the connection timeout.
#
# 			If you do not do this, the heartbeat signal has no effect, and if no data is received
# 			from the master, the slave can make repeated reconnection attempts, creating zombie
# 			dump threads.
#
# 		) If the slave does need to reconnect, the first retry occurs immediately after the timeout.
#
# 			MASTER_CONNECT_RETRY specifies the interval between reconnection attempts, and MASTER_RETRY_COUNT
# 			limits the number of reconnection attempts.
#
# 			If both the default settings are used, the slave waits 60 seconds between reconnection attempts
# 			(MASTER_CONNECT_RETRY=60), and keeps attempting to reconnect at this rate for 24 hours
# 			(MASTER_RETRY_COUNT=86400)
#
# 			These values are recorded in the master info log and shown in the replication_connection_configuration
# 			Performance Schema table.
#
# 			MASTER_RETRY_COUNT supersedes the --master-retry-count server startup option.
#
# MASTER_DELAY specifies how many seconds behind the master the slave must lag.
#
# An event received from the master is not executed until at least interval seconds later
# than its execution on the master.
#
# The default is 0.
#
# An error occurs if interval is not a nonnegative integer in teh range from 0 to 2^31-1
#
# For more information, see SECTION 17.3.12, "DELAYED REPLICATION"
#
# A CHANGE MASER TO statement employing the MASTER_DELAY option can be executed on a running
# slave when the slave SQL thread is stopped.
#
# MASTER_BIND is for use on replication slaves having multiple network interfaces, and determines
# which of the slave's network interfaces is chosen for connecting to the master.
#
# The address configured with this option, if any, can be seen in the Master_Bind column
# of the output from SHOW_SLAVE_STATUS
#
# In the master info repository table mysql.slave_master_info, the value can be seen
# as the Master_bind column.
#
# The ability to bind a replicaiton slave to a specific network interface is also supported
# by NDB Cluster.
#
# MASTER_lOG_FILE and MASTER_LOG_POS are the coordinates at which the slave I/O thread should
# begin reading from the master the next time the thread starts.
#
# RELAY_LOG_FILE and RELAY_LOG_POS are the coordinates at which the slave SQL thread should
# begin reading from teh relay log the next time the thread starts.
#
# If you specify either of MASTER_LOG_FILE or MASTER_LOG_POS, you cannot specify RELAY_LOG_FILE
# or RELAY_LOG_POS
#
# If you specify either of MASTER_LOG_FILE or MASTER_LOG_POS, you also cannot specify MASTER_AUTO_POSITION = 1
# (described later in this section)
#
# If neither of MASTER_LOG_FILE or MASTER_LOG_POS is specified, the slave uses the last coordinates
# of the slave SQL thread before CHANGE_MASTER_TO was issued.
#
# This ensures that there is no discontinuity in replication, even if the slave SQL thread was
# late compared to the slave I/O thread, when you merely want to change, say, the PW.
#
# A CHANGE MASTER TO statement employing RELAY_LOG_FILE, RELAY_LOG_POS or both options
# can be executed on a running slave when the slave SQL thread is stopped.
#
# Relay logs are preserved if at least one of the slave SQL thread and the slave I/O
# thread is running; if both threads are stopped, all relay log files are deleted
# unless at least one of RELAY_LOG_FILE or RELAY_LOG_POS is specified.
#
# RELAY_LOG_FILE can use either an absolute or relative path, and uses teh same base name
# as MASTER_LOG_FILE
#
# When MASTER_AUTO_POSITION = 1 is used with CHANGE MASTER TO, the slave attempts to connect
# to the master using the GTID-based replication protocol.
#
# This option can be used with CHANGE MASTER TO only if both the slave SQL and slave I/O
# threads are stopped.
#
# Both the slave and the master must have GTIDs enabled (GTID MODE=ON, ON_PERMISSIVE or OFF_PERMISSIVE
# on the slave, and GTID_MODE=ON on the master)
#
# Auto-positioning is used for the connection, so the coordinates represented by MASTER_LOG_FILE
# and MASTER_LOG_POS are not used, and the use of either or both of these options together with
# MASTER_AUTO_POSITION = 1 causes an error.
#
# If multi-source replication is enabled on the slave, you need to set the MASTER_AUTO_POSITION = 1
# option for each applicable replication channel.
#
# With MASTER_AUTO_POSITION = 1 set, in the initial connection handshake, the slave sends a 
# GTID set containing the transactions that it has already received, committed, or both.
#
# The master responds by sending all transactions recorded in its binary log whose GTID
# is not included in the GTID set sent by the slave.
#
# This exchange ensures that the master only sends the transactions with a GTID that the slave
# has not already recorded or committed.
#
# If the slave receives transactions from more than one master, as in the case of a 
# diamond topology, the auto-skip function ensures that the transactions are not applied twice.
#
# For details of how the GTID set sent by the slave is computed, see SECTION 17.1.3.3,
# "GTID AUTO-POSITIONING"
#
# If any of the transactions that should be sent by the master have been purged from the
# master's binary log, or added to the set of GTIDs in the gtid_purged system variable
# by another method, the master sends the error:
#
# 		ER_MASTER_HAS_PURGED_REQUIRED_GTIDS
#
# to the slave, and replication does not start.
#
# The GTIDs of the missing purged transactions are identified and listed in the master's
# error log in the warning message:
#
# 		ER_FOUND_MISSING_GTIDS
#
# Also, if during the exchange of transactions it is found that the slave has recorded
# or committed transactions with the master's UUID in the GTID, but the master itself
# has not committed them, the master sends the error:
#
# 		ER_SLAVE_HAS_MORE_GTIDS_THAN_MASTER
#
# to the slave and replication does not start.
#
# For information on how to handle these situations, see SECTION 17.1.3.3,
# "GTID AUTO-POSITIONING"
#
# You can see whether replication is running with auto-positoning enabled
# by checking the Performance Schema replication_connection_status table
# or the output of SHOW_SLAVE_STATUS
#
# Disabling the MASTER_AUTO_POSITION option again makes the slave revert
# to file-based replication, in which case you must also specify one or both
# of the MASTER_LOG_FILE or MASTER_LOG_POS options.
#
# IGNORE_SERVER_IDS takes a comma-separated list of 0 or more server IDs.
#
# Events originating from the corresponding servers are ignored, with the 
# exception of log rotation and deletion events, which are still recorded
# in the relay log.
#
# In circular replication, the originating server normally acts as the terminator
# of its own events, so that they are not applied more than once.
#
# Thus, this option is useful in circular replication when one of the servers
# in the circle is removed.
#
# Suppose that you have a circular replication setup with 4 servers,
# having server IDs 1, 2, 3 and 4 and server 3 fails.
#
# When bridging the gap by starting replication from server 2 to server
# 4, you can include IGNORE_SERVER_IDS = (3) in the CHANGE_MASTER_TO
# statement that you issue on server 4 to tell it to use server 2 as its
# master instead of server 3.
#
# Doing so causes it to ignore and not to propagate any statements
# that originated with the server that is no longer in use.
#
# if IGNORE_SERVER_IDS contains the server's own ID and the server was
# started with the --replicate-same-server-id option enabled, an error results.
#
# NOTE:
#
# 		When global transaction identifiers (GTIDs) are used for replication,
# 		transactions that have already been applied are automatically ignored,
# 		so the IGNORE_SERVER_IDS function is not required and is deprecated.
#
# 		If gtid_mode=ON is set for the server, a deprecation warning is issued
# 		if you include the IGNORE_SERVER_IDS option in a CHANGE_MASTER_TO 
# 		statement.
#
# The master info repository and the output of SHOW_SLAVE_STATUS provide the list
# of servers that are currently ignored.
#
# For more information, see SECTION 17.2.4.2, "SLAVE STATUS lOGS" and
# SECTION 13.7.6.34, "SHOW SLAVE STATUS SYNTAX"
#
# If a CHANGE_MASTER_TO statement is issued without any IGNORE_SERVER_IDS
# option, any existing list is preserved.
#
# To clear the list of ignored servers, it is necessary to use the option
# with an empty list:
#
# 		CHANGE MASTER TO IGNORE_SERVER_IDS = ();
#
# RESET SLAVE ALL clears IGNORE_SERVER_IDS
#
# NOTE:
#
# 		A deprecation warning is issued if SET GTID_MODE=ON is issued
# 		when any channel has existing server IDs set with IGNORE_SERVER_IDS
#
# 		Before starting GTID-based replication, check for and clear all ignored
# 		server ID lists on the servers involved.
#
# 		The SHOW_SLAVE_STATUS statement displays the list of ignore IDs,
# 		if there is one.
#
# 		If you do receive the deprecation warning, you can still clear a 
# 		list after gtid_mode=ON is set by issuing a CHANGE_MASTER_TO statement
# 		containing the IGNORE_SERVER_IDS option with an empty list.
#
# Invoking CHANGE_MASTER_TO causes the previous values for MASTER_HOST, MASTER_PORT,
# MASTER_LOG_FILE and MASTER_LOG_POS to be written to the error log, along with
# other information about the slave's state prior to execution.
#
# CHANGE MASTER TO causes an implicit commit of an ongoing transaction.
#
# See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# From MySQL 5.7, the strict requirements to execute STOP_SLAVE prior to issuing
# any CHANGE_MASTER_TO statement (and START_SLAVE afterward) is removed.
#
# Instead of depending on whether the slave is stopped, the behavior of CHANGE MASTER TO
# depends on the states of the slave SQL thread and slave I/O threads; which of these
# threads is stopped or running now determines the options that can or cannot be used
# with a CHANGE MASTER TO statement at a given point in time.
#
# The rules for making this determinaiton are listed here:
#
# 		) If hte SQL thread is stopped, you can execute CHANGE MASTER TO using
# 			any combination that is otherwise allowed of RELAY_LOG_FILE, RELAY_LOG_POS,
# 			and MASTER_DELAY options, even if the slave I/O thread is running.
#
# 			No other options may be used with this statement when the I/O thread
# 			is running.
#
# 		) If the I/O thread is stopped, you can execute CHANGE MASTER TO using any
# 			of the options for this statement (in any allowed combination) except:
#
# 				RELAY_LOG_FILE
#
# 				RELAY_LOG_POS
#
# 				MASTER_DELAY
#
# 			even when the SQL thread is running.
#
# 			These three options may not be used when the I/O thread is running.
#
# 		) Both the SQL thread and the I/O thread must be stopped before issuing a 
# 			CHANGE MASTER TO statement that employs MASTER_AUTO_POSITION = 1
#
# You can check the current state of the slave SQL and I/O threads using SHOW_SLAVE_STATUS
#
# For more information, see SECTION 17.3.8, "SWITCHING MASTERS DURING FAILOVER"
#
# If you are using statement-based replication and temporary tables, it is possible
# for a CHANGE MASTER TO statement following a STOP SLAVE statement to leave behind
# temporary tables on the slave.
#
# A warning (ER_WARN_OPEN_TEMP_TABLES_MUST_BE_ZERO) is now issued whenever this occurs.
#
# You can avoid this in such cases by making sure that the value of the SLAVE_OPEN_TEMP_TABLES
# system status variable is equal to 0 prior to executing such a CHANGE MASTER TO statement.
#
# CHANGE_MASTER_TO is useful for setting up a slave when you have the snapshot of the master
# and have recorded the master binary log coordinates corresponding to the time of hte
# snapshot.
#
# After loading the snapshot into the slave to synchronize it with the master, you can run
# CHANGE MASTER TO MASTER_LOG_FILE='log_name', MASTER_LOG_POS=log_pos on the slave to specify
# the coordinates at which the slave should begin reading the master binary log.
#
# The following example changes the master server the slave uses and establishes the master
# binary log coordinates from which the slave begins reading.
#
# This is used when you want to set up the slave to replicate the master:
#
# 		CHANGE MASTER TO
# 			MASTER_HOST='master2.example.com',
# 			MASTER_USER='replication',
# 			MASTER_PASSWORD='password',
# 			MASTER_PORT=3306,
# 			MASTER_LOG_FILE='master2-bin.001',
# 			MASTER_LOG_POS=4,
# 			MASTER_CONNECT_RETRY=10;
#
# The next example shows an operation that is less frequently employed.
#
# It is used when the slave has relay log files that you want it to execute
# again for some reason.
#
# To do this, the master need not be reachable.
#
# You need only use CHANGE_MASTER_TO and start the SQL thread (START SLAVE SQL_THREAD):
#
# 		CHANGE MASTER TO
# 			RELAY_LOG_FILE='slave-relay-bin.006',
# 			RELAY_LOG_POS=4025;
#
# The following table shows the maximum permissible length for hte string-valued options.
#
# OPTION 			MAX LENGTH
# 
# MASTER_HOST 			60
# 					
# MASTER_USER 			96
#
# MASTER_PASSWORD 	32
#
# MASTER_LOG_FILE 	511
#
# RELAY_LOG_FILE 		511
#
# MASTER_SSL_CA 		511
#
# MASTER_SSL_CAPATH 	511
#
# MASTER_SSL_CERT 	511
#
# MASTER_SSL_CRL 		511
#
# MASTER_SSL_CRLPATH 511
#
# MASTER_SSL_KEY 		511
#
# MASTER_SSL_CIPHER 	511
#
# MASTER_TLS_VERSION 511
#
# MASTER_PUBLIC_KEY_PATH 511
#
# 13.4.2.2 CHANGE REPLICATION FILTER SYNTAX
#
# CHANGE REPLICATION FILTER filter[, filter]
# 		[, ---] [FOR CHANNEL channel]
#
# filter:
# 		REPLICATE_DO_DB = (db_list)
#   | REPLICATE_IGNORE_DB = (db_list)
#   | REPLICATE_DO_TABLE = (tbl_list)
#   | REPLICATE_IGNORE_TABLE = (tbl_list)
#   | REPLICATE_WILD_DO_TABLE = (wild_tbl_list)
# 	 | REPLICATE_WILD_IGNORE_TABLE = (wild_tbl_list)
#   | REPLICATE_REWRITE_DB = (db_pair_list)
#
# db_list:
# 		db_name[, db_name][, ---]
#
# tbl_list:
# 		db_name.table_name[, db_name.table_name[, ---]
# wild_tbl_list:
# 		'db_pattern.table_pattern'[, 'db_pattern.table_pattern'][, ---]
#
# db_pair_list:
# 		(db_pair)[, (db_pair)][, ---]
#
# db_pair:
# 		from_db, to_db
#
# CHANGE REPLICATION FILTER sets one or more replicaiton filtering rules on the
# slave in teh same way as starting the slave mysqld with replication filtering
# options such as:
#
# 		--replicate-do-db
#
# 		or
#
# 		--replicate-wild-ignore-table
#
# Unlike the case with the server options, this statement does not require
# restarting the server to take effect, only that the slave SQL thread
# be stopped using STOP_SLAVE_SQL_THREAD first (and restarted with START_SLAVE_SQL_THREAD afterwards).
#
# CHANGE_REPLICATION_FILTER requires the REPLICATION_SLAVE_ADMIN or SUPER privilege
#
# Use the FOR CHANNEL channel clause to make a replicaiton filter specific to a replication
# channel, for example on a multi-source replication slave.
#
# Filters applied without a specific FOR CHANNEL clause are considered global filters,
# meaning that they are applied to all replication channels.
#
# 	NOTE:
#
# 		Global replication filters cannot be set on a MySQL server instance that is configured
# 		for Group Replication, because filtering transactions on some servers would make the group
# 		unable to reach agreement on a consistent state.
#
# 		Channel specific replication filters can be set on replication channels that are not
# 		directly involved with Group Replication, such as where a group member also acts
# 		as a replication slave to a master that is outside the group.
#
# 		They cannot be set on the group_replication_applier or group_replication_recovery channels
#
# The following list shows the CHANGE REPLICATION FILTER options and how they relate to 
# --replicate-* server options:
#
# 		) REPLICATE_DO_DB: Include updates based on database name. Equivalent to --replicate-do-db
#
# 		) REPLICATE_IGNORE_DB: Exclude updates based on database name. Equivalent to --replicate-ignore-db
#
# 		) REPLICATE_DO_TABLE: Include updates based on table name. Equivalent to --replicate-do-table
#
# 		) REPLICATE_IGNORE_TABLE: Exclude updates based on table name. Equivalent to --replicate-ignore-table
#
# 		) REPLICATE_WILD_DO_TABLE: Include updates based on wildcard pattern matching table name. Equivalent to --replicate-wild-do-table
#
# 		) REPLICATE_WILD_IGNORE_TABLE: Exclude updates based on wildcard pattern matching table name. Equivalent to --replicate-wild-ignore-table
#
# 		) REPLICATE_REWRITE_DB: Perform updates on slave after substituting new name on slave for specified database on master.
#
# 											Equivalent to --replicate-rewrite-db
#
# The precise effects of REPLICATE_DO_DB and REPLICATE_IGNORE_DB filters are dependent on whether statement-based
# or row-based replication is in effect.
#
# See SECTION 17.2.5, "HOW SERVERS EVALUATE REPLICATION FILTERING RULES", for more information.
#
# Multiple replication filtering rules can be created in a single CHANGE REPLICATION FILTER statement
# by separating the rules with commas, as shown here:
#
# 		CHANGE REPLICATION FILTER
# 			REPLICATE_DO_DB = (d1), REPLICATE_IGNORE_DB = (d2);
#
# Issuing the statement just shown is equivalent to starting the slave mysqld with the options
# --replicate-do-db=d1 --replicate-ignore-db=d2
#
# On a multi-score replicaiton slave, which uses multiple replicaiton channels to process transaction
# from different sources, use the FOR CHANNEL channel clause to set a replication filter on
# a replication channel:
#
# 		CHANGE REPLICATION FILTER REPLICATE_DO_DB = (d1) FOR CHANNEL channel_1;
#
# This enables you to create a channel specific replication filter to filter out selected
# data from a source.
#
# When a FOR CHANNEL clause is provided, the replication filter statement acts
# on that slave replication channel removing any existing replication filter which
# has the same filter type as the specified replication filters, and replacing them
# with the specified filter.
#
# Filter types not explicitly listed in the statement are not modified
#
# If issued against a slave replication channel which is not configured,
# the statement fails with an ER_SLAVE_CONFIGURATION error
#
# If issued against Group Replication channels, the statement fails with an
# ER_SLAVE_CHANNEL_OPERATION_NOT_ALLOWED error
#
# On a replication slave with multiple replicaiton channels configured, issuing
# CHANGE_REPLICATION_FILTER with no FOR CHANNEL clause configures the replication
# filter for every configured slave replication channel, and for the global replication filters.
#
# For every filter type, if the filter type is listed in the statement, then any existing
# filter rules of that type are replaced by the filter rules specified in the most recently
# issued statement, otherwise the old value of the filter type is retained.
#
# For more information see SECTION 17.2.5.4, "REPLICATION CHANNEL BASED FILTERS"
#
# If the same filtering rule is specified multiple times, only the last such rule
# is actually used.
#
# For example, the two statements shown here have exactly the same effect,
# because the first REPLICATE_DO_DB rule in the first statement is ignored:
#
# 		CHANGE REPLICATION FILTER
# 			REPLICATE_DO_DB = (db1, db2), REPLICATE_DO_DB = (db3, db4);
#
# 		CHANGE REPLICATION FILTER
# 			REPLICATE_DO_DB = (db3, db4);
#
# CAUTION:
#
# 		This behavior differs from that of the --replicate-* filter options
# 		where specifying the same option multiple times causes the
# 		creation of multiple filter rules.
#
# Names of tables and database not containing any special characters need
# not be quoted.
#
# Values used with REPLICATION_WILD_TABLE and REPLICATION_WILD_IGNORE_TABLE
# are string expressions, possibly containing (special) wildard characters,
# and so must be quoted.
#
# This is shown in the following example statements:
#
# 		CHANGE REPLICATION FILTER
# 			REPLICATE_WILD_DO_TABLE = ('db1.old%');
#
# 		CHANGE REPLICATION FILTER
# 			REPLICATE_WILD_IGNORE_TABLE = ('db1.new%', 'db2.new%');
#
# Values used with REPLICATE_REWRITE_DB represents pairs of database names;
#
# Each such value must be enclosed in parentheses.
#
# The following statement rewrites statements occurring on database db1
# on the master to database db2 on the slave:
#
# 		CHANGE REPLICATION FILTER REPLICATE_REWRITE_DB = ((db1, db2));
#
# The statement just shown contains two sets of parentheses, one enclosing the pair
# of database names, and the other enclosing the entire list.
#
# This is perhaps more easily seen in the following example, which creates two
# rewrite-db rules, one rewriting database dbA to dbB, and one rewriting database dbC to dbD:
#
# 		CHANGE REPLICATION FILTER
# 			REPLICATE_REWRITE_DB = ((dbA, dbB), (dbC, dbD));
#
# The CHANGE_REPLICATION_FILTER statement replaces replication filtering rules only
# for the filter types and replication channels affected by the statement, and leaves
# other rules and channels unchanged.
#
# If you want to unset all filters of a given type, set the filter's value to an
# explicitly empty list, as shown in this example, which removes all existing
# REPLICATE_DO_DB and REPLICATE_IGNORE_DB rules:
#
# 		CHANGE REPLICATION FILTER
# 			REPLICATE_DO_DB = (), REPLICATE_IGNORE_DB = ();
#
# Setting a filter to empty in this way removes all existing rules, does not create
# any new ones, and does not restore any rules set at mysqld startup using --replicate-*
# options on the command line or in the configuration file.
#
# The RESET_SLAVE_ALL statement removes channel specific replication filters that were
# set on channels deleted by the statement.
#
# When the deleted channel or channels are recreated, any global replication filters
# specified for the slave are copied to them, and no channel specific replication
# filters are applied.
#
# For more information, see SECTION 17.2.5, "HOW SERVERS EVALUATE REPLICATION FILTERING RULES"
#
# 13.4.2.3 MASTER_POS_WAIT() SYNTAX
#
# 		SELECT MASTER_POS_WAIT('master_log_file', master_log_pos [, timeout][, channel])
#
# This is actually a function, not a statement.
#
# It is used to ensure that the slave has read and executed events up to a given position
# in the master's binary log.
#
# See SECTION 12.23, "MISCELLANEOUS FUNCTIONS", for a full description.
#
# 13.4.2.4 RESET SLAVE SYNTAX
#
# 		RESET SLAVE [ALL] [channel_option]
#
# 		channel_option:
# 			FOR CHANNEL channel
#
# RESET_SLAVE makes the slave forget its replication channel position in the master's binary log.
#
# This statement is meant to be used for a clean start:
#
# 		it clears the master info and relay log info repositories, deletes all the relay
# 		log files, and starts a new relay log file.
#
# It also resets to 0 the applicaiton delay specified with the MASTER_DELAY option to
# CHANGE MASTER TO RESET_SLAVE does not change the values of gtid_executed or gtid_purged.
#
# NOTE:
#
# 		All relay log files are deleted, even if they have not been completely executed by the
# 		slave SQL thread.
#
# 		(This is a condition likely to exist on a replication slave if you have issued
# 		a STOP_SLAVE statement or if hte slave is highly loaded)
#
# To use RESET_SLAVE, the slave replication threads must be stopped, so on a running slave
# use STOP_SLAVE before issuing RESET_SLAVE
#
# To use RESET_SLAVE on a Group Replication group member, the member status must be 
# OFFLINE, meaning that the plugin is loaded but the member does not currently belong
# to any group.
#
# A group member can be taken offline by using a STOP_GROUP_REPLICATION statement.
#
# The optional FOR CHANNEL channel clause enables you to name which replication channel
# the statement applies to.
#
# Providing a FOR CHANNEL channel clause applies the RESET SLAVE statement to a specific
# replication channel.
#
# Combining a FOR CHANNEL channel clause with the ALL option deletes the specified
# channel.
#
# If no channel is named and no extra channels exist, the statement applies to the
# default channel.
#
# Issuing a RESET_SLAVE_ALL statement without a FOR CHANNEL channel clause when
# multiple replication channels exist deletes all replication channels and recreates
# only the default channel.
#
# See SECTION 17.2.3, "REPLICATION CHANNELS" for more information.
#
# RESET_SLAVE does not change any replication connection parameters such as
# master host, master port, master user or master password.
#
# 		) From MySQL 8.0.13, when master_info_repository=TABLE is set on the server
# 			(which is the default from MySQL 8.0), replication connection parameters
# 			are preserved in the crash-safe InnoDB table mysql.slave_master_info as
# 			part of the RESET_SLAVE operation.
#
# 			They are also retained in memory.
#
# 			In the event of a server crash or deliberate restart after issuing
# 			RESET_SLAVE but before issuing START_SLAVE, the replication connection
# 			parameters are retrieved from the table and reused for the new connection.
#
# 		) When master_info_repository=FILE is set on the server, replication connection
# 			parameters are only retained in memory.
#
# 			If the slave mysqld is restarted immediately after issuing RESET_SLAVE due
# 			to a server crash or deliberate restart, the connection parameters are lost.
#
# 			In that case, you must issue a CHANGE_MASTER_TO statement after the server
# 			start to respecify the connection parameters before issuing START_SLAVE
#
# If you want to reset the connection parameters intentionally, you need to use RESET_SLAVE_ALL,
# which clears the connection parameters.
#
# In that case, you must issue a CHANGE_MASTER_TO statement after the server start to
# specify the new connection parameters.
#
# RESET SLAVE ALL clears the IGNORE_SERVER_IDS list set by CHANGE_MASTER_TO
#
# RESET_SLAVE does not change any replication filter settings (such as --replicate-ignore-table)
# for channels affected by the statement.
#
# However, RESET SLAVE ALL removes the replication filters that were set on the channels
# deleted by the statement.
#
# When the deleted channel or channels are recreated, any global replication filters
# specified for the slave are copied to them, and no channel specific replication filters
# are applied.
#
# For more information, see SECTION 17.2.5.4, "REPLICATION CHANNEL BASED FILTERS"
#
# RESET SLAVE causes an implicit commit of an ongoing transaction.
#
# See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# If the slave SQL thread was in the middle of replicating temporary tables when it was
# stopped, and RESET_SLAVE is issued, these replicated temporary tables are deleted on
# the slave.
#
# RESET SLAVE does not reset the heartbeat period (Slave_heartbeat_period) or SSL_VERIFY_SERVER_CERT
#
# NOTE:
#
# 		When used on an NDB Cluster replication slave SQL node, RESET SLAVE clears the mysql.ndb_apply_status
# 		table.
#
# 		You should keep in mind when using this statement that ndb_apply_status uses the NDB storage engine
# 		and so is shared by all SQL nodes attached to the slave cluster.
#
# 		You can override this behavior by issuing SET GLOBAL @@ndb_clear_apply_status=OFF prior to executing
# 		RESET SLAVE, which keeps the slave from purging the ndb_apply_status table in such cases.
#
# 13.4.2.5 SET GLOBAL SQL_SLAVE_SKIP_COUNTER_SYNTAX
#
# 		SET GLOBAL sql_slave_skip_counter = N
#
# This statement skips the next N events from the master.
#
# This is useful for recovering from replication stops caused by a statement.
#
# When using this statement, it is important to understand that the binary log is actually
# organized as a sequence of groups known as event groups.
#
# Each event group consists of a sequence of events.
#
# 		) For transactional tables, an event group corresponds to a transaction
#
# 		) For nontransactional tables, an event group corresponds to a single SQL statement
#
# NOTE:
#
# 		A single transaction can contain changes to both transactional and nontransactional tables
#
# When you use SET_GLOBAL_sql_slave_skip_counter to skip events and the result is in the middle
# of a group, the slave continues to skip events until it reaches the end of the group.
#
# Execution then starts with the next event group
#
# 13.4.2.6 START SLAVE SYNTAX
#
# 		START SLAVE [thread_types] [until_option] [connection_options] [channel_option]
#
# 		thread_types:
# 			[thread_type [, thread_type] ---]
#
# 		thread_type:
# 			IO_THREAD | SQL_THREAD
#
# 		until_option:
# 			UNTIL { 	{SQL_BEFORE_GTIDS | SQL_AFTER_GTIDS} = gtid_set
# 					|  MASTER_LOG_FILE = 'log_name', MASTER_LOG_POS = log_pos
# 					|  RELAY_LOG_FILE = 'log_name', RELAY_LOG_POS = log_pos
# 					|  SQL_AFTER_MTS_GAPS }
#
# 		connection_options:
# 			[USER='user_name'] [PASSWORD='user_pass'] [DEFAULT_AUTH='plugin_name'] [PLUGIN_DIR='plugin_dir']
#
# 		channel_option:
# 			FOR CHANNEL channel
#
# 		gtid_set:
# 			uuid_set [, uuid_set] ---
# 			| ''
#
# 		uuid_set:
# 			uuid:interval[:interval]---
#
# 		uuid:
# 			hhhhhhhh-hhhh-hhhh-hhhh-hhhhhhhh
#
# 		h:
# 			[0-9,A-F]
#
# 		interval:
# 			n[-n]
# 		
# 			(n >= 1)
#
# START_SLAVE with no thread_type options starts both of the slave threads.
#
# The I/O thread reads events from the master server and stores them in the relay
# log.
#
# The SQL thread reads events from the relay log and executes them.
#
# START_SLAVE requires the REPLICATION_SLAVE_ADMIN or SUPER privilege.
#
# If START_SLAVE succeeds in starting the slave threads, it returns without any
# error.
#
# However, even in that case, it might be that the slave threads start and then
# later stop (for example, because they do not manage to connect to the master or read its
# binary log, or some other problem)
#
# START_SLAVE does not warn you about this.
#
# You must check the slave's error log for error messages generated by the slave threads,
# or check that they are running satisfactorally with SHOW_SLAVE_STATUS
#
# START SLAVE causes an implicit commit of an ongoing transaction.
#
# See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# gtid_next must be set to AUTOMATIC before issuing this statement.
#
# The optional FOR CHANNEL channel clause enables you to name which replication
# channel the statement applies to.
#
# Providing a FOR CHANNEL channel clause applies the START SLAVE statement to a 
# specific replication channel.
#
# If no clause is named and no extra channels exist, the statement applies to the
# default channel.
#
# If a START SLAVE statement does not have a channel defined when using multiple
# channels, this statement starts the specified threads for all channels.
#
# This statement is disallowed for the group_replication_recovery channel
#
# See SECTION 17.2.3, "REPLICATION CHANNELS" for more information
#
# MySQL supports pluggable user-password authentication with START SLAVE
# with the USER, PASSWORD, DEFAULT_AUTH and PLUGIN_DIR options, as described
# in the following list:
#
# 		) USER: User name. Cannot be set to an empty or null string, or left unset if PASSWORD is used
#
# 		) PASSWORD: Password
#
# 		) DEFAULT_AUTH: Name of plugin; default is MySQL native authentication
#
# 		) PLUGIN_DIR: Location of plugin
#
# You cannot use the SQL_THREAD option when specifying any of USER, PASSWORD,
# DEFAULT_AUTH or PLUGIN_DIR, unless the IO_THREAD option is also provided.
#
# See SECTION 6.3.10, "PLUGGABLE AUTHENTICATION" for more information
#
# If an insecure connection is used with any of these options, the server issues the warning:
#
# 		SENDING PASSWORDS IN PLAIN TEXT WITHOUT SSL/TLS IS EXTREMELY INSECURE
#
# START_SLAVE_---_UNTIL supports two additional options for use with global transaction
# identifiers (GTIDs) (see SECTION 17.1.3, "REPLICATION WITH GLOBAL TRANSACTION IDENTIFIERS")
#
# Each of these takes a set of one or more global transaction identifiers gtid_set
# as an argument (see GTID Sets, for more information)
#
# When no thread_type is specified, START SLAVE UNTIL SQL_BEFORE_GTIDS causes the slave
# SQL thread to process transactions until it has reached the first transaction whose
# GTID is listed in the gtid_set.
#
# START SLAVE UNTIL SQL_AFTER_GTIDS causes the slave threads to process all transactions
# until the last transaction in the gtid_set has been processed by both threads.
#
# In other words, START SLAVE UNTIL SQL_BEFORE_GTIDS causes the slave SQL thread to process
# all transactions occurring before the first GTID in the gtid_set is reached, and 
# START SLAVE UNTIL SQL_AFTER_GTIDS causes the slave threads to handle all transactions,
# including those whose GTIDs are found in gtid_set, until each has encountered a transaction
# whose GTID is not part of the set.
#
# SQL_BEFORE_GTIDS and SQL_AFTER_GTIDS each support the SQL_THREAD and IO_THREAD options,
# although using IO_THREAD with them currently has no effect.
#
# For example, START SLAVE SQL THREAD UNTIL SQL_BEFORE_GTIDS = <value> causes the salve
# SQL thread to process all transactions originating from the master whose server_uuid is
# <value> until it encounters the transaction having <sequence number>.
#
# It then stops without processing this transaction.
#
# In other words, all transactions up to and including the transaction with sequence
# number 10 are processed.
#
# Executing START SLAVE SQL_THREAD UNTIL SQL_AFTER_GTIDS = <Value>:<range>
#
# on the other hand, would cause the salve SQL thread to obtain all transactions just
# mentioned from the master, including all of the transactions having the sequence
# numbers of the range - and then stop without processing any additional transactions;
#
# that is, the transaction having sequence number equal to the last one in the range,
# would be the last transaction fetched by the slave SQL thread.
#
# When using a multithreaded slave with slave_preserve_commit_order=0 set, there is a 
# chance of gaps in the sequence of transactions that have been executed from the
# relay log in the following cases:
#
# 		) Killing the coordinator thread
#
# 		) After an error occurs in the applier threads
#
# 		) mysqld shuts down unexpectedly
#
# Use the START_SLAVE_UNTIL_SQL_AFTER_MTS_GAPS statement to cause a multithreaded
# slave's worker threads to only run until no more gaps are found in the relay log,
# and then to stop.
#
# This statement can take an SQL_THREAD option, but the effects of the statement
# remain unchanged.
#
# It has no effect on the slave I/O thread (and cannot be used with the IO_THREAD option)
#
# Issuing START_SLAVE on a multithreaded slave with gaps in the sequence of transactions
# executed from the relay log generates a warning.
#
# In such a situation, the solution is to use START_SLAVE_UNTIL_SQL_AFTER_MTS_GAPS,
# then issue RESET_SLAVE to remove any remaining relay logs.
#
# See SECTION 17.4.1.34, "REPLICATION AND TRANSACTION INCONSISTENCIES" for more information.
#
# To change a failed multithreaded slave to single-threaded mode, you can issue the following
# series of statements, in the order shown:
#
# 		START SLAVE UNTIL SQL_AFTER_MTS_GAPS;
#
# 		SET @@GLOBAL.slave_parallel_workers = 0;
#
# 		START SLAVE SQL_THREAD;
#
# NOTE:
#
# 		It is possible to view the entire text of a running START SLAVE --- statement,
# 		including any USER or PASSWORD values used, in the output of SHOW PROCESSLIST.
#
# 		This is also true for the text of a running CHANGE_MASTER_TO statement, including
# 		any values it employs for MASTER_USER or MASTER_PASSWORD
#
# START_SLAVE sends an acknowledgement to the user after both the I/O thread and the SQL
# thread have started.
#
# However, the I/O thread may not yet have connected.
#
# For this reason, a successful START_SLAVE causes SHOW_SLAVE_STATUS to show Slave_SQL_Running=Yes,
# but it does not guarantee that Slave_IO_Running=Yes (because Slave_IO_Running=Yes only if the
# I/O thread is running and connected)
#
# For more information, see SECTION 13.7.6.34, "SHOW SLAVE STATUS SYNTAX", and SECTION 17.1.7.1, "CHECKING REPLICATION STATUS"
#
# You can add IO_THREAD and SQL_THREAD options to the statement to name which of the threads to start.
#
# The SQL_THREAD option is disallowed when specifying any of USER, PASSWORD, DEFAULT_AUTH or
# PLUGIN_DIR, unless the IO_THREAD option is also provided.
#
# An UNTIL clause (until_option, in the preceding grammar) may be added to specify that the slave
# should start and run until the SQL thread reaches a given point in the master binary log,
# specified by the MASTER_LOG_POS and MASTER_LOG_FILE options, or a given point in the slave
# relay log, indicated with the RELAY_LOG_POS and RELAY_LOG_FILE options.
#
# When the SQL thread reaches the point specified, it stops.
#
# If the SQL_THREAD option is specified in the statement, it starts only the SQL thread
#
# Otherwise, it starts both slave threads.
#
# If the SQL thread is running, the UNTIL clause is ignored and a warning is issued.
#
# You cannot use an UNTIL clause with the IO_THREAD option
#
# It is also possible with START SLAVE UNTIL to specify a stop point relative to a given GTID
# or set of GTIDs using one of the options SQL_BEFORE_GTIDS or SQL_AFTER_GTIDS, as explained
# previously here.
#
# When using one of these options, you can specify SQL_THREAD, IO_THREAD - both of these -
# or neither of them.
#
# If you specify only SQL_THREAD, then only the slave SQL thread is affected by the statement;
#
# if only IO_THREAD is used, then only the slave I/O is affected
#
# If both SQL_THREAD and IO_THREAD are used, or if neither of them is used, then both the
# SQL and I/O threads are affected by the statement.
#
# For an UNTIL clause, you must specify any of the following:
#
# 		) Both a log file name and a position in that file
#
# 		) Either of SQL_BEFORE_GTIDS or SQL_AFTER_GTIDS
#
# 		) SQL_AFTER_MTS_GAPS
#
# Do not mix master and relay log options. Do not mix log file options with GTID options.
#
# The UNTIL clause is not supported for multithreaded slaves except when also using
# SQL_AFTER_MTS_GAPS
#
# If UNTIL is used on a multithreaded slave without SQL_AFTER_MTS_GAPS, the slave operates
# in single-threaded (sequential) mode for replication until the point specified by
# the UNTIL clause is reached.
#
# Any UNTIL condition is reset by a subsequent STOP_SLAVE statement, a START_SLAVE statement
# that includes no UNTIL clause, or a server restart.
#
# When specifying a log file and position, you can use the IO_THREAD option with START SLAVE --- UNTIL
# even though only the SQL thread is affected by this statement.
#
# The IO_THREAD option is ignored in such cases.
#
# The preceding restriction does not apply when using one of the GTID options
# (SQL_BEFORE_GTIDS and SQL_AFTER_GTIDS); the GTID options support both SQL_THREAD
# and IO_THREAD, as explained previously in this section.
#
# The UNTIL clause can be useful for debugging replication, or to cause replication
# to proceed until just before the point where you want to avoid having the slave
# replicate an event.
#
# For example, if an unwise DROP_TABLE statement was executed on the master, you acn
# use UNTIL to tell the slave to execute up to that point but no farther.
#
# To find what the event is, use mysqlbinlog with the master binary log or slave relay
# log, or by using a SHOW_BINLOG_EVENTS statement.
#
# If you are using UNTIL to have the slave process replicated queries in sections,
# it is recommended that you start the slave with the --skip-slave-start option to prevent
# the SQL thread from running when the slave server starts.
#
# It is probably best to use this option in an option file rather than on the
# command line, so that an unexpected server restart does not cause it to be forgotten.
#
# The SHOW_SLAVE_STATUS statement includes output fields that display the current values
# of the UNTIL condition.
#
# In very old versions of MySQL (before 4.0.5), this statement was called SLAVE START.
# That syntax now produces an error.
#
# 13.4.2.7 STOP SLAVE SYNTAX
#
# 		STOP SLAVE [thread_types]
#
# 		thread_types:
# 			[thread_type [, thread_type] ---]
#
# 		thread_type: IO_THREAD | SQL_THREAD
#
# 		channel_option:
# 			FOR CHANNEL channel
#
# Stops the slave threads.
#
# STOP_SLAVE requires the REPLICATION_SLAVE_ADMIN or SUPER privilege.
#
# Recommended best practice is to execute STOP SLAVE on the slave before
# stopping the slave server (see SECTION 5.1.17, "THE SERVER SHUTDOWN PROCESS", for more information)
#
# When using the row-based logging format:
#
# 	You should execute STOP SLAVE or STOP SLAVE SQL_THREAD on the slave prior to shutting down
# 	the slave server if you are replicating any tables that use a nontransactional storage engine
# 	(see the Note later in this section)
#
# Like START_SLAVE, this statement may be used with the IO_THREAD and SQL_THREAD options to
# name the thread or threads to be stopped.
#
# STOP SLAVE causes an implicit commit of an ongoing transaction.
#
# See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# gtid_next must be set to AUTOMATIC before issuing this statement.
#
# You can control how long STOP SLAVE waits before timing out by setting the rpl_stop_slave_timeout
# system variable.
#
# This can be used to avoid deadlocks between STOP SLAVE and other slave SQL statements using
# different client connections to the slave.
#
# When the timeout value is reached, the issuing client returns an error message and
# stops waiting, but the STOP SLAVE intstruction remains in effect.
#
# Once the slave threads are no longer busy, the STOP SLAVE statement is executed and 
# the slave stops.
#
# Some CHANGE MASTER TO statements are allowed while the slave is running, depending on
# the states of the slave SQL and I/O threads.
#
# However, using STOP SLAVE prior to executing CHANGE MASTER TO in such cases is still
# supported.
#
# See SECTION 13.4.2.1,"CHANGE MASTER TO SYNTAX", and SECTION 17.3.8, "SWITCHING MASTERS DURING FAILOVER",
# for more information.
#
# The optional FOR CHANNEL channel clause enables you to name which replication channel the statement
# applies to.
#
# Providing a FOR CHANNEL channel clause applies the STOP SLAVE statement to a specific replication
# channel.
#
# If no channel is named and no extra channels exist, the statement applies to the default channel.
#
# If a STOP SLAVE statement does not name a channel when using multiple channels, this statement stops the
# specified threads for all channels..
#
# This statement cannot be used with the group_replication_recovery channel.
#
# See SECTION 17.2.3, "REPLICATION CHANNELS" for more information.
#
# When using statement-based replication:
#
# 		changing the master while it has open temporary tables is potentially unsafe.
#
# 		This is one of the reasons why statement-based replication of temporary tables
# 		is not recommended.
#
# 		You can find out whether there are any temporary tables on the slave by checking
# 		the value of Slave_open_temp_tables; when using statement-based replication, this value
# 		should be 0 before executing CHANGE MASTER TO.
#
# If there are any temporary tables open on the slave, issuing a CHANGE MASTER TO statement
# after issuing a STOP SLAVE causes an ER_WARN_OPEN_TEMP_TABLES_MUST_BE_ZERO warning.
#
# When using a multithreaded slave (slave_parallel_workers is a nonzero value), any gaps in the
# sequence of transactions executed from the relay log are closed as part of stopping the
# worker threads.
#
# If the slave is stopped unexpectedly (for example due to an error in a worker thread,
# or another thread issuing KILL), while a STOP_SLAVE statement is executing, the sequence
# of executed transactions from the relay log may become inconsistent.
#
# See SECTION 17.4.1.34, "REPLICATION AND TRANSACTION INCONSISTENCIES" for more information.
#
# If the current replication event group has modified one or more nontransactional tables,
# STOP SLAVE waits for up to 60 seconds for the event grop to complete, unless you issue
# a KILL_QUERY or KILL_CONNECTION statement for the slave SQL thread.
#
# If the event group remains incomplete after the timeout, an error message is logged.
#
# 13.4.3 SQL STATEMENTS FOR CONTROLLING GROUP REPLICATION
#
# 13.4.3.1 START GROUP_REPLICATION SYNTAX
# 13.4.3.2 STOP GROUP_REPLICATION SYNTAX
#
# 13.4.3.3 FUNCTION WHICH CONFIGURES GROUP REPLICATION PRIMARY
# 13.4.3.4 FUNCTIONS WHICH CONFIGURE THE GROUP REPLICATION MODE
#
# 13.4.3.5 FUNCTIONS TO INSPECT AND CONFIGURE THE MAXIMUM CONSENSUS INSTANCES OF A GROUP
#
# This section provides information about the statements used for controlling group replication.
#
# 13.4.3.1 START GROUP_REPLICATION SYNTAX
#
# 		START GROUP_REPLICATION
#
# Starts group replication.
#
# This statement requires the GROUP_REPLICATION_ADMIN or SUPER privilege.
#
# If super_read_only=ON and the member should join as a primary, super_read_only
# is set to OFF once Group Replication successfully starts.
#
# 13.4.3.2 STOP GROUP_REPLICATION SYNTAX
#
# 		STOP GROUP_REPLICATION
#
# Stops Group Replication.
#
# This statement requires the GROUP_REPLICATION_ADMIN or SUPER privilege.
#
# As soon as you issue STOP_GROUP_REPLICATION the member is set to super_read_only=ON,
# which ensures that no writes can be made to the member while Group Replication stops.
#
# Any other replication channels running on the member are also stopped.
#
# 		WARNING:
#
# 			Use this statement with extreme caution because it removes the server
# 			instance from the group, meaning it is no longer protected by Group
# 			Replication's consistency guarantee mechanisms.
#
# 			To be completely safe, ensure that your applications can no longer
# 			connect to the instance before issuing this statement to avoid
# 			any chance of stale reads.
#
# 13.4.3.3 FUNCTION WHICH CONFIGURES GROUP REPLICATION PRIMARY
#
# The following funciton enables you to configure which member of a single-primary
# replication group is the primary.
#
# 		) group_replication_set_as_primary()
#
# 			Appoints a specific member of the group as the new primary,
# 			overriding any election process.
#
# 			Pass in member_uuid which is the server_uuid of the member that you
# 			want to become the new primary.
#
# 			Must be issued on a member of a replication group running in single-primary
# 			mode.
#
# 			Syntax:
#
# 				STRING group_replication_set_as_primary(member_uuid)
#
# 			Return value:
#
# 			A string containing the result of the operation, for example whether it was 
# 			successful or not.
#
# 			Example:
#
# 				SELECT group_replication_set_as_primary(member_uuid)
#
# 			For more information, see SECTION 18.4.2.1, "CHANGING A GROUP'S PRIMARY MEMBER"
#
# 13.4.3.4 FUNCTIONS WHICH CONFIGURE THE GROUP REPLICATION MODE
#
# The following functions enable you to control the mode which a replication group is running in,
# either single-primary or multi-primary mode.
#
# 		) group_replication_switch_to_single_primary_mode()
#
# 			Changes a group running in multi-primary mode to single-primary mode, without the need to stop
# 			Group Replication.
#
# 			Must be issued on a member of a replication group running in multi-primary mode.
#
# 			Syntax:
#
# 				STRING group_replication_switch_to_single_primary_mode([str])
#
# 			Arguments:
#
# 				str: A string containing the UUID of a member of the group which should become the
# 						new single primary.
#
# 						Other members of the group become secondaries.
#
# 			Return value:
#
# 			A string containing the result of the operation, for example whether it was successful or not.
#
# 			Example:
#
# 				SELECT group_replication_switch_to_single_primary_mode(member_uuid);
#
# 			For more information, see SECTION 18.4.2.2, "CHANGING A GROUP'S MODE"
#
# 		) group_replication_switch_to_multi_primary_mode()
#
# 			Changes a group running in single-primary mode to multi-primary mode.
#
# 			Must be issued on a member of a replication group running in single-primary mode.
#
# 			Syntax:
#
# 				STRING group_replication_switch_to_multi_primary_mode()
#
# 			This function has no parameters
#
# 			Return value:
#
# 			A string containing the result of the operation, for example whether it was successful or not.
#
# 			Example:
#
# 				SELECT group_replication_switch_to_multi_primary_mode()
#
# 			All members which belong to the group becomes primaries.
#
# 			For more information, see SECTION 18.4.2.2, "CHANGING A GROUP'S MODE"
#
# 13.4.3.5 FUNCTIONS TO INSPECT AND CONFIGURE THE MAXIMUM CONSENSUS INSTANCES OF A GROUP
#
# The following functions enable you to inspect and configure the maximum number of consensus
# instances that a group can execute in parallel.
#
# 		) group_replication_get_write_concurrency()
#
# 			Check the maximum number of consensus instances that a group can execute in parallel.
#
# 			Syntax:
#
# 				STRING group_replication_get_write_concurrency()
#
# 			This function has no parameters.
#
# 			Return value:
#
# 			Any resulting error as a string
#
# 			Example:
#
# 				SELECT group_replication_get_write_concurrency()
#
# 			For more information, see SECTION 18.4.2.3, "USING GROUP REPLICATION GROUP WRITE CONSENSUS"
#
# 		) group_replication_set_write_concurrency()
#
# 			Configures the maximum number of consensus instances that a group can execute in parallel.
#
# 			Syntax:
#
# 				STRING group_replication_set_write_concurrency(instances)
#
# 			Arguments:
#
# 				) members: Sets the maximum number of consensus instances that a group can execute
# 								in parallel.
#
# 								Default value is 10, valid values are integers in the range of 10 to 200
#
# 			Return value:
#
# 			Any resulting error as a string.
#
# 			Example:
#
# 				SELECT group_replication_set_write_concurrency(instances);
#
# 			For more information, see SECTION 18.4.2.3, "USING GROUP REPLICATION GROUP WRITE CONSENSUS"
#
# 13.5 PREPARED SQL STATEMENT SYNTAX
#
# 13.5.1 PREPARE SYNTAX
# 13.5.2 EXECUTE SYNTAX
# 13.5.3 DEALLOCATE PREPARE SYNTAX
#
# MySQL 8.0 provides support for server-side prepared statements.
#
# This support takes advantage of the efficient client/server binary protocol.
#
# Using prepared statements with placeholders for parameter values has the following benefits:
#
# 		) Less overhead for parsing the statement each time it is executed.
#
# 			Typically, database applications process large volumes of almost-identical
# 			statements, with only changes to literal or variables values in clauses such as
# 			WHERE for queries and deletes, SET for updates, and VALUES for inserts.
#
# 		) Protection against SQL injection attacks. The parameter values can contain unescaped
# 			SQL quote and delimiter characters.
#
# PREPARED STATEMENTS IN APPLICATION PROGRAMS
#
# You can use server-side prepared statements through client programming interfaces, including
# the MySQL C API CLIENT LIBRARY or MySQL CONNECTOR/C for C programs, MySQL Connector/J for
# java programs, and MySQL Connector/NET for programs using .NET tech.
#
# For example, the C API provides a set of function calls that make up its prepared statement
# API.
#
# See SECTION 28.7.8, "C API PREPARED STATEMENTS"
#
# Other language interfaces can provide support for prepared statements that use the binary
# protocol by linking in the C client library, one example being the mysqli extension, available
# in PHP 5.0 and later.
#
# PREPARED STATEMENTS IN SQL SCRIPTS
#
# An alternative SQL interface to prepared statements is available.
#
# This interface is not as efficient as using the binary protocol through a prepared
# statement API, but requires no programming because it is available directly at the SQL level:
#
# 		) You can use it when no programming interface is available to you
#
# 		) You can use it from any program that can send SQL statements to the server to be executed,
# 			such as the mysql client program.
#
# 		) You can use it even if the client is using an old version of the client library, as long
# 			as you connect to a server running MySQL 4.1 or higher
#
# SQL syntax for prepared statements is intended to be used for situations such as these:
#
# 		) To test how prepared statements work in your application before coding it
#
# 		) To use prepared statements when you do not have access to a programming API that supports them
#
# 		) To interactively troubleshoot application issues with prepared statements.
#
# 		) To create a test case that reproduces a problem with prepared statements, so that you can file a bug report
#
# PREPARE, EXECUTE, AND DEALLOCATE PREPARE STATEMENTS
#
# SQL syntax for prepared statements is based on three SQL statements:
#
# 		) PREPARE prepares a statement for execution (see SECTION 13.5.1, "PREPARE SYNTAX")
#
# 		) EXECUTE executes a prepared statement (see SECTION 13.5.2, "EXECUTE SYNTAX")
#
# 		) DEALLOCATE_PREPARE releases a prepared statement (see SECTION 13.5.3, "DEALLOCATE PREPARE SYNTAX")
#
# The following examples show two equivalent ways of preparing a statement that computes the 
# hypotenuse of a triangle given the lengths of the two sides.
#
# The first example shows how to create a prepared statement by using a string literal to supply
# the text of the statement:
#
# 		PREPARE stmt1 FROM 'SELECT SQRT(POW(?,2) + POW(?,2)) AS hypotenuse';
# 		SET @a = 3;
# 		SET @b = 4;
# 		EXECUTE stmt1 USING @a, @b;
# 		+---------------------+
# 		| hypotenuse 		    |
# 		+---------------------+
# 		| 		 5 				 |
# 		+---------------------+
# 		DEALLOCATE PREPARE stmt1;
#
# The second example is similar, but supplies the text of the statement as a user variable:
#
# 		SET @s = 'SELECT SQRT(POW(?,2) + POW(?,2)) AS hypotenuse';
# 		PREPARE stmt2 FFROM @s;
# 		SET @a = 6;
# 		SET @b = 8;
# 		EXECUTE stmt2 USING @a, @b;
# 		+------------------+
# 		| hypotenuse 		 |
# 		+------------------+
# 		| 		10 			 |
# 		+------------------+
# 		DEALLOCATE PREPARE stmt2;
#
# Here is an additional example that demonstrates how to choose the table on which
# to perform a query at runtime, by storing the name of the table as a user variable:
#
# 		USE test;
# 		CREATE TABLE t1 (a INT NOT NULL);
# 		INSERT INTO t1 VALUES (4), (8), (11), (32), (80);
#
# 		SET @table = 't1';
# 		SET @s = CONCAT('SELECT * FROM ', @table);
# 
# 		PREPARE stmt3 FROM @s;
# 		EXECUTE stmt3;
# 		+------+
# 		| a 	 |
# 		+------+
# 		| 4 	 |
# 		| 8 	 |
# 		| 11   |
# 		| 32 	 |
# 		| 80   |
# 		+------+
#
# 		DEALLOCATE PREPARE stmt3;
#
# A prepared statement is specific to the session in which it was created.
#
# If you terminate a session without deallocating a previously prepared statement,
# the server deallocates it automatically.
#
# A prepared statement is also global to the session. If you create a prepared statement
# within a stored routine, it is not deallocated when the stored routine ends.
#
# To guard against too many prepared statements being created simultaneously, set the
# max_prepared_stmt_count system variable.
#
# To prevent the use of prepared statements, set the value to 0.
#
# SQL SYNTAX ALLOWED IN PREPARED STATEMENTS
#
# The following SQL statements can be used as prepared statements:
#
# 		ALTER TABLE
# 		ALTER USER
# 		ANALYZE TABLE
#
# 		CACHE INDEX
# 		CALL
# 		CHANGE MASTER
#
# 		CHECKSUM {TABLE | TABLES}
# 		COMMIT
# 		{CREATE | DROP} INDEX
#
# 		{CREATE | RENAME | DROP} DATABASE
# 		{CREATE | DROP} TABLE
# 		{CREATE | RENAME | DROP} USER
#
# 		{CREATE | DROP} VIEW
# 		DELETE
# 		DO
#
# 		FLUSH {TABLE | TABLES | TABLES WITH READ LOCK | HOSTS | PRIVILEGES
# 			| LOGS | STATUS | MASTER | SLAVE | USER_RESOURCES}
# 		GRANT
# 		INSERT
#
# 		INSTALL PLUGIN
# 		KILL
# 		LOAD INDEX INTO CACHE
#
# 		OPTIMIZE TABLE
# 		RENAME TABLE
# 		REPAIR TABLE
#
#  	REPLACE
# 		RESET {MASTER | SLAVE}
# 		REVOKE
#
# 		SELECT
# 		SET
# 		SHOW {WARNINGS | ERRORS}
#
# 		SHOW BINLOG EVENTS
# 		SHOW CREATE {PROCEDURE | FUNCTION | EVENT | TABLE | VIEW}
# 		SHOW {MASTER | BINARY} LOGS
# 
# 		SHOW {MASTER | SLAVE} STATUS
# 		SLAVE {START | STOP}
# 		TRUNCATE TABLE
#
# 		UNINSTALL PLUGIN
# 		UPDATE
#
# For compliance with the SQL standard, which states that diagnostics statements are not preparable,
# MySQL does not support the following as prepared statements:
#
# 		) SHOW WARNINGS, SHOW COUNT(*) WARNINGS
#
# 		) SHOW ERRORS, SHOW COUNT(*) ERRORS
#
# 		) Statements containing any reference to the warning_count or error_count system variable.
#
# Other statements are not supported in MySQL 8.0
#
# Generally, statements not permitted in SQL preapred statements are also not permitted in stored
# programs.
#
# Exceptions are noted in SECTION C.1, "RESTRICTIONS ON STORED PROGRAMS"
#
# Metadata changes to tables or views referred to by prepared statements are detected
# and cause automatic repreparation of the statement when it is next executed.
#
# For more information, see SECTION 8.10.3, "CACHING OF PREPARED STATEMENTS AND STORED PROGRAMS"
#
# Placeholders can be used for the arguments of the LIMIT clause when using prepared statements.
#
# See SECTION 13.2.10, "SELECT SYNTAX"
#
# In prepared CALL statements used with PREPARE and EXECUTE, placeholder support for OUT and
# INOUT parameters is available beginning with MySQL 8.0
#
# See SECTION 13.2.1, "CALL SYNTAX", for an example and a workaround for earlier versions.
#
# Placeholders can be used for IN parameters regardless of version.
#
# SQL syntax for prepared statements cannot be used in nested fashion. That is, a statement
# passed to PREPARE cannot itself be a PREPARE, EXECUTE or DEALLOCATE_PREPARE statement.
#
# SQL syntax for prepared statements is distinct from using prepared statement API calls.
#
# For example, you cannot use the mysql_stmt_prepare() C API function to prepare a
# PREPARE, EXECUTE or DEALLOCATE_PREPARE statement.
#
# SQL syntax for prepared statements can be used within stored procedures, but not
# in stored functions or triggers.
#
# However, a cursor cannot be used for a dynamic statement that is prepared and
# executed with PREPARE and EXECUTE.
#
# The statement for a cursor is checked at cursor creation time, so the statement
# cannot be dynamic.
#
# SQL syntax for prepared statements does not support multi-statements (that is,
# multiple statements within a single string separated by ; characters)
#
# To write C programs that use the CALL SQL statement to execute stored procedures
# that contain prepared statements, the CLIENT_MULTI_RESULTS flag must be enabled.
#
# This is because each CALL returns a result to indicate the call status, in addition
# to any result sets that might be returned by statements executed within the procedure.
#
# CLIENT_MULTI_RESULTS can be enabled when you call mysql_real_connect(), either
# explicitly by passing the CLIENT_MULTI_RESULTS flag itself, or implicitly by
# passing CLIENT_MULTI_STATEMENTS (which also enables CLIENT_MULTI_RESULTS)
#
# For additional information, see SECTION 13.2.1, "CALL SYNTAX"
#
# 13.5.1 PREPARE SYNTAX
#
# 		PREPARE stmt_name FROM preparable_stmt
#
# The PREPARE statement prepares a SQL statement and assigns it a name, stmt_name, by which
# to refer to the statement later.
#
# The prepared statement is executed with EXECUTE and released with DEALLOCATE_PREPARE
#
# For examples, see SECTION 13.5, "PREPARED SQL STATEMENT SYNTAX"
#
# Statement names are not case-sensitive. preparable_stmt is either a string literal or
# a user variable that contains the text of the SQL statement.
#
# The text must represent a single statement, not multiple statements.
#
# Within the statement, ? characters can be used as parameter markers to indicate
# where data values are to be bound to the query later when you execute it.
#
# The ? characters should not be enclosed within quotation marks, even if you
# intend to bind them to string values.
#
# Parameter markers can be used only where data values should appear, not for
# SQL keywords, identifiers, and so forth.
#
# If a prepared statement with the given name already exists. It is deallocated implicitly
# before the new statement is prepared.
#
# This means that if the new statement contains an error and cannot be prepared,
# an error is returned and no statement with the given name exists.
#
# The scope of a prepared statement is the session within which it is created,
# which as several implications:
#
# 		) A prepared statement created in one session is not available to other sessions
#
# 		) When a session ends, whether normally or abnormally, its prepared statements
# 			no longer exist.
#
# 			If auto-reconnect is enabled, the client is not notified that the connection
# 			was lost.
#
# 			For this reason, clients may wish to disable auto-reconnect
#
# 			See SECTION 28.7.24, "C API AUTOMATIC RECONNECTION CONTROL"
#
# 		) A prepared statement created within a stored program continues to exist
# 			after the program finishes executing and can be executed outside the 
# 			program later.
#
# 		) A statement prepared in stored program context cannot refer to stored procedure
# 			or function parameters or local variables because they go out of scope when the
# 			program ends and would be unavailable were the statement to be executed later outside
# 			the program.
#
# 			As a workaround, refer instead to user-defined variables, which also have
# 			session scope;
#
# 			See SECTION 9.4, "USER-DEFINED VARIABLES"
#
# 13.5.2 EXECUTE SYNTAX
#
# 		EXECUTE stmt_name
# 			[USING @var_name [, @var_name] ---]
#
# After preparing a statement with PREPARE, you execute it with an EXECUTE statement
# that refers to the prepared statement name.
#
# If the prepared statement contains any parameter markers, you must supply a USING
# clause that lists user variables containing the values to be bound to the parameters.
#
# Parameter values can be supplied only by user variables, and the USING clause must
# name exactly as many variables as the number of parameter markers in the statement.
#
# You can execute a given prepared statement multiple times, passing different variables
# to it or setting the variables to different values before each execution.
#
# For examples, see SECTION 13.5, "PREPARED SQL STATEMENT SYNTAX"
#
# 13.5.3 DEALLOCATE PREPARE SYNTAX
#
# 		{DEALLOCATE | DROP} PREPARE stmt_name
#
# To deallocate a prepared statement produced with PREPARE, use a DEALLOCATE_PREPARE
# statement that refers to the prepared statement name.
#
# Attempting to execute a prepared statement after deallocating it results in an
# error.
#
# If too many prepared statements are created and not deallocated by either the
# DEALLOCATE PREPARE statement or the end of the session, you might encounter
# the upper limit enforced by the max_prepared_stmt_count system variable.
#
# For examples, see SECTION 13.5, "PREPARED SQL STATEMENT SYNTAX"
#
# 13.6 COMPOUND-STATEMENT SYNTAX
#
# 		13.6.1 BEGIN --- END COMPOUND-STATEMENT SYNTAX
# 		13.6.2 STATEMENT LABEL SYNTAX
# 
# 		13.6.3 DECLARE SYNTAX
# 		13.6.4 VARIABLES IN STORED PROGRAMS
#
# 		13.6.5 FLOW CONTROL STATEMENTS
# 		13.6.6 CURSORS
# 	
# 		13.6.7 CONDITION HANDLING
#
# This section desccribes the syntax for the BEGIN_---_END compound statement and other
# statements that can be used in the body of stored programs:
#
# 		Stored procedures and functions, triggers, and events.
#
# These objects are defined in terms of SQL code that is stored on the server
# for later invocation
#
# (See CHAPTER 24, STORED PROGRAMS AND VIEWS)
#
# A compound statement is a block that can contain other blocks; declarations
# for variables, condition handlers and cursors; and flow control constructs 
# such as loops and conditional tests.
#
# 13.6.1 BEGIN --- END COMPOUND-STATEMENT SYNTAX
#
# 		[begin_label:] BEGIN
# 			[statement_list]
# 		END [end_label]
#
# BEGIN_---_END syntax is used for writing compound statements, which can appear
# within stored programs (stored procedures and functions, triggers and events)
#
# A compound statement can contain multiple statements, enclosed by the BEGIN 
# and END keywords.
#
# statement_list represents a list of one or more statements, each terminated
# by a semicolon (;) statement delimiter.
#
# The statement_list itself is optional, so the empty compound statement
# (BEGIN END) is legal.
#
# BEGIN_---_END blocks can be nested.
#
# Use of multiple statements requires that a client is able to send statement
# strings containing the ; statement delimiter.
#
# In the mysql command-line client, this is handled with the delimiter command.
#
# Changing the ; end-of-statement delimiter (for example, to //) permit ; to
# be used in a program body.
#
# For an example, see SECTION 24.1, "DEFINING STORED PROGRAMS"
#
# A BEGIN_---_END block can be labeled. See SECTION 13.6.2, "STATEMENT LABEL SYNTAX"
#
# The optional [NOT] ATOMIC clause is not supported.
#
# This means that no transactional savepoint is set at the start of the instruction
# block and the BEGIN clause used in this context has no effect on the current transaction.
#
# NOTE:
#
# 		Within all stored programs, the parser treats BEGIN_[WORK] as the beginning of a 
# 		BEGIN_---_END block.
#
# 		To begin a transaction in this context, use START_TRANSACTION instead.
#
# 13.6.2 STATEMENT LABEL SYNTAX
#
# 		[begin_label:] BEGIN
# 			[statement_list]
# 		END [end_label]
#
# 		[begin_label:] LOOP
# 			statement_list
# 		END LOOP [end_label]
#
# 		[begin_label:] REPEAT
# 			statement_list
# 		UNTIL search_condition
# 		END REPEAT [end_label]
#
# 		[begin_label:] WHILE search_condition DO
# 			statement_list
# 		END WHILE [end_label]
#
# Labels are permitted for BEGIN_---_END blocks and for the LOOP, REPEAT and WHILE statements.
#
# Label use for those statements follows these rules:
#
# 		) begin_label must be followed by a colon
#
# 		) begin_label can be given without end_label.
#
# 			If end_label is present, it must be the same as begin_label
#
# 		) end_label cannot be given without begin_label
#
# 		) Labels at the same nesting level must be distinct.
#
# 		) Labels can be up to 16 characters long
#
# To refer to a label within the labeled construct, use an ITERATE 
# or LEAVE statement.
#
# The following example uses those statements to continue iterating or
# terminate the loop:
#
# 		CREATE PROCEDURE doiterate(p1 INT)
# 		BEGIN
# 			label1: LOOP
# 				SET p1 = p1 + 1;
# 				IF p1 < 10 THEN ITERATE label1; END IF;
# 				LEAVE label1;
# 			END LOOP label1;
# 		END;
#
# The scope of a block label does not include the code for handlers declared within the
# block.
#
# For details, see SECTION 13.6.7.2, "DECLARE --- HANDLER SYNTAX"
#
# 13.6.3 DECLARE SYNTAX
#
# The DECLARE statement is used to define various items local to a program:
#
# 		) Local variables. See SECTION 13.6.4, "VARIABLES IN STORED PROGRAMS"
#
# 		) Conditions and handlers. See SECTION 13.6.7, "CONDITION HANDLING"
#
# 		) Cursors. See SECTION 13.6.6, "CURSORS"
#
# DECLARE is permitted only inside a BEGIN_---_END compound statement and must be
# at its start, before any other statements.
#
# Declarations must follow a certain order.
#
# Cursor declarations must appear before handler declarations.
#
# Variable and condition declarations must appear before cursor or
# handler declarations.
#
# 13.6.4 VARIABLES IN STORED PROGRAMS
#
# 13.6.4.1 LOCAL VARIABLE DECLARE SYNTAX
# 13.6.4.2 LOCAL VARIABLE SCOPE AND RESOLUTION
#
# System variables and user-defined variables can be used in stored programs, just as they can be used
# outside stored-program context.
#
# In addition, stored programs can use DECLARE to define local variables, and stored routines
# (procedures and functions) can be declared to take parameters that communicate
# values between the routine and its caller.
#
# 		) To declare local variables, use the DECLARE statement, as described in SECTION 13.6.4.1, "LOCAL VARIABLE DECLARE SYNTAX"
#
# 		) Variables can be set directly with the SET statement. See SECTION 13.7.5.1, "SET SYNTAX FOR VARIABLE ASSIGNMENT"
#
# 		) Results from queries can be retrieved into local variables using SELECT_---_INTO_var_list or by opening a cursor
# 			and using FETCH_---_INTO_var_list
#
# 			See SECTION 13.2.10.1, "SELECT --- INTO SYNTAX", and SECTION 13.6.6, "CURSORS"
#
# For information about the scope of local variables and how MySQL resolves ambiguous names, see SECTION 13.6.4.2,
# "LOCAL VARIABLE SCOPE AND RESOLUTION"
#
# It is not permitted to assign the value DEFAULT to stored procedure or function parameters or stored
# program local variables (for example with a SET var_name = DEFAULT statement)
#
# In MySQL 8.0, this results in a syntax error
#
# 13.6.4.1 LOCAL VARIABLE DECLARE SYNTAX
#
# 		DECLARE var_name [, var_name] --- type [DEFAULT value]
#
# This statement declares local variables within stored programs.
#
# To provide a default value for a variable, include a DEFAULT clause.
#
# The value can be specified as an expression; it need not be a constant.
#
# If the DEFAULT clause is missing, the initial value is NULL
#
# Local variables are treated like stored routine parameters with respect to data
# type and overflow checking.
#
# See SECTION 13.1.17, "CREATE PROCEDURE AND CREATE FUNCTION SYNTAX"
#
# Variable declarations must appear before cursor or handler declarations
#
# Local variable names are not case-sensitive. Permissible characters and quoting rules
# are the same as for other identifiers, as described in SECTION 9.2, "SCHEMA OBJECT NAMES"
#
# The scope of a local variable is the BEGIN_---_END block within which it is declared.
#
# The variable can be referred to in blocks nested within the declaring block, except
# those blocks that declare a variable with the same name.
#
# For examples of variable declarations, see SECTION 13.6.4.2, "LOCAL VARIABLE SCOPE AND RESOLUTION"
#
# 13.6.4.2 LOCAL VARIABLE SCOPE AND RESOLUTION
#
# The scope of a local variable is the BEGIN_---_END block within which it is declared.
#
# The variable can be referred to in blocks nested within the declaring block, except
# those blocks that declare a variable with the same name.
#
# Because local variables are in scope only during stored program execution, references
# to them are not permitted in prepared statements created within a stored program.
#
# Prepared statement scope is the current session, not the stored program, so the
# statement could be executed after the program ends, at which point the variables
# would no longer be in scope.
#
# For example, SELECT --- INTO local_var cannot be used as a prepared statement.
#
# This restriction also applies to stored procedure and function parameters.
#
# See SECTION 13.5.1, "PREPARE SYNTAX"
#
# A local variable should not have the same name as a table column.
#
# If an SQL statement, such as SELECT_---_INTO statement, contains a reference
# to a column and a declared local variable with the same name, MySQL currently
# interprets the reference as the name of a variable.
#
# Consider the following procedure definition:
#
# 		CREATE PROCEDURE sp1 (x VARCHAR(5))
# 		BEGIN
# 			DECLARE xname VARCHAR(5) DEFAULT 'bob';
# 			DECLARE newname VARCHAR(5);
# 			DECLARE xid INT;
#
# 			SELECT xname, id INTO newname, xid
# 				FROM table1 WHERE xname = xname;
# 			SELECT newname;
# 		END;
#
# MySQL interprets xname in the SELECT statement as a reference to the 
# xname variable rather than the xname column.
#
# Consequently, when the procedure sp1() is called, the newname variable
# returns the value 'bob' regardless of the value of the table1.xname
# column.
#
# Similarly, the cursor definition in the following procedure contains a SELECT
# statement that refers to xname.
#
# MySQL interprets this as a reference to the variable of that name rather
# than a column reference.
#
# 		CREATE PROCEDURE sp2 (x VARCHAR(5))
# 		BEGIN
# 			DECLARE xname VARCHAR(5) DEFAULT 'bob';
# 			DECLARE newname VARCHAR(5);
# 			DECLARE xid INT;
# 			DECLARE done TINYINT DEFAULT 0;
# 			DECLARE cur1 CURSOR FOR SELECT xname, id FROM table1;
# 			DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = 1;
#
# 			OPEN cur1;
# 			read_loop: LOOP
# 				FETCH FROM cur1 INTO newname, xid;
# 				IF done THEN LEAVE read_loop; END IF
# 				SELECT newname;
# 			END LOOP;
# 			CLOSE cur1;
# 		END;
#
# See also SECTION C.1, "RESTRICTIONS ON STORED PROGRAMS"
#
# 13.6.5 FLOW CONTROL STATEMENTS
#
# 13.6.5.1 CASE SYNTAX
# 13.6.5.2 IF SYNTAX
#
# 13.6.5.3 ITERATE SYNTAX
# 13.6.5.4 LEAVE SYNTAX
#
# 13.6.5.5 LOOP SYNTAX
# 13.6.5.6 REPEAT SYNTAX
#
# 13.6.5.7 RETURN SYNTAX
# 13.6.5.8 WHILE SYNTAX
#
# MySQL supports the IF, CASE, ITERATE, LEAVE LOOP, WHILE and REPEAT constructs for flow control
# within stored programs.
#
# It also supports RETURN within stored functions.
#
# Many of these constructs contain other statements, as indicated by the grammar specifications
# in the following sections.
#
# Such constructs may be nested.
#
# For example, an IF statement might contain a WHILE loop, which itself contains a CASE statement.
#
# MySQL does not support FOR Loops.
#
# 13.6.5.1 CASE SYNTAX
#
# 		CASE case_value
# 			WHEN when_value THEN statement_list
# 			[WHEN when_value THEN statement_list]
# 			[ELSE statement_list]
# 		END CASE
#
# Or:
#
# 		CASE
# 			WHEN search_condition THEN statement_list
# 			[WHEN search_condition THEN statement_list]
# 			[ELSE statement_list]
# 		END CASE
#
# The CASE statement for stored programs implements a complex conditional construct.
#
# NOTE:
#
# 		There is also a CASE expression, which differs from the CASE statement described here.
#
# 		See SECTION 12.4, "CONTROL FLOW FUNCTIONS"
#
# 		The CASE statement cannot have an ELSE NULL clause, and it is terminated with 
# 		END CASE instead of END
#
# For the first syntax, case_value is an expression.
#
# This value is compared to the when_value expression in each WHEN clause until one of them
# is equal.
#
# When an equal when_value is found, the corresponding THEN clause statement_list executes.
#
# If no when_value is equal, the ELSE clause statement_list executes, if there is one.
#
# This syntax cannot be used to test for equalit with NULL because NULL = NULL is false.
#
# See SECTION 3.3.4.6, "WORKING WITH NULL VALUES"
#
# For the second syntax, each WHEN clause search_condition expression is evaluated until one
# is true, at which point its corresponding THEN clause statement_list executes.
#
# If no search_condition is equal, the ELSE clause statement_list executes, if there is one.
#
# If no when_value or search_condition matches the value tested and the CASE statement contains
# no ELSE clause, a:
#
# 		Case not found for CASE statement
#
# error results.
#
# Each statement_list consists of one or more SQL statements; an empty statement_list
# is not permitted.
#
# To handle situations where no value is matched by any WHEN clause, use an ELSE containing
# an empty BEGIN_---_END block, as shown in this example.
#
# (The indentation used here in the ELSE clause is for purposes of clarity only,
# and is not otherwise significant)
#
# DELIMITER |
#
# CREATE PROCEDURE p()
# 		BEGIN
# 			DECLARE v INT DEFAULT 1;
#
# 			CASE v
# 				WHEN 2 THEN SELECT v;
# 				WHEN 3 THEN SELECT 0;
# 				ELSE
# 					BEGIN
# 					END;
# 			END CASE;
# 		END;
# 		|
#
# 13.6.5.2 IF SYNTAX
#
# IF search_condition THEN statement_list
# 		[ELSEIF search_condition THEN statement_list] ---
# 		[ELSE statement_list]
# END IF
#
# The IF statement for stored programs implements a basic conditional construct.
#
# NOTE:
#
# 		There is also an IF() funciton, which differs from the IF statement described here.
#
# 		See SECTION 12.4, "CONTROL FLOW FUNCTIONS"
#
# 		The IF statement can have THEN, ELSE and ELSEIF clauses, and it is
# 		terminated with END IF.
#
# If the search_condition evaluates to true, the corresponding THEN or ELSEIF clause
# statement_list executes.
#
# If no search_condition matches, the ELSE clause statement_list executes.
#
# Each statement_list consists of one or more SQL statements; an empty statement_list is
# not permitted.
#
# An IF --- END IF block, like all other flow-control blocks used within stored programs,
# must be terminated with a semicolon, as shown in this example:
#
# 		DELIMITER //
#
# 		CREATE FUNCTION SimpleCompare(n INT, m INT)
# 			RETURNS VARCHAR(20)
#
# 			BEGIN
# 				DECLARE s VARCHAR(20);
#
# 				IF n > m THEN SET s = '>';
# 				ELSEIF n = m THEN SET s = '=';
# 				ELSE SET s = '<';
# 				END IF;
#
# 				SET s = CONCAT(n, ' ', s, ' ', m);
#
# 				RETURN s;
# 			END //
#
# 		DELIMITER;
#
# AS with other flow-control constructs, IF --- END IF blocks may be nested within other
# flow-control constructs, including other IF statements.
#
# Each IF must be terminated by its own END IF followed by a semicolon.
#
# You can use indentation to make nested flow-control blocks more easily readable
# by humans (although this is not required by MySQL), as shown here:
#
# 		DELIMITER //
#
# 		CREATE FUNCTION VerboseCompare (n INT, m INT)
# 			RETURNS VARCHAR(50)
#
# 			BEGIN
# 				DECLARE s VARCHAR(50);
#
# 				IF n = m THEN SET s = 'equals';
# 				ELSE
# 					IF n > m THEN SET s = 'greater';
# 					ELSE SET s = 'less';
# 					END IF;
#
# 					SET s = CONCAT('is', s, ' than');
# 				END IF;
#
# 				SET s = CONCAT(n, ' ', s, ' ', m, '.');
#
# 				RETURN s;
# 			END //
#
# 		DELIMITER ;
#
# In this example, the inner IF is evaluated only if n is not equal to m
#
# 13.6.5.3 ITERATE SYNTAX
#
# 		ITERATE label
#
# ITERATE can appear only within LOOP, REPEAT, and WHILE statements.
#
# ITERATE means "start the loop again"
#
# For an example, see SECTION 13.6.5.5, "LOOP SYNTAX"
#
# 13.6.5.4 LEAVE SYNTAX
#
# 		LEAVE label
#
# This statement is used to exit the flow control construct that has the given label.
#
# If the label is for the outermost stored program block, LEAVE exits the program
#
# LEAVE can be used within BEGIN_---_END or loop constructs (LOOP, REPEAT, WHILE)
#
# For an example, see SECTION 13.6.5.5, "LOOP SYNTAX"
#
# 13.6.5.5 LOOP SYNTAX
#
# 		[begin_label:] LOOP
# 			statement_list
# 		END LOOP [end_label]
#
# LOOP implements a simple loop construct, enabling repeated execution of the statement
# list, which consists of one or more statements, each terminated by a semicolon (;) 
# statement delimiter.
#
# The statements within the loop are repeated until the loop is terminated.
#
# Usually, this is accomplished with a LEAVE statement.
#
# Within a stored function, RETURN can also be used, which exits the function entirely
#
# Neglecting to include a loop-termination statement results in an infinite loop
#
# A LOOP statement can be labeled. For the rules regarding label use, see SECTION 13.6.2, "STATEMENT LABEL SYNTAX"
#
# Example:
#
# 		CREATE PROCEDURE doiterate(p1 INT)
# 		BEGIN
# 			label1: LOOP
# 				SET p1 = p1 + 1;
# 				IF p1 < 10 THEN
# 					ITERATE label1;
# 				END IF;
# 				LEAVE label1;
# 			END LOOP label1;
# 			SET @x = p1;
# 		END;
#
# 13.6.5.6 REPEAT SYNTAX
#
# 	[begin_label:] REPEAT
# 		statement_list
# 	UNTIL search_condition
# 	END REPEAT [end_label]
#
# The statement list within a REPEAT statement is repeated until the search_condition expression
# is true.
#
# Thus, a REPEAT always enters the loop at least once.
#
# statement_list consists of one or more statements, each terminated by a semicolon (;) statement delimiter
#
# A REPEAT statement can be labeled. For the rules regarding label use, see SECTION 13.6.2,
# "STATEMENT LABEL SYNTAX"
#
# Example:
#
# 		delimiter //
#
# 		CREATE PROCEDURE dorepeat(p1 INT)
# 		BEGIN
# 			SET @x = 0;
# 			REPEAT
# 				SET @x = @x + 1;
# 			UNTIL @x > p1 END REPEAT;
# 		END
# 		//
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		CALL dorepeat(1000)//
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		SELECT @x//
# 		+---------+
# 		| @x 	    |
# 		+---------+
# 		| 1001 	 |
# 		+---------+
# 		1 row in set (0.00 sec)
#
# 13.6.5.7 RETURN SYNTAX
#
# 		RETURN expr
#
# The RETURN statement terminates execution of a stored function and returns the value expr to
# the function caller.
#
# There must be at least one RETURN statement in a stored function.
#
# There may be more than one if the function has multiple exit points.
#
# This statement is not used in stored procedures, triggers, or events.
#
# The LEAVE statement can be used to exit a stored program of those types.
#
# 13.6.5.8 WHILE SYNTAX
#
# 		[begin_label:] WHILE search_condition DO
# 			statement_list
# 		END WHILE [end_label]
#
# The statement list within a WHILE statement is repeated as long as the search_condition
# expression is true.
#
# statement_list consists of one or more SQL statements, each terminated by a semicolon
# (;) statement delimiter.
#
# A WHILE statement can be labeled. For the rules regarding label use,
# see SECTION 13.6.2, "STATEMENT LABEL SYNTAX"
#
# Example:
#
# 		CREATE PROCEDURE dowhile()
# 		BEGIN
# 			DECLARE v1 INT DEFAULT 5;
#
# 			WHILE v1 > 0 DO
# 				---
# 				SET v1 = v1 - 1;
# 			END WHILE;
# 		END;
#
# 13.6.6 CURSORS
#
# 13.6.6.1 CURSOR CLOSE SYNTAX
# 13.6.6.2 CURSOR DECLARE SYNTAX
# 13.6.6.3 CURSOR FETCH SYNTAX
# 13.6.6.4 CURSOR OPEN SYNTAX
#
# MySQL supports cursors inside stored programs.
#
# The syntax is as in embedded SQL. Cursors have these properties:
#
# 		) Asensitive: The server may or may not make a copy of its result table
#
# 		) Read only: Not updatable
#
# 		) Nonscrollable: Can be traversed only in one direction and cannot skip rows
#
# Cursor declarations must appear before handler declarations and after variable and condition
# declarations.
#
# Example:
#
# 		CREATE PROCEDURE curdemo()
# 		BEGIN
# 			DECLARE done INT DEFAULT FALSE;
# 			DECLARE a CHAR(16);
# 			DECLARE b, c INT;
# 			DECLARE cur1 CURSOR FOR SELECT id, data FROM test.t1;
# 			DECLARE cur2 CURSOR FOR SELECT i FROM test.t2;
# 			DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
#
# 			OPEN cur1;
# 			OPEN cur2;
#
# 			read_loop: LOOP
# 				FETCH cur1 INTO a, b;
# 				FETCH cur2 INTO c;
# 				IF done THEN
# 					LEAVE read_loop;
# 				END IF;
# 				IF b < c THEN
# 					INSERT INTO test.t3 VALUES (a,b);
# 				ELSE
# 					INSERT INTO test.t3 VALUES (a,c);
# 				END IF;
# 			END LOOP;
#
# 			CLOSE cur1;
# 			CLOSE cur2;
# 		END;
#
# 13.6.6.1 CURSOR CLOSE SYNTAX
#
# 		CLOSE cursor_name
#
# This statement closes a previously opened cursor.
#
# For an example, see SECTION 13.6.6, "CURSORS"
#
# An error occurs if the cursor is not open.
#
# If not closed explicitly, a cursor is closed at the end of the
# BEGIN_---_END block in which it was declared.
#
# 13.6.6.2 CURSOR DECLARE SYNTAX
#
# 		DECLARE cursor_name CURSOR FOR select_statement
#
# This statement declares a cursor and associates it with a SELECT statement that
# retrieves the rows to be traversed by the cursor.
#
# To fetch the rows later, use a FETCH statement.
#
# The number of columns retrieved by the SELECT statement must match the number
# of output variables specified in the FETCH statement.
#
# The SELECT statement cannot have an INTO clause.
#
# Cursor declarations must appear before handler declarations and after variable
# and condition declarations.
#
# A stored program may contain multiple cursor declarations, but each cursor declared
# in a given block must have a unique name.
#
# For an example, see SECTION 13.6.6, "CURSORS"
#
# For information available through SHOW statements, it is possible in many cases
# to obtain equivalent information by using a cursor with an INFORMATION_SCHEMA table.
#
# 13.6.6.3 CURSOR FETCH SYNTAX
#
# 		FETCH [[NEXT] FROM] cursor_name INTO var_name [, var_name] ---
#
# This statement fetches the next row for the SELECT statement associated with the
# specified cursor (which must be open), and advances the cursor pointer.
#
# If a row exists, the fetched columns are stored in the named variables.
#
# The number of columns retrieved by the SELECT statement must match the number
# of output variables specified in the FETCH statement.
#
# If no more rows are available, a No Data condition occurs with SQLSTATE
# value '02000'
#
# To detect this condition, you can set up a handler for it (or for a NOT FOUND condition)
# 
# For an example, see SECTION 13.6.6, "CURSORS"
#
# Be aware that another operation, such as a SELECT or another FETCH, may also cause
# the handler to execute by raising the same condition.
#
# If it is necessary to distinguish which operation raised the condition,
# place the operation within its own BEGIN_---_END block so that it can be
# associated with its own handler.
#
# 13.6.6.4 CURSOR OPEN SYNTAX
#
# 		OPEN cursor_name
#
# This statement opens a previously declared cursor. For an example, see SECTION 13.6.6, "CURSORS"
#
# 13.6.7 CONDITION HANDLING
#
# 13.6.7.1 DECLARE --- CONDITION SYNTAX
# 13.6.7.2 DECLARE --- HANDLER SYNTAX
#
# 13.6.7.3 GET DIAGNOSTICS SYNTAX
# 13.6.7.4 RESIGNAL SYNTAX
#
# 13.6.7.5 SIGNAL SYNTAX
# 13.6.7.6 SCOPE RULES FOR HANDLERS
#
# 13.6.7.7 THE MYSQL DIAGNOSTICS AREA
# 13.6.7.8 CONDITION HANDLING AND OUT OR INOUT PARAMETERS
#
# Conditions may arise during stored program execution that require special handling,
# such as exiting the current program block or continuing execution.
#
# Handlers can be defined for general conditions such as warnings or exceptions,
# or for specific conditions such as particular error code.
#
# Specific conditions can be assigned names and referred to that way in handlers.
#
# To name a condition, use the DECLARE_---_CONDITION statement.
#
# To declare a handler, use the DECLARE_---_HANDLER statement.
#
# See SECTION 13.6.7.1, "DECLARE --- CONDITION SYNTAX", and SECTION 13.6.7.2, "DECLARE_---_HANDLER SYNTAX"
#
# For information about how the server chooses handlers when a condition occurs,
# see SECTION 13.6.7.6, "SCOPE RULES FOR HANDLERS"
#
# To raise a condition, use the SIGNAL statement. To modify condition information within
# a condition handler, use RESIGNAL.
#
# See SECTION 13.6.7.1, "DECLARE --- CONDITION SYNTAX", and SECTION 13.6.7.2, "DECLARE --- HANDLER SYNTAX"
#
# To retrieve information from the diagnostics area, use the GET_DIAGNOSTICS statement (see SECTION 13.6.7.3,
# "GET DIAGNOSTICS SYNTAX")
#
# For information about the diagnostics area, see SECTION 13.6.7.7, "THE MYSQL DIAGNOSTICS AREA"
#
# 13.6.7.1 DECLARE --- CONDITION SYNTAX
#
# DECLARE condition_name CONDITION FOR condition_value
#
# condition_value: {
# 		mysql_error_code
# 	 | SQLSTATE [VALUE] sqlstate_value
# }
#
# The DECLARE_---_CONDITION statement declares a named error condition, associating
# a name with a condition that needs specific handling.
#
# The name can be referred to in a subsequent DECLARE_---_HANDLER statement (see SECTION 13.6.7.2, "DECLARE_---_HANDLER SYNTAX")
#
# Condition declarations must appear before cursor or handler declarations.
#
# The condition_value for DECLARE_---_CONDITION indicates the specific condition or class of conditions
# to associate with the condition name.
#
# It can take the following forms:
#
# 		) mysql_error_code: An integer literal indicating a MySQL error code.
#
# 			Do not use MySQL error code 0 because that indicates success rather than an error
# 			condition.
#
# 			For a list of MySQL error codes, see SECTION B.3, "SERVER ERROR MESSAGE REFERENCE"
#
# 		) SQLSTATE[VALUE] sqlstate_value: A 5-character string literal indicating an SQLSTATE value.
#
# 			Do not use SQLSTATE values that begin with '00' because those indicate success rather
# 			than an error condition.
#
# 			For a list of SQLSTATE values, see SECTION B.3, "SERVER ERROR MESSAGE REFERENCE"
#
# Condition names referred to in SIGNAL or use RESIGNAL statements must be associated with
# SQLSTATE values, not MySQL error codes.
#
# Using names for conditions can help make stored program code clearer.
#
# For example, this handler applies to attempts to drop a nonexistent table,
# but that is apparent only if you know that 1051 is the MySQL error code for "unknown table":
#
# 		DECLARE CONTINUE HANDLER FOR 1051
# 			BEGIN
# 				-- body of handler
# 			END;
#
# By declaring a name for the condition, the purpose of the handler is more readily seen:
#
# 		DECLARE no_such_table CONDITION FOR 1051;
# 		DECLARE CONTINUE HANDLER FOR no_such_table
# 			BEGIN
# 				-- body of handler
# 			END;
#
# Here is a named condition for the same condition, but based on the corresponding
# SQLSTATE value rather than the MySQL error code:
#
# 		DECLARE no_such_table CONDITION FOR SQLSTATE '42S02';
# 		DECLARE CONTINUE HANDLER FOR no_such_table
# 			BEGIN
# 				-- body of handler
# 			END;
#
# 13.6.7.2 DECLARE --- HANDLER SYNTAX
#
# 	DECLARE handler_action HANDLER
# 		FOR condition_value [, condition_value]
# 		statement
#
# 	handler_action: {
# 		CONTINUE
# 	 | EXIT
#   | UNDO
# 	}
#
# 	condition_value: {
# 		mysql_error_code
#   | SQLSTATE [VALUE] sqlstate_value
# 	 | condition_name
# 	 | SQLWARNING
# 	 | NOT FOUND
#   | SQLEXCEPTION
#  }
#
# The DECLARE_---_HANDLER statement specifies a handler that deals with one or more
# conditions.
#
# If one of these conditions occurs, the specified statement executes.
#
# statement can be a simple statement such as SET var_name = value, or a compound statement
# written using BEGIN and END (see SECTION 13.6.1, "BEGIN --- END COMPOUND-STATEMENT SYNTAX")
#
# Handler declarations must appear after variable or condition declarations.
#
# The handler_action value indicates what action the handler takes after execution
# of the handler statement:
#
# 		) CONTINUE: Execution of the current program continues
#
# 		) EXIT: Execution terminates for the BEGIN_---_END compound statement in which the handler
# 			is declared.
#
# 			This is true even if the condition occurs in an inner block.
#
# 		) UNDO: Not supported.
#
# The condition_value for DECLARE_---_HANDLER indicates the specific condition or class
# of conditions that activates the handler.
#
# It can take the following forms:
#
# 		) mysql_error_code: An integer literal indicating a MySQL error code, such as 1051 to specify "Unknown table":
#
# 			DECLARE CONTINUE HANDLER FOR 1051
# 				BEGIN
# 					-- body of handler
# 				END;
#
# 			Do not use MySQL error code 0 because that indicates success rather than an error condition.
#
# 			For a list of MySQL error codes, see SECTION B.3, "SERVER ERROR MESSAGE REFERENCE"
#
# 		) SQLSTATE[VALUE] sqlstate_value: A 5-character string literal indicating an SQLSTATE value,
# 			such as '42S01' to specify "unknown table":
#
# 				DECLARE CONTINUE HANDLER FOR SQLSTATE '42S02'
# 					BEGIN
# 						-- body of handler
# 					END;
#
# 			Do not use SQLSTATE values that begin with '00' because those indicate success
# 			rather than an error condition.
#
# 			For a list of SQLSTATE values, see SECTION B.3, "SERVER ERROR MESSAGE REFERENCE"
#
# 		) condition_name: A condition name previously specified with DECLARE_---_CONDITION
#
# 			A condition name can be associated with a MySQL error code or SQLSTATE value.
#
# 			See SECTION 13.6.7.1, "DECLARE --- CONDITION SYNTAX"
#
# 		) SQLWARNING: Shorthand for the class of SQLSTATE values that begin with '01'
#
# 			DECLARE CONTINUE HANDLER FOR SQLWARNING
# 				BEGIN
# 					--- body of handler
# 				END;
#
# 		) NOT FOUND:
#
# 			Shorthand for the class of SQLSTATE values that begin with '02'
#
# 			This is relevant within the context of cursors and is used to control
# 			what happens when a cursor reaches the end of a data set.
#
# 			If no more rows are available, a No Data condition occurs with SQLSTATE 
# 			value '02000'
#
# 			To detect this condition, you can set up a handler for it or for a NOT FOUND
# 			condition.
#
# 				DECLARE CONTINUE HANDLER FOR NOT FOUND
# 					BEGIN
# 						-- body of handler
# 					END;
#
# 			For another example, see SECTION 13.6.6, "CURSORS"
#
# 			The NOT FOUND condition also occurs for SELECT --- INTO var_list statements
# 			that retrieve no rows.
#
# 		) SQLEXCEPTION:
#
# 			Shorthand for the class of SQLSTATE values that do not begin with 
# 			'00', '01', or '02'
#
# 				DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
# 					BEGIN
# 						-- body of handler
# 					END;
#
# For information about how the server chooses handlers when a condition occurs,
# see SECTION 13.6.7.6, "SCOPE RULES FOR HANDLERS"
#
# If a condition occurs for which no handler has been declared, the action taken
# depends on the condition class:
#
# 		) For SQLEXCEPTION conditions, the stored program terminates at the statement
# 			that raised the condition, as if there were an EXIT handler.
#
# 			If the program was called by another stored program, the calling program
# 			handles the condition using the handler selection rules applied to
# 			its own handlers.
#
# 		) For SQLWARNING conditions, the program continues executing, as if there were a CONTINUE handler.
#
# 		) For NOT FOUND conditions, if the condition was raised normally, the action is CONTINUE.
#
# 			If it was raised by SIGNAL or RESIGNAL, the action is EXIT.
#
# The following example uses a handler for SQLSTATE '23000', which occurs for a 
# duplicate-key error:
#
# 		CREATE TABLE test.t (s1 INT, PRIMARY KEY (s1));
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		delimiter //
#
# 		CREATE PROCEDURE handlerdemo ()
# 		BEGIN
# 			DECLARE CONTINUE HANDLER FOR SQLSTATE '23000' SET @x2 = 1;
# 			SET @x = 1;
# 			INSERT INTO test.t VALUES (1);
# 			SET @x = 2;
# 			INSERT INTO test.t VALUES (1);
# 			SET @x = 3;
# 		END;
# 		//
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		CALL handlerdemo()//
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		SELECT @x//
# 		+------------+
# 		| @x 			 |
# 		+------------+
# 		| 3 			 |
# 		+------------+
# 		1 row in set (0.00 sec)
#
# Notice that @x is 3 after the procedure executes, which shows that execution continued
# to the end of the procedure after the error occurred.
#
# If the DECLARE_---_HANDLER statement had not been present, MySQL would have taken the
# default action (EXIT) after the second INSERT failed due to the PRIMARY KEY constraint,
# and SELECT @x would have returned 2.
#
# To ignore a condition, declare a CONTINUE handler for it and associate it with an empty block.
#
# For example:
#
# 		DECLARE CONTINUE HANDLER FOR SQLWARNING BEGIN END;
#
# The scope of a block label does not include the code for handlers declared within the block.
#
# Therefore, the statement associated with a handler cannot use ITERATE or LEAVE to refer
# to labels for blocks that enclose the handler declaration.
#
# Consider the following example, where the REPEAT block has a label of retry:
#
# 		CREATE PROCEDURE p ()
# 		BEGIN
# 			DECLARE i INT DEFAULT 3;
# 			retry:
# 				REPEAT
# 					BEGIN
# 						DECLARE CONTINUE HANDLER FOR SQLWARNING
# 							BEGIN
# 								ITERATE retry; #Illegal
# 							END;
# 						IF i < 0 THEN
# 							LEAVE retry; #legal
# 						END IF;
# 						SET i = i - 1;
# 					END;
# 				UNTIL FALSE END REPEAT;
# 		END;
#
# The retry label is in scope for the IF statement within the block.
#
# It is not in scope for the CONTINUE handler, so the reference there is invalid
# and results in an error:
#
# 		ERROR 1308 (42000): LEAVE with no matching label: retry
#
# To avoid references to outer labels in handlers, use one of these strategies:
#
# 		) To leave the block, use an EXIT handler. If no block cleanup is required, the
# 			BEGIN_---_END handler body can be empty:
#
# 				DECLARE EXIT HANDLER FOR SQLWARNING BEGIN END;
#
# 			Otherwise, put the cleanup statements in the handler body:
#
# 				DECLARE EXIT HANDLER FOR SQLWARNING
# 					BEGIN
# 						block cleanup statements
# 					END;
#
# 		) To continue execution, set a status variable in a CONTINUE handler that can be checked
# 			in the enclosing block to determine whether the handler was invoked.
#
# 			The following example uses the variable done for this purpose:
#
# 				CREATE PROCEDURE p ()
# 				BEGIN
# 					DECLARE i INT DEFAULT 3;
# 					DECLARE done INT DEFAULT FALSE;
# 					retry:
# 						REPEAT
# 							BEGIN
# 								DECLARE CONTINUE HANDLER FOR SQLWARNING
# 									BEGIN
# 										SET done = TRUE;
# 									END;
# 								IF done OR i < 0 THEN
# 									LEAVE retry;
# 								END IF;
# 								SET i = i - 1;
# 							END;
# 						UNTIL FALSE END REPEAT;
# 				END; 
#
# 13.6.7.3 GET DIAGNOSTICS SYNTAX
#
# 		GET [CURRENT | STACKED] DIAGNOSTICS
# 		{
# 			statement_information_item
# 			[, statement_information_item] ---
# 		 | CONDITION condition_number
# 			condition_information_item
# 		 	[, condition_information_item] ---
# 		}
#
# 		statement_information_item:
# 			target = statement_information_item_name
#
# 		condition_information_item:
# 			target = condition_information_item_name
#
# 		statement_information_item_name:
# 			NUMBER
# 		 | ROW_COUNT
#
# 		condition_information_item_name: {
# 			CLASS_ORIGIN
# 		 | SUBCLASS_ORIGIN
# 		 | RETURNED_SQLSTATE
# 		 | MESSAGE_TEXT
# 		 | MYSQL_ERRNO
# 		 | CONSTRAINT_CATALOG
# 		 | CONSTRAINT_SCHEMA
# 		 | CONSTRAINT_NAME
# 		 | CATALOG_NAME
# 		 | SCHEMA_NAME
# 		 | TABLE_NAME
# 		 | COLUMN_NAME
# 		 | CURSOR_NAME
# 		}
#
# 		condition_number, target:
# 			(see following discussion)
#
# SQL statements produce diagnostic information that populates the diagnostics area.
#
# The GET_DIAGNOSTICS statement enables applications to inspect this information.
#
# (You can also use SHOW_WARNINGS or SHOW_ERRORS to see conditions or errors)
#
# No special privileges are required to execute GET_DIAGNOSTICS
#
# The keyword CURRENT means to retrieve information from the current diagnostics area.
#
# The keyword STACKED means to retrieve information from the second diagnostics
# area, which is available only if the current context is a condition handler.
#
# If neither keyword is given, the default is to use the current diagnostics area.
#
# The GET_DIAGNOSTICS statement is typically used in a handler within a stored program.
#
# It is a MySQL extension that GET_[CURRENT]_DIAGNOSTICS is permitted outside
# handler context to check the execution of any SQL statement.
#
# For example, if you invoke the mysql client program, you can enter these statements
# at the prompt:
#
# 		DROP TABLE test.no_such_table;
# 		ERROR 1051 (42S02): Unknown table 'test.no_such_table'
# 		GET DIAGNOSTICS CONDITION 1
# 			@p1 = RETURNED_SQLSTATE, @p2 = MESSSAGE_TEXT;
# 		SELECT @p1, @p2;
# 		+-------+------------------------------------+
# 		| @p1   | @p2 									      |
# 		+-------+------------------------------------+
# 		| 42S02 | Unknown table 'test.no_such_table' |
# 		+-------+------------------------------------+
#
# This extension applies only to the current diagnostics area.
#
# It does not apply to the second diagnostics area because GET STACKED DIAGNOSTICS
# is permitted only if the current context is a condition handler.
#
# If that is not the case, a:
#
# 	 GET STACKED DIAGNOSTICS when handler not active
#
# error occurs
#
# For a description of the diagnostics area, see SECTION 13.6.7.7, "THE MySQL DIAGNOSTICS AREA"
#
# Briefly, it contains two kinds of information:
#
# 		) Statement information, such as the number of conditions that occurred or the affected-rows count
#
# 		) Condition information, such as the error code and message.
#
# 			If a statement raises multiple conditions, this part of the diagnostics area has
# 			a condition area for each one.
#
# 			If a statement raises no conditions, this part of the diagnostics area is empty.
#
# For a statement that produces three conditions, the diagnostics area contains statement
# and condition information like this:
#
# 		Statement information:
# 			row count
# 			--- other statement information items ---
# 		Condition area list:
# 			Condition area 1:
# 				error code for condition 1
# 				error message for condition 1
# 				--- other condition information items ---
# 			Condition area 2:
# 				error code for condition 2:
# 				error message for condition 2
# 				--- other condition information items ---
# 			Condition area 3:
# 				error code for condition 3
# 				error message for condition 3
# 				--- other condition information items --
#
# GET_DIAGNOSTICS can obtain either statement or condition information, but not
# both in the same statement:
#
# 		) To obtain statement information, retrieve the desired statement items into target
# 			variables.
#
# 			This instance of GET_DIAGNOSTICS assigns the number of available conditions
# 			and the rows-affected count to the user variables @p1 and @p2:
#
# 				GET DIAGNOSTICS @p1 = NUMBER, @p2 = ROW_COUNT;
#
# 		) To obtain condition information, specify the condition number and retrieve the desired
# 			condition items into target variables.
#
# 			This instance of GET_DIAGNOSTICS assigns the SQLSTATE value and error message
# 			to the user variables @p3 and @p4:
#
# 				GET DIAGNOSTICS CONDITION 1
# 					@p3 = RETURNED_SQLSTATE, @p4 = MESSAGE_TEXT;
#
# The retreival list specifies one or more target = item_name assignments, separated by commas.
#
# Each assignment names a target variable and either a statement_information_item_name or
# condition_information_item_name designator, depending on whether the statement retrieves
# statement or condition information.
#
# Valid target designators for storing item information can be stored procedure or
# function parameters, stored program local variables declared with DECLARE,
# or user-defined variables.
#
# Valid condition_number designators can be stored procedure or function parameters,
# stored program local variables declared with DECLARE, user-defined variables,
# system variables, or literals.
#
# A character literal may include a _charset introducer.
#
# A warning occurs if the condition number is not in the range from 1 to the
# number of condition areas that have information.
#
# In this case, the warning is added to the diagnostics area without clearing it.
#
# When a condition occurs, MySQL does not populate all condition items recognized
# by GET_DIAGNOSTICS.
#
# For example:
#
# 		GET DIAGNOSTICS CONDITION 1
# 			@p5 = SCHEMA_NAME, @p6 = TABLE_NAME;
# 		SELECT @p5, @p6;
# 		+------+-----------+
# 		| @p5  | @p6 		 |
# 		+------+-----------+
# 		|  	 | 			 |
# 		+------+-----------+
#
# In standard SQL, if there are multiple conditions, the first condition relates
# to the SQLSTATE value returned for the previous SQL statement.
#
# In MySQL, this is not guaranteed.
#
# To get the main error, you cannot do this:
#
# 		GET DIAGNOSTICS CONDITION 1 @errno = MYSQL_ERRNO;
#
# Instead, retrieve the condition count first, then use it to specify
# which condition number to inspect:
#
# 		GET DIAGNOSTICS @cno = NUMBER;
# 		GET DIAGNOSTICS CONDITION @cno @errno = MYSQL_ERRNO;
#
# For information about permissible statement and condition information items,
# and which ones are populated when a condition occurs, see DIAGNOSTICS AREA INFORMATION ITEMS
#
# Here is an example that uses GET_DIAGNOSTICS and an exception handler in stored procedure
# context to assess the outcome of an insert operation.
#
# If the insert was successful, the procedure uses GET_DIAGNOSTICS to get the rows-affected
# count.
#
# This shows that you can use GET_DIAGNOSTICS multiple times to retrieve information about
# a statement as long as the current diagnostics area has not been cleared.
#
# 		CREATE PROCEDURE do_insert(value INT)
# 		BEGIN
# 			--- DECLARE variables to hold diagnostics area information
# 			DECLARE code CHAR(5) DEFAULT '00000';
# 			DECLARE msg TEXT;
# 			DECLARE rows INT;
# 			DECLARE result TEXT;
# 			-- Declare exception handler for failed insert
# 			DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
# 				BEGIN
# 					GET DIAGNOSTICS CONDITION 1
# 						code = RETURNED_SQLSTATE, msg = MESSAGE_TEXT;
# 				END;
#
# 			-- Perform the insert
# 			INSERT INTO t1 (int_col) VALUES(value);
# 			-- Check whether the insert was successful
# 			IF code = '00000' THEN
# 				GET DIAGNOSTICS rows = ROW_COUNT;
# 				SET result = CONCAT('insert succeeded, row count = ',rows);
# 			ELSE
# 				SET result = CONCAT('insert failed, error = ',code,', message = ',msg);
# 			END IF;
# 			-- Say what happened
# 			SELECT result;
# 		END;
#
# Suppose that t1.int_col is an integer column that is declared as NOT NULL.
#
# The procedure produces these results when invoked to insert non-NULL and
# NULL values, respectively:
#
# 		CALL do_insert(1);
# 		+---------------------------------+
# 		| result 							    |
# 		+---------------------------------+
# 		| insert succeeded, row count = 1 |
# 		+---------------------------------+
#
# 		CALL do_insert(NULL);
# 		+-------------------------------------------------------------------------+
# 		| result 																					  |
# 		+-------------------------------------------------------------------------+
# 		| insert failed, error = 23000, message = Column 'int_col' cannot be null |
# 		+-------------------------------------------------------------------------+
#
# When a condition handler activates, a push to the diagnostics area stack occurs:
#
# 		) The first (current) diagnostics area becomes the second (stacked) diagnostics area
# 			and a new current diagnostics area is created as a copy of it.
#
# 		) GET_[CURRENT]_DIAGNOSTICS and GET_STACKED_DIAGNOSTICS can be used within the handler
# 			to access the contents of the current and stacked diagnostics areas.
#
# 		) Initially, both diagnostics areas return the same result, so it is possible to get
# 			information from the current diagnostics area about the condition that activated
# 			the handler, as long as you execute no statements within the handler that change
# 			its current diagnostics area.
#
# 		) However, statements executing within the handler can modify the current diagnostics area,
# 			clearing and setting its contents according to the normal rules (see HOW THE DIAGNOSTICS AREA IS CLEARED AND POPULATED)
#
# 			A more reliable way to obtain information about the handler-activating condition is to use the
# 			stacked diagnostics area, which cannot be modified by statements executing within the handler
# 			except RESINGAL.
#
# 			For information about when the current diagnostics area is set and cleared, see
# 			SECTION 13.6.7.7, "THE MYSQL DIAGNOSTICS AREA"
#
# The next example shows how GET STACKED DIAGNOSTICS can be used within a handler to obtain
# information about the handled exception, even after the current diagnostics area has been
# modified by handler statements.
#
# Within a stored procedure p(), we attempt to insert two values into a table that contains a
# TEXT NOT NULL column.
#
# The first value is a non-NULL string and the second is NULL
#
# The column prohibits NULL values, so the first insert succeeds but the
# second causes an exception.
#
# The procedure includes an exception handler that maps attempts to insert
# NULL into inserts of the empty string:
#
# 		DROP TABLE IF EXISTS t1;
# 		CREATE TABLE t1 (c1 TEXT NOT NULL);
# 		DROP PROCEDURE IF EXISTS p;
# 		delimiter //
# 		CREATE PROCEDURE p ()
# 		BEGIN
# 			-- Declare variables to hold diagnostics area information
# 			DECLARE errcount INT;
# 			DECLARE errno INT;
# 			DECLARE msg TEXT;
# 			DECLARE EXIT HANDLER FOR SQLEXCEPTIOn
# 			BEGIN
# 				-- Here the current DA is nonempty because no prior statements
# 				-- executing within the handler have cleared it
# 				GET CURRENT DIAGNOSTICS CONDITION 1
# 					errno = MYSQL_ERRNO, msg = MESSAGE_TEXT;
# 				SELECT 'current DA before mapped insert' AS op, errno, msg;
# 				GET STACKED DIAGNOSTICS CONDITION 1
# 					errno = MYSQL_ERRNO, msg = MESSAGE_TEXT;
# 				SELECT 'stacked DA before mapped insert' AS op, errno, msg;
#
# 				-- Map attempted NULL insert to empty string insert
# 				INSERT INTO t1 (c1) VALUES('');
#
# 				-- Here the current DA should be empty (if the INSERT succeeded),
# 				-- so check whether there are conditions before attempting to
# 				-- obtain condition information
# 				GET CURRENT DIAGNOSTICS errcount = NUMBER;
# 				IF errcount = 0
# 				THEN
# 					SELECT 'mapped insert succeeded, current DA is empty' AS op;
# 				ELSE
# 					GET CURRENT DIAGNOSTICS CONDITION 1
# 						errno = MYSQL_ERRNO, msg = MESSAGE_TEXT;
# 					SELECT 'current DA after mapped insert' AS op, errno, msg;
# 				END IF ;
# 				GET STACKED DIAGNOSTICS CONDITION 1
# 					errno = MYSQL_ERRNO, msg = MESSAGE_TEXT;
# 				SELECT 'stacked DA after mapped insert' AS op, errno, msg;
# 			END;
# 			INSERT INTO t1 (c1) VALUES('string 1');
# 			INSERT INTO t1 (c1) VALUES(NULL);
# 		END;
# 		//
# 		delimiter ;
# 		CALL p();
# 		SELECT * FROM t1;
#
# When the handler activates, a copy of the current diagnostics area is pushed to the
# diagnostics area stack.
#
# The handler first displays the contents of the current and stacked diagnostics areas,
# which are both the same initially:
#
# 		+-------------------------------------------+-----------+-------------------------------------+
# 		| op 													  | errno 	  | msg 											 |
# 		+-------------------------------------------+-----------+-------------------------------------+
# 		| current DA before mapped insert 			  | 1048 	  | Column 'c1' cannot be null 			 |
# 		+-------------------------------------------+-----------+-------------------------------------+
#
# 		+-------------------------------------------+-----------+-------------------------------------+
# 		| op 													  | errno 	  | msg 											 |
# 		+-------------------------------------------+-----------+-------------------------------------+
# 		| stacked DA before mapped insert 			  | 1048 	  | Column 'c1' cannot be null 			 |
# 		+-------------------------------------------+-----------+-------------------------------------+
#
# Statements executing after the GET_DIAGNOSTICS statements may reset the current diagnostics area.
#
# Statements may reset the current diagnostics area.
#
# For example, the handler maps the NULL insert to an empty string insert and displays the result.
#
# The new insert succeeds and clears the current diagnostics area, but the stacked diagnostics
# area remains unchanged and still contains information about the condition that activated the handler:
#
# 		+-----------------------------------------------+
# 		| op 															|
# 		+-----------------------------------------------+
# 		| mapped insert succeeded, current DA is empty  |
# 		+-----------------------------------------------+
#
# 		+-----------------------------------------------+---------+--------------------------------+
# 		| op 															| errno 	 | msg 									 |
# 		+-----------------------------------------------+---------+--------------------------------+
# 		| stacked DA after mapped insert 					| 1048 	 | Column 'c1' cannot be null     |
# 		+-----------------------------------------------+---------+--------------------------------+
#
# When the condition handler ends, its current diagnostics area is popped from the stack and the stacked
# diagnostics area becomes the current diagnostics area in the stored procedure.
#
# After the procedure returns, the table contains two rows.
#
# The empty row results from the attempt to insert NULL that was mapped to an empty string insert:
#
# 		+---------------+
# 		| c1 				 |
# 		+---------------+
# 		| string 1 		 |
# 		| 					 |
# 		+---------------+
#
# In the preceding example, the first two GET_DIAGNOSTICS statements within the condition handler
# that retrieve information from the current and stacked diagnostics areas return the same values.
#
# This will not be the case if statements that reset the current diagnostics area execute earlier
# within the handler.
#
# Suppose that p() is rewritten to place the DECLARE statements within the handler definition
# rather than preceding it:
#
# 		CREATE PROCEDURE p ()
# 		BEGIN
# 			DECLARE EXIT HANDLER FOR SQLEXCEPTION
# 			BEGIN
# 				-- Declare variables to hold diagnostics area information
# 				DECLARE errcount INT;
# 				DECLARE errno INT;
# 				DECLARE msg TEXT;
# 				GET CURRENT DIAGNOSTICS CONDITION 1
# 					errno = MYSQL_ERRNO, msg = MESSAGE_TEXT;
# 				SELECT 'current DA before mapped insert' AS op, errno, msg;
# 				GET STACKED DIAGNOSTICS CONDITION 1
# 					errno = MYSQL_ERRNO, msg = MESSAGE_TEXT;
# 				SELECT 'stacked DA before mapped insert' AS op, errno, msg;
# 		---
#
# In this case, the result is version dependent:
#
# 		) Before MySQL 5.7.2, DECLARE does not change the current diagnostics area, so the first two
# 			GET_DIAGNOSTICS statements return the same result, just as in the original version of p()
#
# 			In MySQL 5.7.2, work was done to ensure that all nondiagnostic statements populate the
# 			diagnostics area, per the SQL standard.
#
# 			DECLARE is one of them, so in 5.7.2 and higher, DECLARE statements executing at the 
# 			beginning of the handler clear the current diagnostics area and the GET_DIAGNOSTICS
# 			statements produce different results:
#
# 				+-----------------------------------+-----------+-----------+
# 				| op 											| errno 	   | msg 		|
# 				+-----------------------------------+-----------+-----------+
# 				| current DA before mapped insert   | NULL 		| NULL 		|
# 				+-----------------------------------+-----------+-----------+
#
# 				+-----------------------------------+-----------+---------------------------------------+
# 				| op 											| errno 	   | msg 											 |
# 				+-----------------------------------+-----------+---------------------------------------+
# 				| stacked DA before mapped insert   | 1048 		| Column 'c1' cannot be null 				 |
# 				+-----------------------------------+-----------+---------------------------------------+
#
# To avoid this issue within a condition handler when seeking to obtain information about the condition
# that activated the handler, be sure to access the stacked diagnostics area, not the current
# diagnostics area.
#
# 13.6.7.4 RESIGNAL SYNTAX
#
# 		RESIGNAL [condition_value]
# 			[SET signal_information_item
# 			[, signal_information_item] ---]
#
# 		condition_value: {
# 			SQLSTATE [VALUE] sqlstate_value
# 		 | condition_name
# 		}
#
# 		signal_information_item:
# 			condition_information_item_name = simple_value_specification
#
# 		condition_information_item_name: {
# 			CLASS_ORIGIN
# 		 | SUBCLASS_ORIGIN
# 		 | MESSAGE_TEXT
# 		 | MYSQL_ERRNO
# 		 | CONSTRAINT_CATALOG
# 		 | CONSTRAINT_SCHEMA
# 		 | CONSTRAINT_NAME
# 		 | CATALOG_NAME
# 		 | SCHEMA_NAME
# 		 | TABLE_NAME
# 		 | COLUMN_NAME
# 		 | CURSOR_NAME
# 		}
#
# 		condition_name, simple_value_specification:
# 			(see following discussion)
#
# RESIGNAL passes on the error condition information that is available during
# execution of a condition handler within a compound statement inside a stored
# procedure or function, trigger, or event.
#
# RESIGNAL may change some or all information before passing it on.
#
# RESIGNAL is related to SIGNAL, but instead of originating a condition
# as SIGNAL does, RESIGNAL relays existing condition information, possibly
# after modifying it.
#
# RESIGNAL makes it possible to both handle an error and return the error information.
#
# Otherwise, by executing an SQL statement within the handler, information that caused
# the handler's activation is destroyed.
#
# RESIGNAL also can make some procedures shorter if a given handler can handle part
# of a situation, then pass the condition "up the line" to another handler.
#
# No privileges are required to execute the RESIGNAL statement.
#
# All forms of RESIGNAL require that the current context be a condition handler.
#
# Otherwise, RESIGNAL is illegal and a:
#
# 		RESIGNAL when handler not active
#
# error occurs.
#
# To retrieve information from the diagnostics area, use the GET_DIAGNOSTICS
# statement (see SECTION 13.6.7.3, "GET DIAGNOSTICS SYNTAX")
#
# FOr information about the diagnostics area, see SECTION 13.6.7.7,
# "THE MYSQL DIAGNOSTICS AREA"
#
# 		) RESIGNAL OVERVIEW
#
# 		) RESIGNAL ALONE
#
# 		) RESIGNAL WITH NEW SIGNAL INFORMATION
#
# 		) RESIGNAL WITH A CONDITION VALUE AND OPTIONAL NEW SIGNAL INFORMATION
#
# 		) RESIGNAL REQUIRES CONDITION HANDLER CONTEXT
#
# RESIGNAL OVERVIEW
#
# For condition_value and signal_information_item, the definitions and rules are the same
# for RESIGNAL as for SIGNAL.
#
# For example, the condition_value can be an SQLSTATE value, and the value can indicate errors,
# warnings, or "not found".
#
# For additional information, see SECTION 13.6.7.5, "SIGNAL SYNTAX"
#
# The RESIGNAL statement takes condition_value and SET clauses, both of which are optional.
#
# This leads to several possible uses:
#
# 		) RESIGNAL alone:
#
# 			RESIGNAL;
#
# 		) RESIGNAL with new signal information:
#
# 			RESIGNAL SET signal_information_item [, signal_information_item] ---;
#
# 		) RESIGNAL with a condition value and possibly new signal information:
#
# 			RESIGNAL condition_value
# 				[SET signal_information_item [, signal_information_item] ---];
#
# These use cases all cause changes to the diagnostics and condition areas:
#
# 		) A diagnostics area contains one or more condition areas.
#
# 		) A condition area contains condition information items, such as the SQLSTATE value, MYSQL_ERRNO,
# 			or MESSAGE_TEXT
#
# There is a stack of diagnostics areas.
#
# When a handler takes control, it pushes a diagnostics area to the top of the stack, so there
# are two diagnostics areas during handler execution:
#
# 		) The first (current) diagnostics area, which starts as a copy of the last diagnostics area,
# 			but will be overwritten by the first statement in the handler that changes the current
# 			diagnostics area.
#
# 		) The last (stacked) diagnostics area, which has the condition areas that were set up before the handler
# 			took control.
#
# The maximum number of condition areas in a diagnostics area is determined by the value of the max_error_count
# system variable.
#
# See DIAGNOSTICS AREA-RELATED SYSTEM VARIABLES
#
# RESIGNAL ALONE
#
# A simple RESIGNAL alone means "pass on the error with no change."
#
# It restores the last diagnostics area and makes it the current diagnostics area.
#
# That is, it "Pops" the diagnostics area stack.
#
# Within a condition handler that catches a condition, one use for RESIGNAL alone is to
# perform some other actions, and then pass on without change the original condition
# information (the information that existed before entry into the handler)
#
# Example:
#
# 		DROP TABLE IF EXISTS xx;
# 		delimiter //
# 		CREATE PROCEDURE p ()
# 		BEGIN
# 			DECLARE EXIT HANDLER FOR SQLEXCEPTION
# 			BEGIN
# 				SET @error_count = @error_count + 1;
# 				IF @a = 0 THEN RESIGNAL; END IF;
# 			END;
# 			DROP TABLE xx;
# 		END//
# 		delimiter ;
# 		SET @error_count = 0;
# 		SET @a = 0;
# 		CALL p();
#
# Suppose that the DROP TABLE xx statement fails. The diagnostics area stack looks like this:
#
# 		DA 1. ERROR 1051 (42S02): Unknown table 'xx'
#
# Then execution enters the EXIT handler. It starts by pushing a diagnostics area to the top of
# the stack, which now looks like this:
#
# 		DA 1. ERROR 1051 (42S02): Unknown table 'xx'
# 		DA 2. ERROR 1051 (42S02): Unknown table 'xx'
#
# At this point, the contents of the first (current) and second (stacked) diagnostics areas are
# teh same.
#
# The first diagnostics area may be modified by statements executing subsequently within
# the handler.
#
# Usually a procedure statement clears the first diagnostics area.
#
# BEGIN is an exception, it does not clear, it does nothing.
#
# SET is not an exception, it clears,performs the operation, and produces a result of
# "success"
#
# The diagnostics area stack now looks like this:
#
# 		DA 1. ERROR 0000 (00000): Successful operation
# 		DA 2. ERROR 1051 (42S02): Unknown table 'xx'
#
# At this point, if @a = 0, RESIGNAL pops the diagnostics area stack, which now looks
# like this:
#
# 		DA 1. ERROR 1051 (42S02): Unknown table 'xx'
#
# And that is what the caller sees.
#
# If @a is not 0, the handler simply ends, which means that there is no more use for the
# current diagnostics area (it has been "handled"), so it can be thrown away, causing
# the stacked diagnostics area to become the current diagnostics area again.
#
# The diagnostics area stack looks like this:
#
# 		DA 1. ERROR 0000 (00000): Successful operation
#
# The details make it look complex, but the end result is quite useful:
#
# 		Handlers can execute without destroying information about the condition
# 		that caused activation of the handler.
#
# RESIGNAL WITH NEW SIGNAL INFORMATION
#
# RESIGNAL with a SET clause provides new signal information, so the statement means
# "pass on the error with changes"
#
# 		RESIGNAL SET signal_information_item [, signal_information_item] ---;
#
# As with RESIGNAL alone the idea is to pop the diagnostics area stack so that
# the original information will go out.
#
# Unlike RESIGNAL alone, anything specified in the SET clause changes.
#
# Example:
#
# 		DROP TABLE IF EXISTS xx;
# 		delimiter //
# 		CREATE PROCEDURE p ()
# 		BEGIN
# 			DECLARE EXIT HANDLER FOR SQLEXCEPTION
# 			BEGIN
# 				SET @error_count = @error_count + 1;
# 				IF @a = 0 THEN RESIGNAL SET MYSQL_ERRNO = 5; END IF;
# 			END;
# 			DROP TABLE xx;
# 		END//
# 		delimiter ;
# 		SET @error_count = 0;
# 		SET @a = 0;
# 		CALL p();
#
# Remember from the previous discussion that RESIGNAL alone results in a diagnostics
# area stack like this:
#
# 		DA 1. ERROR 1051 (42S02): Unknown table 'xx'
#
# The RESIGNAL SET MYSQL_ERRNO = 5 statement results in this stack instead, which is what the caller sees:
#
# 		DA 1. ERROR 5 (42S02): Unknown table 'xx'
#
# In other words, it changes the error number, and nothing else.
#
# The RESIGNAL statement can change any or all of the signal information items, making
# the first condition area of the diagnostics area look quite different.
#
# RESIGNAL WITH A CONDITION VALUE AND OPTIONAL NEW SIGNAL INFORMATION
#
# RESIGNAL with a condition value means "push a condition into the current diagnostics area"
#
# If the SET clause is present, it also changes the error information.
#
# 		RESIGNAL condition_value
# 			[SET signal_information_item [, signal_information_item] ---];
#
# This form of RESIGNAL restores the last diagnostics area and makes it the current
# diagnostics area.
#
# That is, it "pops" the diagnostics area stack, which is the same as what a simple
# RESIGNAL alone would do.
#
# However, it also changes the diagnostics area depending on the condition value or
# signal information.
#
# Example:
#
# 		DROP TABLE IF EXISTS xx;
# 		delimiter //
# 		CREATE PROCEDURE p ()
# 		BEGIN
# 			DECLARE EXIT HANDLER FOR SQLEXCEPTION
# 			BEGIN
# 				SET @error_count = @error_count + 1;
# 				IF @a = 0 THEN RESIGNAL SQLSTATE '45000' SET MYSQL_ERRNO=5; END IF;
# 			END;
# 			DROP TABLE xx;
# 		END//
# 		delimiter ;
# 		SET @error_count = 0;
# 		SET @a = 0;
# 		SET @@max_error_count = 2;
# 		CALL p();
# 		SHOW ERRORS;
#
# This is similar to the previous example, and the effects are the same, except that if
# RESIGNAL happens, the current condition area looks different at the end.
#
# (The reason the condition adds to rather than replaces the existing condition is the
# use of a condition value)
#
# The RESIGNAL statement includes a condition value (SQLSTATE '45000'), so it adds a new
# condition area, resulting in a diagnostics area stack that looks like this:
#
# 		DA 1. (condition 2) ERROR 1051 (42S02): Unknown table 'xx'
# 				(condition 1) ERROR 5 (45000) Unknown table 'xx'
#
# The result of CALL_p() and SHOW_ERRORS for this example is:
#
# 		CALL p();
# 		ERROR 5 (45000): Unknown table 'xx'
# 		SHOW ERRORS;
# 		+--------+---------+--------------------------------------------------+
# 		| Level  | Code    | Message 														 |
# 		+--------+---------+--------------------------------------------------+
# 		| Error  | 1051 	 | Unknown table 'xx' 										 |
# 		| Error  | 5 		 | Unknown table 'xx' 										 |
# 		+--------+---------+--------------------------------------------------+
#
# RESIGNAL REQUIRES CONDITION HANDLER CONTEXT
#
# All forms of RESIGNAL require that the current context be a condition handler.
#
# Otherwise, RESIGNAL is illegal and a RESIGNAL when handler not active error
# occurs.
#
# For example:
#
# 		CREATE PROCEDURE p () RESIGNAL;
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		CALL p();
# 		ERROR 1645 (0K000): RESIGNAL when handler not active
#
# Here is a more dificult example:
#
# 		delimiter //
# 		CREATE FUNCTION f () RETURNS INT
# 		BEGIN
# 			RESIGNAL;
# 			RETURN 5;
# 		END//
# 		CREATE PROCEDURE p ()
# 		BEGIN
# 			DECLARE EXIT HANDLER FOR SQLEXCEPTION SET @a=f();
# 			SIGNAL SQLSTATE '55555';
# 		END//
# 		delimiter ;
# 		CALL p();
#
# RESIGNAL occurs within the stored function f()
#
# Although f() itself is invoked within the context of the EXIT handler, execution
# within f() has its own context, which is not handler context.
#
# Thus, RESIGNAL within f() results in a "handler not active" error.
#
# 13.6.7.5 SIGNAL SYNTAX
#
# 		SIGNAL condition_value
# 			[SET signal_information_item
# 			[, signal_information_item] ---]
#
# 		condition_value: {
# 			SQLSTATE [VALUE] sqlstate_value
# 		 | condition_name
# 		}
#
# 		signal_information_item:
# 			condition_information_item_name = simple_value_specification
#
# 		condition_information_item_name: {
# 			CLASS_ORIGIN
# 		 | SUBCLASS_ORIGIN
# 		 | MESSAGE_TEXT
# 		 | MYSQL_ERRNO
# 		 | CONSTRAINT_CATALOG
# 		 | CONSTRAINT_SCHEMA
# 		 | CONSTRAINT_NAME
# 		 | CATALOG_NAME
# 		 | SCHEMA_NAME
# 		 | TABLE_NAME
# 		 | COLUMN_NAME
# 		 | CURSOR_NAME
# 		}
#
# 		condition_name, simple_value_specification:
# 			(see following discussion)
#
# SIGNAL is the way to "return" an error.
#
# SIGNAL provides error information to a handler, to an outer portion
# of the application, or to the client.
#
# Also, it provides control over the error's characteristics (error number,
# SQLSTATE value, message)
#
# Without SIGNAL, it is necessary to resort to workarounds such as deliberately
# referring to a nonexistent table to cause a routine to return an error.
#
# No privileges are required to execute the SIGNAL statement.
#
# To retrieve information from the diagnostics area, use the GET_DIAGNOSTICS
# statement (see SECTION 13.6.7.3, "GET DIAGNOSTICS SYNTAX")
#
# For information about the diagnostics area, see SECTION 13.6.7.7, "THE MYSQL DIAGNOSTICS AREA"
#
# 		) SIGNAL OVERVIEW
#
# 		) SIGNAL CONDITION INFORMATION ITEMS
#
# 		) EFFECT OF SIGNALS ON HANDLERS, CURSORS AND STATEMENTS
#
# SIGNAL OVERVIEW
#
# The condition_value in a SIGNAL statement indicates the error value to be returned.
#
# It can be an SQLSTATE value (a 5-character string literal) or a condition_name
# that refers to a named condition previously defined with DECLARE_---_CONDITION
# (See SECTION 13.6.7.1, "DECLARE --- CONDITION SYNTAX")
#
# An SQLSTATE value can indicate errors, warnings, or "not found"
#
# The first two characters of the value indicates its error class, as discussed
# in SIGNAL CONDITION INFORMATION ITEMS.
#
# Some signal values cause statement termination: see EFFECT OF SIGNALS ON HANDLERS, CURSORS AND STATEMENTS
#
# The SQLSTATE value for a SIGNAL statement should not start with '00' because such values
# indicate success and are not valid for signaling an error.
#
# This is true whether the SQLSTATE value is specified directly in the SIGNAL statement or in
# a named condition referred to in the statement.
#
# If the value is invalid, a BAD SQLSTATE error occurs.
#
# To signal a generic SQLSTATE value, use '45000', which means "unhandled user-defined exception"
#
# The SIGNAL statement optionally includes a SET clause that contains multiple signal items,
# in a list of condition_information_item_name = simple_value_specification assignments,
# separated by commas.
#
# Each condition_information_item_name may be specified only once in the SET clause.
#
# Otherwise, a Duplicate condition information item error occurs.
#
# Valid simple_value_specification designators can be specified using stored procedure
# or function parameters, stored program local variables declared with DECLARE,
# user-defined variables, system variables, or literals.
#
# A character literal may include a _charset introducer.
#
# For information about permissible condition_information_item_name values, see SIGNAL CONDITION INFORMATION ITEMS
#
# The following procedure signals an error or warning depending on the value of pval, its
# input parameter:
#
# 		CREATE PROCEDURE p (pval INT)
# 		BEGIN
# 			DECLARE speciality CONDITION FOR SQLSTATE '45000';
# 			IF pval = 0 THEN
# 				SIGNAL SQLSTATE '01000';
# 			ELSEIF pval = 1 THEN
# 				SIGNAL SQLSTATE '45000'
# 					SET MESSAGE_TEXT = 'An error occurred';
# 			ELSEIF pval = 2 THEN
# 				SIGNAL speciality
# 					SET MESSAGE_TEXT = 'An error occurred';
# 			ELSE
# 				SIGNAL SQLSTATE '01000'
# 					SET MESSAGE_TEXT = 'A warning occurred', MYSQL_ERRNO = 1000;
# 				SIGNAL SQLSTATE '45000'
# 					SET MESSAGE_TEXT = 'An error occurred', MYSQL_ERRNO = 1001;
# 			END IF;
# 		END;
#
# If pval is 0, p() signals a warning because SQLSTATE values that begin with '01' are signals
# in the warning class.
#
# The warning does not terminate the procedure, and can be seen with SHOW_WARNINGS after the
# procedure returns.
#
# If pval is 1, p() signals an error and sets the MESSAGE_TEXT condition information item.
#
# The error terminates the procedure, and the text is returned with the error information.
#
# If pval is 2, the same error is signaled, although the SQLSTATE value is specified using
# a named condition in this case.
#
# If pval is anything else, p() first signals a warning and sets the message text and error
# number condition information items.
#
# This warning does not terminate the procedure, so execution continues and p() then signals
# an error.
#
# The error does terminate the procedure. The message text and error number set by the warning
# are replaced by the values set by the error, which are returned with the error information.
#
# SIGNAL is typically used within stored programs, but it is a MySQL extension that it is permitted
# outside handler context.
#
# For example, if you invoke the mysql client program, you can enter any of these statements
# at the prompt:
#
# 		SIGNAL SQLSTATE '77777';
#
# 		CREATE TRIGGER t_bi BEFORE INSERT ON t
# 			FOR EACH ROW SIGNAL SQLSTATE '77777';
#
# 		CREATE EVENT e ON SCHEDULE EVERY 1 SECOND
# 			DO SIGNAL SQLSTATE '77777';
#
# SIGNAL executes according to the following rules:
#
# 	If the SIGNAL statement indicates a particular SQLSTATE value, that value is used to
# 	signal the condition specified.
#
# Example:
#
# 		CREATE PROCEDURE p (divisor INT)
# 		BEGIN
# 			IF divisor = 0 THEN
# 				SIGNAL SQLSTATE '22012';
# 			END IF;
# 		END;
#
# If the SIGNAL statement uses a named condition, the condition must be declared in some
# scope that applies to the SIGNAL statement, and must be defined using an SQLSTATE value,
# not a MySQL error number.
#
# Example:
#
# 		CREATE PROCEDURE p (divisor INT)
# 		BEGIN
# 			DECLARE divide_by_zero CONDITION FOR SQLSTATE '22012';
#			IF divisor = 0 THEN
# 				SIGNAL divide_by_zero;
# 			END IF;
# 		END;
#
# If the named condition does not exist in the scope of the SIGNAL statement, an 
# Undefined CONDITION error occurs.
#
# If SIGNAL refers to a named condition that is defined with a MySQL error number rather
# than an SQLSTATE value, a:
#
# 	 SIGNAL/RESIGNAL can only use a CONDITION defined with SQLSTATE error
#
# occurs.
#
# The following statements cause that error because the named condition is associated
# with a MySQL error number:
#
# 		DECLARE no_such_table CONDITION FOR 1051;
# 		SIGNAL no_such_table;
#
# If a condition with a given name is declared multiple times in different scopes,
# the declaration with the most local scope applies.
#
# Consider the following procedure:
#
# 		CREATE PROCEDURE p (divisor INT)
# 		BEGIN
# 			DECLARE my_error CONDITION FOR SQLSTATE '45000';
# 			IF divisor = 0 THEN
# 				BEGIN
# 					DECLARE my_error CONDITION FOR SQLSTATE '22012';
# 					SIGNAL my_error;
# 				END;
# 			END IF;
# 			SIGNAL my_error;
# 		END;
#
# If divisor is 0, the first SIGNAL statement executes.
#
# The innermost my_error condition declaration applies, raising SQLSTATE '22012'
#
# If divisor is not 0, the second SIGNAL statement executes. The outermost my_error
# condition declaration applies, raising SQLSTATE '45000'
#
# For information about how the server chooses handlers when a condition occurs,
# see SECTION 13.6.7.6, "SCOPE RULES FOR HANDLERS"
#
# Signals can be raised within exception handlers:
#
# 		CREATE PROCEDURE p ()
# 		BEGIN
# 			DECLARE EXIT HANDLER FOR SQLEXCEPTION
# 			BEGIN
# 				SIGNAL SQLSTATE VALUE '99999'
# 					SET MESSAGE_TEXT = 'An error occurred';
# 			END;
# 			DROP TABLE no_such_table;
# 		END;
#
# CALL p() reaches the DROP_TABLE statement.
#
# There is no table named no_such_table, so the error handler is activated.
#
# The error handler destroys the original error ("No such table") and makes a
# new error with SQLSTATE '99999' and message An error occurred.
#
# SIGNAL CONDITION INFORMATION ITEMS
#
# The following table lists the names of diagnostics area condition information items
# that can be set in a SIGNAL (or RESIGNAL) statement.
#
# All items are standard SQL except MYSQL_ERRNO, which is a MySQL extension.
#
# For more information about these items see SECTION 13.6.7.7, "THE MYSQL DIAGNOSTICS AREA"
#
# 		Item Name 					Definition
# 		---------- 					----------
# 		CLASS_ORIGIN 				VARCHAR(64)
# 		SUBCLASS_ORIGIN 			VARCHAR(64)
#
# 		CONSTRAINT_CATALOG 		VARCHAR(64)
# 		CONSTRAINT_SCHEMA 		VARCHAR(64)
#
# 		CONSTRAINT_NAME 			VARCHAR(64)
# 		CATALOG_NAME 				VARCHAR(64)
#
# 		SCHEMA_NAME 				VARCHAR(64)
# 		TABLE_NAME 					VARCHAR(64)
#
# 		COLUMN_NAME 				VARCHAR(64)
# 		CURSOR_NAME 				VARCHAR(64)
#
# 		MESSAGE_TEXT 				VARCHAR(128)
# 		MYSQL_ERRNO 				SMALLINT UNSIGNED
#
# The character set of character items is UTF-8
#
# It is illegal to assign NULL to a condition information item in a SIGNAL statement.
#
# A SIGNAL statement always specifies an SQLSTATE value, either directly, or indirectly
# by referring to a named condition defined with an SQLSTATE value.
#
# The first two characters of an SQLSTATE value are its class, and the class
# determines the default value for the condition information items:
#
# 		) Class = '00' (success)
#
# 			Illegal. SQLSTATE values that begin with '00' indicate success and are not valid for SIGNAL
#
# 		) Class = '01' (warning)
#
# 			MESSAGE_TEXT = 'Unhandled user-defined warning condition';
# 			MYSQL_ERRNO = ER_SIGNAL_WARN
#
# 		) Class = '02' (not found)
#
# 			MESSAGE_TEXT = 'Unhandled user-defined not found condition';
# 			MYSQL_ERRNO = ER_SIGNAL_NOT_FOUND
#
# 		) Class > '02' (exception)
#
# 			MESSAGE_TEXT = 'Unhandled user-defined exception condition';
# 			MYSQL_ERRNO = ER_SIGNAL_EXCEPTION
#
# For legal classes, the other condition information items are set as follows:
#
# 		CLASS_ORIGIN = SUBCLASS_ORIGIN = '';
# 		CONSTRAINT_CATALOG = CONSTRAINT_SCHEMA = CONSTRAINT_NAME = '';
# 		CATALOG_NAME = SCHEMA_NAME = TABLE_NAME = COLUMN_NAME = '';
# 		CURSOR_NAME = '';
#
# The error values that are accessible after SIGNAL executes are the SQLSTATE value
# raised by the SIGNAL statement and the MESSAGE_TEXT and MySQL_ERRNO items.
#
# These values are available from the C API:
#
# 		) mysql_sqlstate() returns the SQLSTATE value
#
# 		) mysql_errno() returns the MYSQL_ERRNO value
#
# 		) mysql_error() returns the MESSAGE_TEXT value
#
# At the SQL level, the output from SHOW_WARNINGS and SHOW_ERRORS indicates the MYSQL_ERRNO
# and MESSAGE_TEXT values in the Code and Message columns.
#
# To retrieve information from the diagnostics area, use the GET_DIAGNOSTICS statement
# (see SECTION 13.6.7.3, "GET DIAGNOSTICS SYNTAX")
#
# For information about the diagnostics area, see SECTION 13.6.7.7, "THE MYSQL DIAGNOSTICS AREA"
#
# EFFECT OF SIGNALS ON HANDLERS, CURSORS, AND STATEMENTS
#
# Signals have different effects on statement execution depending on the signal class.
#
# The class determines how severe an error is. MySQL ignores the value of the sql_mode
# system variable; in particular, strict SQL mode does not matter.
#
# MySQL also ignores IGNORE: The intent of SIGNAL is to raise a user-generated error
# explicitly, so a signal is never ignored.
#
# In the following descriptions, "unhandled" means that no handler for the signaled
# SQLSTATE value has been defined with DECLARE_---_HANDLER
#
# 		) Class = '00' (success)
#
# 			Illegal. SQLSTATE values that begin with '00' indicates success and are not valid for SIGNAL
#
# 		) Class = '01' (warning)
#
# 			The value of the warning_count system variable goes up.
#
# 			SHOW_WARNINGS shows the signal. SQLWARNING handlers catch the signal.
#
# 			Warnings cannot be returned from stored functions because the RETURN statement that causes
# 			the function to return clears the diagnostic area.
#
# 			The statement thus clears any warnings that may have been present there 
# 			(and resets warning_count to 0)
#
# 		) Class = '02' (not found)
#
# 			NOT FOUND handlers catch the signal.
#
# 			There is no effect on cursors. If the signal is unhandled in a stored function,
# 			statements end.
#
# 		) Class > '02' (exception)
#
# 			SQLEXCEPTION handlers catch the signal.
#
# 			if the signal is unhandled in a stored function, statements end.
#
# 		) Class = '40'
#
# 			Treated as an ordinary exception
#
# 13.6.7.6 SCOPE RULES FOR HANDLERS
#
# A stored program may include handlers to be invoked when certain conditions occur within
# the program.
#
# The applicability of each handler depends on its location within the program definition
# and on the condition or conditions that it handles:
#
# 		) A handler declared in a BEGIN_---_END block is in scope only for the SQL statements
# 			following the handler declarations in the block.
#
# 			If the handler itself raises a condition, it cannot handle that condition, nor 
# 			can any other handlers declared in the block.
#
# 			In the following example, handlers H1 and H2 are in scope for conditions
# 			raised by statements stmt1 and stmt2.
#
# 			But neither H1 nor H2 are in scope for conditions raised in the body of H1 or H2.
#
# 				BEGIN -- outer block
# 					DECLARE EXIT HANDLER FOR ---; --- handler H1
# 					DECLARE EXIT HANDLER FOR ---; --- handler H2
# 					stmt1;
# 					stmt2;
# 				END;
#
# 		) A handler is in scope only for the block in which it is declared, and cannot be
# 			activated for conditions occurring outside that block.
#
# 			In the following example, handler H1 is in scope for stmt1 in the inner block,
# 			but not for stmt2 in the outer block:
#
# 				BEGIN -- outer block
# 					BEGIN -- inner block
# 						DECLARE EXIT HANDLER FOR ---; --- handler H1
# 						stmt1;
# 					END;
# 					stmt2;
# 				END;
#
# 		) A handler can be specific or general.
#
# 			A specific handler is for a MySQL error code, SQLSTATE value, or condition name.
#
# 			A general handler is for a condition in the SQLWARNING, SQLEXCEPTION, or NOT FOUND
# 			class.
#
# 			Condition specificity is related to condition precedence, as described later.
#
# Multiple handlers can be declared in different scopes and with different specifities.
#
# For example, there might be a specific MySQL error code handler in an outer block,
# and a general SQLWARNING handler in an inner block.
#
# Or there might be handlers for a specific MySQL error code and the general SQLWARNING
# class in the same block.
#
# Whether a handler is activated depends not only on its own scope and condition value,
# but on what other handlers are present.
#
# When a condition occurs in a stored program, the server searches for applicable handlers
# in the current scope (current BEGIN_---_END block)
#
# If there are no applicable handlers, the search continues outward with the handlers
# in each successive containing scope (block).
#
# When the server finds one or more applicable handlers at a given scope, it chooses
# among them based on condition precdence:
#
# 		) A MySQL error code handler takes precedence over an SQLSTATE value handler
#
# 		) An SQLSTATE value handler takes precedence over general SQLWARNING, SQLEXCEPTION,
# 			or NOT FOUND handlers.
#
# 		) An SQLEXCEPTION handler takes precedence over an SQLWARNING handler
#
# 		) It is possible to have several applicable handlers with the same precedence.
#
# 			For example, a statement could generate multiple warnings with different#
# 			error codes, for each of which an error-specific handler exists.
#
# 			In this case, the choice of which handler the server activates is
# 			nondeterministic, and may change depending on the circumstances under which
# 			the condition occurs.
#
# One implication of the handler selection rules is that if multiple applicable handlers
# occur in different scopes, handlers with the most local scope take precedence
# over handlers in outer scopes, even over those for more specific conditions.
#
# If there is no appropriate handler when a condition occurs, the action taken depends
# on the class of the condition:
#
# 		) For SQLEXCEPTION conditions, the stored program terminates at the statement that
# 			raised the condition, as if there were an EXIT handler.
#
# 			If the program was called by another stored program, the calling program handles
# 			the condition using the handler selection rules applied to its own handlers.
#
# 		) For SQLWARNING conditions, the program continues executing, as if there were a CONTINUE handler
#
# 		) For NOT FOUND conditions, if the condition was raised normally, the action is CONTINUE.
#
# 			If it was raised by SIGNAL or RESIGNAL, the action is EXIT.
#
# The following examples demonstrate how MySQL applies the handler selection rules.
#
# This procedure contains two handlers, one for the specific SQLSTATE value ('42S02')
# that occurs for attempts to drop a nonexistent table, and one for the general
# SQLEXCEPTION class:
#
# 		CREATE PROCEDURE p1()
# 		BEGIN
# 			DECLARE CONTINUE HANDLER FOR SQLSTATE '42S02'
# 				SELECT 'SQLSTATE handler was activated' AS msg;
# 			DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
# 				SELECT 'SQLEXCEPTION handler was activated' AS msg;
#
# 			DROP TABLE test.t;
# 		END;
#
# Both handlers are declared in the same block and have the same scope.
#
# However, SQLSTATE handlers take precedence over SQLEXCEPTION handlers,
# so if the table t is nonexistent, the DROP_TABLE statement raises a condition
# that activates the SQLSTATE handler:
#
# 		CALL p1();
# 		+--------------------------------------+
# 		| msg 										   |
# 		+--------------------------------------+
# 		| SQLSTATE handler was activated 		|
# 		+--------------------------------------+
#
# This procedure contains the same two handlers. But this time, the DROP_TABLE statement
# and SQLEXCEPTION handler are in an inner block relative to the SQLSTATE handler:
#
# 		CREATE PROCEDURE p2()
# 		BEGIN -- outer block
# 				DECLARE CONTINUE HANDLER FOR SQLSTATE '42S02'
# 					SELECT 'SQLSTATE handler was activated' AS msg;
# 			BEGIN -- inner block
# 				DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
# 					SELECT 'SQLEXCEPTION handler was activated' AS msg;
#
# 				DROP TABLE test.t; -- occurs within inner block
# 			END;
# 		END;
#
# In this case, the handler that is more local to where the condition occurs
# takes precedence.
#
# The SQLEXCEPTION handler activates, even though it is more general than the
# SQLSTATE handler:
#
# 		CALL p2();
# 		+-------------------------------------------------+
# 		| msg 														  |
# 		+-------------------------------------------------+
# 		| SQLEXCEPTION handler was activated 				  |
# 		+-------------------------------------------------+
#
# In this procedure, one of the handlers is declared in a block inner to the scope of the
# DROP_TABLE statement:
#
# 		CREATE PROCEDURE p3()
# 		BEGIN -- outer block
# 			DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
# 				SELECT 'SQLEXCEPTION handler was activated' AS msg;
# 			BEGIN -- inner block
# 				DECLARE CONTINUE HANDLER FOR SQLSTATE '42S02'
# 					SELECT 'SQLSTATE handler was activated' AS msg;
# 			END;
#
# 			DROP TABLE test.t; -- occurs within outer block
# 		END;
#
# Only the SQLEXCEPTION handler applies because the other one is not in scope
# for the condition raised by the DROP_TABLE:
#
# 		CALL p3();
# 		+------------------------------------------+
# 		| msg 												 |
# 		+------------------------------------------+
# 		| SQLEXCEPTION handler was activated 		 |
# 		+------------------------------------------+
#
# In this procedure, both handlers are declared in a block inner to the scope of the
# DROP_TABLE statement:
#
# 		CREATE PROCEDURE p4()
# 		BEGIN -- Outer block
# 			BEGIN -- Inner block
# 				DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
# 					SELECT 'SQLEXCEPTION handler was activated' AS msg;
# 				DECLARE CONTINUE HANDLER FOR SQLSTATE '42S02'
# 					SELECT 'SQLSTATE handler was activated' AS msg;
# 			END;
#
# 			DROP TABLE test.t -- Occurs within outer block
# 		END;
#
# Neither handler applies because they are not in scope for the DROP_TABLE 
#
# The condition raised by the statement goes unhandled and terminates the procedure
# with an error:
#
# 		CALL p4();
# 		ERROR 1051 (42S02): UNKNOWN TABLE 'test.t'
#
# 13.6.7.7 THE MYSQL DIAGNOSTICS AREA
#
# SQL statements produce diagnostic information that populates the diagnostics area.
#
# Standard SQL has a diagnostics area stack, containing a diagnostics area for each
# nested execution context.
#
# Standard SQL also supports GET_STACKED_DIAGNOSTICS syntax for referring to the
# second diagnostics area during condition handler execution.
#
# The following discussion describes the structure of the diagnostics area in MySQL,
# the information items recognized by MySQL, how statements clear and set the
# diagnostics area, and how diagnostics areas are pushed to and popped from the stack.
#
# 		) DIAGNOSTICS AREA STRUCTURE
#
# 		) DIAGNOSTICS AREA INFORMATION ITEMS
#
# 		) HOW THE DIAGNOSTICS AREA IS CLEARED AND POPULATED
#
# 		) HOW THE DIAGNOSTICS AREA STACK WORKS
#
# 		) DIAGNOSTICS AREA-RELATED SYSTEM VARIABLES
#
# DIAGNOSTICS AREA STRUCTURE
#
# The diagnostics area contains two kinds of information:
#
# 		) Statement information, such as the number of conditions that occurred or the affected-rows
# 			count.
#
# 		) Condition information, such as the error code and message.
#
# 			If a statement raises multiple conditions, this part of the diagnostics area has
# 			a condition area for each one.
#
# 			If a statement raises no conditions, this part of the diagnostics area is empty.
#
# For a statement that produces three conditions, the diagnostics area contains statement
# and condition information like this:
#
# 		Statement information:
# 			row count
# 			--- other statement information items ---
# 		Condition area list:
# 			Condition area 1:
# 				error code for condition 1
# 				error message for condition 1
# 				--- other condition information items ---
# 			Condition area 2:
# 				error code for condition 2:
# 				error message for condition 2
# 				--- other condition information items ---
# 			Condition area 3:
# 				error code for condition 3
# 				error message for condition 3
# 				--- other condition information items ---
#
# DIAGNOSTICS AREA INFORMATION ITEMS
#
# The diagnostics area contains statement and condition information items.
#
# Numeric items are integers.
#
# The character set for character items is UTF-8. No item can be NULL.
#
# If a statement or condition item is not set by a statement that populates
# the diagnostics area, its value is 0 or the empty string, depending on the
# item data type.
#
# The statement information part of the diagnostics area contains these items:
#
# 		) NUMBER: An integer indicating the number of condition areas that have information
#
# 		) ROW_COUNT: An integer indicating the number of rows affected by the statement 
#
# 			ROW_COUNT has the same value as the ROW_COUNT() function (see SECTION 12.15,
# 			"INFORMATION FUNCTIONS")
#
# The condition information part of the diagnostics area contains a condition area for
# each condition.
#
# Condition areas are numbered from 1 to the value of the NUMBER statement condition
# item
#
# If NUMBER is 0, there are no condition areas
#
# Each condition area contains the items in the following list.
#
# All items are standard SQL except MySQL_ERRNO, which is a MySQL extension.
#
# The definitions apply for conditions generated other than by a signal (that is,
# by a SIGNAL or RESIGNAL statement)
#
# For nonsignal conditions, MySQL populates only those condition items not
# described as always empty.
#
# The effects of signals on the condition area are described later.
#
# 		) CLASS_ORIGIN: A string containing the class of the RETURNED_SQLSTATE value.
#
# 			If the RETURNED_SQLSTATE value begins with a class value defined in SQL
# 			standards document ISO 9075-2 (SECTION 24.1, SQLSTATE), CLASS_ORIGIN is 'ISO 9075'
#
# 			Otherwise, CLASS_ORIGIN is 'MySQL'
#
# 		) SUBCLASS_ORIGIN: A string containing the subclass of the RETURNED_SQLSTATE value.
#
# 			If CLASS_ORIGIN is 'ISO 9075' or RETURNED_SQLSTATE ends with '000'
#
# 			SUBCLASS_ORIGIN is 'ISO 9075'. Otherwise, SUBCLASS_ORIGIN is 'MySQL'
#
# 		) RETURNED_SQLSTATE: A string that indicates the SQLSTATE value for the condition
#
# 		) MESSAGE_TEXT: A string that indicates the error message for the condition
#
# 		) MYSQL_ERRNO: An integer that indicates the MySQL error code for the condition
#
# 		) CONSTRAINT_CATALOG, CONSTRAINT_SCHEMA, CONSTRAINT_NAME: Strings that indicate the catalog,
# 			schema, and name for a violated constraint.
#
# 			They are always empty.
#
# 		) CATALOG_NAME, SCHEMA_NAME, TABLE_NAME, COLUMN_NAME: Strings that indicate the catalog, schema, table,
# 			and column related to the condition.
#
# 			They are always empty.
#
# 		) CURSOR_NAME: A string that indicates the cursor name. This is always empty
#
# For the RETURNED_SQLSTATE, MESSAGE_TEXT and MYSQL_ERRNO values for particular errors,
# see SECTION B.3, "SERVER ERROR MESSAGE REFERENCE"
#
# If a SIGNAL (or RESIGNAL) statement populates the diagnostics area, its SET clause can
# assign to any condition information item except RETURNED_SQLSTATE any value that is
# legal for the item data type.
#
# SIGNAL also sets the RETURNED_SQLSTATE value, but not directly in its SET clause.
#
# That value comes from the SIGNAL statement SQLSTATE argument.
#
# SIGNAL also sets statement information items. It sets NUMBER to 1.
#
# It sets ROW_COUNT to -1 for errors and 0 otherwise.
#
# HOW THE DIAGNOSTICS AREA IS CLEARED AND POPULATED
#
# Nondiagnostic SQL statements populate the diagnostics area automatically,
# and its contents can be set explicitly with the SIGNAL and RESIGNAL statements.
#
# The diagnostics area can be examined with GET_DIAGNOSTICS to extract specific
# items, or with SHOW_WARNINGS or SHOW_ERRORS to see conditions of errors.
#
# SQL statements clear and set the diagnostics area as follows:
#
# 		) When the server starts executing a statement after parsing it, it clears the diagnostics
# 			area for nondiagnostic statements.
#
# 			Diagnostic statements do not clear the diagnostics area.
#
# 			These statements are diagnostic:
#
# 				) GET_DIAGNOSTICS
#
# 				) SHOW_ERRORS
#
# 				) SHOW_WARNINGS
#
# 		) If a statement raises a condition, the diagnostics area is cleared of conditions
# 			that belong to earlier statements.
#
# 			The exception is that conditions raised by GET DIAGNOSTICS and RESIGNAL
# 			are added to the diagnostics area without clearing it.
#
# Thus, even a statement that does not normally clear the diagnostics area when it begins
# executing clears it if the statement raises a condition.
#
# The following example shows the effect of various statements on the diagnostics area,
# using SHOW_WARNINGS to display information about conditions stored there.
#
# This DROP_TABLE statement clears the diagnostics area and populates it when the condition occurs:
#
# 		DROP TABLE IF EXISTS test.no_such_table;
# 		Query OK, 0 rows affected, 1 warning (0.01 sec)
#
# 		SHOW WARNINGS;
# 		+--------+-----------+----------------------------------------+
# 		| Level  | Code 		| Message 										  |
# 		+--------+-----------+----------------------------------------+
# 		| note   | 1051 		| Unknown table 'test.no_such_table' 	  |
# 		+--------+-----------+----------------------------------------+
# 		1 row in set (0.00 sec)
#
# This SET statement generates an error, so it clears and populates the
# diagnostics area:
#
# 		SET @x = @@x;
# 		ERROR 1193 (HY000): Unknown system variable 'x'
#
# 		SHOW WARNINGS;
# 		+--------+---------+-------------------------------------------+
# 		| Level  | Code 	 | Message 												|
# 		+--------+---------+-------------------------------------------+
# 		| Error  | 1193 	 | Unknown system variable 'x' 					|
# 		+--------+---------+-------------------------------------------+
# 		1 row in set (0.00 sec)
#
# The previous SET statement produced a single condition, so 1 is the only valid
# condition number for GET_DIAGNOSTICS at this point.
#
# The following statement uses a condition number of 2, which produces a warning
# that is added to the diagnostics area without clearing it:
#
# 		GET DIAGNOSTICS CONDITION 2 @p = MESSAGE_TEXT;
# 		Query OK, 0 rows affected, 1 warning (0.00 sec)
#
# 		SHOW WARNINGS;
# 		+-------+-----------+----------------------------------------+
# 		| Level | Code 	  | Message 										 |
# 		+-------+-----------+----------------------------------------+
# 		| Error | 1193 	  | Unknown system variable 'xx' 			 |
# 		| Error | 1753 	  | Invalid condition number 					 |
# 		+-------+-----------+----------------------------------------+
# 		2 rows in set (0.00 sec)
#
# Now there are two conditions in the diagnostics area, so the same 
# GET_DIAGNOSTICS statement succeeds:
#
# 		GET DIAGNOSTICS CONDITION 2 @p = MESSAGE_TEXT;
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		SELECT @p;
# 		+-------------------------------------+
# 		| @p 											  |
# 		+-------------------------------------+
# 		| Invalid condition number 			  |
# 		+-------------------------------------+
# 		1 row in set (0.01 sec)
#
# HOW THE DIAGNOSTICS AREA STACK WORKS
#
# When a push to the diagnostics area stack occurs, the first (current) diagnostics area
# becomes the second (stacked) diagnostics area and a new current diagnostics area
# is created as a copy of it.
#
# Diagnostics areas are pushed to and popped from the stack under the following circumstances:
#
# 		) Execution of a stored program
#
# 			A push occurs before the program executes and a pop occurs afterward.
#
# 			If the stored program ends while handlers are executing, there can be more than
# 			one diagnostics area to pop; this occurs due to an exception for which there
# 			are no appropriate handlers or due to RETURN in the handler.
#
# 			Any warning or error conditions in the popped diagnostics areas then are added to the
# 			current diagnostics area, except that, for triggers, only errors are added.
#
# 			When the stored program ends, the caller sees these conditions in its current
# 			diagnostics area.
#
# 		) Execution of a condition handler within a stored program
#
# 			When a push occurs as a result of condition handler activation, the stacked
# 			diagnostics area is the area that was current within the stored program
# 			prior to the push.
#
# 			The new now-current diagnostics area is the handler's current diagnostics area.
#
# 			GET_[CURRENT]_DIAGNOSTICS and GET_STACKED_DIAGNOSTICS can be used within the
# 			handler to access the contents of the current (handler) and stacked (stored program)
# 			diagnostics areas.
#
# 			Initially, they return the same result, but statements executing within the
# 			handler modify the current diagnostics area, clearing and setting its contents
# 			according to the normal rules (see HOW THE DIAGNOSTICS AREA IS CLEARED AND POPULATED)
#
# 			The stacked diagnostics area cannot be modified by statements executing within the
# 			handler except RESIGNAL.
#
# 			If the handler executes successfully, the current (handler) diagnostics area is popped
# 			and the stacked (stored program) diagnostics area again becomes the current diagnostics area.
#
# 			Conditions added to the handler diagnostics area during handler execution are added to the
# 			current diagnostics area.
#
# 		) Execution of RESIGNAL
#
# 			The RESIGNAL statement passes on the error condition information that is available during
# 			execution of a condition handler within a compound statement inside a stored program.
#
# 			RESIGNAL may change some or all information before passing it on, modifying the diagnostics
# 			stack as described in SECTION 13.6.7.4, "RESIGNAL SYNTAX"
#
# DIAGNOSTICS AREA-RELATED SYSTEM VARIABLES
#
# Certain system variables control or are related to some aspects of the diagnostics area:
#
# 		) max_error_count controls the number of condition areas in the diagnostics area.
#
# 			If more conditions than this occur, MySQL silently discards information for the excess
# 			conditions.
#
# 			(Conditions added by RESIGNAL are always added, with older conditions being discarded
# 			as necessary to make room)
#
# 		) warning_count indicates the number of conditions that occurred.
#
# 			This includes errors, warnings, and notes.
#
# 			Normally, NUMBER and warning_count are teh same.
#
# 			However, as the number of conditions generated exceeds max_error_count,
# 			the value of warning_count continues to rise whereas NUMBER remains capped
# 			at max_error_count because no additional conditions are stored in the
# 			diagnostics area.
#
# 		) error_count indicates the number of errors that occurred.
#
# 			This value includes "not found" and exception conditions, but excludes
# 			warnings and notes.
#
# 			Like warning_count, its value can exceed max_error_count
#
# 		) If the sql_notes system variable is set to 0, notes are not stored and do
# 			not increment warning_count
#
# Example:
#
# 		If max_error_count is 10, the diagnostics area can contain a maximum of 10 condition areas.
#
# 		Suppose that a statement raises 20 conditions, 12 of which are errors.
#
# 		In that case, the diagnostics area contains the first 10 conditions, NUMBER
# 		is 10, warning_count is 20 and error_count is 12.
#
# Changes to the value of max_error_count have no effect until the next attempt to
# modify the diagnostics area.
#
# If the diagnostics area contains 10 condition areas and max_error_count is set
# to 5, that has no immediate effect on the size or content of the diagnostics area.
#
# 13.6.7.8 CONDITION HANDLING AND OUT OR INOUT PARAMETERS
#
# If a stored procedure exits with an unhandled exception, modified values of OUT and
# INOUT parameters are not propogated back to the caller.
#
# If an exception is handled by a CONTINUE or EXIT handler that contains a RESIGNAL
# statement, execution of RESIGNAL pops the Diagnostics Area stack, thus signalling the
# exception (that is, the information that existed before entry into the handler)
#
# If the exception is an error, the values of OUT and INOUT parameters are not propogated
# back to the caller.
#
# 13.7 DATABASE ADMINISTRATION STATEMENTS
#
# 13.7.1 ACCOUNT MANAGEMENT STATEMENTS
# 13.7.2 RESOURCE GROUP MANAGEMENT STATEMENTS
#
# 13.7.3 TABLE MAINTENANCE STATEMENTS
# 13.7.4 COMPONENT, PLUGIN, AND USER-DEFINED FUNCTION STATEMENTS
#
# 13.7.5 SET SYNTAX
# 13.7.6 SHOW SYNTAX
#
# 13.7.7 OTHER ADMINISTRATIVE STATEMENTS
#
# 13.7.1 ACCOUNT MANAGEMENT STATEMENTS
#
# 13.7.1.1 ALTER USER SYNTAX
# 13.7.1.2 CREATE ROLE SYNTAX
#
# 13.7.1.3 CREATE USER SYNTAX
# 13.7.1.4 DROP ROLE SYNTAX
#
# 13.7.1.5 DROP USER SYNTAX
# 13.7.1.6 GRANT SYNTAX
#
# 13.7.1.7 RENAME USER SYNTAX
# 13.7.1.8 REVOKE SYNTAX
#
# 13.7.1.9 SET DEFAULT ROLE SYNTAX
# 13.7.1.10 SET PASSWORD SYNTAX
#
# 13.7.1.11 SET ROLE SYNTAX
#
# MySQL account information is stored in teh tables of the mysql system database.
#
# This database and the access control system are discussed extensively in
# CHAPTER 5, MYSQL SERVER ADMINISTRATION, which you should consult for additional details.
#
# IMPORTANT:
#
# 		Some MySQL releases introduce changes to the grant tables to add new privileges
# 		or features.
#
# 		To make sure that you can take advantage of any new capabilities, update your
# 		grant tables to the current structure whenever you upgrade MySQL.
#
# 		See SECTION 4.4.5, "MYSQL_UPGRADE -- CHECK AND UPGRADE MYSQL TABLES"
#
# When the read_only system variable is enabled, account-management statements
# require the CONNECTION_ADMIN or SUPER privilege, in addition to any other
# required privileges.
#
# This is because they modify tables in the mysql system database
#
# Account management statements are atomic and crash safe.
#
# For more information, see SECTION 13.1.1, "ATOMIC DATA DEFINITION STATEMENT SUPPORT"
#
# 13.7.1.1 ALTER USER SYNTAX
#
# 	ALTER USER [IF EXISTS]
# 		user [auth_option] [, user [auth_option]] ---
# 		[REQUIRE {NONE | tls_option [[AND] tls_option] ---}]
# 		[WITH resource_option [resource_option] ---]
# 		[password_option | lock_option] ---
#
# 	ALTER USER [IF EXISTS] USER() user_func_auth_option
#
# 	ALTER USER [IF EXISTS]
# 		user DEFAULT ROLE
# 		{NONE | ALL | role [, role ] ---}
#
# 	user:
# 		(see Section 6.2.4, "Specifying Account Names")
#
# 	auth_option: {
# 		IDENTIFIED BY 'auth_string'
# 			[REPLACE 'current_auth_string']
# 			[RETAIN CURRENT_PASSWORD]
# 	 | IDENTIFIED WITH auth_plugin
# 	 | IDENTIFIED WITH auth_plugin BY 'auth_string'
# 			[REPLACE 'current_auth_string']
# 			[RETAIN CURRENT PASSWORD]
# 	 | IDENTIFIED WITH auth_plugin AS 'hash_string'
# 	 | DISCARD OLD PASSWORD
# }
#
# user_func_auth_option: {
# 		IDENTIFIED BY 'auth_string'
# 			[REPLACE 'current_auth_string']
# 			[RETAIN CURRENT PASSWORD]
# 	 | DISCARD OLD PASSWORD
# }
#
# tls_option: {
# 	 SSL
# | X509
# | CIPHER 'cipher'
# | ISSUER 'issuer'
# | SUBJECT 'subject'
# }
#
# resource_option: {
# 		MAX_QUERIES_PER_HOUR count
#   | MAX_UPDATES_PER_HOUR count
#   | MAX_CONNECTIONS_PER_HOUR count
# 	 | MAX_USER_CONNECTIONS count
# }
#
# password_option: {
# 		PASSWORD EXPIRE [DEFAULT | NEVER | INTERVAL N DAY]
# 	 | PASSWORD HISTORY {DEFAULT | N}
#   | PASSWORD REUSE INTERVAL {DEFAULT | N DAY}
#   | PASSWORD REQUIRE CURRENT [DEFAULT | OPTIONAL]
# }
#
# lock_option: {
# 		ACCOUNT LOCK
#   | ACCOUNT UNLOCK
# }
#
# The ALTER_USER statement modifies MySQL accounts.
#
# It enables authentication, role, SSL/TLS, resource-limit and password-management
# properties to be modified for existing accounts.
#
# It can also be used to lock and unlock accounts.
#
# In most cases, ALTER_USER requires the global CREATE_USER privilege, or the UPDATE
# privilege for the mysql system database.
#
# The exceptions are:
#
# 		) Any client who connects to the server using a nonanonymous account can change the password
# 			for that account.
#
# 			(In particular, you can change your own password)
#
# 			To see which account the server authenticated you as, invoke the CURRENT_USER() function:
#
# 				SELECT CURRENT_USER();
#
# 		) For DEFAULT ROLE syntax, ALTER_USER requires these privileges:
#
# 			) Setting the default roles for another user requires the global CREATE_USER
# 				privilege, or the UPDATE privilege for the mysql.default_roles system table.
#
# 			) Setting the default roles for yourself requires no special privileges, as long 
# 				as the roles you want as the default have been granted to you.
#
# 		) Statements that modify secondary passwords require these privileges:
#
# 			) The APPLICATION_PASSWORD_ADMIN privilege is required to use the RETAIN CURRENT PASSWORD
# 				or DISCARD OLD PASSWORD clause for ALTER_USER statements that apply to your own account.
#
# 				The privilege is required to manipulate your own secondary password because most users
# 				require only one password.
#
# 			) If an account is to be permitted to manipulate secondary passwords for all accounts,
# 				it should be granted the CREATE_USER privilege rather than APPLICATION_PASSWORD_ADMIN
#
# When the read_only system variable is enabled, ALTER_USER additionally requires the CONNECTION_ADMIN
# or SUPER privilege.
#
# By default, an error occurs if you try to modify a user that does not exist.
#
# If the IF EXISTS clause is given, the statement produces a warning for each named user that
# does not exist, rather than an error.
#
# 	IMPORTANT:
#
# 		Under some circumstances, ALTER_USER may be recorded in server logs or on the client
# 		side in a history file such as ~/.mysql_history, which means that cleartext passwords
# 		may be read by anyone having read access to that information.
#
# 		For information about the conditions under which this occurs for the server logs
# 		and how to control it, see SECTION 6.1.2.3, "PASSWORDS AND LOGGING"
#
# 		For similar information about client-side logging, see SECTION 4.5.1.3, "MYSQL CLIENT LOGGING"
#
# There are several aspects to the ALTER_USER statement, described under the following topics:
#
# 		) ALTER USER OVERVIEW
#
# 		) ALTER USER AUTHENTICATION OPTIONS
#
# 		) ALTER USER ROLE OPTIONS
#
# 		) ALTER USER SSL/TLS OPTIONS
#
# 		) ALTER USER RESOURCE-LIMIT OPTIONS
#
# 		) ALTER USER PASSWORD-MANAGEMENT OPTIONS
#
# 		) ALTER USER ACCOUNT-LOCKING OPTIONS
#
# 		) ALTER USER BINARY LOGGING
#
# ALTER USER OVERVIEW
#
# For each affected account, ALTER_USER modifies the corresponding row in the mysql.user
# system table to reflect the properties specified in the statement.
#
# Unspecified properties retain their current values.
#
# Each account name uses the format described in SECTION 6.2.4, "SPECIFYING ACCOUNT NAMES"
#
# The host name part of the account name, if omitted, defaults to '%'
#
# It is also possible to specify CURRENT_USER or CURRENT_USER() to refer to the account
# associated with the current session.
#
# For one syntax only, the account may be specified with the USER() function:
#
# 		ALTER USER USER() IDENTIFIED BY 'auth_string';
#
# This syntax enables changing your own password without naming your account literally.
#
# (The syntax also supports the REPLICATE RETAIN CURRENT PASSWORD, and DISCARD OLD PASSWORD
# clauses described at ALTER USER AUTHENTICATION OPTIONS)
#
# For ALTER_USER syntaxes that permit an auth_option value to follow a user value,
# auth_option indicates how the account authenticates by specifying an account authentication
# plugin, credentials (for example, a password), or both.
#
# Each auth_option value applies only to the account named immediately preceding it.
#
# Following the user specifications, the statement may include options for SSL/TLS,
# resource-limit, password-management, and locking properties.
#
# All such options are global to the statement and apply to all accounts named
# in the statement.
#
# Example:
#
# 		Change an account's password and expire it. As a result, the user must connect with
# 		the named password and choose a new one at the next connection:
#
# 			ALTER USER 'jeffrey'@'localhost'
# 				IDENTIFIED BY 'new_password' PASSWORD EXPIRE;
#
# Example:
#
# 		Modify an account to use the sha256_password authentication plugin and the given
# 		password.
#
# 		Require that a new password be chosen every 180 days:
#
# 			ALTER USER 'jeffrey'@'localhost'
# 				IDENTIFIED WITH sha256_password BY 'new_password'
# 				PASSWORD EXPIRE INTERVAL 180 DAY;
#
# Example:
#
# 		Lock or unlock an account
#
# 			ALTER USER 'jeffrey'@'localhost' ACCOUNT LOCK;
# 			ALTER USER 'jeffrey'@'localhost' ACCOUNT UNLOCK;
#
# Example:
#
# 		Require an account to connect using SSL and establish a limit of 20 connections per hour:
#
# 			ALTER USER 'jeffrey'@'localhost'
# 				REQUIRE SSL WITH MAX_CONNECTIONS_PER_HOUR 20;
#
# Example:
#
# 		Alter multiple accounts, specifying some per-account properties and some global properties:
#
# 			ALTER USER
# 				'jeffrey'@'localhost'
# 					IDENTIFIED BY 'jeffrey_new_password',
# 				'jeanne'@'localhost',
# 				'josh'@'localhost'
# 					IDENTIFIED BY 'josh_new_password'
# 					REPLACE 'josh_current_password'
# 					RETAIN CURRENT PASSWORD
# 				REQUIRE SSL WITH MAX_USER_CONNECTIONS 2
# 				PASSWORD HISTORY 5;
#
# The IDENTIFIED BY value following jeffrey applies only to its immediately preceeding
# account, so it changes the password to 'jeffrey_new_password' only for jeffrey.
#
# For jeanne, there is no per-account value (thus leaving the password unchanged)
#
# For josh, IDENTIFIED BY establishes a new password ('josh_new_password'), REPLACE
# is specified to verify that hte user issuing the ALTER_USER statement knows the
# current password ('josh_current_password'), and that current password is also retained
# as the account secondary password.
#
# (As a result, josh can connect with either the primary or secondary password)
#
# The remaining properties apply globally to all accounts named in the statement,
# so for both accounts:
#
# 		) Connections are required to use SSL
#
# 		) The account can be used for a maximum of two simultaneous connections
#
# 		) Password changes cannot reuse any of the five most recent PWs
#
# Example:
#
# 		Discard the secondary password for josh, leaving the account with only
# 		its primary password:
#
# 			ALTER USER 'josh'@'localhost' DISCARD OLD PASSWORD;
#
# In the absence of a particular type of option, the account remains unchanged
# in that respect.
#
# FOr example, with no locking option, the locking state of the account is not
# changed.
#
# ALTER USER AUTHENTICATION OPTIONS
#
# An account name may be followed by an auth_option authentication option that specifies
# the account authentication plugin, credentials or both.
#
# It may also include a password-verification clause that specifies the account current password
# to be replaced, and clauses that manage whether an account has a seondary password.
#
# NOTE:
#
# 		Clauses for password verification and secondary passwords apply only to accounts that
# 		store credentials internally in the mysql.user system table
#
# 		(mysql_native_password, sha256_password, or caching_sha2_password)
#
# 		For accounts that use plugins that perform authentication against an
# 		external credential system, password management must be handled externally
# 		against the system as well.
#
# 	) auth_plugin names an authentication plugin.
#
# 		The plugin name can be a quoted string literal or an unquoted name.
#
# 		Plugin names are stored in the plugin column of the mysql.user system system table.
#
# 		For auth_option syntaxes that do not specify an authentication plugin, the default
# 		plugin is indicated by the value of the default_authentication_plugin system
# 		variable.
#
# 		For descriptions of each plugin, see SECTION 6.5.1, "AUTHENTICATION PLUGINS"
#
# 	) Credentials are stored in the mysql.user system table.
#
# 		An 'auth_string' or 'hash_string' value specifies account credentials, either
# 		as a cleartext (unencrypted) string or hashed in the format except by the
# 		authentication plugin associated with the account, respectively:
#
# 			) For syntaxes that use 'auth_string', the string is cleartext and is passed
# 				to the authentication plugin for possible hashing.
#
# 				The result returned by the plugin is stored in the mysql.user table
#
# 				A plugin may use the value as specified, in which case no hashing occurs.
#
# 			) For syntaxes that use 'hash_string', the string is assumed to be already
# 				hashed in the format required by the authentication plugin.
#
# 				If the hash format is inappropriate for the plugin, it will not be usable
# 				and correct authentication of client connections will not occur.
#
# 	) The REPLACE 'current_auth_string' clause is available as of MySQL 8.0.13
#
# 		If given:
#
# 			) REPLACE specifies the account current password to be replaced, as a cleartext (unencrypted) string
#
# 			) The clause must be given if password changes for the are required to specify the current password,
# 				as verification that the user attempting to make the change actually knows the current PW.
#
# 			) THe clause is optional if password changes for the account may but need not specify the current password
#
# 			) The statement fails if the clause is given but does not match the current password, even if the clause is optional
#
# 			) REPLACE can be specified only when changing the account password for the current user
#
# 		For more information about password verification by specifying the current password, see SECTION 6.3.8, "PASSWORD MANAGEMENT"
#
# 	) The RETAIN CURRENT PASSWORD and DISCARD OLD PASSWORD clauses implement dual-password capability
# 		and are available as of MySQL 8.0.14
#
# 		Both are optional, but if given, have the following effects:
#
# 			) RETAIN CURRENT PASSWORD retains an account current password as its secondary password,
# 				replacing any existing secondary password.
#
# 				The new password becomes the primary password, but clients can use the account to connect
# 				to the server using either the primary or secondary password.
#
# 				(Exception: if the new password specified by the ALTER_USER statement is empty, the secondary
# 					password becomes empty as well, even if RETAIN CURRENT PASSWORD is given)
#
# 			) If you specify RETAIN CURRENT PASSWORD for an account that has an empty primary password, the statement fails
#
# 			) If an account has a secondary password and you change its primary password without specifying
# 				RETAIN CURRENT PASSWORD, the secondary password remains unchanged.
#
# 			) If you change the authentication plugin assigned to the account, the secondary password is discarded.
#
# 				If you change the authentication plugin and also specify RETAIN CURRENT PASSWORD, the statement fails.
#
# 			) DISCARD OLD PASSWORD discards the secondary password, if one exists.
#
# 				The account retains only its primary password, and clients can use the account
# 				to connect to the server only with the primary password.
#
# 		FOr more information about use of dual passwords, see SECTION 6.3.8, "PASSWORD MANAGEMENT"
#
# ALTER_USER permits these auth_option syntaxes:
#
# 		) IDENTIFIED BY 'auth_string' [REPLACE 'current_auth_string'] [RETAIN CURRENT PASSWORD]
#
# 			Sets the account authentication plugin to the default plugin, passes the cleartext 
# 			'auth_string' value to the plugin for hashing, and stores the result in the account
# 			row in the mysql.user system table.
#
# 			The REPLACE clause, if given, specifies the account current password, as described
# 			previously in this section.
#
# 			The RETAIN CURRENT PASSWORD clause, if given, causes the account current password 
# 			to be retained as its secondary password, as described previously in this section.
#
# 		) IDENTIFIED WITH auth_plugin
#
# 			Sets the account authentication plugin to auth_plugin, clears the credentials
# 			to the empty string (the credentials are associated with the old authentication
# 			plugin, not the new one), and stores the result in the account row in the mysql.user
# 			system table
#
# 			In addition, the password is marked expired.
#
# 			the user must choose a new one when next connecting
#
# 		) IDENTIFIED WITH auth_plugin BY 'auth_string' [REPLACE 'current_auth_string'] [RETAIN CURRENT PASSWORD]
#
# 			Sets the account authentication plugin to auth_plugin, passes the cleartext 'auth_string'
# 			value to the plugin for hashing, and stores the result in the account row
# 			in the mysql.user system table
#
# 			The REPLACE clause, if given, specifies the account current password, as described
# 			previously in this section
#
# 			The RETAIN CURRENT PASSWORD clause, if given, causes the account current password
# 			to be retained as its secondary password, as described previously in this section.
#
# 		) IDENTIFIED WITH auth_plugin AS 'hash_string'
#
# 			Sets the account authentication plugin to auth_plugin and stores the hashed
# 			'hash_string' value as in the mysql.user account row.
#
# 			The string is assumed to be already hashed in the format required by the plugin
#
# 		) DISCARD OLD PASSWORD
#
# 			Discards the account secondary password, if there is one, as described previously
#
# Example:
#
# 		Specify the password as cleartext; the default plugin is used:
#
# 			ALTER USER 'jeffrey'@'localhost'
# 				IDENTIFIED BY 'password';
#
# Example:
#
# 		Specify the authentication plugin, along with a cleartext password value:
#
# 			ALTER USER 'jeffrey'@'localhost'
# 				IDENTIFIED WITH mysql_native_password
# 								BY 'password';
#
# Example:
#
# 		Like the preceding example, but in addition, specify the current password as a cleartext
# 		value to satisfy any account requirement that the user making the change knows that password:
#
# 			ALTER USER 'jeffrey'@'localhost'
# 				IDENTIFIED WITH mysql_native_password
# 								BY 'password'
# 								REPLACE 'current_password';
#
# 		The preceding statement fails unless the current user is jeffrey because REPLACE is permitted
# 		only for changes to the current user's password.
#
# Example:
#
# 		Establish a new primary password and retain the existing password as the secondary password:
#
# 			ALTER USER 'jeffrey'@'localhost'
# 				IDENTIFIED BY 'new_password'
# 				RETAIN CURRENT PASSWORD;
#
# Example:
#
# 		Discard the secondary password, leaving the account with only its primary password:
#
# 			ALTER USER 'jeffrey'@'localhost' DISCARD OLD PASSWORD;
#
# Example:
#
# 		Specify the authentication plugin, along with a hashed password value:
#
# 			ALTER USER 'jeffrey'@'localhost'
# 				IDENTIFIED WITH mysql_native_password
# 					AS '<value>';
#
# For additional information about setting passwords and authentication plugins,
# see SECTION 6.3.7, "ASSIGNING ACCOUNT PASSWORDS" and SECTION 6.3.10, "PLUGGABLE AUTHENTICATION"
#
# ALTER USER ROLE OPTIONS
#
# ALTER_USER_---_DEFAULT_ROLE defines which roles become active when the user connects to the server
# and authenticates, or when the user executes the SET_ROLE_DEFAULT statement during a session.
#
# ALTER_USER_---_DEFAULT_ROLE is alternative syntax for SET_DEFAULT_ROLE (see SECTION 13.7.1.9, "SET DEFAULT ROLE SYNTAX")
#
# However, ALTER_USER can set the default for only a single user, whereas SET_DEFAULT_ROLE can set the
# default for multiple users.
#
# On the other hand, you can specify CURRENT_USER as the user name for the
# ALTER_USER statement, whereas you cannot for SET_DEFAULT_ROLE
#
# Each user account name uses the format described previously.
#
# Each role name uses the format described in SECTION 6.2.5, "SPECIFYING ROLE NAMES"
#
# For example:
#
# 		ALTER USER 'joe'@'10.0.0.1' DEFAULT ROLE administrator, developer;
#
# The host name part of the role name, if omitted, defaults to '%'
#
# The clause following the DEFAULT ROLE keywords permites these values:
#
# 		) NONE: Set the default to NONE (no roles)
#
# 		) ALL: Set the default to all roles granted to the account
#
# 		) role [, role] ---. Set the default to the named roles, which must exist
# 			and be granted to the account at the time ALTER_USER_---_DEFAULT_ROLE
# 			is executed.
#
# ALTER USER SSL/TLS OPTIONS
#
# MySQL can check X.509 cert attributes in addition to the usual authentication
# that is based on the user name and credentials.
#
# For background information on the use of SSL/TLS with MySQL, see SECTION 6.4, "USING ENCRYPTED CONNECTIONS"
#
# To specify SSL/TLS-related options for a MySQL account, use a REQUIRE clause that specifies
# one or more tls_option values.
#
# Order of REQUIRE options does not matter, but no option can be specified twice.
#
# The AND keyword is optional between REQUIRE options.
#
# ALTER_USER permits these tls_option values:
#
# 		) NONE:
#
# 			Indicates that all accounts named by the statement have no SSL or X.509 requirements.
#
# 			Unencrypted connections are permitted if the user name and password are valid.
#
# 			Encrypted connections can be used, at the client's option, if the client has the proper
# 			certificate and key files.
#
# 				ALTER USER 'jeffrey'@'localhost' REQUIRE NONE;
#
# 			Clients attempt to establish a secure connection by default.
#
# 			For clients that have REQUIRE NONE, the connection attempt falls back to
# 			an unencrypted connection if a secure connection cannot be established.
#
# 			To require an encrypted connection, a client need specify only the --ssl-mode=REQUIRED
# 			option; the connection attempt fails if a secure connection cannot be established.
#
# 		) SSL:
#
# 			Tells the server to permit only encrypted connections for all accounts named by the
# 			statement.
#
# 				ALTER USER 'jeffrey'@'localhost' REQUIRE SSL;
#
# 			Clients attempt to establish a secure connection by default. For accounts that have
# 			REQUIRE SSL, the connection attempt fails if a secure connection cannot be
# 			established.
#
# 		) X509
#
# 			For all accounts named by the statement, requires that clients present a valid certificate,
# 			but the exact certificate, issuer and subject do not matter.
#
# 			The only requirement is that it should be possible to verify its signature with one of
# 			the CA certificates.
#
# 			Use of X.509 certificates always implies encryption, so the SSL option is unnecessary
# 			in this case.
#
# 				ALTER USER 'jeffrey'@'localhost' REQUIRE X509;
#
# 			For accounts with REQUIRE X509, clients must specify the --ssl-key and --ssl-cert
# 			options to connect.
#
# 			(It is recommended but not required that --ssl-ca also be specified so that the
# 			public certificate provided by the server can be verified)
#
# 			This is true for ISSUER and SUBJECT as well because those REQUIRE options imply the
# 			requirements of X509
#
# 		) ISSUER 'issuer'
#
# 			For all accounts named by the statement, requires that clients present a valid X.509
# 			certificate issued by CA 'issuer'.
#
# 			If a client presents a certificate that is valid but has a different issuer;
# 			the server rejects the connection.
#
# 			Use of X.509 certificates always implies encryption, so the SSL option is
# 			unnecessary in this case.
#
# 				ALTER USER 'jeffrey'@'localhost'
# 					REQUIRE ISSUER '/C=SE/ST=Stockholm/L=Stockholm/
# 						0=MySQL/CN=CA/emailAddress=ca@example.com';
#
# 			Because ISSUER implies the requirements of X509, clients must specify
# 			the --ssl-key and --ssl-cert options to connect.
#
# 			(It is recommended but not required that --ssl-ca also be specified
# 			so that the public certificate provided by the server can be verified)
#
# 		) SUBJECT 'subject'
#
# 			For all accounts named by the statement, requires that clients present a valid
# 			X.509 certificate containing the subject subject.
#
# 			If a client presents a certificate that is valid but has a different subject,
# 			the server rejects the connection.
#
# 			Use of X.509 certificates always implies encryption, so the SSL option is unnecessary
# 			in this case.
#
# 				ALTER USER 'jeffrey'@'localhost'
# 					REQUIRE SUBJECT '/C=SE/ST=Stockholm/L=Stockholm/
# 						0=MySQL demo client certificate/
# 						CN=client/emailAddress=client@example.com';
#
# 			MySQL does a simple string comparison of the 'subject' value to the value
# 			in the certificate, so lettercase and component ordering must be given
# 			exactly as present in the certificate.
#
# 			Because SUBJECT implies the requirements of X509, clients must specify the
# 			--ssl-key and --ssl-cert options to connect.
#
# 			(It is recommended but not required that --ssl-ca also be specified so that
# 			the public certificate provided by the server can be verified.)
#
# 		) CIPHER 'cipher'
#
# 			For all accounts named by the statement, requires a specific cipher method
# 			for encrypting connections.
#
# 			This option is needed to ensure that ciphers and key lengths of sufficient
# 			strength are used.
#
# 			Encryption can be weak if old algorithms using short encryption keys are used.
#
# 				ALTER USER 'jeffrey'@'localhost'
# 					REQUIRE CIPHER 'EDH-RSA-DES-CBC3-SHA';
#
# 			The SUBJECT, ISSUER and CIPHER options can be combined in the REQUIRE clause:
#
# 				ALTER USER 'jeffrey'@'localhost'
# 					REQUIRE SUBJECT '/C=SE/ST=Stockholm/L=Stockholm/
# 						0=MySQL demo client certificate/
# 						CN=client/emailAddress=client@example.com'
# 					AND ISSUER '/C=SE/ST=Stockholm/L=Stockholm/
# 						0=MySQL/CN=CA/emailAddress=ca@example.com'
# 					AND CIPHER 'EDH-RSA-DES-CBC3-SHA';
#
# ALTER USER RESOURCE-LIMIT OPTIONS
#
# It is possible to place limits on use of server resources by an account, as discussed
# in SECTION 6.3.6, "SETTING ACCOUNT RESOURCE LIMITS"
#
# To do so, use a WITH clause that specifies one or more resource_option values.
#
# Order of WITH options does not matter, except that if a given resource limit is
# specified multiple times, the last instance takes precedence.
#
# ALTER_USER permits these resource_option values:
#
# 		) MAX_QUERIES_PER_HOUR count, MAX_UPDATES_PER_HOUR count, MAX_CONNECTIONS_PER_HOUR count
#
# 			For all accounts named by the statement, these options restrict how many queries,
# 			updates and connections to the server are permitted to each account during any
# 			given one-hour period.
#
# 			If count is 0 (the default), this means that there is no limitation for the account.
#
# 		) MAX_USER_CONNECTIONS count
#
# 			For all accounts named by the statement, restricts the maximum number of simultaneous
# 			connections to the server by each account.
#
# 			A nonzero count specifies the limit for the account explicitly.
#
# 			If count is 0 (the default), the server determines the number of simultaneous
# 			connections for the account from the global value of the max_user_connections
# 			system variable.
#
# 			If max_user_connections is also zero, there is no limit for the account.
#
# 			Example:
#
# 				ALTER USER 'jeffrey'@'localhost'
# 					WITH MAX_QUERIES_PER_HOUR 500 MAX_UPDATES_PER_HOUR 100;
#
# ALTER USER PASSWORD-MANAGEMENT OPTIONS
#
# ALTER_USER supports several password_option values for password management:
#
# 		) Password expiration options: You can expire an account password manually and establish
# 			its password expiration policy.
#
# 			Policy options do not expire the password.
#
# 			Instead, they determine how the server applies automatic expiration to the
# 			account based on password age, which is assessed from the date and time of
# 			the most recent account password change.
#
# 		) Password reuse options: You can restrict password reuse based on number of password
# 			changes, time elapsed or both.
#
# 		) Password verification-required options: You can indicate whether attempts to change an
# 			account password must specify the current password, as verification that the
# 			user attempting to make the change actually knows the current password.
#
# This section describes the syntax for password-management options. For information about
# establishing policy for password management, see SECTION 6.3.8, "PASSWORD MANAGEMENT"
#
# If multiple password-management options of a given type (PASSWORD EXPIRE, PASSWORD HISTORY,
# PASSWORD REUSE INTERVAL, PASSWORD REQUIRE) are specified, the last one takes precedence.
#
# 		NOTE:
#
# 			Password-management options apply only to accounts that store credentials internally
# 			in the mysql.user system table (mysql_native_password, sha256_password,
# 			or caching_sha2_password)
#
# 			For accounts that use plugins that perform authentication against an external
# 			credential system, password management must be handled externally against that
# 			system as well.
#
# A client has an expired password if the account password was expired manually or the password
# age is considered greater than its permitted lifetime per the automatic expiration policy.
#
# In this case, the server either disconnects the client or restricts the operations permitted
# to it (see SECTION 6.3.9, "SERVER HANDLING OF EXPIRED PASSWORDS")
#
# Operations performed by a restricted client result in an error until the user establishes
# a new account password.
#
# NOTE:
#
# 		It is possible to "reset" a password by setting it to its current value.
#
# 		As a matter of good policy, it is preferable to choose a different password.
#
# 		DBAs can enforce non-reuse by establishing an appropriate password-reuse policy.
#
# 		See PASSWORD REUSE POLICY
#
# ALTER_USER permits these password_option values for controlling password expiration:
#
# 		) PASSWORD EXPIRE
#
# 			Immediately marks the password expired for all accounts named by the statement.
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD EXPIRE;
#
# 		) PASSWORD EXPIRE DEFAULT
#
# 			Sets all accounts named by the statement so that the global expiration
# 			policy applies, as specified by the default_password_lifetime system variable.
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD EXPIRE DEFAULT;
#
# 		) PASSWORD EXPIRE NEVER
#
# 			This expiration option overrides the global policy for all accounts named by the
# 			statement.
#
# 			For each, it disables password expiration so that the password never expires.
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD EXPIRE NEVER;
#
# 		) PASSWORD EXPIRE INTERVAL N DAY
#
# 			This expiration option overrides the global policy for all accounts named
# 			by the statement.
#
# 			For each, it sets the password lifetime to N days.
#
# 			The following statement requires the password to be changed every 180 days:
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD EXPIRE INTERVAL 180 DAY;
#
# ALTER_USER permits these password_option values for controlling reuse of previous passwords
# based on required minimum number of password changes:
#
# 		) PASSWORD HISTORY DEFAULT
#
# 			Sets all acounts named by the statement so that the global policy about password
# 			history length applies, to prohibit reuse of passwords before the number of
# 			changes specified by the password_history system variable.
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD HISTORY DEFAULT;
#
# 		) PASSWORD HISTORY N
#
# 			This history-length option overrides the global policy for all accounts
# 			named by the statement.
#
# 			For each, it sets the password history length to N passwords, to prohibit
# 			reusing any of the N most recently chosen passwords.
#
# 			The following statement prohibits reuse of any of the previous 6 passwords:
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD HISTORY 6;
#
# ALTER_USER permits these password_option values for controlling reuse of previous
# passwords based on time elapsed:
#
# 		) PASSWORD REUSE INTERVAL DEFAULT
#
# 			Sets all statements named by the account so that the global policy about time
# 			elapsed applies, to prohibit reuse of passwords newer than the number of days
# 			specified by the password_reuse_interval system variable.
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD REUSE INTERVAL DEFAULT;
#
# 		) PASSWORD REUSE INTERVAL N DAY
#
# 			This time-elapsed option overrides the global policy for all accounts named
# 			by the statement.
#
# 			For each, it sets the password reuse interval to N days, to prohibit reuse
# 			of passwords newer than that many days.
#
# 			The following statement prohibits password reuse for 360 days:
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD REUSE INTERVAL 360 DAY;
#
# ALTER_USER permits these password_option values for controlling whether attempts to
# change an account password must specify the current password, as verification
# that the user attempting to make the change actually knows the current password:
#
# 		) PASSWORD REQUIRE CURRENT
#
# 			This verification option overrides the global policy for all accounts named
# 			by the statement.
#
# 			For each, it requires that password changes specify the current password.
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD REQUIRE CURRENT;
#
# 		) PASSWORD REQUIRE CURRENT OPTIONAL
#
# 			This verification option overrides the global policy for all accounts named by
# 			the statement.
#
# 			For each, it does not require that password changes specify the current
# 			password.
#
# 			(The current password may but need not be given)
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD REQUIRE CURRENT OPTIONAL;
#
# 		) PASSWORD REQUIRE CURRENT DEFAULT
#
# 			Sets all statements named by the account so that the global policy
# 			about password verification applies, as specified by the password_require_current
# 			system variable.
#
# 				ALTER USER 'jeffrey'@'localhost' PASSWORD REQUIRE CURRENT DEFAULT;
#
# ALTER USER ACCOUNT-LOCKING OPTIONS
#
# MySQL supports account locking and unlocking using the ACCOUNT LOCK and ACCOUNT UNLOCK
# options, which specify the locking state for an account.
#
# For additional discussion, see SECTION 6.3.12, "USER ACCOUNT LOCKING"
#
# If multiple account-locking options are specified, the last one takes precedence.
#
# ALTER USER BINARY LOGGING
#
# ALTER_USER is written to the binary log if it succeeds, but not if it fails; in that
# case, rollback occurs and no changes are made.
#
# A statement written to the binary log includes all named users.
#
# If the IF EXISTS clause is given, this includes even users that do not exist
# and were not altered.
#
# If the original statement changes the credentials for a user, the statement
# written to the binary log specifies the applicable authentication plugin for
# that user, determined as follows:
#
# 		) The plugin named in the original statement, if one was specified.
#
# 		) Otherwise, the plugin associated with the user account if the user exists,
# 			or the default authentication plugin if the user does not exist.
#
# 			(If the statement written to the binary log must specify a particular
# 			authentication plugin for a user, include it in the original statement)
#
# If the server adds the default authentication plugin for any users in the statement
# written to the binary log, it writes a warning to the error log naming those users.
#
# 13.7.1.2 CREATE ROLE SYNTAX
#
# 		CREATE ROLE [IF NOT EXISTS] role [, role] ---
#
# CREATE_ROLE creates one or more roles, which are named collections of privileges.
#
# To use this statement, you must have the global CREATE_ROLE or CREATE_USER privilege.
#
# When the read_only system variable is enabled, CREATE_ROLE additionally requires the
# CONNECTION_ADMIN or SUPER privilege.
#
# A role when created is locked, has no passwords, and is assigned the default authentication plugin.
#
# CREATE_ROLE either succeeds for all named roles or rolls back and has no effect if any error
# occurs.
#
# By default, an error occurs if you try to create a role that already exists.
#
# If the IF NOT EXISTS clause is given, the statement produces a warning for each named
# role that already exists, rather than an error.
#
# The statement is written to the binary log if it succeeds, but not if it fails; in that case,
# rollback occurs and no changes are made.
#
# A statement written to the binary log includes all named roles.
#
# If the IF NOT EXISTS clause is given, this includes even roles that already
# exist and were not created.
#
# Each role name uses the format described in SECTION 6.2.5, "SPECIFYING ROLE NAMES"
#
# For example:
#
# 		CREATE ROLE 'administrator', 'developer';
# 		CREATE ROLE 'webapp'@'localhost';
#
# The host name part of the role name, if omitted, defaults to '%'
#
# For role usage examples, see SECTION 6.3.4, "USING ROLES"
#
# 13.7.1.3 CREATE USER SYNTAX
#
# 		CREATE USER [IF NOT EXISTS]
# 			user [auth_option] [, user [auth_option]] ---
# 			DEFAULT ROLE role [, role ] ---
# 			[REQUIRE {NONE | tls_option [[AND] tls_option] ---}]
# 			[WITH resource_option [resource_option] ---]
# 			[password_option | lock_option] ---
#
# 		user:
# 			(see Section 6.2.4, "SPECIFYING ACCOUNT NAMES")
#
# 		auth_option: {
# 			IDENTIFIED BY 'auth_string'
# 		 | IDENTIFIED WITH auth_plugin
# 		 | IDENTIFIED WITH auth_plugin BY 'auth_string'
# 		 | IDENTIFIED WITH auth_plugin AS 'hash_string'
# 		}
#
# 		tls_option: {
# 			SSL
# 		 | X509
# 		 | CIPHER 'cipher'
# 		 | ISSUER 'issuer'
# 		 | SUBJECT 'subject'
# 		}
#
# 		resource_option: {
# 			MAX_QUERIES_PER_HOUR count
# 		 | MAX_UPDATES_PER_HOUR count
# 		 | MAX_CONNECTIONS_PER_HOUR count
# 		 | MAX_USER_CONNECTIONS count
# 		}
#
# 		password_option: {
# 			PASSWORD EXPIRE [DEFAULT | NEVER | INTERVAL N DAY]
# 		 | PASSWORD HISTORY {DEFAULT | N}
# 		 | PASSWORD REUSE INTERVAL {DEFAULT | N DAY}
# 		 | PASSWORD REQUIRE CURRENT [DEFAULT | OPTIONAL]
# 		}
#
# 		lock_option: {
# 			ACCOUNT LOCK
# 		 | ACCOUNT UNLOCK
# 		}
#
# The CREATE_USER statement creates new MySQL accounts.
#
# It enables authentication, role, SSL/TLS, resource-limit, and password-management
# properties to be established for new accounts.
#
# It also controls whether accounts are initially locked or unlocked.
#
# To use CREATE_USER, you must have the global CREATE_USER privilege, or the
# INSERT privilege for the mysql system database.
#
# When the read_only system variable is enabled, CREATE_USER additionally requires
# the CONNECTION_ADMIN or SUPER privilege.
#
# CREATE_USER either succeeds for all named users or rolls back and has no effect
# if any error occurs.
#
# By default, an error occurs if you try to create a user that already exists.
#
# If the IF NOT EXISTS clause is given, the statement produces a warning for each
# named user that already exists, rather than an error.
#
# iMPORTANT:
#
# 		Under some circumstances,  CREATE_USER may be recorded in server logs or on the
# 		client side in a history file such as ~/.mysql_history, which means that cleartext
# 		passwords may be read by anyone having read access to that information.
#
# 		For information about the conditions under which this occurs for the server
# 		logs and how to control it, see SECTION 6.1.2.3, "PASSWORDS AND LOGGING"
#
# 		For similar information about client-side logging, see SECTION 4.5.1.3,
# 		"MYSQL CLIENT LOGGING"
#
# There are several aspects to the CREATE_USER statement, described under the following
# topics:
#
# 		) CREATE USER OVERVIEW
#
# 		) CREATE USER AUTHENTICATION OPTIONS
#
# 		) CREATE USER ROLE OPTIONS
#
# 		) CREATE USER SSL/TLS OPTIONS
#
# 		) CREATE USER RESOURCE-LIMIT OPTIONS
#
# 		) CREATE USER PASSWORD-MANAGEMENT OPTIONS
#
# 		) CREATE USER ACCOUNT-LOCKING OPTIONS
#
# 		) CREATE USER BINARY LOGGING
#
# CREATE USER OVERVIEW
#
# For each account, CREATE_USER creates a new row in the mysql.user system table.
#
# The account row reflects the properties specified in the statement.
#
# Unspecified properties are set to their default values:
#
# 		) Authentication: The authentication plugin defined by the default_authentication_plugin
# 			system variable, and empty credentials.
#
# 		) Default role: NONE
#
# 		) SSL/TLS: NONE
#
# 		) Resource limits: Unlimited
#
# 		) Password management: PASSWORD EXPIRE DEFAULT PASSWORD HISTORY DEFAULT PASSWORD REUSE INTERVAL DEFAULT 
# 			PASSWORD REQUIRE CURRENT DEFAULT
#
# 		) Account locking: ACCOUNT UNLOCK
#
# An account when first created has no privileges and a default role of NONE.
#
# To assign privileges or roles, use the GRANT statement.
#
# Each account name uses the format described in SECTION 6.2.4, "SPECIFYING ACCOUNT NAMES"
#
# For example:
#
# 		CREATE USER 'jeffrey'@'localhost' IDENTIFIED BY 'password';
#
# The host name part of the account name, if omitted, defaults to '%'
#
# Each user value naming an account may be followed by an optional auth_option value
# that indicates how the account authenticates.
#
# These values enable account authentication plugins and credentials (for example, a password)
# to be specified.
#
# Each auth_option value applies only to the account named immediately preceding it.
#
# Following the user specifications, the statement may include options for SSL/TLS, resource-limit,
# password-management, and locking properties.
#
# All such options are global to the statement and apply to all accounts named in the statement.
#
# Example:
#
# 		Create an account that uses the default authentication plugin and the given password.
#
# 		Mark the password expired so that the user must choose a new one at the first
# 		connection to the server:
#
# 			CREATE USER 'jeffrey'@'localhost'
# 				IDENTIFIED BY 'new_password' PASSWORD EXPIRE;
#
# Example:
#
# 		Create an account that uses the sha256_password authentication plugin and
# 		the given password.
#
# 		Require that a new password be chosen every 180 days:
#
# 			CREATE USER 'jeffrey'@'localhost'
# 				IDENTIFIED WITH sha256_password BY 'new_password'
# 				PASSWORD EXPIRE INTERVAL 180 DAY;
#
# Example:
#
# 		Create multiple accounts, specifying some per-account properties
# 		and some global properties:
#
# 			CREATE USER
# 				'jeffrey'@'localhost' IDENTIFIED WITH mysql_native_password
# 															BY 'new_password1',
# 				'jeanne'@'localhost' IDENTIFIED WITH sha256_password
# 															BY 'new_password2'
# 				REQUIRE X509 WITH MAX_QUERIES_PER_HOUR 60
# 				PASSWORD HISTORY 5
# 				ACCOUNT LOCK;
#
# Each auth_option value (IDENTIFIED WITH --- BY in this case) applies only
# to the account named immediately preceding it, so each account uses
# the immediately following authentication plugin and password.
#
# The remaining properties apply globally to all accounts named in the statement,
# so for both accounts:
#
# 		) Connections must be made using a valid X.509 certificate
#
# 		) Up to 60 queries per hour are permitted.
#
# 		) Password changes cannot reuse any of the five most recent passwords
#
# 		) The account is locked initially, so effectively it is a placeholder and
# 			cannot be used until an administrator unlocks it.
#
# CREATE USER AUTHENTICATION OPTIONS
#
# An account name may be followed by an auth_option authentication option that
# specifies the account authentication plugin, credentials, or both:
#
# 		) auth_plugin names an authentication plugin. The plugin name can be a quoted
# 			string literal or an unquoted name.
#
# 			Plugin names are stored in the plugin column of the mysql.user system table
#
# 			For auth_option syntaxes that do not specify an authentication plugin,
# 			the default plugin is indicated by the value of the default_authentication_plugin
# 			system variable.
#
# 			For descriptions of each plugin, see SECTION 6.5.1, "AUTHENTICATION PLUGINS"
#
# 		) Credentials are stored in the mysql.user system table.
#
# 			An 'auth_string' or 'hash_string' value specifies account credentials,
# 			either as a cleartext (unencrypted) string or hashed in the format
# 			expected by the authentication plugin associated with the account, respectively:
#
# 				) For syntaxes that use 'auth_string', the string is cleartext and is passed
# 					to the authentication plugin for possible hashing.
#
# 					The result returned by the plugin is stored in the mysql.user
# 					table.
#
# 					plugin may use the value as specified, in which case no hashing occurs.
#
# 				) For syntaxes that use 'hash_string', the string is assumed to be already
# 					hashed in the format required by the authentication plugin.
#
# 					If the hash format is inappropriate for the plugin, it will not be usable
# 					and correct authentication of client connections will not occur.
#
# CREATE_USER permits these auth_option syntaxes:
#
# 		) IDENTIFIED BY 'auth_string'
#
# 			Sets the account authentication plugin to the default plugin, passes the cleartext
# 			'auth_string' value to the plugin for hashing, and stores the result in the account
# 			row in the mysql.user system table
#
# 		) IDENTIFIED WITH auth_plugin
#
# 			Sets the account authentication plugin to auth_plugin, clears the credentials
# 			to the empty string, and stores the result in the account row in the mysql.user
# 			system table.
#
# 		) IDENTIFIED WITH auth_plugin BY 'auth_string'
#
# 			Sets the account authentication plugin to auth_plugin, passes the cleartext
# 			'auth_string' value to the plugin for hashing, and stores the result in the
# 			account row in the mysql.user system table
#
# 		) IDENTIFIED WITH auth_plugin AS 'hash_string'
#
# 			Sets the account authentication plugin to auth_plugin and stores the hashed
# 			'hash_string' value as is in the mysql.user account row
#
# 			The string is assumed to be already hashed in the format required by the plugin
#
# Example:
#
# 		Specify the password as cleartext; the default plugin is used:
#
# 			CREATE USER 'jeffrey'@'localhost'
# 				IDENTIFIED BY 'password';
#
# Example:
#
# 		Specify the authentication plugin, along with a cleartext password value:
#
# 			CREATE USER 'jeffrey'@'localhost'
# 				IDENTIFIED WITH mysql_native_password BY 'password';
#
# In each case, the password value stored in the account row is the cleartext value
# 'password' after it has been hashed by the authentication plugin associated with
# the account.
#
# For additional information about setting passwords and authentication plugins,
# see SECTION 6.3.7, "ASSIGNING ACCOUNT PASSWORDS", and SECTION 6.3.10, "PLUGGABLE AUTHENTICATION"
#
# CREATE USER ROLE OPTIONS
#
# The DEFAULT ROLE clause defines which roles become active when the user connects to the server
# and authenticates, or when the user executes the SET_ROLE_DEFAULT statement during a session.
#
# Each role name uses the format described in SECTION 6.2.5, "SPECIFYING ROLE NAMES"
#
# For example:
#
# 		CREATE USER 'joe'@'10.0.0.1' DEFAULT ROLE administrator, developer;
#
# The host name part of the role name, if omitted, defaults to '%'
#
# The DEFAULT ROLE clause permits a list of one or more comma-separated role names.
#
# These roles need not exist at the time CREATE_USER is executed
#
# CREATE USER SSL/TLS OPTIONS
#
# MySQL can check X.509 certificate attributes in addition to the usual authentication
# that is based on the user name and credentials.
#
# For background information on the use of SSL/TLS with MySQL, see SECTION 6.4, "USING ENCRYPTED CONNECTIONS"
#
# To specify SSL/TLS-related options for a MySQL account, use a REQUIRE clause that specifies
# one or more tls_option values.
#
# Order of REQUIRE options does not matter, but no option can be specified twice.
#
# The AND keyword is optional between REQUIRE options.
#
# CREATE_USER permits these tls_option values:
#
# 		) NONE
#
# 			Indicates that all accounts named by the statement have no SSL or X.509
# 			requirements.
#
# 			Unencrypted connections are permitted if the user name and password are
# 			valid.
#
# 			Encrypted connections can be used, at the client's option, if the client
# 			has the proper certificate and key files.
#
# 				CREATE USER 'jeffrey'@'localhost' REQUIRE NONE;
#
# 			Clients attempt to establish a secure connection by default.
#
# 			For clients that have REQUIRE NONE, the connection attempt falls
# 			back to an unencrypted connection if a secure connection cannot be
# 			established.
#
# 			To require an encrypted connection, a client need specify only the
# 			--ssl-mode=REQUIRED option; the connection attempt fails if a secure
# 			connection cannot be established.
#
# 			NONE is the default if no SSL-related REQUIRE options are specified
#
# 		) SSL
#
# 			Tells the server to permit only encrypted connections for all accounts
# 			named by the statement.
#
# 				CREATE USER 'jeffrey'@'localhost' REQUIRE SSL;
#
# 			Clients attempt to establish a secure connection by default.
#
# 			For accounts that have REQUIRE SSL, the connection attempt fails
# 			if a secure connection cannot be established.
#
# 		) X509
#
# 			For all accounts named by the statement, requires that clients present
# 			a valid certificate, but the exact certificate, issuer, and subject do
# 			not matter.
#
# 			The only requirement is that it be possible to verify its signature
# 			with one of the CA certificates.
#
# 			Use of X.509 certificates always implies encryption, so the SSL
# 			option is unnecessary in this case.
#
# 				CREATE USER 'jeffrey'@'localhost' REQUIRE X509;
#
# 			For accounts with REQUIRE X509, clients must specify the --ssl-key
# 			and --ssl-cert options to connect.
#
# 			(It is recommended but not required that --ssl-ca also be specified
# 			so that the public certificate provided by the server can be verified)
#
# 			This is true for ISSUER and SUBJECT as well because those REQUIRE
# 			options imply the requirements of X509.
#
# 		) ISSUER 'issuer'
#
# 			For all accounts named by the statement, requires that clients present a valid
# 			X.509 certificate issued by CA 'issuer'
#
# 			If a client presents a certificate that is valid but has a different
# 			issuer, the server rejects the connection.
#
# 			Use of X.509 certificates always implies encryption, so the SSL option
# 			is unnecessary in this case.
#
# 				CREATE USER 'jeffrey'@'localhost'
# 					REQUIRE ISSUER '/C=SE/ST=Stockholm/L=Stockholm/
# 						0=MySQL/CN=CA/emailAddress=ca@example.com';
#
# 			Because ISSUER implies the requirements of X509, clients must specify
# 			the --ssl-key and --ssl-cert options to connect.
#
# 			(It is recommended but not required that --ssl-ca also be specified
# 				so that the public certificate provided by teh server can be verified)
#
# 		) SUBJECT 'subject'
#
# 			For all accounts named by the statement, requires that clients present a valid
# 			X.509 certificate containing the subject subject.
#
# 			If a client presents a certificate that is valid but has a different subject,
# 			the server rejects teh connection.
#
# 			Use of X.509 certificates always implies encryption, so the SSL option is 
# 			unnecessary in this case.
#
# 				CREATE USER 'jeffrey'@'localhost'
# 					REQUIRE SUBJECT '/C=SE/ST=Stockholm/L=Stockholm/
# 						0=MySQL demo client certificate/
# 						CN=client/emailAddress=client@example.com';
#
# 			MySQL does a simple string comparison of the 'subject' value to the value
# 			in the certificate, so lettercase and component ordering must be given exactly
# 			as present in the certificate.
#
# 			Because SUBJECT implies the requirements of X509, clients must specify the
# 			--ssl-key and --ssl-cert options to connect.
#
# 			(It is recommended but not required that --ssl-ca also be specified so that hte public
# 			certificate provided by the server can be verified)
#
# 		) CIPHER 'cipher'
#
# 			For all accounts named by the statement, requires a specific cipher method for encrypting
# 			connections.
#
# 			This option is needed to ensure that ciphers and key lengths of sufficient
# 			strength are used.
#
# 			Encryption can be weak if old algorithms using short encryption keys are used.
#
# 				CREATE USER 'jeffrey'@'localhost'
# 					REQUIRE CIPHER 'EDH-RSA-DES-CBC3-SHA';
#
# The SUBJECT, ISSUER and CIPHER options can be combined in the REQUIRE clause:
#
# 		CREATE USER 'jeffrey'@'localhost'
# 			REQUIRE SUBJECT '/C=SE/ST=Stockholm/L=Stockholm/
# 				0=MySQL demo client certificate/
# 				CN=client/emailAddress=client@example.com'
# 			AND ISSUER '/C=SE/ST=Stockholm/L=Stockholm/
# 				0=MySQL/CN=CA/emailAddress=ca@example.com'
# 			AND CIPHER 'EDH-RSA-DES-CBC3-SHA';
#
# CREATE USER RESOURCE-LIMIT OPTIONS
#
# It is possible to place limits on use of server resources by an account, as discussed
# in SECTION 6.3.6, "SETTING ACCOUNT RESOURCE LIMITS"
#
# To do so, use a WITH clause that specifies one or more resource_option values.
#
# Order of WITH options does not matter, except that if a given resource limit is specified
# multiple times, the last instance takes precedence.
#
# CREATE_USER permits these resource_option values:
#
# 		) MAX_QUERIES_PER_HOUR count, MAX_UPDATES_PER_HOUR count, MAX_CONNECTIONS_PER_HOUR count
#
# 			For all accounts named by the statement, these options restrict how many queries, updates
# 			and connections to the server are permitted to each account during any given one-hour
# 			period.
#
# 			If count is 0 (the default), this means that there is no limitation for the account
#
# 		) MAX_USER_CONNECTIONS count
#
# 			For all accounts named by the statement, restricts the maximum number of simultaneous
# 			connections to the server by each account.
#
# 			A nonzero count specifies the limit for the account explicitly.
#
# 			If count is 0 (the default), the server determines the number of simultaneous
# 			connections for the account from the global value of the max_user_connections
# 			system variable.
#
# 			If max_user_connections is also zero, there is no limit for the account
#
# Example:
#
# 		CREATE USER 'jeffrey'@'localhost'
# 			WITH MAX_QUERIES_PER_HOUR 500 MAX_UPDATES_PER_HOUR 100;
#
# CREATE USER PASSWORD-MANAGEMENT OPTIONS
#
# CREATE_USER supports several password_option values for password management:
#
# 		) Password expiration options:
#
# 			You can expire an account password manually and establish its password
# 			expiration policy.
#
# 			Policy options do not expire the password.
#
# 			Instead, they determine how the server applies automatic expiration
# 			to the account based on password age, which is assessed from the date
# 			and time of the most recent account password change.
#
# 		) Password reuse options: You can restrict password reuse based on number
# 			of password changes, time elapsed, or both.
#
# 		) Password verification-required options: You can indicate whether attempts to change
# 			an account password must specify the current password, as verification that the
# 			user attempting to make the change actually knows the current password.
#
# This section describes the syntax for password-management options.
#
# For information about establishing policy for password management, see
# SECTION 6.3.8, "PASSWORD MANAGEMENT"
#
# If multiple password-management options of a given type (PASSWORD EXPIRE,
# PASSWORD HISTORY, PASSWORD REUSE INTERVAL, PASSWORD REQUIRE) are specified,
# the last one takes precedence.
#
# NOTE:
#
# 		Password-management options apply only to accounts that store credentials
# 		internally in the mysql.user system table (mysql_native_password,
# 		sha256_password, or caching_sha2_password)
#
# 		For accounts that use plugins that perform authentication against an
# 		external credential system, password management must be handled
# 		externally against that system as well.
#
# A client has an expired password if the account password was expired manually
# or the password age is considered greater than its permitted lifetime per
# the automatic expiration policy.
#
# In this case, the server either disconnects the client or restricts the operations
# permitted to it (see SECTION 6.3.9, "SERVER HANDLING OF EXPIRED PASSWORDS")
#
# Operations performed by a restricted client result in an error until the user
# establishes a new account password.
#
# CREATE_USER permits these password_option values for controlling password expiration:
#
# 		) PASSWORD EXPIRE
#
# 			Immediately marks the password expired for all accounts named by the statement
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD EXPIRE;
#
# 		) PASSWORD EXPIRE DEFAULT
#
# 			Sets all accounts named by the statement so that the global expiration policy
# 			applies, as specified by the default_password_lifetime system variable.
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD EXPIRE DEFAULT;
#
# 		) PASSWORD EXPIRE NEVER
#
# 			This expiration option overrides the global policy for all accounts named by the
# 			statement.
#
# 			For each, it disables password expiration so that the password never expires.
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD EXPIRE NEVER;
#
# 		) PASSWORD EXPIRE INTERVAL N DAY
#
# 			This expiration option overrides the global policy for all accounts named by the
# 			statement.
#
# 			For each, it sets the password lifetime to N days.
#
# 			The following statement requires the password to be changed every 180 days:
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD EXPIRE INTERVAL 180 DAY;
#
# CREATE_USER permits these password_option values for controlling reuse of previous passwords
# based on required minimum number of password changes:
#
# 		) PASSWORD HISTORY DEFAULT
#
# 			Sets all accounts named by the statement so that the global policy about password
# 			history length applies, to prohibit reuse of passwords before the number of
# 			changes specified by the password_history system variable.
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD HISTORY DEFAULT;
#
# 		) PASSWORD HISTORY N
#
# 			This history-length option overrides the global policy for all accounts named by
# 			the statement.
#
# 			For each, it sets the password history length to N passwords, to prohibit
# 			reusing any of the N most recently chosen passwords.
#
# 			The following statement prohibits reuse of any of the previous 6 passwords:
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD HISTORY 6;
#
# CREATE_USER permits these password_option values for controlling reuse of previous
# passwords based on time elapsed:
#
# 		) PASSWORD REUSE INTERVAL DEFAULT
#
# 			Sets all statements named by the account so that the global policy about time
# 			elapsed applies, to prohibit reuse of passwords newer than the number of days
# 			specified by the password_reuse_interval system variable.
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD REUSE INTERVAL DEFAULT;
#
# 		) PASSWORD REUSE INTERVAL N DAY
#
# 			This time-elapsed option overrides the global policy for all accounts
# 			named by the statement.
#
# 			For each, it sets the password reuse interval to N days, to prohibit
# 			reuse of passwords newer than that many days.
#
# 			The following statement prohibits password reuse for 360 days:
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD REUSE INTERVAL 360 DAY;
#
# CREATE_USER permits these password_option values for controlling whether attempts
# to change an account password must specify the current password, as verification
# that hte user attempting to make the change actually knows the current PW:
#
# 		) PASSWORD REQUIRE CURRENT
#
# 			This verification option overrides the global policy for all accounts named
# 			by the statement.
#
# 			For each, it requires that password changes specify the current password.
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD REQUIRE CURRENT;
#
# 		) PASSWORD REQUIRE CURRENT OPTIONAL
#
# 			This verification option overrides the global policy for all accounts
# 			named by the statement.
#
# 			For each, it does not require that password changes specify the current
# 			password.
#
# 			(The current password may but need not be given)
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD REQUIRE CURRENT OPTIONAL;
#
# 		) PASSWORD REQUIRE CURRENT DEFAULT
#
# 			Sets all statements named by the account so that the global policy about
# 			password verification applies, as specified by the password_require_current
# 			system variable.
#
# 				CREATE USER 'jeffrey'@'localhost' PASSWORD REQUIRE CURRENT DEFAULT;
#
# CREATE USER ACCOUNT-LOCKING OPTIONS
#
# MySQL supports account locking and unlocking using the ACCOUNT LOCK and ACCOUNT UNLOCK
# options, which specify the locking state for an account.
#
# For additional discussion, see SECTION 6.3.12, "USER ACCOUNT LOCKING"
#
# If multiple account-locking options are specified, the last one takes
# precedence.
#
# CREATE USER BINARY LOGGING
#
# CREATE_USER is written to the binary log if it succeeds, but not if it fails; in that case,
# rollback occurs and no changes are made.
#
# A statement written to the binary log includes all named users.
#
# If the IF NOT EXISTS clause is given, this includes even users that already exist
# and were not created.
#
# The statement written to the binary log specifies an authentication plugin for each
# user, determined as follows:
#
# 		) The plugin named in the original statement, if one was specified
#
# 		) Otherwise, the default authentication plugin.
#
# 			In particular, if a user u1 already exists and uses a nondefault authentication
# 			plugin, the statement written to the binary log for CREATE USER IF NOT EXISTS u1
# 			names the default authentication plugin.
#
# 			(If the statement written to the binary log must specify a nondefault authentication
# 			plugin for a user, include it in the original statement)
#
# If the server adds the default authentication plugin for any nonexisting users in the
# statement written to the binary log, it writes a warning to the error log naming those
# users.
#
# 13.7.1.4 DROP ROLE SYNTAX
#
# 		DROP ROLE [IF EXISTS] role [, role ] ---
#
# DROP_ROLE removes one or more roles (named collections of privileges)
#
# To use this statement, you must have the global DROP_ROLE or CREATE_USER
# privilege.
#
# When the read_only system variable is enabled, DROP_ROLE additionally requires
# the CONNECTION_ADMIN or SUPER privilege.
#
# Roles named in the mandatory_roles system variable value cannot be dropped.
#
# DROP_ROLE either succeeds for all named roles or rolls back and has no effect
# if any error occurs.
#
# By default, an error occurs if you try to drop a role that does not exist.
#
# If the IF EXISTS clause is given, the statement produces a warning for each named
# role that does not exist, rather than an error.
#
# The statement is written to the binary log if it succeeds, but not if it fails;
# in that case, rollback occurs and no changes are made.
#
# A statement written to the binary log includes all named roles.
#
# If the IF EXISTS clause is given, this includes even roles that do not exist
# and were not dropped.
#
# Each role name uses the format described in SECTION 6.2.5, "SPECIFYING ROLE NAMES"
#
# For example:
#
# 		DROP ROLE 'administrator', 'developer';
# 		DROP ROLE 'webapp'@'localhost';
#
# The host name part of the role name, if omitted, defaults to '%'
#
# A dropped role is automatically revoked from any user account (or role)
# to which the role was granted.
#
# Within any current session for such an account, its privileges are adjusted
# for the next statement executed.
#
# 13.7.1.5 DROP USER SYNTAX
#
# 		DROP USER [IF EXISTS] user [, user] ---
#
# The DROP_USER statement removes one or more MySQL accounts and their privileges.
#
# It removes privilege rows for the account from all grant tables.
#
# Roles named in the mandatory_roles system variable value cannot be dropped.
#
# To use DROP_USER, you must have the global CREATE_USER privilege, or the
# DELETE privilege for the mysql system database.
#
# When the read_only system variable is enabled, DROP_USER additionally
# requires the CONNECTION_ADMIN or SUPER privilege.
#
# DROP_USER either succeeds for all named users or rolls back and has no effect
# if any error occurs.
#
# By default, an error occurs if you try to drop a user that does not exist.
#
# If the IF EXISTS clause is given, the statement produces a warning for each
# named user that does not exist, rather than an error.
#
# The statement is written to the binary log if it succeeds, but not if it fails;
# in that case, rollback occurs and no changes are made.
#
# A statement written to the binary log includes all named users.
#
# If the IF EXISTS clause is given, this includes even users that do not exist
# and were not dropped.
#
# Each account name uses the format described in SECTION 6.2.4, "SPECIFYING ACCOUNT NAMES"
#
# For example:
#
# 		DROP USER 'jeffrey'@'localhost';
#
# The host name part of the account name, if omitted, defaults to '%'
#
# IMPORTANT:
#
# 		DROP_USER does not automatically close any open user sessions.
#
# 		Rather, in the event that a user with an open session is dropped,
# 		the statement does not take effect until that user's session is closed.
#
#		Once the session is closed, the user is dropped, and that user's next
# 		attempt to log in will fail.
#
# 		This is by design.
#
# DROP_USER does not automatically drop or invalidate databases or objects
# within them that the old user created.
#
# This includes stored programs or views for which the DEFINER attribute names
# the dropped user.
#
# Attempts to access such objects may produce an error if they execute in definer
# security context.
#
# (For information about security context, see SECTION 24.6, "ACCESS CONTROL FOR STORED
# PROGRAMS AND VIEWS")
#
# 13.7.1.6 GRANT SYNTAX
#
# 		GRANT
# 			priv_type [(column_list)]
# 				[, priv_type [(column_list)]] ---
# 			ON [object_type] priv_level
# 			TO user_or_role [, user_or_role] ---
# 			[WITH GRANT OPTION]
#
# 		GRANT PROXY ON user_or_role
# 			TO user_or_role [, user_or_role] ---
# 			[WITH GRANT OPTION]
#
# 		GRANT role [, role] ---
# 			TO user_or_role [, user_or_role] ---
# 			[WITH ADMIN OPTION]
#
# 		object_type: {
# 			TABLE
# 		 | FUNCTION
# 		 | PROCEDURE
# 		}
#
# 		priv_level: {
# 			*
# 		 | *.*
# 		 | db_name.*
# 		 | db_name.tbl_name
# 		 | tbl_name
# 		 | db_name.routine_name
# 		}
#
# 		user_or_role: {
# 			user
# 		 | role
# 		}
#
# 		user:
# 			(see Section 6.2.4, "Specifying Account Names")
#
# 		role:
# 			(see Section 6.2.5, "Specifying Role Names")
#
# The GRANT statement assigns privileges and roles to MySQL user accounts and roles.
#
# There are several aspects to the GRANT statement, described under the following
# topics:
#
# 		) GRANT GENERAL OVERVIEW
#
# 		) OBJECT QUOTING GUIDELINES
#
# 		) ACCOUNT NAMES
#
# 		) PRIVILEGES SUPPORTED BY MYSQL
#
# 		) GLOBAL PRIVILEGES
#
# 		) DATABASE PRIVILEGES
#
# 		) TABLE PRIVILEGES
#
# 		) COLUMN PRIVILEGES
#
# 		) STORED ROUTINE PRIVILEGES
#
# 		) PROXY USER PRIVILEGES
#
# 		) GRANTING ROLES
#
# 		) OTHER ACCOUNT CHARACTERISTICS 
#
# 		) MYSQL AND STANDARD SQL VERSIONS OF GRANT
#
# GRANT GENERAL OVERVIEW
#
# The GRANT statement enables system administrators to grant privileges and roles,
# which can be granted to user accounts and roles.
#
# These syntax restrictions apply: 
#
# 		) GRANT cannot mix granting both privileges and roles in the same statement.
#
# 			A given GRANT statement must grant either privileges or roles.
#
# 		) The ON clause distinguishes whether the statement grants privileges or roles:
#
# 			) With ON, the statement grants privileges.
#
# 			) Without ON, the statement grants roles
#
# 			) It is permitted to assign both privileges and roles to an account, but you
# 				must use separate GRANT statements, each with syntax appropriate to what is
# 				to be granted.
#
# For more information about roles, see SECTION 6.3.4, "USING ROLES"
#
# To use GRANT, you must have the GRANT_OPTION privilege, and you must have the
# privileges that you are granting.
#
# When the read_only system variable is enabled, GRANT additionally requires the
# CONNECTION_ADMIN or SUPER privilege.
#
# GRANT either succeeds for all named users and roles or rolls back and has no
# effect if any error occurs.
#
# The statement is written to the binary log only if it succeeds for all
# named users and roles.
#
# The REVOKE statement is related to GRANT and enables administrators to remove
# account privileges.
#
# See SECTION 13.7.1.8, "REVOKE SYNTAX"
#
# Each account name uses the format described in SECTION 6.2.4, "SPECIFYING ACCOUNT NAMES"
#
# Each role name uses the format described in SECTION 6.2.5, "SPECIFYING ROLE NAMES"
#
# For example:
#
# 		GRANT ALL ON db1.* TO 'jeffrey'@'localhost';
# 		GRANT 'role1', 'role2' TO 'user1'@'localhost', 'user2'@'localhost';
# 		GRANT SELECT ON world.* TO 'role3';
#
# The host name part of the account or role name, if omitted, defaults to '%'
#
# Normally, a database administrator first uses CREATE_USER to create an account
# and define its nonprivilege characteristics such as its password, whether it uses
# secure connections, and limits on access to server resources, then uses GRANT
# to define its privileges.
#
# ALTER_USER may be used to change the nonprivilege characteristics of existing accounts.
#
# For example:
#
# 		CREATE USER 'jeffrey'@'localhost' IDENTIFIED BY 'password';
# 		GRANT ALL ON db1.* TO 'jeffrey'@'localhost';
# 		GRANT SELECT ON db2.invoice TO 'jeffrey'@'localhost';
# 		ALTER USER 'jeffrey'@'localhost' WITH MAX_QUERIES_PER_HOUR 90;
#
# From the mysql program, GRANT responds with Query OK, 0 rows affected when executed
# successfully.
#
# To determine what privileges result from the operation, use SHOW_GRANTS
#
# See SECTION 13.7.6.21, "SHOW GRANTS SYNTAX"
#
# 	IMPORTANT:
#
# 		Under some circumstances, GRANT may be recorded in server logs or on the
# 		client side in a history file such as ~/.mysql_history, which means that 
# 		cleartext passwords may be read by anyone having read access to that
# 		information.
#
# 		For information about the conditions under which this occurs for the
# 		server logs and how to control it, see SECTION 6.1.2.3, "PASSWORDS AND LOGGING"
#
# 		For similar information about client-side logging, see SECTION 4.5.1.3,
# 		"MYSQL CLIENT LOGGING"
#
# GRANT supports host names up to 60 characters long.
#
# User names can be up to 32 characters. Database, table, column and routine names
# can be up to 64 characters.
#
# 		WARNING:
#
# 			Do not attempt to change the permissible length for user names by altering
# 			the mysql.user system table.
#
# 			Doing so results in unpredictable behavior which may even make it impossible
# 			for users to log into the MySQL server.
#
# 			Never alter the structure of tables in the mysql system database in any manner
# 			except by means of the procedure described in SECTION 4.4.5, "MYSQL_UPGRADE --- CHECK AND UPGRADE MYSQL TABLES"
#
# OBJECT QUOTING GUIDELINES
#
# Several objects within GRANT statements are subject to quoting, although quoting is optional
# in many cases:
#
# 		Account, role, database, table, column and routine names.
#
# For example, if a user_name or host_name value in an account name is legal as an unquoted
# identifier, you need not quote it.
#
# However, quotation marks are necessary to specify a user_name string containing special characters
# (such as -), or a host_name string containing special characters or wildcard characters
# (such as %); for example, 'test-user'@'%.com'
#
# Quote the user name and host name separately
#
# To specify quoted values:
#
# 		) Quote database, table, column and routine names as identifiers
#
# 		) Quote user names, and host names as identifiers or as strings
#
# 		) Quote passwords as strings
#
# For string-quoting and identifier-quoting guidelines, see SECTION 9.1.1, "STRING LITERALS",
# and SECTION 9.2, "SCHEMA OBJECT NAMES"
#
# The _ and % wildcards are permitted when specifying database names in GRANT statements
# that grant privileges at the database level (GRANT --- ON db_name.*)
#
# This means, for example, that to use a _ character as part of a database name
# specify it as \_ in the GRANT statement, to prevent the user from being able
# to access additional databases matching the wildcard pattern;
#
# For example, GRANT --- ON `foo\_bar`.* TO ---
#
# When a database name not is used to grant privileges at the database level, but as
# a qualifier for granting privileges to some other objects such as a table
# or routine (for example, GRANT --- ON db_name.tbl_name), wildcard characters
# are treated as normal characters.
#
# ACCOUNT NAMES
#
# A user value in a GRANT statement indicates a MySQL account to which the statement
# applies.
#
# To accommodate granting rights to users from arbitrary hosts, MySQL supports
# specifying the user value in the form 'user_name'@'host_name'
#
# You can specify wildcards in the host name. For example, 'user_name'@'%.example.com'
# applies to user_name for any host in the example.com domain, and 'user_name'@'198.51.100.%'
# applies to user_name for any host in the 198.51.100 class C subnet.
#
# The simple form 'user_name' is a synonym for 'user_name'@'%'
#
# MySQL does not support wildcards in user names.
#
# To refer to an anonymous user, specify an account with an empty user name
# with the GRANT statement:
#
# 		GRANT ALL ON test.* TO ''@'localhost' ---;
#
# In this case, any user who connects from the local host with the correct password
# for the anonymous user will be permitted access, with the privileges associated
# with the anonymous-user account.
#
# For additional information about user name and host name values in account names,
# see SECTION 6.2.4, "SPECIFYING ACCOUNT NAMES"
#
# WARNING:
#
# 		If you permit local anonymous users to connect to the MySQL server, you should
# 		also grant privileges to all local users as 'user_name'@'localhost'
#
# 		Otherwise, the anonymous user account for localhost in the mysql.user system
# 		table is used when named users try to log in to the MySQL server from the local
# 		machine.
#
# 		For details, see SECTION 6.2.6, "ACCESS CONTROL, STAGE 1: CONNECTION VERIFICATION"
#
# 		To determine whether this issue applies to you, execute the following query, which
# 		lists any anonymous users:
#
# 			SELECT host, User FROM mysql.user WHERE User='';
#
# 		To avoid the problem just described, delete the local anonymous user account
# 		using this statement:
#
# 			DROP USER ''@'localhost';
#
# PRIVILEGES SUPPORTED BY MYSQL
#
# The following tables summarize the permissible static and dynamic priv_type privilege types
# that can be specified for the GRANT and REVOKE statements, and the levels at which each
# privilege can be granted.
#
# For additional information about each privilege, see SECTION 6.2.1, "PRIVILEGES PROVIDED BY MYSQL"
#
# For information about the differences between static and dynamic privileges,
# see SECTION 6.2.2, "STATIC VERSUS DYNAMIC PRIVILEGES"
#
# TABLE 13.9 PERMISSIBLE STATIC PRIVILEGES FOR GRANT AND REVOKE
#
# PRIVILEGE 					MEANING AND GRANTABLE LEVELS
#
# ALL_[PRIVILEGES] 			Grant all privileges at specified access level except GRANT_OPTION and PROXY
#
# ALTER 							Enable use of ALTER_TABLE. Levels: Global, database, table
#
# ALTER_ROUTINE 				Enable stored routines to be altered or dropped. Levels: Global, database, routine
#
# CREATE 						Enable database and table creation. levels: Global, database, table
#
# CREATE_ROLE 					Enable role creation. Level: Global
#
# CREATE_ROUTINE 				Enable stored routine creation. Levels: Global, database
#
# CREATE_TABLESPACE 			Enable tablespaces and log file groups to be created, altered, or dropped. Level: Global
#
# CREATE_TEMPORARY_TABLES	Enable use of CREATE_TEMPORARY_TABLE. Levels: Global, database
#
# CREATE_USER 					Enable use of CREATE_USER, DROP_USER, RENAME_USER, and REVOKE_ALL_PRIVILEGES. Level: Global
#
# CREATE_VIEW 					Enable views to be created or altered. Levels: Global, database, table
#
# DELETE 						Enable use of DELETE. Level: Global, database, table
#
# DROP 							Enable databases, tables, and views to be dropped. Levels: Global, database, table
#
# DROP_ROLE 					Enable roles to be dropped. Level: Global
#
# EVENT 							Enable use of events for the Event Scheduler. levels: Global, database
#
# EXECUTE 						Enable the user to execute stored routines. Levels: Global, database, routine
#
# FILE 							Enable the user to cause the server to read or write files. Level: Global
#
# GRANT_OPTION 				Enables privileges to be granted to or removed from other accounts. Levels: Global, database, table, routine, proxy
#
# INDEX 							Enables indexes to be created or dropped. Levels: Global, database, table
#
# INSERT 						Enable use of INSERT. Levels: Global, database, table, column
#
# LOCK_TABLES 					Enable use of LOCK_TABLES on tables for which you have the SELECT privilege. Levels: Global, database
#
# PROCESS 						Enable the user to see all processes with SHOW_PROCESSLIST. Level: Global
#
# PROXY 							Enable user proxying. Level: From user to user
#
# REFERENCES 					Enable foreign key creation. Levels: Global, database, table, column
#
# RELOAD 						Enable use of FLUSH operations. Level: Global
#
# REPLICATION_CLIENT 		Enable the user to ask where master or slave servers are. Level: Global
#
# REPLICATION_SLAVE 			Enable replication slaves to read binary log events from the master. Level: Global
#
# SELECT 						Enable use of SELECT. Levels: Global, database, table, column
#
# SHOW_DATABASES 				Enable SHOW_DATABASES to show all databases. Level: Global
#
# SHOW_VIEW 					Enable use of SHOW_CREATE_VIEW. Levels: Global, database, table
#
# SHUTDOWN 						Enable use of mysqladmin shutdown. Level: Global
#
# SUPER 							Enable use of other administrative operations such as CHANGE_MASTER_TO,
# 									KILL, PURGE_BINARY_LOGS, SET_GLOBAL and mysqladmin debug command. Level: Global
#
# TRIGGER 						Enable trigger operations. Levels: Global, database, table
#
# UPDATE 						Enable use of UPDATE. Levels: Global, database, table, column
#
# USAGE 							Synonym for "no privileges"
#
# TABLE 13.10 PERMISSIBLE DYNAMIC PRIVILEGES FOR GRANT AND REVOKE
#
# PRIVILEGE 							Meaning and Grantable Levels
#
# APPLICATION_PASSWORD_ADMIN		Enable dual password administration. Level: Global
#
# AUDIT_ADMIN 							Enable audit log configuration. Level: Global
#
# BACKUP_ADMIN 						Enable backup administration. level: Global
#
# BINLOG_ADMIN 						Enable binary log control. Level: Global
#
# BINLOG_ENCRYPTION_ADMIN 			Enable activation and deactivation of binary log encryption. Level: Global
#
# CONNECTION_ADMIN 					Enable connection limit/restriction control. Level: Global
#
# ENCRYPTION_KEY_ADMIN 				Enable InnoDB key rotation. Level: Global
#
# FIREWALL_ADMIN 						Enable firewall rule administration, any user. Level: GLobal
#
# FIREWALL_USER 						Enable firewall rule administration, self. Level: Global
#
# GROUP_REPLICATION_ADMIN 			Enable Group Replication control. level: Global
#
# PERSIST_RO_VARIABLES_ADMIN 		Enable persisting read-only system variables. Level: Global
#
# REPLICATION_SLAVE_ADMIN 			Enable regular replication control. Level: GLobal
#
# RESOURCE_GROUP_ADMIN 				Enable resource group administration. Level: Global
#
# RESOURCE_GROUP_USER 				Enable resource group administration. Level: Global
#
# ROLE_ADMIN 							Enable use of WITH ADMIN OPTION. Level: GLobal
#
# SESSION_VARIABLES_ADMIN 			Enable setting restricted session system variables. Level: Global
#
# SET_USER_ID 							Enable setting non-self DEFINER values. Level: Global
#
# SYSTEM_VARIABLES_ADMIN 			Enable modifying or persisting global system variables. Level: Global
#
# VERSION_TOKEN_ADMIN 				Enable use of Version Tokens UDFs. Level: Global
#
# XA_RECOVER_ADMIN 					Enable XA_RECOVER execution. Level: Global
#
# A trigger is associated with a table. To create or drop a trigger, you must have the TRIGGER privilege
# for the table, not the trigger.
#
# In GRANT statements, the ALL_[PRIVILEGES] or PROXY privilege must be named by itself
# and cannot be specified along with other privileges.
#
# ALL_[PRIVILEGES] stands for all privileges available for the level at which privileges
# are to be granted except for the GRANT_OPTION and PROXY privileges.
#
# MySQL account information is stored in the tables of the mysql system database.
#
# For additional details, consult SECTION 6.2, "THE MYSQL ACCESS PRIVILEGE SYSTEM",
# which discusses the mysql system database and the access control system extensively.
#
# If the grant tables hold privilege rows that contain mixed-case database or table
# names and the lower_case_table_names system variable is set to a nonzero value,
# REVOKE cannot be used to revoke these privileges.
#
# It will be necessary to manipulate the grant tables directly.
#
# (GRANT will not create such rows when lower_case_table_names is
# set, but such rows might have been created prior to setting that variable.
#
# The lower_case_table_names setting can only be configured at server startup)
#
# Privileges can be granted at several levels, depending on the syntax used
# for the ON clause.
#
# For REVOKE, the same ON syntax specifies which privileges to remove.
#
# For the global, database, table and routine levels, GRANT_ALL assigns only
# the privileges that exist at the level you are granting.
#
# For example, GRANT ALL ON db_name.* is a database-level statement,
# so it does not grant any global-only privileges such as FILE.
#
# Granting ALL does not assign the GRANT_OPTION or PROXY privilege.
#
# The object_type clause, if present, should be specified as TABLE FUNCTION,
# or PROCEDURE when the following object is a table, a stored function,
# or a stored procedure.
#
# The privileges that a user holds for a database, table, column, or routine
# are formed additively as the logical OR of the account privileges at each
# of the privilege levels.
#
# FOr example, if a user has a global SELECT privilege, the privilege cannot be
# denied by an absence of the privilege at the database, table, or column level.
#
# Details of the privilege-checking procedure are presented in SECTION 6.2.7,
# "ACCESS CONTROL, STAGE 2: REQUEST VERIFICATION"
#
# If you are using table, column, or routine privileges for even one user,
# the server examines table, column, and routine privileges for all users
# and this slows down MySQL a bit.
#
# Similarly, if you limit the number of queries, updates or connections
# for any users, the server must monitor these values.
#
# MySQL enables you to grant privileges on databases or tables that do not
# exist.
#
# For tables, the privileges to be granted must include the CREATE privilege.
#
# This behavior is by design, and is intended to enable the DB admin to prepare
# user accounts and privileges for databases or tables that are to be 
# created at a later time.
#
# IMPORTANT:
#
# 		MySQL does not automatically revoke any privileges when you drop
# 		a database or table.
#
# 		However, if you drop a routine, any routine-level privileges granted
# 		for that routine are revoked.
#
# GLOBAL PRIVILEGES
#
# Global privileges are administrative or apply to all databases on a given
# server.
#
# To assign global privileges, use ON *.* syntax:
#
# 		GRANT ALL ON *.* TO 'someuser'@'somehost';
# 		GRANT SELECT, INSERT ON *.* TO 'someuser'@'somehost';
#
# The CREATE_TABLESPACE, CREATE_USER, FILE, PROCESS, RELOAD, REPLICATION_CLIENT,
# REPLICATION_SLAVE, SHOW_DATABASES, SHUTDOWN and SUPER static privileges are
# administrative and can only be granted globally.
#
# Dynamic privileges are all global and can only be granted globally.
#
# Other privileges can be granted globally or at more specific levels.
#
# The effect of GRANT_OPTION granted at the global level differs for static
# and dynamic privileges:
#
# 		) GRANT_OPTION granted for any static global privilege applies to all static global privileges
#
# 		) GRANT_OPTION granted for any dynamic privilege applies only to that dynamic privilege
#
# GRANT ALL at the global level grants all static global privileges and all currently registered
# dynamic privileges.
#
# A dynamic privilege registered subsequent to execution of the GRANT statement is not granted
# retroactively to any account.
#
# MySQL stores global privileges in the mysql.user system table
#
# DATABASE PRIVILEGES
#
# Database privileges apply to all objects in a given database. To assign database-level
# privileges, use ON db_name.* syntax:
#
# 		GRANT ALL ON mydb.* TO 'someuser'@'somehost';
# 		GRANT SELECT, INSERT ON mydb.* TO 'someuser'@'somehost';
#
# If you use ON * syntax (rather than ON *.*), privileges are assigned at the database
# level for the default database.
#
# An error occurs if there is no default database.
#
# The CREATE, DROP, EVENT, GRANT_OPTION, LOCK_TABLES and REFERENCES privileges can be
# specified at the database level.
#
# Table or routine privileges also can be specified at the database level, in which
# case they apply to all tables or routines in the database.
#
# MySQL stores database privileges in the mysql.db system table
#
# TABLE PRIVILEGES
#
# Table privileges apply to all columns in a given table. To assign table-level privileges,
# use ON db_name.tbl_name syntax:
#
# 		GRANT ALL ON mydb.mytbl TO 'someuser'@'somehost';
# 		GRANT SELECT, INSERT ON mydb.mytbl TO 'someuser'@'somehost';
#
# If you specify tbl_name rather than db_name.tbl_name, the statement applies to tbl_name
# in the default database.
#
# An error occurs if there is no default database.
#
# The permissible priv_type values at the table level are ALTER, CREATE_VIEW, CREATE,
# DELETE, DROP, GRANT_OPTION, INDEX, INSERT, REFERENCES, SELECT, SHOW_VIEW, TRIGGER
# and UPDATE.
#
# Table-level privileges apply to base tables and views.
#
# They do not apply to tables created with CREATE_TEMPORARY_TABLE, even if
# the table names match.
#
# For information about TEMPORARY table privileges, see SECTION 13.1.20.3, "CREATE TEMPORARY TABLE SYNTAX"
#
# MySQL stores table privileges in the mysql.tables_priv system table
#
# COLUMN PRIVILEGES
#
# Column privileges apply to single columns in a given table.
#
# Each privilege to be granted at the column level must be followed by the column
# or columns, enclosed within parentheses.
#
# 		GRANT SELECT (col1), INSERT (col1, col2) ON mydb.mytbl TO 'someuser'@'somehost';
#
# The permissible priv_type values for a column (that is, when you use a column_list clause)
# are INSERT, REFERENCES, SELECT, and UPDATE.
#
# MySQL stores column privileges in the mysql.columns_priv system table.
#
# STORED ROUTINE PRIVILEGES
#
# The ALTER_ROUTINE, CREATE_ROUTINE, EXECUTE and GRANT_OPTION privileges apply to
# stored routines (procedures and functions).
#
# They can be granted at the global and database levels.
#
# Except for CREATE_ROUTINE, these privileges can be granted at the routine level
# for individual routines.
#
# 		GRANT CREATE ROUTINE ON mydb.* TO 'someuser'@'somehost';
# 		GRANT EXECUTE ON PROCEDURE mydb.myproc TO 'someuser'@'somehost';
#
# The permissible priv_type values at the routine level are ALTER_ROUTINE, EXECUTE
# and GRANT_OPTION
#
# CREATE_ROUTINE is not a routine-level privilege because you must have the privilege
# at the global or database level to create a routine in the first place.
#
# MySQL stores routine-level privileges in the mysql.procs_priv system table
#
# PROXY USER PRIVILEGES
#
# The PROXY privilege enables one user to be a proxy for another.
#
# The proxy user impersonates or takes the identity of the proxied user;
# that is, it assumes the privileges of the proxied user.
#
# 		GRANT PROXY ON 'localuser'@'localhost' TO 'externaluser'@'somehost';
#
# When PROXY is granted, it must be the only privilege named in the GRANT
# statement, and the only permitted WITH option is WITH GRANT OPTION.
#
# Proxying requires that the proxy user authenticate through a plugin that
# returns the name of the proxied user ot the server when the proxy
# user connects, and that the proxy user have the PROXY privilege for the
# proxied user.
#
# For details and examples, see SECTION 6.3.11, "PROXY USERS"
#
# MySQL stores proxy privileges in the mysql.proxies_priv system table
#
# GRANTING ROLES
#
# GRANT syntax without an ON clause grants roles rather than individual privileges.
#
# A role is a named collection of privileges; See SECTION 6.3.4, "USING ROLES"
#
# For example:
#
# 		GRANT 'role1', 'role2' TO 'user1'@'localhost', 'user2'@'localhost';
#
# Each role to be granted must exist, as well as each user account or role
# to which it is to be granted.
#
# If the GRANT statement includes the WITH ADMIN OPTION clause, each named user
# becomes able to grant the named roles to other users or roles, or revoke
# them from other users or roles.
#
# This includes the ability to use WITH ADMIN OPTION itself
#
# It is possible to create circular references with GRANT. For example:
#
# 		CREATE USER 'u1', 'u2';
# 		CREATE ROLE 'r1', 'r2';
#
# 		GRANT 'u1' TO 'u1'; -- SImple loop: u1 => u1
# 		GRANT 'r1' TO 'r1'; -- simple loop: r1 => r1
#
# 		GRANT 'r2' TO 'u2'; 
# 		GRANT 'u2' TO 'r2'; -- mixed user/role loop: u2 => r2 => u2
#
# Circular grant references are permitted, but add no new privileges or roles to the grantee
# because a user or role already has its privileges and roles.
#
# OTHER ACCOUNT CHARACTERISTICS 
#
# The optional WITH clause is used to enable a user to grant privileges to other users.
#
# The WITH GRANT OPTION clause gives the user the ability to give to other users
# any privileges the user has at the specified privilege level.
#
# To grant the GRANT_OPTION privilege to an account without otherwise changing its 
# privileges, do this:
#
# 		GRANT USAGE ON *.* TO 'someuser'@'somehost' WITH GRANT OPTION;
#
# Be careful to whom you give the GRANT_OPTION privilege because two users with
# different privileges may be able to combine privileges.
#
# You cannot grant another user a privilege which you yourself do not have; the GRANT_OPTION
# privilege enables you to assign only those privileges which you yourself possess.
#
# Be aware that when you grant a user the GRANT_OPTION privilege at a particular privilege level,
# any privileges that user possesses (or may be given in the future) at that level, 
# can also be granted by that user to other users.
#
# Suppose that you grant a user the INSERT privilege on a database.
#
# If you then grant the SELECT privilege on the database and specify WITH GRANT OPTION,
# that user can give to other users not only the SELECT privilege, but also INSERT.
#
# If oyu then grant the UPDATE privilege to the user on the database,
# the user can grant INSERT, SELECT and UPDATE.
#
# For a nonadminsitrative user, you should not grant the ALTER privilege globally
# or for the mysql system database.
#
# If you do that,the user can try to subvert the privilege system by renaming tables.
#
# FOr additional information about security risks associated with a particular privileges,
# See SECTION 6.2.1, "PRIVILEGES PROVIDED BY MYSQL"
#
# MYSQL AND STANDARD SQL VERSIONS OF GRANT
#
# The biggest differences between the MySQL and standard SQL versions of GRANT are:
#
# 		) MySQL associates privileges with the combination of a host name and user name
# 			and not with only a user name.
#
# 		) Standard SQL does not have global or database-level privileges, nor does it support
# 			all the privilege types that MySQL supports.
#
# 		) MySQL does not support the standard SQL UNDER privilege
#
# 		) Standard SQL privileges are structured in a hierarchial manner.
#
# 			If you remove a user, all privileges the user has been granted are revoked.
#
# 			This is also true in MySQL, if you use DROP_USER.
#
# 			See SECTION 13.7.1.5, "DROP USER SYNTAX"
#
# 		) In standard SQL, when you drop a table, all privileges for the table are revoked.
#
# 			In standard SQL, when you revoke a privilege, all privileges that were granted based
# 			on that privilege are also revoked.
#
# 			In MySQL, privileges can be dropped with DROP_USER or REVOKE statements.
#
# 		) In MySQL, it is possible to have the INSERT privilege for only some of the
# 			columns in a table.
#
# 			In this case, you can still execute INSERT statements on the table, provided
# 			that you insert values only for those columns for which you have the INSERT privilege.
#
# 			The omitted columns are set to their implicit default values if strict SQL mode
# 			is not enabled.
#
# 			In strict mode, the statement is rejected if any of the omitted columns have no
# 			default value.
#
# 			(Standard SQL requires you to have the INSERT privilege on all columns)
#
# 			For information about strict SQL mode and implicit default values, see SECTION 5.1.11,
# 			"SERVER SQL MODES" and SECTION 11.7, "DATA TYPE DEFAULT VALUES"
#
# 13.7.1.7 RENAME USER SYNTAX
#
# 		RENAME USER old_user TO new_user
# 			[, old_user TO new_user] ---
#
# The RENAME_USER statement renames existing MySQL accounts.
#
# An error occurs for old accounts that do not exist or new accounts that already exist.
#
# To use RENAME_USER, you must have the global CREATE_USER privilege, or the UPDATE
# privilege for the mysql system database.
#
# When the read_only system variable is enabled, RENAME_USER additionally
# requires the CONNECTION_ADMIN or SUPER privilege.
#
# Each account name uses the format described in SECTION 6.2.4, "SPECIFYING ACCOUNT NAMES"
#
# For example:
#
# 		RENAME USER 'jeffrey'@'localhost' TO 'jeff'@'127.0.0.1';
#
# The host name part of the account name, if omitted, defaults ot '%'
#
# RENAME_USER causes the privileges held by the old user to be those held by the
# new user.
#
# However, RENAME_USER does not automatically drop or invalidate databases or
# objects within them that the old user created.
#
# This includes stored programs or views for which the DEFINER attribute names
# the old user.
#
# Attempts to access such objects may produce an error if they execute in definer
# security context.
#
# (For information about security context, see SECTION 24.6, "ACCESS CONTROL FOR STORED PROGRAMS AND VIEWS")
#
# The privilege changes take effect as indicated in SECTION 6.2.8, "WHEN PRIVILEGE CHANGES TAKE EFFECT"
#
# 13.7.1.8 REVOKE SYNTAX
#
# REVOKE
# 		priv_type [(column_list)]
# 			[, priv_type [(column_list)] ---
# 		ON [object_type] priv_leveL
# 		FROM user_or_role [, user_or_role] ---
#
# REVOKE ALL [PRIVILEGES], GRANT OPTION
# 		FROM user_or_role [, user_or_role] ---
#
# REVOKE PROXY ON user_or_role
# 		FROM user_or_role [, user_or_role] ---
#
# REVOKE role [, role ] ---
# 		FROM user_or_role [, user_or_role ] ---
#
# user_or_role: {
# 		user
# 	 | role
# }
#
# user:
# 		(see SECTION 6.2.4, "SPECIFYING ACCOUNT NAMES")
#
# role:
# 		(see SECTION 6.2.5, "SPECIFYING ROLE NAMES")
#
# The REVOKE statment enables system administrators to revoke privileges
# and roles, which can be revoked from user accounts and roles.
#
# For information about roles, see SECTION 6.3.4, "USING ROLES"
#
# When the read_only system variable is enabled, REVOKE requires the CONNECTION_ADMIN
# or SUPER privilege in addition to any other required privileges described in the
# following discussion.
#
# REVOKE either succeeds for all named users and roles or rolls back and has no effect
# if any error occurs.
#
# The statement is written to the binary log only if it succeeds for all named users
# and roles.
#
# Each account name uses the format described in SECTION 6.2.4, "SPECIFYING ACCOUNT NAMES"
#
# Each role name uses the format described in SECTION 6.2.5, "SPECIFYING ROLE NAMES"
#
# For example:
#
# 		REVOKE INSERT ON *.* FROM 'jeffrey'@'localhost';
# 		REVOKE 'role1', 'role2' FROM 'user1'@'localhost', 'user2'@'localhost';
# 		REVOKE SELECT ON world.* FROM 'role3';
#
# The host name part of the account or role name, if omitted, defaults to '%'
#
# For details on the levels at which privileges exist, the permissible priv_type,
# priv_level and object_type values, and the syntax for specifying users and
# passwords, see SECTION 13.7.1.6, "GRANT SYNTAX"
#
# To use the first REVOKE syntax, you must have the GRANT_OPTION privilege,
# and you must have the privileges that you are revoking.
#
# To revoke all privileges, use the second syntax, which drops all global,
# database, table, column and routine privileges for the named users or roles:
#
# 		REVOKE ALL PRIVILEGES, GRANT OPTION
# 			FROM user_or_role [, user_or_role] ---
#
# REVOKE ALL PRIVILEGES, GRANT OPTION does not revoke any roles.
#
# To use this REVOKE syntax, you must have the global CREATE_USER privilege,
# or the UPDATE privilege for the mysql system database.
#
# The syntax for which the REVOKE keyword is followed by one or more role names
# takes a FROM clause indicating one or more users or roles from which to revoke
# the roles.
#
# Roles named in the mandatory_roles system variable value cannot be revoked.
#
# A revoked role immediately affects any user account from which it was revoked,
# such that within any current session for the account, its privileges are adjusted
# for the next statement executed.
#
# Revoking a role revokes the role itself, not the privileges that it represents.
#
# If an account is granted a role that includes a given privilege, and is also granted
# the privilege explicitly or another role that includes the privilege, the account still
# is granted that privilege after the first role is revoked.
#
# For example, if an account is granted two roles that each include SELECT, the account
# still can select after either role is revoked.
#
# REVOKE ALL ON *.* (at the global level) revokes all granted static global privileges
# and all granted dynamic privileges.
#
# User accounts and roles from which privileges and roles are to be revoked must exist,
# but the roles to be revoked need not be currently granted to them.
#
# REVOKE removes privileges, but does not drop mysql.user system table entries.
#
# To remove a user account entirely, use DROP_USER. See SECTION 13.7.1.5, "DROP USER SYNTAX"
#
# If the grant tables hold privilege rows that contain mixed-case database or table names
# and the lower_case_table_names system variable is set to a nonzero value, REVOKE cannot
# be used to revoke these privileges.
#
# It will be necessary to manipulate the grant tables directly.
#
# (GRANT will not create such rows when lower_case_table_names is set, but such
# rows might have been created prior to setting the variable.
#
# The lower_case_table_names setting  can only be configured when initializing the server)
#
# When successfully executed from the mysql program, REVOKE responds with Query OK, 0 rows affected.
#
# To determine what privileges remain after the operation, use SHOW_GRANTS.
#
# See SECTION 13.7.6.21, "SHOW GRANTS SYNTAX"
#
# 13.7.1.9 SET DEFAULT ROLE SYNTAX
#
# 		SET DEFAULT ROLE
# 			{NONE | ALL | role [, role ] ---}
# 			TO user [, user ] ---
#
# For each user named immediately after the TO keyword, this statement defines which
# roles become active when the user connects to the server and authenticates, or when
# the user executes the SET_ROLE_DEFAULT statement during a session.
#
# SET_DEFAULT_ROLE is alternative syntax for ALTER_USER_---_DEFAULT_ROLE
# (See SECTION 13.7.1.1, "ALTER USER SYNAX")
#
# However, ALTER_USER can set the default for only a single user, whereas SET_DEFAULT_ROLE
# can set the default for multiple users.
#
# On the other hand, you can specify CURRENT_USER as the user name for the ALTER_USER
# statement, whereas you cannot for SET_DEFAULT_ROLE
#
# SET_DEFAULT_ROLE requires these privileges:
#
# 		) Setting the default roles for another user requires the global CREATE_USER privilege,
# 			or the UPDATE privilege for the mysql.default_roles system table.
#
# 		) Setting the default roles for yourself requires no special privileges, as long as the
# 			roles you want as the default have been granted to you.
#
# Each role name uses the format described in SECTION 6.2.5, "SPECIFYING ROLE NAMES"
#
# For example:
#
# 		SET DEFAULT ROLE administrator, developer TO 'joe'@'10.0.0.1';
#
# The host name part of the role name, if omitted, defaults to '%'
#
# The clause following the DEFAULT ROLE keywords permits these values:
#
# 		) NONE: Set the default to NONE (no roles)
#
# 		) ALL: Set the default to all roles granted to the account
#
# 		) role [, role ] ---: Set the default to the named roles, which must exist and be granted
# 			to the account at the time SET_DEFAULT_ROLE is executed.
#
# NOTE:
#
# 		SET_DEFAULT_ROLE and SET_ROLE_DEFAULT are different statements:
#
# 			) SET_DEFAULT_ROLE defines which account roles to activate by default within account sessions.
#
# 			) SET_ROLE_DEFAULT sets the active roles within the current session to the current account default roles.
#
# 13.7.1.10 SET PASSWORD SYNTAX
#
# 		SET PASSWORD [FOR user] = 'auth_string'
# 			[REPLACE 'current_auth_string']
# 			[RETAIN CURRENT PASSWORD]
#
# The SET_PASSWORD statement assigns a password to a MySQL user account.
#
# It may also include a password-verification clause that specifies the account
# current password to be replaced, and a clause that manages whether an account
# has a secondary password.
#
# 'auth_string' and 'current_auth_string' each represents a cleartext (unencrypted) password.
#
# NOTE:
#
# 		Clauses for password verification and secondary passwords apply only to accounts
# 		that store credentials interally in the mysql.user system table (mysql_native_password, sha256_password,
# 		or caching_sha2_password)
#
# 		For accounts that use plugins that perform authentication against an external
# 		credential system, password management must be handled externally against that
# 		system as well.
#
# The REPLACE 'current_auth_string' clause is available as of MySQL 8.0.13.
#
# If given:
#
# 		) REPLACE specifies the account current password to be replaced, as a cleartext (unencrypted) string
#
# 		) The clause must be given if password changes for the account are required to specify
# 			the current password, as verification that the user attempting to make the change
# 			actually knows the current password.
#
# 		) The clause is optional if password changes for the account may but need not specify the current password
#
# 		) The statement fails if the clause is given but does not match the current password, even if the clause is optional
#
# 		) REPLACE can be specified only when changing the account password for the current user
#
# For more information about password verification by specifying the current password, see SECTION 6.3.8,
# "PASSWORD MANAGEMENT"
#
# The RETAIN CURRENT PASSWORD clause implements dual-password capability and is available
# as of MySQL 8.0.14.
#
# If given:
#
# 		) RETAIN CURRENT PASSWORD retains an account current password as its secondary password,
# 			replacing any existing secondary password.
#
# 			The new password becomes the primary password, but clients can use the account to
# 			connect to the server using either the primary or secondary password.
#
# 			(Exception:
#
# 				If the new password specified by the SET_PASSWORD statement is empty,
# 				the secondary password becomes empty as well, even if RETAIN CURRENT PASSWORD
# 				is given)
#
# 		) If you specify RETAIN CURRENT PASSWORD for an account that has an empty primary password, the statement fails
#
# 		) If an account has a secondary password and you change its primary password without specifying
# 			RETAIN CURRENT PASSWORD, the secondary password remains unchanged.
#
# For more information about use of dual passwords, see SECTION 6.3.8, "PASSWORD MANAGEMENT"
#
# NOTE:
#
# 		Rather than using SET_PASSWORD_---_=_'auth_string' syntax, ALTER_USER syntax is the preferred
# 		statement for account alterations, including assigning passwords.
#
# 		For example:
#
# 			ALTER USER user IDENTIFIED BY 'auth_string';
#
# IMPORTANT:
#
# 		Under some circumstances, SET_PASSWORD may be recorded in server logs or on the client
# 		side in a history file such as ~/.mysql_history, which means that cleartext passwords
# 		may be read by anyone having read access t othat information.
#
# 		For information about the conditions under which this occurs for the server logs
# 		and how to control it, see SECTION 6.1.2.3, "PASSWORDS AND LOGGING"
#
# 		For similar information about client-side logging, see SECTION 4.5.1.3, "MYSQL CLIENT LOGGING"
#
# SET_PASSWORD can be used with or without a FOR clause that explicitly names a user account:
#
# 		) With a FOR user clause, the statement sets the password for the named account, which must exist:
#
# 			SET PASSWORD FOR 'jeffrey'@'localhost' = 'auth_string';
#
# 		) With no FOR user clause, the statement sets the password for the current user:
#
# 				SET PASSWORD = 'auth_string';
#
# 			Any client who connects to the server using a nonanonymous account can change the
# 			password for that account.
#
# 			(In particular, you can change your own password)
#
# 			To see which account the server authenticated you as, invoke the CURRENT_USER()
# 			function:
#
# 				SELECT CURRENT_USER();
#
# If a FOR user clause is given, the account name uses the format described in SECTION 6.2.4,
# SPECIFYING ACCOUNT NAMES".
#
# For example:
#
# 		SET PASSWORD FOR 'bob'@'%.example.org' = 'auth_string';
#
# The host name part of the account name, if omitted, defaults to '%'
#
# SET_PASSWORD interprets the string as a cleartext string, passes it to the
# authentication plugin associated with the account, and stores the result 
# returned by the plugin in the account row in the mysql.user system table.
#
# (The plugin is given the opportunity to hash the value into the encryption
# format it expects.
#
# The plugin may use the value as specified, in which case no hashing occurs)
#
# Setting the password for a named account (with a FOR clause) requires the
# UPDATE privilege for the mysql system database.
#
# Setting the password for yourself (for a nonanonymous account with no 
# FOR clause) requires no special privileges.
#
# Statements that modify secondary passwords require these privileges:
#
# 		) The APPLICATION_PASSWORD_ADMIN privilege is required to use the RETAIN
# 			CURRENT PASSWORD clause for SET_PASSWORD statements that apply to your
# 			own account.
#
# 			the privilege is required to manipulate your own secondary password
# 			because most users require only one password
#
# 		) If an account is to be permitted to manipulate secondary passwords for
# 			all accounts, it should be granted the CREATE_USER privilege rather than
# 			APPLICATION_PASSWORD_ADMIN
#
# When the read_only system variable is enabled, SET_PASSWORD requires the CONNECTION_ADMIN
# or SUPER privilege in addition to any other required privileges.
#
# For additional information about setting passwords and authentication plugins, see
# SECTION 6.3.7, "ASSIGNING ACCOUNT PASSWORDS", and SECTION 6.3.10, "PLUGGABLE AUTHENTICATION"
#
# 13.7.1.11 SET ROLE SYNTAX
#
# 		SET ROLE {
# 			DEFAULT
# 		 | NONE
# 		 | ALL
# 		 | ALL EXCEPT role [, role ] ---
# 		 | role [, role ] ---
# 		}
#
# SET_ROLE modifies the current user's effective privileges within the current session by
# specifying which of its granted roles are active.
#
# Granted roles include those granted explicitly to the user and those named
# in the mandatory_roles system variable value.
#
# Privileges that the user has been granted directly (rather than through roles) remain
# unaffected by changes to the active roles.
#
# Each role name uses the format described in SECTION 6.2.5, "SPECIFYING ROLE NAMES".
#
# For example:
#
# 		SET ROLE DEFAULT;
# 		SET ROLE 'role1', 'role2';
# 		SET ROLE ALL;
# 		SET ROLE ALL EXCEPT 'role1', 'role2';
#
# The host name part of the role name, if omitted, defaults to '%'
#
# The statement permits these role specifiers:
#
# 		) DEFAULT:
#
# 			Activate the account default roles. Default roles are those specified with SET_DEFAULT_ROLE
#
# 			When a user connects to the server and authenticates successfully, the server determines
# 			which roles to activate as the default roles.
#
# 			If the activate_all_roles_on_login system variable is enabled, the server activates all granted
# 			roles.
#
# 			Otherwise, The server executes SET_ROLE_DEFAULT implicitly.
#
# 			The server activates only default roles that can be activated.
#
# 			The server wites warnings to its error log for default roles that cannot be	
# 			activated, but the client receives no warnings.
#
# 			If a user executes SET_ROLE_DEFAULT during a session, an error occurs if any
# 			default role cannot be activated (for example, if it does not exist or is not
# 			granted to the user)
#
# 			In this case, the currrent active roles are not changed.
#
# 		) NONE:
#
# 			Set the active roles to NONE (no active roles)
#
# 		) ALL:
#
# 			Activate all roles granted to the account
#
# 		) ALL EXCEPT role [, role ] ---:
#
# 			Activate all roles granted to the account except those named.
#
# 			The named roles need not exist or be granted to the account.
#
# 		) role [, role ] ---: 
#
# 			Activate the named roles, which must be granted to the account.
#
# NOTE:
#
# 		SET_DEFAULT_ROLE and SET_ROLE_DEFAULT are different statements:
#
# 			) SET_DEFAULT_ROLE defines which account roles to activate by default within account sessions.
#
# 			) SET_ROLE_DEFAULT sets the active roles within the current session to the current account default roles.
#
# 13.7.2 RESOURCE GROUP MANAGEMENT STATEMENTS
#
# 13.7.2.1 ALTER RESOURCE GROUP SYNTAX
# 13.7.2.2 CREATE RESOURCE GROUP SYNTAX
#
# 13.7.2.3 DROP RESOURCE GROUP SYNTAX
# 13.7.2.4 SET RESOURCE GROUP SYNTAX
#
# MySQL supports creation and management of resource groups, and permits assigning threads
# running within the server to particular groups so that threads execute according to the
# resources available to the group.
#
# This section describes the SQL statements available for resource group management.
#
# For general discussion of the resource group capability, see SECTION 8.12.5, "RESOURCE GROUPS"
#
# 13.7.2.1 ALTER RESOURCE GROUP SYNTAX
#
# 		ALTER RESOURCE GROUP group_name
# 			[VCPU [=] vcpu_spec [, vcpu_spec] ---]
# 			[THREAD_PRIORITY [=] N]
# 			[ENABLE|DISABLE [FORCE]]
#
# 		vcpu_spec: {N | M - N}
#
# ALTER_RESOURCE_GROUP is used for resource group management (see SECTION 8.12.5, "RESOURCE GROUPS")
#
# This statement alters modifiable attributes of an existing resource group.
#
# It requires the RESOURCE_GROUP_ADMIN privilege.
#
# group_name identifies which resource group to alter. If the group does not exist,
# an error occurs.
#
# The attributes for CPU affinity, priority, and whether the group is enabled can be
# modified with ALTER_RESOURCE_GROUP.
#
# These attributes are specified the same way as described for CREATE_RESOURCE_GROUP
# (see SECTION 13.7.2.2, "CREATE RESOURCE GROUP SYNTAX")
#
# Only the attributes specified are altered. Unspecified attributes retain their
# current values.
#
# The FORCE modifier is used with DISABLE.
#
# It determines statement behavior if the resource group has any threads assigned to it:
#
# 		) If FORCE is not given, existing threads in the group continue to run until they terminate,
# 			but new threads cannot be assigned to the group.
#
# 		) If FORCE is given, existing threads in the group are moved to their respective default group
# 			(system threads to SYS_default, user threads to USR_default)
#
# The name and type attributes are set at group creation time and cannot be modified thereafter
# with ALTER_RESOURCE_GROUP.
#
# Examples:
#
# 		) Alter a group CPU affinity:
#
# 			ALTER RESOURCE GROUP rg1 VCPU = 0-63;
#
# 		) Alter a group thread priority:
#
# 			ALTER RESOURCE GROUP rg2 THREAD_PRIORITY = 5;
#
# 		) Disable a group, moving any threads assigned to it to the default groups:
#
# 			ALTER RESOURCE GROUP rg3 DISABLE FORCE;
#
# Resource group management is local to the server on which it occurs.
#
# ALTER_RESOURCE_GROUP statements are not written to the binary log and are not
# replicated.
#
# 13.7.2.2 CREATE RESOURCE GROUP SYNTAX
#
# 		CREATE RESOURCE GROUP group_name
# 			TYPE = {SYSTEM|USER}
# 			[VCPU [=] vcpu_spec [, vcpu_spec] ---]
# 			[THREAD_PRIORITY [=] N]
# 			[ENABLE|DISABLE]
#
# 		vcpu_spec: {N | M - N}
#
# CREATE_RESOURCE_GROUP is used for resource group management (see SECTION 8.12.5, "RESOURCE GROUPS")
#
# This statement creates a new resource group and assigns its initial attribute values.
#
# It requires the RESOURCE_GROUP_ADMIN privilege.
#
# group_name identifies which resource group to create. If the group already exists,
# an error occurs.
#
# The TYPE attribute is required. It should be SYSTEM for a system resource group,
# USER for a user resource group.
#
# The group type affects permitted THREAD_PRIORITY values, as described later.
#
# The VCPU attribute indicates the CPU affinity; that is, the set of virtual CPUs
# the group can use:
#
# 		) If VCPU is not given, the resource group has no CPU affinity and can use all available CPUs.
#
# 		) If VCPU is given, the attribute value is a list of comma-separated CPU numbers or ranges:
#
# 			) Each number must be an integer in the range from 0 to the number of CPUs - 1.
#
# 				For example, on a system with 64 CPUs, the number can range from 0 to 63.
#
# 			) A range is given in the form M - N, where M is less than or equal to N and both
# 				numbers are in the CPU range.
#
# 			) If a CPU number is an integer outside the permitted range or is not an integer,
# 				an error occurs.
#
# Example VCPU specifiers (these are all equivalent):
#
# 		VCPU = 0,1,2,3,9,10
# 		VCPU = 0-3,9-10
# 		VCPU = 9,10,0-3
# 		VCPU = 0,10,1,9,3,2
#
# The THREAD_PRIORITY attribute indicates the priority for threads assigned to the group:
#
# 		) If THREAD_PRIORITY is not given, the default priority is 0
#
# 		) If THREAD_PRIORITY is given, the attribute value must be in the range from -20 (highest priority)
# 			to 19 (lowest priority)
#
# 			The priority for system resource groups must be in the range from -20 to 0.
#
# 			The priority for user resource groups must be in the range from 0 to 19
#
# 			Use of different ranges for system and user groups ensures that user threads
# 			never have a higher priority than system threads.
#
# ENABLE and DISABLE specify that the resource group is initially enabled or disabled.
#
# If neither is specified, the group is enabled by default.
#
# A disabled group cannot have threads assigned to it.
#
# Examples:
#
# 		) Create an enabled user group that has a single GPU and the lowest priority:
#
# 			CREATE RESOURCE GROUP rg1
# 				TYPE = USER
# 				VCPU = 0
# 				THREAD_PRIORITY = 19;
#
# 		) Create a disabled system group that has no CPU affinity (can use all CPUs) and the
# 			highest priority:
#
# 				CREATE RESOURCE GROUP rg2
# 					TYPE = SYSTEM
# 					THREAD_PRIORITY = -20
# 					DISABLE;
#
# Resource group management is local to the server on which it occurs.
#
# CREATE_RESOURCE_GROUP statements are not written to the binary log and 
# are not replicated.
#
# 13.7.2.3 DROP RESOURCE GROUP SYNTAX
#
# 		DROP RESOURCE GROUP group_name [FORCE]
#
# DROP_RESOURCE_GROUP is used for resource group management (see SECTION 8.12.5, "RESOURCE GROUPS")
#
# This statement drops a resource group. It requires the RESOURCE_GROUP_ADMIN privilege.
#
# group_name identifies which resource group to drop. If the group does not exist, an error occurs.
#
# The FORCE modifier determines statement behavior if the resource group has any threads assigned to it:
#
# 		) If FORCE is not given and any threads are assigned to the group, an error occurs.
#
# 		) If FORCE is given, existing threads in the group are moved to their respective default group
# 			(system threads to SYS_default, user threads to USR_default)
#
# Examples:
#
# 		) Drop a group, failing if the group contains any threads:
#
# 			DROP RESOURCE GROUP rg1;
#
# 		) Drop a group and move existing threads to the default groups:
#
# 			DROP RESOURCE GROUP rg2 FORCE;
#
# Resource group management is local to the server on which it occurs.
#
# DROP_RESOURCE_GROUP statements are not written to the binary log and are not replicated.
#
# 13.7.2.4 SET RESOURCE GROUP SYNTAX
#
# 		SET RESOURCE GROUP group_name
# 			[FOR thread_id [, thread_id] ---]
#
# SET_RESOURCE_GROUP is used for resource group management (see SECTION 8.12.5, "RESOURCE GROUPS")
#
# This statement assigns threads to a resource group.
#
# It requires the RESOURCE_GROUP_ADMIN or RESOURCE_GROUP_USER privilege.
#
# group_name identifies which resource group to be assigned. Any thread_id values indicate
# threads to assign to the group.
#
# Thread IDs can be determined from the Performance Schema threads table.
#
# If the resource group or any named thread ID does not exist, an error occurs.
#
# With no FOR clause, the statement assigns the current thread for the session to the
# resource group.
#
# With a FOR clause that names thread IDs, the statement assigns those threads to the
# resource group.
#
# For attempts to assign a system thread to a user resource group or a user thread to
# a system resource group, a warning occurs.
#
# Examples:
#
# 		) Assign the current session thread to a group:
#
# 			SET RESOURCE GROUP rg1;
#
# 		) Assign the named threads to a group:
#
# 			SET RESOURCE GROUP rg2 FOR 14, 78, 4;
#
# Resource group management is local to the server on which it occurs.
#
# SET_RESOURCE_GROUP statements are not written to the binary log and are not replicated.
#
# An alternative to SET_RESOURCE_GROUP is the RESOURCE_GROUP optimizer hint, which assigns
# individual statements to a resource group.
#
# See SECTION 8.9.2, "OPTIMIZER HINTS"
#
# 13.7.3 TABLE MAINTENANCE STATEMENTS
#
# 13.7.3.1 ANALYZE TABLE SYNTAX
# 13.7.3.2 CHECK TABLE SYNTAX
#
# 13.7.3.3 CHECKSUM TABLE SYNTAX
# 13.7.3.4 OPTIMIZE TABLE SYNTAX
#
# 13.7.3.5 REPAIR TABLE SYNTAX
#
# 13.7.3.1 ANALYZE TABLE SYNTAX
#
# 		ANALYZE [NO_WRITE_TO_BINLOG | LOCAL]
# 			TABLE tbl_name [, tbl_name] ---
#
# 		ANALYZE [NO_WRITE_TO_BINLOG | LOCAL]
# 			TABLE tbl_name
# 			UPDATE HISTOGRAM ON col_name [, col_name] ---
# 				[WITH N BUCKETS]
#
# 		ANALYZE [NO_WRITE_TO_BINLOG | LOCAL]
# 			TABLE tbl_name
# 			DROP HISTOGRAM ON col_name [, col_name] ---
#
# ANALYZE_TABLE generates table statistics:
#
# 		) ANALYZE_TABLE without either HISTOGRAM clause performs a key distribution analysis
# 			and stores the distribution for the named table or tables.
#
# 			For MyISAM tables, ANALYZE_TABLE for key distribution analysis is equivalent to using
# 			myisamchk --analyze
#
# 		) ANALYZE_TABLE with the UPDATE HISTOGRAM clause generates histogram statistics for the named
# 			table columns and stores them in the data dictionary.
#
# 			Only one table name is permitted for this syntax
#
# 		) ANALYZE_TABLE with the DROP HISTOGRAM clause removes histogram statistics for the named table
# 			columns from the data dictionary.
#
# 			Only one table name is permitted for this syntax.
#
# NOTE:
#
# 		If the innodb_read_only system variable is enabled, ANALYZE_TABLE may fail because it
# 		cannot update statistics tables in the data dictionary, which use InnoDB.
#
# 		For ANALYZE_TABLE operations that update the key distribution, failure may occur even
# 		if the operation updates the table itself (for example, if it is a MyISAM table)
#
# 		To obtain the updated distribution statistics, set information_schema_stats_expiry=0
#
# This statement requires SELECT and INSERT privileges for the table.
#
# ANALYZE_TABLE works with InnoDB, NDB and MyISAM tables. It does not work with views.
#
# ANALYZE_TABLE is supported for partitioned tables, and you can use ALTER TABLE --- ANALYZE PARTITION
# to analyze one or more partitions, for more information, see SECTION 13.1.9, "ALTER TABLE SYNTAX"
# and SECTION 23.3.4, "MAINTENANCE OF PARTITIONS"
#
# During the analysis, the table is locked with a read lock for InnoDB and MyISAM.
#
# By default, the server writes ANALYZE_TABLE statements to the binary log so that they replicate
# to replication slaves.
#
# To suppress logging, specify the optional NO_WRITE_TO_BINLOG keyword or its alias LOCAL.
#
# 		) ANALYZE TABLE OUTPUT
#
# 		) KEY DISTRIBUTION ANALYSIS
#
# 		) HISTOGRAM STATISTICS ANALYSIS
#
# ANALYZE TABLE OUTPUT
#
# ANALYZE_TABLE returns a result set with the columns shown in the following table.
#
# 		COLUMN 					VALUE
#
# 		Table 					The table name
#
# 		Op 						analyze or histogram
#
# 		Msg_type 				status, error, info, note or warning
#
# 		Msg_text 				An informational message
#
# KEY DISTRIBUTION ANALYSIS
#
# ANALYZE_TABLE without either HISTOGRAM clause performs a key distribution analysis
# and stores the distribution for the table or tables.
#
# Any existing histogram statistics remain unaffected.
#
# If the table has not changed since the last key distribution analysis,
# the table is not analyzed again.
#
# MySQL uses the stored key distribution to decide the order in which tables should be
# joined for joins on something other than a constant.
#
# In addition, key distributions can be used when deciding which indexes to use
# for a specific table within a query.
#
# For more information on how key distribution analysis works within InnoDB,
# see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
# and SECTION 15.8.10.3, "ESTIMATING ANALYZE TABLE COMPLEXITY FOR INNODB TABLES"
#
# Also see SECTION 15.6.1.6, "LIMITS ON INNODB TABLES"
#
# In particular, when you enable the innodb_stats_persistent option,
# you must run ANALYZE_TABLE after loading substansial data into an InnoDB
# table, or creating a new index for one.
#
# To check the stored key distribution cardinality, use the SHOW_INDEX statement
# or the INFORMATION_SCHEMA STATISTICS table.
#
# See SECTION 13.7.6.22, "SHOW INDEX SYNTAX", and SECTION 25.25, "THE INFORMATION_SCHEMA STATISTICS TABLE"
#
# HISTOGRAM STATISTICS ANALYSIS
#
# ANALYZE_TABLE with the HISTOGRAM clauses enables management of histogram statistics for table
# column values.
#
# For information about histogram statistics, see SECTION 8.9.6, "OPTIMIZER STATISTICS"
#
# These histogram operations are available:
#
# 		) ANALYZE_TABLE with an UPDATE HISTOGRAM clause generates histogram statistics for the
# 			named table columns and stores them in the data dictionary.
#
# 			Only one table name is permitted for this syntax.
#
# 			The optional WITH N BUCKETS clauses specifies the number of buckets for the histogram.
#
# 			The value of N must be an integer in the range from 1 to 1024.
#
# 			If this clause is omitted, the number of buckets is 100.
#
# 		) ANALYZE_TABLE with a DROP HISTOGRAM clause removes histogram statistics for the named
# 			table columns from the data dictionary.
#
# 			Only one table name is permitted for this syntax.
#
# Stored histogram management statements affect only the named columns.
#
# Consider these statements:
#
# 		ANALYZE TABLE t UPDATE HISTOGRAM ON c1, c2, c3 WITH 10 BUCKETS;
# 		ANALYZE TABLE t UPDATE HISTOGRAM ON c1, c3 WITH 10 BUCKETS;
# 		ANALYZE TABLE t DROP HISTOGRAM ON c2;
#
# The first statement updates the histograms for columns c1, c2, and c3, replacing
# any existing histograms for those columns.
#
# The second statement updates the histograms for c1 and c3, leaving the c2 histogram
# unaffected.
#
# The third statement removes the histogram for c2, leaving those for c1 and c3 unaffected.
#
# Histogram generation is not supported for encrypted tables (to avoid exposing data in the statistics) or TEMPORARY tables.
#
# Histogram generation applies to columns of all data types except geometry types (spatial data) and JSON.
#
# Histograms can be generated for stored and virtual generated columns.
#
# Histograms cannot be generated for columns that are covered by single-column unique indexes.
#
# Histogram management statements attempt to perform as much of the requested operation as possible,
# and report diagnostic messages for the remainder.
#
# For example, if an UPDATE HISTOGRAM statement names multiple columns, but some of them do not
# exist or have an unsupported data type, histograms are generated for the other columns,
# and messages are produced for the invalid columns.
#
# The histogram_generation_max_mem_size system variable controls the maximum amount of memory
# available for histogram generation.
#
# The global and session values may be set at runtime.
#
# Changing the global histogram_generation_max_mem_size value requires privileges
# sufficient to set global system variables.
#
# Changing the session histogram_generation_max_mem_size value requires privileges
# sufficient to set restricted session system variables.
#
# See SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES"
#
# For information about memory allocations performed for histogram generation, monitor
# the Performance Schema memory/sql/histograms instrument.
#
# See SECTION 26.12.16.10, "MEMORY SUMMARY TABLES"
#
# Histograms are affected by these DDL statements:
#
# 		) DROP_TABLE removes histograms for columns in the dropped table
#
# 		) DROP_DATABASE removes histograms for any table in the dropped database because the statement drops all tables in the database
#
# 		) RENAME_TABLE does not remove histograms. Instead, it renames histograms for the renamed table to be associated with the new table name
#
# 		) ALTER_TABLE statements that remove or modify a column remove histograms for that column
#
# 		) ALTER_TABLE_---_CONVERT_TO_CHARACTER_SET removes histograms for character columns because they are
# 			affected by the change of character set.
#
# 			Histograms for noncharacter columns remain unaffected.
#
# 13.7.3.2 CHECK TABLE SYNTAX
#
# 		CHECK TABLE tbl_name [, tbl_name] --- [option] ---
#
# 		option: {
# 			FOR UPGRADE
# 		 | QUICK
# 		 | FAST
# 		 | MEDIUM
# 		 | EXTENDED
# 		 | CHANGED
# 		}
#
# CHECK_TABLE checks a table or tables for errors.
#
# CHECK_TABLE can also check views for problems, such as tables that are referenced
# in the view definition that no longer exist.
#
# To check a table, you must have some privilege for it.
#
# CHECK_TABLE works for InnoDB, MyISAM, ARCHIVE and CSV tables.
#
# Before running CHECK_TABLE on InnoDB tables, see CHECK TABLE USAGE NOTES FOR INNODB TABLES.
#
# CHECK_TABLE is supported for partitioned tables, and you can use ALTER TABLE --- CHECK PARTITION
# to check one or more partitions; for more information, see SECTION 13.1.9, "ALTER TABLE SYNTAX",
# and SECTION 23.3.4, "MAINTENANCE OF PARTITIONS"
#
# CHECK_TABLE ignores virtual generated columns that are not indexed.
#
# 		) CHECK TABLE OUTPUT
#
# 		) CHECKING VERSION COMPATIBILITY
#
# 		) CHECKING DATA CONSISTENCY
#
# 		) CHECK TABLE USAGE NOTES FOR INNODB TABLES
#
# 		) CHECK TABLE USAGE NOTES FOR MYISAM TABLES
#
# CHECK TABLE OUTPUT
#
# CHECK_TABLE returns a result set with the columns shown in the following table.
#
# 		COLUMN 				VALUE
#
# 		Table 				The table name
#
# 		Op 					Always check
#
# 		Msg_type 			status, error, info, note or warning
#
# 		Msg_text 			An informational message
#
# The statement might produce many rows of information for each checked table.
#
# The last row has a Msg_type value of status and the Msg_text normally should be
# OK.
#
# Table is already up to date means that the storage engine for the table indicated
# that there was no need to check the table.
#
# CHECKING VERSION COMPATIBILITY
#
# The FOR UPGRADE option checks whether the named tables are compatible with the
# current version of MySQL.
#
# With FOR UPGRADE, the server checks each table to determine whether there
# have been any incompatible changes in any of the table's data types or indexes
# since the table was created.
#
# If not, the check succeeds.
#
# Otherwise, if there is a possible incompatibility, the server runs a full check
# on the table (which might take some time)
#
# Incompatibilities might occur because the storage format for a data type has
# changed or because its sort order has changed.
#
# Our aim is to avoid these changes, but occasionally they are necessary to correct
# problems that would be worse than an incompatibility between releases.
#
# FOR UPGRADE discovers these incompatibilities:
#
# 		) The indexing order for end-space in TEXT columns for InnoDB and MyISAM tables
# 			changed between MySQL 4.1 and MySQL 5.0
#
# 		) The storage method of the new DECIMAL data type changed between MySQL 5.0.3 and 5.0.5
#
# 		) Changes are sometimes made to character sets or collations that require table indexes
# 			to be rebuilt.
#
# 			For details about such changes, see SECTION 2.11.1.3, "CHANGES IN MYSQL 8.0"
#
# 			For information about rebuilding tables, see SECTION 2.11.3, "REBUILDING OR REPAIRING TABLES OR INDEXES"
#
# 		) MySQL 8.0 does not support the YEAR(2) data type permitted in older versions of MySQL.
#
# 			For tables containing YEAR(2) columns, CHECK_TABLE recommends REPAIR_TABLE, which converts
# 			YEAR(2) to YEAR(4)
#
# 		) Trigger creation time is maintained.
#
# 		) A table is reported as needing a rebuild if it contains old temporal columns in pre
# 			5.6.4 format (TIME, DATETIME, and TIMESTAMP columns without support for fractional
# 			seconds precision) and the avoid_temporal_upgrade system variable is disabled.
#
# 			This helps mysql_upgrade detect and upgrade tables containing old temporal
# 			columns.
#
# 			If avoid_temporal_upgrade is enabled, FOR UPGRADE ignores the old temporal
# 			columns present in the table; consequently, mysql_upgrade does not upgrade them.
#
# 			To check for tables that contain such temporal columns and need a rebuild,
# 			disable avoid_temporal_upgrade before executing CHECK_TABLE_---_FOR_UPGRADE
#
# 		) Warnings are issued for tables that use nonnative partitioning because nonnative
# 			partitioning is removed in MySQL 8.0
#
# 			See CHAPTER 23, PARTITIONING
#
# CHECKING DATA CONSISTENCY
#
# The following table shows the other check options that can be given.
#
# These options are passed to the storage engine, which may use or ignore them.
#
# 		TYPE 					MEANING
#
# 		QUICK 				Do not scan the rows to check for incorrect links. Applies to InnoDB and MyISAM tables and views.
#
# 		FAST 					Check only tables that have not been closed properly. Ignored for InnoDB; applies only to MyISAM tables and views.
#
# 		CHANGED 				Check only tables that have been changed since the last check or that have not been closed
# 								properly.
#
# 								Ignored for InnoDB; applies only to MyISAM tables and views.
#
# 		MEDIUM 				Scan rows to verify that deleted links are valid.
#
# 								This also calculates a key checksum for the rows and verifies this with
# 								a calculated checksum for the keys.
#
# 								Ignored for InnoDB; applies only to MyISAM tables and views.
#
# 		EXTENDED 			Do a full key lookup for all keys for each row.
#
# 								This ensures that the table is 100% consistent, but takes a long time.
# 								Ignored for InnoDB; applies only to MyISAM tables and views.
#
# You can combine check options, as in the following example that does a quick check on the table
# to determine whether it was closed properly:
#
# 		CHECK TABLE test_table FAST QUICK;
#
# NOTE:
#
# 		If CHECK_TABLE finds no problems with a table that is marked as "corrupted" or
# 		"not closed properly", CHECK_TABLE may remove the mark.
#
# If a table is corrupted, the problem is most likely in the indexes and not in teh data part.
#
# All of the preceeding check types check the indexes thoroughly and should thus find
# most errors.
#
# To check a table that you assume is okay, use no check options or the QUICK option.
#
# The latter should be used when you are in a hurry and can take the very small
# risk that QUICK does not find an error in the data file.
#
# (In most cases, under normal usage, MySQL should find any error in the data file.
#
# If this happens, the table is marked as "corrupted" and cannot be used until it is
# repaired.)
#
# FAST and CHANGED are mostly intended to be used from a script (for example, to be executed
# from cron) to check tables periodically.
#
# In most cases, FAST is to be preferred over CHANGED. (The only case when it is not preferred
# is when you suspect that you have found a bug in the MyISAM code)
#
# EXTENDED is to be used only after you have run a normal check but still get errors from
# a table when MySQL tries to update a row or find a row by key.
#
# This is very unlikely if a normal check has succeeded.
#
# Use of CHECK_TABLE_---_EXTENDED might influence execution plans generated by the query optimizer.
#
# Some problems reported by CHECK_TABLE cannot be corrected automatically:
#
# 		) Found row where the auto_increment column has the value 0
#
# 			This means that you have a row in the table where the AUTO_INCREMENT index column
# 			contains the value 0.
#
# 			(It is possible to create a row where the AUTO_INCREMENT column is 0 by explicitly
# 			setting the column to 0 with an UPDATE statement)
#
# 			This is not an error in itself, but could cause trouble if you decide to dump the
# 			table and restore it or do an ALTER_TABLE on the table.
#
# 			In this case, the AUTO_INCREMENT column changes value according to the rules
# 			of AUTO_INCREMENT columns, which could cause problems such as a duplicate-key
# 			error.
#
# 			To get rid of the warning, execute an UPDATE statement to set the column to some value
# 			other than 0.
#
# CHECK TABLE USAGE NOTES FOR INNODB TABLES
#
# The following notes apply to InnoDB tables:
#
# 		) If CHECK_TABLE encounters a corrupt page, the server exits to prevent error propagation
# 			(Bug #10132)
#
# 			If the corruption occurs in a secondary index but table data is readable, running
# 			CHECK_TABLE can still cause a server exit.
#
# 		) If CHECK_TABLE encounters a corrupted DB_TRX_ID or DB_ROLL_PTR field in a clustered index,
# 			CHECK_TABLE can cause InnoDB to access an invalid undo log record, resulting
# 			in an MVCC-related server exit.
#
# 		) If CHECK_TABLE encounters errors in InnoDB tables or indexes, it reports an error, and usually
# 			marks the index and sometimes marks the table as corrupted, preventing further use
# 			of the index or table.
#
# 			Such errors include an incorrect number of entries in a secondary index or incorrect links.
#
# 		) If CHECK_TABLE finds an incorrect number of entries in a secondary index, it reports an error
# 			but does not cause a server exit or prevent access to the file.
#
# 		) CHECK_TABLE surveys the index page structure, then surveys each key entry.
#
# 			It does not validate the key pointer to a clustered record or follow
# 			the path for BLOB pointers.
#
# 		) When an InnoDB table is stored in its own .ibd file, the first 3 pages of the
# 			.ibd file contain header information rather than table or index data.
#
# 			The CHECK_TABLE statement does not detect inconsistencies that affect only
# 			the header data.
#
# 			To verify the entire contents of an InnoDB .ibd file, use the innochecksum
# 			command.
#
# 		) When running CHECK_TABLE on large InnoDB tables, other threads may be blocked
# 			during CHECK_TABLE execution.
#
# 			To avoid timeouts, the semaphore wait threshold (600 seconds) is extended
# 			by 2 hours (7200 seconds) for CHECK_TABLE operations.
#
# 			If InnoDB detects semaphore waits of 240 seconds or more, it starts
# 			printing InnoDB monitor output to the error log.
#
# 			If a lock request extends beyond the semaphore wait threshold, InnoDB
# 			aborts the process.
#
# 			To avoid the possibility of a semaphore wait timeout entirely, run
# 			CHECK_TABLE_QUICK instead of CHECK_TABLE
#
# 		) CHECK_TABLE functionality for InnoDB SPATIAL indexes includes an R-tree
# 			validity check and a check to ensure that the R-tree row count matches
# 			the clustered index.
#
# 		) CHECK_TABLE supports secondary indexes on virtual generated columns, which
# 			are supported by InnoDB.
#
# 		) As of MySQL 8.0.14, InnoDB supports parallel index reads, which helps improve
# 			CHECK_TABLE performance.
#
# 			The innodb_parallel_read_threads session variable must be set to a value
# 			greater than 1 for parallel index reads to occur.
#
# 			THe default value is 4.
#
# 			The actual number of threads used to perform a parallel index read
# 			is determined by the innodb_parallel_read_threads setting or the
# 			number of index subtrees to scan, whichever is smaller.
#
# 			The pages read into the buffer pool during the scan are kept at the tail
# 			of the buffer pool LRU list so that they can be discarded quickly when
# 			free buffer pool pages are required.
#
# CHECK TABLE USAGE NOTES FOR MYISAM TABLES
#
# The following notes apply to MyISAM tables:
#
# 		) CHECK_TABLE updates key statistics for MyISAM tables
#
# 		) If CHECK_TABLE output does not return OK or Table is already up to date,
# 			you should normally run a repair of the table.
#
# 			See SECTION 7.6, "MyISAM TABLE MAINTENANCE AND CRASH RECOVERY"
#
# 		) If none of the CHECK_TABLE options QUICK, MEDIUM or EXTENDED are specified,
# 			the default check type for dynamic-format MyISAM tables is MEDIUM.
#
# 			This has the same result as running myisamchk --medium-check tbl_name
# 			on the table.
#
# 			The default check type also is MEDIUM for static-format MyISAM tables,
# 			unless CHANGED or FAST is specified.
#
# 			In that case, the default is QUICK.
#
# 			The row scan is skipped for CHANGED and FAST because the rows are
# 			very seldom corrupted.
#
# 13.7.3.3 CHECKSUM TABLE SYNTAX
#
# 		CHECKSUM TABLE tbl_name [, tbl_name] --- [QUICK | EXTENDED]
#
# CHECKSUM_TABLE reports a checksum for the contents of a table.
#
# You can use this statement to verify that the contents are the same before
# and after a backup, rollback, or other operation that is intended to put
# the data back to a known state.
#
# This statement requires the SELECT privilege for the table.
#
# This statement is not supported for views. If you run CHECKSUM_TABLE against
# a view, the Checksum value is always NULL, and a warning is returned.
#
# For a nonexistent table, CHECKSUM_TABLE returns NULL and generates a warning
#
# During the checksum operation, the table is locked with a read lock for InnoDB
# and MyISAM.
#
# PERFORMANCE CONSIDERATIONS
#
# By default, the entire table is read row by row and the checksum is calculated.
#
# For large tables, this could take a long time, thus you would only perform this
# operation occasionally.
#
# This row-by-row calculation is what you get with the EXTENDED clause, with InnoDB
# and all other storage engines other than MyISAM, and with MyISAM tables not created
# with the CHECKSUM=1 clause.
#
# For MyISAM tables created with the CHECKSUM=1 clause, CHECKSUM_TABLE or CHECKSUM_TABLE_---_QUICK
# returns the "live" table checksum that can be returned very fast.
#
# If the table does not meet all these conditions, the QUICK method returns NULL.
#
# THe QUICK method is not supported with InnoDB tables. See SECTION 13.1.20, "CREATE TABLE SYNTAX"
# for the syntax of the CHECKSUM clause.
#
# The checksum value depends on the table row format. If the row format changes, the checksum
# also changes.
#
# For example, the storage format for temporal types such as TIME, DATETIME and TIMESTAMP
# changed in MySQL 5.6 prior to MySQL 5.6.5, so if a 5.5 table is upgraded to MySQL 5.6,
# the checksum value may change.
#
# IMPORTANT:
#
# 		If the checksums for two tables are different, then it is almost certain that
# 		the tables are different in some way.
#
# 		However, because the hashing function used by CHECKSUM_TABLE is not guaranteed
# 		to be collision-free, there is a slight chance that two tables which are not
# 		identical can produce the same checksum.
#
# 13.7.3.4 OPTIMIZE TABLE SYNTAX
#
# 		OPTIMIZE [NO_WRITE_TO_BINLOG | LOCAL]
# 			TABLE tbl_name [, tbl_name] ---
#
# OPTIMIZE_TABLE reorganizes the physical storage of table data and associated
# index data, to reduce storage space and improve I/O efficiency when accessing
# the table.
#
# The exact changes made to each table depend on the storage engine used by that
# table.
#
# Use OPTIMIZE_TABLE in these cases, depending on the type of table:
#
# 		) After doing substansial insert, update or delete operations on an
# 			InnoDB table that has its own .ibd file because it was created with the
# 			innodb_file_per_table option enabled.
#
# 			The table and indexes are reorganized, and disk space can be reclaimed
# 			for use by the operating system.
#
# 		) After doing substansial insert, update or delete operations on columns
# 			that are part of a FULLTEXT index in an InnoDB table.
#
# 			Set the configuration option innodb_optimize_fulltext_only=1 first
#
# 			To keep the index maintenance period to a reasonable time, set the
# 			innodb_ft_num_word_optimize option to specify how many words to update
# 			in the search index, and run a sequence of OPTIMIZE TABLE statements
# 			until the search index is fully updated.
#
# 		) After deleting a large part of a MyISAM or ARCHIVE table, or making many
# 			changes to a MyISAM or ARCHIVE table with variable-length rows
# 			(tables that have VARCHAR, VARBINARY, BLOB or TEXT columns)
#
# 			Deleted rows are maintained in a linked list and subsequent INSERT
# 			operations reuse old row positions.
#
# 			You can use OPTIMIZE TABLE to reclaim the unused space and to
# 			defragment the data file.
#
# 			After extensive changes to a table, this statement may also
# 			improve performance of statements that use the table, 
# 			sometimes significantly.
#
# This statement requires SELECT and INSERT privileges for the table.
#
# OPTIMIZE_TABLE works for InnoDB, MyISAM and ARCHIVE tables.
#
# OPTIMIZE_TABLE is also supported for dynamic columns of in-memory NDB
# tables.
#
# It does not work for fixed-width columns of in-memory tables, nor does it
# work for Disk Data tables.
#
# The performance of OPTIMIZE on NDB Cluster tables can be tuned using
# --ndb_optimization_delay, which controls the length of time to wait
# between processing batches of rows by OPTIMIZE_TABLE.
#
# For more information, see PREVIOUS NDB CLUSTER ISSUES RESOLVED IN NDB
# CLUSTER 7.3
#
# For NDB Cluster tables, OPTIMIZE_TABLE can be interuppted by (for example)
# killing the SQL thread performing the OPTIMIZE operation.
#
# By default, OPTIMIZE_TABLE does not work for tables created using any
# other storage engine and returns a result indicating this lack of support.
#
# You can make OPTIMIZE TABLE work for other storage engines by starting
# mysqld with the --skip-new option.
#
# In this case, OPTIMIZE_TABLE is just mapped to ALTER_TABLE
#
# This statement does not work with views
#
# OPTIMIZE_TABLE is supported for partitioned tables. For information about using
# this statement with partitioned tables and table partitions, see SECTION 23.3.4,
# "MAINTENANCE OF PARTITIONS"
#
# By default, the server writes OPTIMIZE_TABLE statements to the binary log so that
# they replicate to replication slaves.
#
# To suppress logging, specify the optional NO_WRITE_TO_BINLOG keyword or its
# alias LOCAL.
#
# 		) OPTIMIZE TABLE OUTPUT
#
# 		) INNODB DETAILS
#
# 		) MYISAM DETAILS
#
# 		) OTHER CONSIDERATIONS
#
# OPTIMIZE TABLE OUTPUT
#
# OPTIMIZE_TABLE returns a result set with the columns shown in the following table.
#
# 		COLUMN 				VALUE
#
# 		Table 				The table name
#
# 		Op 					Always optimize
#
# 		Msg_type 			status, error, info, note, or warning
#
# 		Msg_text 			An informational message
#
# OPTIMIZE_TABLE table catches and throws any errors that occur while copying
# table statistics from the old file to the newly created file.
#
# For example, if the user ID of the owner of the .MYD or .MYI file is
# different from the user ID of the mysqld process, OPTIMIZE_TABLE generates
# a "cannot change ownership of the file" error unless mysqld is started
# by the root user.
#
# INNODB DETAILS
#
# For InnoDB tables, OPTIMIZE_TABLE is mapped to ALTER_TABLE_---_FORCE,
# which rebuilds the table to update index statistics and free unused
# space in the clustered index.
#
# THis is displayed in the output of OPTIMIZE_TABLE when you run it on
# an InnoDB table, as shown here:
#
# 		OPTIMIZE TABLE foo;
# 		+--------------+------------------+-------------------+-------------------------------------------------------------------+
# 		| Table 			| Op 					 | Msg_type 			| Msg_text 																 			  |
# 		+--------------+------------------+-------------------+-------------------------------------------------------------------+
# 		| test.foo 		| optimize 			 | note 					| Table does not support optimize, doing recreate + analyze instead |
# 		| test.foo 		| optimize 			 | status 				| OK 																					  |
# 		+--------------+------------------+-------------------+-------------------------------------------------------------------+
#
# OPTIMIZE_TABLE uses online DDL for regular and partitioned InnoDB tables, which reduces downtime
# for concurrent DML operations.
#
# The table rebuild triggered by OPTIMIZE_TABLE and performed under the cover by ALTER_TABLE_---_FORCE is completed
# in place.
#
# An exclusive table lock is only taken briefly during the prepare phase and the commit phase of the operation.
#
# During the prepare phase, metadata is updated and an intermediate table is created.
#
# During the commit phase, table metadata changes are committed.
#
# OPTIMIZE_TABLE rebuilds the table using the table copy method under the following conditions:
#
# 		) When the old_alter_table system variable is enabled
#
# 		) When the mysqld --skip-new option is enabled
#
# OPTIMIZE_TABLE using online DDL is not supported for InnoDB tables that contain
# FULLTEXT indexes.
#
# The table copy method is used instead.
#
# InnoDB stores data using a page-allocation method and does not suffer from fragmentation
# in the same way that legacy storage engines (such as MyISAM) will.
#
# When considering whether or not to run optimize, consider the workload of transactions
# that your server will process:
#
# 		) Some level of fragmentation is expected. InnoDB only fills pages 93% full, to leave room
# 			for updates without having to split pages.
#
# 		) Delete operations might leave gaps that leave gaps less filled than desired, which could make
# 			it worthwhile to optimize the table
#
# 		) Updates to rows usually rewrite the data within the same page, depending on the data type
# 			and row format, when sufficient space is available.
#
# 			See SECTION 15.9.1.5, "HOW COMPRESSION WORKS FOR INNODB TABLES" and
# 			SECTION 15.10, "INNODB ROW FORMATS"
#
# 		) High-concurrency workloads might leave gaps in indexes over time, as InnoDB
# 			retains multiple versions of the same data due through its MVCC
# 			mechanism.
#
# 			See SECTION 15.3, "INNODB MULTI-VERSIONING"
#
# MYISAM DETAILS
#
# For MyISAM tables, OPTIMIZE_TABLE works as follows:
#
# 		1. If the table has deleted or split rows, repair the table
#
# 		2. If the index pages are not sorted, sort them
#
# 		3. If the table's statistics are not up to date (and the repair could not be accomplished by sorting the index),
# 			update them.
#
# OTHER CONSIDERATIONS
#
# OPTIMIZE_TABLE is performed online for regular and partitioned InnoDB tables.
#
# Otherwise, MySQL locks the table during the time OPTIMIZE_TABLE is running.
#
# OPTIMIZE_TABLE does not sort R-tree indexes, such as spatial indexes on POINT columns.
# (Bug #23578)
#
# 13.7.3.5 REPAIR TABLE SYNTAX
#
# 		REPAIR [NO_WRITE_TO_BINLOG | LOCAL]
# 			TABLE tbl_name [, tbl_name] ---
# 			[QUICK] [EXTENDED] [USE_FRM]
#
# REPAIR_TABLE repairs a possibly corrupted table, for certain storage engines only.
#
# This statement requires SELECT and INSERT privileges for the table.
#
# Although normally you should never have to run REPAIR_TABLE, if disaster strikes,
# This statement is very likely to get back all your data from a MyISAM table.
#
# If your tables become corrupted often, try to find the reason for it, to
# eliminate the need to use REPAIR_TABLE
#
# See SECTION B.6.3.3, "WHAT TO DO IF MYSQL KEEPS CRASHING", and SECTION 16.2.4, "MYISAM TABLE PROBLEMS"
#
# REPAIR_TABLE checks the table to see whether an upgrade is required.
#
# If so, it performs the upgrade, following the same rule as CHECK_TABLE_---_FOR_UPGRADE
#
# See SECTION 13.7.3.2, "CHECK TABLE SYNTAX", for more information.
#
# IMPORTANT:
#
# 		) Make a backup of a table before performing a table repair operation; under some circumstances
# 			the operation might cause data loss.
#
# 			Possible causes include but are not limited to file system errors.
#
# 			See CHAPTER 7, BACKUP AND RECOVERY
#
# 		) If the server crashes during a REPAIR_TABLE operation, it is essential after restarting it
# 			that you immediately execute another REPAIR_TABLE statement for the table before performing
# 			any other operations on it.
#
# 			In the worst case, you might have a new clean index file without information
# 			about the data file, and then the next operation you perform could overwrite
# 			the data file.
#
# 			This is an unlikely but possible scenario that underscores the value of making a backup first.
#
# 		) In the event that a table on the master becomes corrupted and you run REPAIR_TABLE on it,
# 			any resulting changes to the original table are not propagated to slaves.
#
# 	) REPAIR TABLE STORAGE ENGINE AND PARTITIONING SUPPORT
#
# 	) REPAIR TABLE OPTIONS
#
# 	) REPAIR TABLE OUTPUT
#
# 	) TABLE REPAIR CONSIDERATIONS
#
# REPAIR TABLE STORAGE ENGINE AND PARTITIONING SUPPORT
#
# REPAIR_TABLE works for MyISAM, ARCHIVE and CSV tables.
#
# For MyISAM tables, it has the same effect as myisamchk --recover tbl_name by default.
#
# This statement does not work with views.
#
# REPAIR_TABLE is supported for partitioned tables. However, the USE_FRM option cannot be
# used with this statement on a partitioned table.
#
# You can use ALTER TABLE --- REPAIR PARTITION to repair one or more partitions; for more 
# information, see SECTION 13.1.9, "ALTER TABLE SYNTAX" and SECTION 23.3.4, "MAINTENANCE OF PARTITIONS"
#
# REPAIR TABLE OPTIONS
#
# 		) NO_WRITE_TO_BINLOG or LOCAL
#
# 			By default, the server writes REPAIR_TABLE statements to the binary log so that they
# 			replicate to replication slaves.
#
# 			To suppress logging, specify the optional NO_WRITE_TO_BINLOG keyword or its alias LOCAL
#
# 		) QUICK
#
# 			If you use the QUICK option, REPAIR_TABLE tries to repair only the index file, and not the
# 			data file.
#
# 			This type of repair is like that done by myisamchk --recover --quick
#
# 		) EXTENDED
#
# 			If you use the EXTENDED option, MySQL creates the index row instead of creating
# 			one index at a time with sorting.
#
# 			This type of repair is like that done by myisamchk --safe-recover
#
# 		) USE_FRM
#
# 			the USE_FRM option is available for use if the .MYI index file is missing or if its header
# 			is corrupted.
#
# 			This option tells MySQL not to trust the information in the .MYI file header
# 			and to re-create it using information from the data dictionary.
#
# 			This kind of repair cannot be done with myisamchk.
#
# 			CAUTION:
#
# 				Use the USE_FRM option only if you cannot use regular REPAIR modes.
#
# 				Telling the server to ignore the .MYI file makes important table metadata
# 				stored in the .MYI unavailable to the repair process, which can have deleterious
# 				consequences:
#
# 					) The current AUTO_INCREMENT value is lost
#
# 					) The link to deleted records in the table is lost, which means that free space
# 						for deleted records will remain unoccupied thereafter.
#
# 					) The .MYI header indicates whether the table is compressed. If the server
# 						ignores this information, it cannot tell that a table is compressed and
# 						repair can cause change or loss of table contents.
#
# 						This means that USE_FRM should not be used with compressed tables.
#
# 						That should not be necessary, anyway: Compressed tables are read only,
# 						so they should not become corrupt.
#
# 				If you use USE_FRM for a table that was created by a different version of the
# 				MySQL server than the one you are currently running, REPAIR_TABLE does not attempt
# 				to repair the table.
#
# 				In this case, the result set returned by REPAIR_TABLE contains a line with a 
# 				Msg_type value of error and a Msg_text value of Failed repairing incompatible .FRM file
#
# 				If USE_FRM is used, REPAIR_TABLE does not check the table to see whether an upgrade is required.
#
# REPAIR TABLE OUTPUT
#
# REPAIR_TABLE returns a result set with the columns shown in the following table.
#
# 		COLUMN 				VALUE
#
# 		Table 				The table name
#
# 		Op 					Always repair
#
# 		Msg_type 			status, error, info, note or warning
#
# 		Msg_text 			An informational message
#
# The REPAIR_TABLE statement might produce many rows of information for each repaired table.
#
# The last row has a Msg_type value of status and Msg_test normally should be OK.
#
# For a MyISAM table, if you do not get OK, you should try repairing it with myisamchk --safe-recover
#
# (REPAIR_TABLE does not implement all the options of myisamchk. With myisamchk --safe-recover, you can
# also use options that REPAIR_TABLE does not support, such as --max-record-length)
#
# REPAIR_TABLE table catches and throws any errors that occur while copying table statistics from the old
# corrupted file to the newly created file.
#
# For example, if the user ID of the owner of the .MYD or .MYI file is different from the user ID of the
# mysqld process, REPAIR_TABLE generates a "cannot change ownership of the file" error unless mysqld
# is started by the root user.
#
# TABLE REPAIR CONSIDERATIONS
#
# REPAIR_TABLE upgrades a table if it contains old temporal columns in pre-5.6.4 format
# (TIME, DATETIME, and TIMESTAMP columns without support for fractional seconds precision)
# and the avoid_temporal_upgrade system variable is disabled.
#
# If avoid_temporal_upgrade is enabled, REPAIR_TABLE ignores the old temporal columns present
# in the table and does not upgrade them.
#
# To upgrade tables that contain such temporal columns, disable avoid_temporal_upgrade before
# executing REPAIR_TABLE
#
# You may be able to increase REPAIR_TABLE performance by setting certain system variables,
# see SECTION 8.6.3, "OPTIMIZING REPAIR TABLE STATEMENTS"
#
# 13.7.4 COMPONENT, PLUGIN, AND USER-DEFINED FUNCTION STATEMENTS
#
# 	13.7.4.1 CREATE FUNCTION SYNTAX FOR USER-DEFINED FUNCTIONS
# 	13.7.4.2 DROP FUNCTION SYNTAX
#
# 	13.7.4.3 INSTALL COMPONENT SYNTAX
# 	13.7.4.4 INSTALL PLUGIN SYNTAX
#
# 	13.7.4.5 UNINSTALL COMPONENT SYNTAX
# 	13.7.4.6 UNINSTALL PLUGIN SYNTAX
#
# 13.7.4.1 CREATE FUNCTION SYNTAX FOR USER-DEFINED FUNCTIONS
#
# 		CREATE [AGGREGATE] FUNCTION function_name
# 			RETURNS {STRING|INTEGER|REAL|DECIMAL}
# 			SONAME shared_library_name
#
# A user-defined function (UDF) is a way to extend MySQL with a new function that
# works like a native (built-in) MySQL function such as ABS() or CONCAT()
#
# function_name is the name that should be used in SQL statements to invoke the function.
#
# The RETURNS clause indicates the type of the function's return value.
#
# DECIMAL is a legal value after RETURNS, but currently DECIMAL functions return string
# values and should be written like STRING functions.
#
# shared_library_name is the base name of the shared library file that contains the code that
# implements the function.
#
# The file must be located in the plugin directory. This directory is given by the value 
# of the plugin_dir system variable.
#
# For more information, see SECTION 5.7.1, "INSTALLING AND UNINSTALLING USER-DEFINED FUNCTIONS"
#
# To create a function, you must have the INSERT privilege for the mysql system database.
#
# This is necessary because CREATE_FUNCTION adds a row to the mysql.func system table that
# records the function's name, type, and shared library name.
#
# UDFs registered using CREATE_FUNCTION are listed in the Performance Schema user_defined_functions
# table; See SECTION 26.12.17.6, "THE USER_DEFINED_FUNCTIONS TABLE"
#
# An active function is one that has been loaded with CREATE_FUNCTION and not removed with DROP_FUNCTION.
#
# All active functions are reloaded each time the server starts, unless you start mysqld
# with the --skip-grant-tables option. In this case, UDF initialization is skipped and
# UDFs are unavailable.
#
# For instructions on writing user-defined functions, see SECTION 29.4.2, "ADDING A NEW USER-DEFINED FUNCTION"
#
# For the UDF mechanism to work, functions must be written in C or C++ (or another language that can use
# C calling conventions), your operating system must support dynamic loading and you must have
# compiled mysqld dynamically (not statically)
#
# An AGGREGATE function works exactly like a native MySQL aggregate (summary) function such as
# SUM or COUNT()
#
# NOTE:
#
# 		To upgrade the shared library associated with a UDF, issue a DROP_FUNCTION statement, upgrade
# 		the shared library, and then issue a CREATE_FUNCTION statement.
#
# 		If you upgrade the shared library first and then use DROP_FUNCTION, the server may crash.
#
# 13.7.4.2 DROP FUNCTION SYNTAX
#
# 		DROP FUNCTION function_name
#
# This statement drops the user-defined function (UDF) named function_name
#
# To drop a function, you must have the DELETE privilege for the mysql system database.
#
# This is because DROP_FUNCTION removes a row from the mysql.func system table that
# records the function's name, type and shared library name.
#
# NOTE:
#
# 		To upgrade the shared library associated with a UDF, issue a DROP_FUNCTION statement,
# 		upgrade the shared library, and then issue a CREATE_FUNCTION statement.
#
# 		If you upgrade the shared library first and then use DROP_FUNCTION, the server may crash.
#
# DROP_FUNCTION is also used to drop stored functions (see SECTION 13.1.29, "DROP PROCEDURE AND DROP FUNCTION SYNTAX")
#
# 13.7.4.3 INSTALL COMPONENT SYNTAX
#
# 		INSTALL COMPONENT component_name [, component_name ] ---
#
# This statement installs one or more server components, which become active immediately.
#
# A component provides services that are available to the server and other components;
# see SECTION 5.5, "MYSQL SERVER COMPONENTS"
#
# INSTALL_COMPONENT requires the INSERT privilege for the mysql.component system table.
#
# Example:
#
# 		INSTALL COMPONENT 'file://component1', 'file://component2';
#
# Component names are URNs that begin with file:// and indicate the base name of the file
# that implements the component, located in the directory named by the plugin_dir
# system variable.
#
# Component names do not include any platform-dependent file name suffix such as .so
# or .dll (These naming details are subject to change because component name interpretation
# is itself performed by a service and the component infrastructure makes it possible
# to replace the default service implementation with alternative implementations.)
#
# If any error occurs, the statement fails and has no effect.
#
# For example, this happens if a component name is errornous, a named component
# does not exist or is already installed, or component initialization fails.
#
# A loader service handles component loading, which includes adding installed components
# to the mysql.component system table that serves as a registry.
#
# For subsequent server restarts, any components listed in mysql.component are loaded
# by the loader service during the startup sequence.
#
# This occurs even if the server is started with the --skip-grant-tables option
#
# If a component depends on services not present in the registry and you attempt
# to install the component without also installing the component or components that
# provide the services on which it depends, an error occurs:
#
# 		ERROR 3527 (HY000): Cannot satisfy dependency for service 'component_a'
# 		required by component 'component_b'
#
# To avoid this problem, either install all components in the same statement, or install
# the dependent component after installing any components on which it depends.
#
# 13.7.4.4 INSTALL PLUGIN SYNTAX
#
# 		INSTALL PLUGIN plugin_name SONAME 'shared_library_name'
#
# This statement installs a server plugin. It requires the INSERT privilege for the
# mysql.plugin system table.
#
# plugin_name is the name of the plugin as defined in the plugin descriptor structure
# contained in the library file (see SECTION 29.2.4.2, "PLUGIN DATA STRUCTURES")
#
# Plugin names are not case-sensitive. For maximal compatibility, plugin names should
# be limited to ASCII letters, digits and underscore because they are used in C
# source files, shell command lines, M4 and Bourne shell scripts, and SQL environments.
#
# shared_library_name is the name of the shared library that contains the plugin code.
#
# The name includes the file name extension (for example, libmyplugin.so, libmyplugin.dll
# or libmyplugin.dylib)
#
# The shared library must be located in the plugin directory (the directory named by the
# plugin_dir system variable)
#
# The library must be in the plugin directory itself, not in a subdirectory.
#
# By default, plugin_dir is the plugin directory under the directory named by the
# pkglibdir configuration variable, but it can be changed by setting the value of
# plugin_dir at server startup.
#
# For example, set its value in a my.cnf file:
#
# 		[mysqld]
# 		plugin_dir=/path/to/plugin/directory
#
# If the value of plugin_dir is a relative path name, it is taken to be relative
# to the MySQL base directory (the value of the basedir system variable)
#
# INSTALL_PLUGIN loads and initializes the plugin code to make the plugin available
# for use.
#
# A plugin is initialized by executing its initialization function, which handles
# any setup that the plugin must perform before it can be used.
#
# When the server shuts down, it executes the deinitialization function for each
# plugin that is loaded so that the plugin has a chance to perform any final cleanup.
#
# INSTALL_PLUGIN also registers the plugin by adding a line that indicates the plugin
# name and library file name to the mysql.plugin system table.
#
# At server startup, the server loads and initializes any plugin that is listed in
# mysql.plugin
#
# This means that a plugin is installed with INSTALL_PLUGIN only once, not every
# time the server starts.
#
# Plugin loading at startup does not occur if the server is started with the --skip-grant-tables
# option.
#
# A plugin library can contain multiple plugins. For each of them to be installed, use a separate
# INSTALL_PLUGIN statement.
#
# Each statement names a different plugin, but all of them specify the same library name.
#
# INSTALL_PLUGIN causes the server to read option (my.cnf) files just as during server
# startup.
#
# This enables the plugin to pick up any relevant options from those files. It is
# possible to add plugin options to an option file even before loading a plugin
# (if the loose prefix is used)
#
# It is also possible to uninstall a plugin, edit my.cnf, and install the plugin again.
#
# Restarting the plugin this way enables it to the new option values without a server restart.
#
# For options that control individual plugin loading at server startup, see SECTION 5.6.1,
# "INSTALLING AND UNINSTALLING PLUGINS"
#
# If you need to load plugins for a single server startup when the --skip-grant-tables
# option is given (which tells the server not to read system tables), use the --plugin-load
# option.
#
# See SECTION 5.1.7, "SERVER COMMAND OPTIONS"
#
# To remove a plugin, use the UNINSTALL_PLUGIN statement
#
# For additional information about plugin loading, see SECTION 5.6.1, "INSTALLING AND UNINSTALLING PLUGINS"
#
# To see what plugins are installed, use the SHOW_PLUGINS statement or query the INFORMATION_SCHEMA
# the PLUGINS table.
#
# If you recompile a plugin library and need to reinstall it, you can use either of the following methods:
#
# 		) Use UNINSTALL_PLUGIN to uninstall all plugins in the library, install the new plugin library file
# 			in the plugin directory, and then use INSTALL_PLUGIN to install all plugins in the library.
#
# 			This procedure has the advantage that it can be used without stopping the server.
#
# 			However, if the plugin library contains many plugins, you must issue many INSTALL_PLUGIN
# 			and UNINSTALL_PLUGIN statements.
#
# 		) Stop the server, install the new plugin library file in the plugin directory, and restart the server.
#
# 13.7.4.5 UNINSTALL COMPONENT SYNTAX
#
# 		UNINSTALL COMPONENT component_name [, component_name ] ---
#
# This statement deactivates and uninstalls one or more server components.
#
# A component provides services that are available to the server and other components;
# see SECTION 5.5, "MYSQL SERVER COMPONENTS"
#
# UNINSTALL_COMPONENT is the complement of INSTALL_COMPONENT
#
# It requires the DELETE privilege for the mysql.component system table
#
# Example:
#
# 		UNINSTALL COMPONENT 'file://component1', 'file://component2';
#
# For information about component naming, see SECTION 13.7.4.3, "INSTALLING COMPONENT SYNTAX"
#
# If any error occurs, the statement fails and has no effect. For example, this happens if a 
# component name is errorneous, a named component is not installed, or cannot be uninstalled
# because other installed components depend on it.
#
# A loader service handles component unloading, which includes removing uninstalled
# components from the mysql.component system table that serves as a registry.
#
# As a result, unloaded components are not loaded during the startup sequence
# for subsequent server restarts.
#
# 13.7.4.6 UNINSTALL PLUGIN SYNTAX
#
# 		UNINSTALL PLUGIN plugin_name
#
# This statement removes an installed server plugin. It requires the DELETE privilege
# for the mysql.plugin system table.
#
# UNINSTALL_PLUGIN is the complement of INSTALL_PLUGIN
#
# plugin_name must be the name of some plugin that is listed in the mysql.plugin table.
#
# The server executes the plugin's deinitialization function and removes the row for
# the plugin from the mysql.plugin system table, so that subsequent server restarts will
# not load and initialize the plugin.
#
# UNINSTALL_PLUGIN does not remove the plugin's shared library file
#
# You cannot uninstall a plugin if any table that uses it is open
#
# Plugin removal has implications for the use of associated tables.
#
# For example, if a full-text parser plugin is associated with a FULLTEXT
# index on the table, uninstalling the plugin makes the table unusable.
#
# Any attempt to access the table results in an error.
#
# The table cannot even be opened, so you cannot drop an index for which
# the plugin is used.
#
# THis means that uninstalling a plugin is something to do with care unless
# you do not care about the table contents.
#
# If you are uninstalling a plugin with no intention of reinstalling it later
# and you care about the table contents, you should dump the table with mysqldump
# and remove the WITH PARSER clause from the dumped CREATE_TABLE statement
# so that you can reload the table later.
#
# If you do not care about the table, DROP_TABLE can be used even if any plugins
# associated with the table are missing.
#
# For additional information about plugin loading, see SECTION 5.6.1, "INSTALLING AND UNINSTALLING PLUGINS"
#
# 13.7.5 SET SYNTAX
#
# 13.7.5.1 SET SYNTAX FOR VARIABLE ASSIGNMENT
# 13.7.5.2 SET CHARACTER SET SYNTAX
# 13.7.5.3 SET NAMES SYNTAX
#
# The SET statement has several forms.
#
# Descriptions for those forms that are not associated with a specific server capability
# appear in subsections of this section:
#
# 		) SET_var_name = value enables you to assign values to variables that affect the
# 			operation of the server or clients.
#
# 			See SECTION 13.7.5.1, "SET SYNTAX FOR VARIABLE ASSIGNMENT"
#
# 		) SET_CHARACTER_SET and SET_NAMES assign values to character set and collation
# 			variables associated with the current connection to the server.
#
# 			See SECTION 13.7.5.2, "SET CHARACTER SET SYNTAX", and SECTION 13.7.5.3, "SET NAMES SYNTAX"
#
# Descriptions for the other forms appear elsewhere, grouped with other statements related
# to the capability they help implement:
#
# 		) SET_DEFAULT_ROLE and SET_ROLE set the default role and current role for user accounts.
#
# 			See SECTION 13.7.1.9, "SET DEFAULT ROLE SYNTAX" and SECTION 13.7.1.11, "SET ROLE SYNTAX"
#
# 		) SET_PASSWORD assigns account passwords. See SECTION 13.7.1.10, "SET PASSWORD SYNTAX"
#
# 		) SET RESOURCE GROUP assigns threads to a resource group. See SECTION 13.7.2.4, "SET RESOURCE GROUP SYNTAX"
#
# 		) SET_TRANSACTION_ISOLATION_LEVEL sets the isolation level for transaction processing.
#
# 			See SECTION 13.3.7, "SET TRANSACTION SYNTAX"
#
# 13.7.5.1 SET SYNTAX FOR VARIABLE ASSIGNMENT
#
# 		SET variable = expr [, variable = expr] ---
#
# 		variable: {
# 			user_var name
# 		 | param_name
# 		 | local_var_name
# 		 | {GLOBAL | @@GLOBAL.} system_var_name
# 		 | {PERSIST | @@PERSIST.} system_var_name
# 		 | {PERSIST_ONLY | @@PERSIST_ONLY.} system_var_name
# 		 | [SESSION | @@SESSION. | @@] system_var_name
# 		}
#
# SET syntax for variable assignment enables you to assign values to different types
# of variables that affect the operation of the server or clients:
#
# 		) User-defined variables. See SECTION 9.4, "USER-DEFINED VARIABLES"
#
# 		) Stored procedure and function parameters, and stored program local variables. See SECTION 13.6.4, "VARIABLES IN STORED PROGRAMS"
#
# 		) System variables. See SECTION 5.1.8, "SERVER SYSTEM VARIABLES". System variables also can be set at server startup, as described
# 			in SECTION 5.1.9, "USING SYSTEM VARIABLES"
#
# A SET statement that assigns variable values is not written to the binary log, so in replication
# scenarios it affects only the host on which you execute it.
#
# To affect all replication hosts, execute the statement on each host.
#
# The following sections describe SET syntax for setting variables.
#
# They use the = assignment operator, but the := assignment operator is
# also permitted for this purpose.
#
# 		) USER-DEFINED VARIABLE ASSIGNMENT
#
# 		) PARAMETER AND LOCAL VARIABLE ASSIGNMENT
#
# 		) SYSTEM VARIABLE ASSIGNMENT
#
# 		) SET ERROR HANDLING
#
# 		) MULTIPLE VARIABLE ASSIGNMENT
#
# 		) SYSTEM VARIABLE REFERENCES IN EXPRESSIONS
#
# USER-DEFINED VARIABLE ASSIGNMENT
#
# User-defined variables are created locally within a session and exist only
# within the context of that session; see SECTION 9.4, "USER-DEFINED VARIABLES"
#
# A user-defined variable is written as @var_name and is assigned an expression
# value as follows:
#
# 		SET @var_name = expr;
#
# Examples:
#
# 		SET @name = 43;
# 		SET @total_tax = (SELECT SUM(tax) FROM taxable_transactions);
#
# As demonstrated by those statements, expr can range from simple (a literal value) to more complex
# (the value returned by a scalar subquery)
#
# The Performance Schema user_variable_by_thread table contains information about user-defined
# variables.
#
# See SECTION 26.12.10, "PERFORMANCE SCHEMA USER-DEFINED VARIABLE TABLES"
#
# PARAMETER AND LOCAL VARIABLE ASSIGNMENT
#
# SET applies to parameters and local variables in the contexts of the stored object within
# which they are defined.
#
# The following procedure uses the increment procedure parameter and counter local variable:
#
# 		CREATE PROCEDURE p(increment INT)
# 		BEGIN
# 			DECLARE counter INT DEFAULT 0;
# 			WHILE counter < 10 DO
# 				--- do work ---
# 				SET counter = counter + increment;
# 			END WHILE;
# 		END;
#
# SYSTEM VARIABLE ASSIGNMENT
#
# The MySQL server maintains system variables that configure its operation.
#
# A system variable can have a global value that affects server operation
# as a whole, a session value that affects the current session, or both.
#
# Many system variables are dynamic and can be changed at runtime using the SET
# statement to affect operation of the current server instance.
#
# SET can also be used to persist certain system variables to the mysqld-auto.cnf
# file in the data directory, to affect server operation for subsequent
# startups.
#
# If you change a session system variable, the value remains in effect within
# your session until you change the variable to a different value or the 
# session ends.
#
# The change has no effect on other sessions.
#
# If you change a global system variable, the value is remembered and used to
# initialize the session value for new sessions until you change the variable
# to a different value or the server exits.
#
# The change is visible to any client that accesses the global value.
#
# However, the change affects the corresponding session value only for clients
# that connect after the change.
#
# The global variable change does not affect the session value for any current
# client sessions (not even the session within which the global value change occurs)
#
# To make a global system variable setting permanent so that it applies across
# server restarts, you can persist it to the mysqld-auto.cnf file in the data directory.
#
# It is also possible to make persistent configuration changes by manually modifying
# a my.cnf option file, but that is more cumbersome, and an error in a manually
# entered setting might not be discovered until much later.
#
# SET statements that persist system variables are more convenient and avoid the
# possibility of malformed settings because settings with syntax errors do not
# succeed and do not change server configuration.
#
# For more information about persisting system variables and the mysqld-auto.cnf
# file, see SECTION 5.1.9.3, "PERSISTED SYSTEM VARIABLES"
#
# NOTE:
#
# 		Setting or persisting a global system variable value always requires special
# 		privileges.
#
# 		Setting a session system variable value normally requires no special privileges
# 		and can be done by any user, although there are exceptions.
#
# 		For more information, see SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES"
#
# The following discussion describes the syntax options for setting and persisting
# system variables:
#
# 		) To assign a value to a global system variable, precede the variable name by the
# 			GLOBAL keyword or the @@GLOBAL. qualifier:
#
# 			SET GLOBAL max_connections = 1000;
# 			SET @@GLOBAL.max_connections = 1000;
#
# 		) To assign a value to a session system variable, precede the variable name by the
# 			SESSION or LOCAL keyword, by the @@SESSION., @@LOCAL., or @@ qualifier, or by
# 			no keyword or no modifier at all:
#
# 				SET SESSION sql_mode = 'TRADITIONAL';
# 				SET LOCAL sql_mode = 'TRADITIONAL';
# 				SET @@SESSION.sql_mode = 'TRADITIONAL';
# 				SET @@LOCAL.sql_mode = 'TRADITIONAL';
# 				SET @@sql_mode = 'TRADITIONAL';
# 				SET sql_mode = 'TRADITIONAL';
#
# 			A client can change its own session variables, but not those of any other client.
#
# 		) To persist a global system variable to the mysqld-auto.cnf option file in the data
# 			directory, precede the variable name by the PERSIST keyword or the @@PERSIST.
# 			qualifier:
#
# 				SET PERSIST max_connections = 1000;
# 				SET @@PERSIST.max_connections = 1000;
#
# 			This SET syntax enables you to make configuration changes at runtime that also
# 			persist across server restarts.
#
# 			Like SET_GLOBAL, SET_PERSIST sets the global variable runtime value, but also
# 			writes the variable setting to the mysqld-auto.cnf file (replacing any existing
# 			variable setting if there is one)
#
# 		) To persist a global system variable to the mysqld-auto.cnf file without setting the
# 			global variable runtime value, precede the variable name by the PERSIST_ONLY
# 			keyword or the @@PERSIST_ONLY. qualifier:
#
# 				SET PERSIST_ONLY back_log = 100;
# 				SET @@PERSIST_ONLY.back_log = 100;
# 	
# 			Like PERSIST, PERSIST_ONLY writes the variable setting to mysqld-auto.cnf
#
# 			However, unlike PERSIST, PERSIST_ONLY does not modify the global variable
# 			runtime value.
#
# 			This makes PERSIST_ONLY suitable for configuring read-only system variables
# 			that can be set only at server startup.
#
# To set a global system variable value to the compiled-in MySQL default value or
# a session system variable to the current corresponding global value, set the
# variable to the value DEFAULT.
#
# For example, the following two statements are identical in setting the session
# value of max_join_size to the current global value:
#
# 		SET @@SESSION.max_join_size = DEFAULT;
# 		SET @@SESSION.max_join_size = @@GLOBAL.max_join_size;
#
# Using SET to persist a global system variable to a value of DEFAULT or to its
# literal default value assigns the variable its default value and adds a 
# setting for the variable to mysqld-auto.cnf 
#
# To remove the variable from the file, use RESET_PERSIST
#
# Some system variables cannot be persisted or are persist-restricted.
#
# See SECTION 5.1.9.4, "NONPERSISTIBLE AND PERSIST-RESTRICTED SYSTEM VARIABLES"
#
# A system variable implemented by a plugin can be persisted if the plugin is installed
# when the SET statement is executed.
#
# Assignment of the persisted plugin variable takes effect for subsequent server
# restarts if the plugin is still installed.
#
# If the plugin is no longer installed, the plugin variable will not exist
# when the server reads the mysqld-auto.cnf file
#
# In this case, the server writes a warning to the error log and continues:
#
# 		currently unknown variable 'var_name'
# 		was read from the persisted config file
#
# To display system variable names and values:
#
# 		) Use the SHOW_VARIABLES statement; see SECTION 13.7.6.39, "SHOW VARIABLES SYNTAX"
#
# 		) Several Performance Schema tables provide system variable information. See SECTION 26.12.13, "PERFORMANCE SCHEMA SYSTEM VARIABLE TABLES"
#
# 		) The Performance Schema variables_info table contains information showing when and by
# 			which user each system variable was most recently set.
#
# 			See SECTION 26.12.13.2, "PERFORMANCE SCHEMA VARIABLES_INFO TABLE"
#
# 		) The Performance Schema persisted_variables table provides an SQL interface to the
# 			mysqld-auto.cnf file, enabling its contents to be inspected at runtime using
# 			SELECT statements.
#
# 			See SECTION 26.12.13.1, "PERFORMANCE SCHEMA PERSISTED_VARIABLES TABLE"
#
# SET ERROR HANDLING
#
# If any variable assignment in a SET statement fails, the entire statement fails and no variables
# are changed, nor is the mysqld-auto.cnf file changed.
#
# SET produces an error under the circumstances described here.
#
# Most of the examples show SET statements that use keyword syntax 
# (for example, GLOBAL or SESSION), but the principles are also true
# for statements that use the corresponding modifiers (for example, @@GLOBAL. or @@SESSION.)
#
# 		) Use of SET (any variant) to set a read-only variable:
#
# 			SET GLOBAL version = 'abc';
# 			ERROR 1238 (HY000): Variable 'version' is a read only variable
#
# 		) Use of GLOBAL, PERSIST, or PERSIST_ONLY to set a variable that has only a session value:
#
# 			SET GLOBAL sql_log_bin = ON;
# 			ERROR 1228 (HY000): Variable 'sql_log_bin' is a SESSION
# 			variable and can't be used with SET GLOBAL
#
# 		) Use of SESSION to set a variable that has only a global value:
#
# 			SET SESSION max_connections = 1000;
# 			ERROR 1229 (HY000): Variable 'max_connections' is a 
# 			GLOBAL variable and should be set with SET GLOBAL
#
# 		) Omission of GLOBAL, PERSIST, or PERSIST_ONLY to set a variable that
# 			has only a global value:
#
# 			SET max_connections = 1000;
# 			ERROR 1229 (HY000): Variable 'max_connections' is a
# 			GLOBAL variable and should be set with SET GLOBAL
#
# 		) Use of PERSIST or PERSIST_ONLY to set a variable that cannot be persisted:
#
# 			SET PERSIST port = 3307;
# 			ERROR 1238 (HY000): Variable 'port' is a read only variable
# 			SET PERSIST_ONLY port = 3307;
# 			ERROR 1238 (HY000): Variable 'port' is a non-persistent read only variable
#
# 		) The @@GLOBAL., @@PERSIST., @@PERSIST_ONLY., @@SESSION., and @@ modifiers apply only
# 			to system variables.
#
# 			An error occurs for attempts to apply them to user-defined variables, stored procedure
# 			or function parameters, or stored program local variables.
#
# 		) Not all system variables can be set to DEFAULT. In such cases, assigning DEFAULT results in an error.
#
# 		) An error occurs for attempts to assign DEFAULT to user-defined variables, stored procedure
# 			or function parameters, or stored program local variables.
#
# MULTIPLE VARIABLE ASSIGNMENT
#
# A SET statement can contain multiple variable assignments, separated by commas.
#
# This statement assigns values to a user-defined variable and a system variable:
#
# 		SET @x = 1, SESSION sql_mode = '';
#
# If you set multiple system variables in a single statement, the most recent GLOBAL,
# PERSIST, PERSIST_ONLY or SESSION keyword in the statement is used for following
# assignments that have no keyword specified.
#
# Examples of multiple-variable assignment:
#
# 		SET GLOBAL sort_buffer_size = 1000000, SESSION sort_buffer_size = 1000000;
# 		SET @@GLOBAL.sort_buffer_size = 1000000, @@LOCAL.sort_buffer_size = 1000000;
# 		SET GLOBAL max_connections = 1000, sort_buffer_size = 1000000;
#
# The @@GLOBAL., @@PERSIST., @@PERSIST_ONLY., @@SESSION., and @@ modifiers apply only
# to the immediately following system variable, not any remaining system variables.
#
# This statement sets the sort_buffer_size global value to 50000 and the session
# value to 1000000:
#
# 		SET @@GLOBAL.sort_buffer_size = 50000, sort_buffer_size = 1000000;
#
# SYSTEM VARIABLE REFERENCES IN EXPRESSIONS
#
# To refer to the value of a system variable in expressions, use one of the
# @@-modifiers (except @@PERSIST. and @@PERSIST_ONLY., which are not permitted
# in expressions)
#
# For example, you can retrieve system variable values in a SELECT statement
# like this:
#
# 		SELECT @@GLOBAL.sql_mode, @@SESSION.sql_mode, @@sql_mode;
#
# NOTE:
#
# 		A reference to a system variable in an expression as @@var_name
# 		(with @@ rather than @@GLOBAL. or @@SESSION.) returns the session
# 		value if it exists and the global value otherwise.
#
# 		This differs from SET @@var_name = expr, which always refers to the session value.
#
# 13.7.5.2 SET CHARACTER SYNTAX
#
# 		SET {CHARACTER SET | CHARSET}
# 			 {'charset_name' | DEFAULT}
#
# This statement maps all strings sent between the server and the current client with the
# given mapping.
#
# SET CHARACTER SET sets three session system variables:
#
# 		character_set_client and character_set_results are set to the given character set
#
# 		and character_set_connection to the value of character_set_database
#
# See SECTION 10.4, "CONNECTION CHARACTER SETS AND COLLATIONS"
#
# charset_name may be quoted or unquoted.
#
# The default character set mapping can be restored by using the value DEFAULT.
#
# The default depends on the server configuration.
#
# Some character sets cannot be used as the client character set.
# Attempting to use them with SET_CHARACTER_SET produces an error.
#
# See IMPERMISSIBLE CLIENT CHARACTER SETS
#
# 13.7.5.3 SET NAMES SYNTAX
#
# 		SET NAMES {'charset_name'
# 			[COLLATE 'collation_name'] | DEFAULT}
#
# This statement sets the three session system variables character_set_client, character_set_connection,
# and character_set_results to the given character set.
#
# Setting character_set_connection to charset_name also sets collation_connection to the default collation
# for charset_name.
#
# See SECTION 10.4, "CONNECTION CHARACTER SETS AND COLLATIONS"
#
# The optional COLLATE clause may be used to specify a collation explicitly.
#
# If given, the collation must one of the permitted collations for charset_name.
#
# charset_name and collation_name may be quoted or unquoted.
#
# The default mapping can be restored by using a value of DEFAULT.
# The default depends on the server configuration.
#
# Some character sets cannot be used as the client character set. Attempting ot use
# them with SET_NAMES produces an error.
#
# See IMPERMISSIBLE CLIENT CHARACTER SETS
#
# 13.7.6 SHOW SYNTAX
#
# 		13.7.6.1 SHOW BINARY LOGS SYNTAX
# 		13.7.6.2 SHOW BINLOG EVENTS SYNTAX
#
# 		13.7.6.3 SHOW CHARACTER SET SYNTAX
# 		13.7.6.4 SHOW COLLATION SYNTAX
#
# 		13.7.6.5 SHOW COLUMNS SYNTAX
# 		13.7.6.6 SHOW CREATE DATABASE SYNTAX
#
# 		13.7.6.7 SHOW CREATE EVENT SYNTAX
# 		13.7.6.8 SHOW CREATE FUNCTION SYNTAX
#
# 		13.7.6.9 SHOW CREATE PROCEDURE SYNTAX
# 		13.7.6.10 SHOW CREATE TABLE SYNTAX
#
# 		13.7.6.11 SHOW CREATE TRIGGER SYNTAX
# 		13.7.6.12 SHOW CREATE USER SYNTAX
#
# 		13.7.6.13 SHOW CREATE VIEW SYNTAX
# 		13.7.6.14 SHOW DATABASES SYNTAX
#
# 		13.7.6.15 SHOW ENGINE SYNTAX
# 		13.7.6.16 SHOW ENGINES SYNTAX
#
# 		13.7.6.17 SHOW ERRORS SYNTAX
# 		13.7.6.18 SHOW EVENTS SYNTAX
#
# 		13.7.6.19 SHOW FUNCTION CODE SYNTAX
# 		13.7.6.20 SHOW FUNCTION STATUS SYNTAX
#
# 		13.7.6.21 SHOW GRANTS SYNTAX
# 		13.7.6.22 SHOW INDEX SYNTAX
#
# 		13.7.6.23 SHOW MASTER STATUS SYNTAX
# 		13.7.6.24 SHOW OPEN TABLES SYNTAX
#
# 		13.7.6.25 SHOW PLUGINS SYNTAX
# 		13.7.6.26 SHOW PRIVILEGES SYNTAX
#
# 		13.7.6.27 SHOW PROCEDURE CODE SYNTAX
# 		13.7.6.28 SHOW PROCEDURE STATUS SYNTAX
#
# 		13.7.6.29 SHOW PROCESSLIST SYNTAX
# 		13.7.6.30 SHOW PROFILE SYNTAX
#
# 		13.7.6.31 SHOW PROFILES SYNTAX
# 		13.7.6.32 SHOW RELAYLOG EVENTS SYNTAX
#
# 		13.7.6.33 SHOW SLAVE HOSTS SYNTAX
# 		13.7.6.34 SHOW SLAVE STATUS SYNTAX
#
# 		13.7.6.35 SHOW STATUS SYNTAX
# 		13.7.6.36 SHOW TABLE STATUS SYNTAX
#
# 		13.7.6.37 SHOW TABLES SYNTAX
# 		13.7.6.38 SHOW TRIGGERS SYNTAX
#
# 		13.7.6.39 SHOW VARIABLES SYNTAX
# 		13.7.6.40 SHOW WARNINGS SYNTAX
#
# SHOW has many forms that provide information about databases, tables, columns, or status information
# about the server.
#
# This section describes those following:
#
# 		SHOW {BINARY | MASTER} LOGS
# 		SHOW BINLOG EVENTS [IN 'log_name'] [FROM pos] [LIMIT [offset,] row_count]
#
# 		SHOW CHARACTER SET [like_or_where]
# 		SHOW COLLATION [like_or_where]
#
# 		SHOW [FULL] COLUMNS FROM tbl_name [FROM db_name] [like_or_where]
# 		SHOW CREATE DATABASE db_name
#
# 		SHOW CREATE EVENT event_name
# 		SHOW CREATE FUNCTION func_name
#
# 		SHOW CREATE PROCEDURE proc_name
# 		SHOW CREATE TABLE tbl_name
#
# 		SHOW CREATE TRIGGER trigger_name
# 		SHOW CREATE VIEW view_name
#
# 		SHOW DATABASES [like_or_where]
# 		SHOW ENGINE engine_name {STATUS | MUTEX}
#
# 		SHOW [STORAGE] ENGINES
# 		SHOW ERRORS [LIMIT [offset,] row_count]
#
# 		SHOW EVENTS
# 		SHOW FUNCTION CODE func_name
#
# 		SHOW FUNCTION STATUS [like_or_where]
# 		SHOW GRANTS FOR user
#
# 		SHOW INDEX FROM tbl_name [FROM db_name]
# 		SHOW MASTER STATUS
#
# 		SHOW OPEN TABLES [FROM db_name] [like_or_where]
# 		SHOW PLUGINS
#
# 		SHOW PROCEDURE CODE proc_name
# 		SHOW PROCEDURE STATUS [like_or_where]
#
# 		SHOW PRIVILEGES
# 		SHOW [FULL] PROCESSLIST
#
# 		SHOW PROFILE [types] [FOR QUERY n] [OFFSET n] [LIMIT n]
# 		SHOW PROFILES
#
# 		SHOW RELAYLOG EVENTS [IN 'log_name'] [FROM pos] [LIMIT [offset,] row_count]
# 		SHOW SLAVE HOSTS
#
# 		SHOW SLAVE STATUS [FOR CHANNEL channel]
# 		SHOW [GLOBAL | SESSION] STATUS [like_or_where]
#
# 		SHOW TABLE STATUS [FROM db_name] [like_or_where]
# 		SHOW [FULL] TABLES [FROM db_name] [like_or_where]
#
# 		SHOW TRIGGERS [FROM db_name] [like_or_where]
# 		SHOW [GLOBAL | SESSION] VARIABLES [like_or_where]
#
# 		SHOW WARNINGS [LIMIT [offset,] row_count]
#
# 		like_or_where:
# 			LIKE 'pattern'
# 		 | WHERE expr
#
# If the syntax for a given SHOW statement includes a LIKE_'pattern' part,
# 'pattern' is a string that can contain the SQL % and _ wildcard characters.
#
# The pattern is useful for restricting statement output to matching values.
#
# Several SHOW statements also accept a WHERE clause that provides more flexibility
# in specifying which rows to display.
#
# See SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# Many MySQL APIs (such as PHP) enable you to treat the result returned from a SHOW
# statement as you would a result set from a SELECT, see CHAPTER 28, CONNECTORS AND APIs,
# or your API documentation for more information.
#
# In addition, you can work in SQL with results from queries on tables in the INFORMATION_SCHEMA
# database, which you cannot easily do with results from SHOW statements.
#
# See CHAPTER 25, INFORMATION_SCHEMA TABLES
#
# 13.7.6.1 SHOW BINARY LOGS SYNTAX
#
# 		SHOW BINARY LOGS
# 		SHOW MASTER LOGS
#
# Lists the binary log files on the server.
#
# This statement is used as part of the procedure described in SECTION 13.4.1.1,
# "PURGE BINARY LOGS SYNTAX", that shows how to determine which logs can be purged.
#
# Encrypted binary log files have a 512-byte file header that stores information
# required for encryption and decryption of the file.
#
# This is included in the file size displayed by SHOW_BINARY_LOGS
#
# The Encrypted column shows whether or not the binary log file is encrypted.
#
# Binary log encryption is active if binlog_encryption=ON is set for the 
# server.
#
# Existing binary log files are not encrypted or decrypted if binary log encryption
# is activated or deactivated while the server is running.
#
# 		SHOW BINARY LOGS;
# 		+-----------------------+-----------------+---------------------+
# 		| Log_name 					| File_size 		| Encrypted 			 |
# 		+-----------------------+-----------------+---------------------+
# 		| binlog.000015 			| 724935 			| Yes 					 |
# 		| binlog.000016 			| 733481 			| Yes 					 |
# 		+-----------------------+-----------------+---------------------+
#
# SHOW_MASTER_LOG is equivalent to SHOW_BINARY_LOGS
#
# A user with the SUPER or REPLICATION_CLIENT privilege may execute this statement.
#
# 13.7.6.2 SHOW BINLOG EVENTS SYNTAX
#
# 		SHOW BINLOG EVENTS
# 			[IN 'log_name']
# 			[FROM pos]
# 			[LIMIT [offset,] row_count]
#
# Shows the events in the binary log. If you do not specify 'log_name', the first binary log is displayed.
#
# The LIMIT clause has the same syntax as for the SELECT statement.
#
# See SECTION 13.2.10, "SELECT SYNTAX"
#
# NOTE:
#
# 		Issuing a SHOW_BINLOG_EVENTS with no LIMIT clause could start a very time- and resource-consuming
# 		process because the server returns to the client the complete contents of the binary log
# 		(which includes all statements executed by the server that modify data)
#
# 		As an alternative to SHOW_BINLOG_EVENTS, use the mysqlbinlog utility to save the binary
# 		log to a text file for later examination and analysis.
#
# 		See SECTION 4.6.8, "mysqlbinlog -- Utility for processing binary log files"
#
# SHOW_BINLOG_EVENTS displays the following fields for each event in the binary log:
#
# 		) Log_name
#
# 			THe name of the file that is being listed.
#
# 		) Pos
#
# 			THe position at which the event occurs.
#
# 		) Event_type
#
# 			An identifier that describes the event type
#
# 		) Server_id
#
# 			The server ID of the server on which the event originated
#
# 		) End_log_pos
#
# 			The position at which the next event begins, which is equal to Pos plus the size of the event
#
# 		) Info
#
# 			More detailed information about the event type.
#
# 			The format of this information depends on the event type.
#
# NOTE:
#
# 		Some events relating to the setting of user and system variables are not included
# 		in the output from SHOW_BINLOG_EVENTS
#
# 		To get complete coverage of events within a binary log, use mysqlbinlog
#
# NOTE:
#
# 		SHOW_BINLOG_EVENTS does not work with relay log files.
#
# 		You can use SHOW_RELAYLOG_EVENTS for this purpose.
#
# 13.7.6.3 SHOW CHARACTER SET SYNTAX
#
# 		SHOW CHARACTER SET
# 			[LIKE 'pattern' | WHERE expr]
#
# The SHOW_CHARACTER_SET statement shows all available character sets.
#
# The LIKE clause, if present, indicates which character set names to match.
#
# The WHERE clause can be given to select rows using more general conditions
# , as discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS".
#
# For example:
#
# 		SHOW CHARACTER SET LIKE 'latin%';
# 		+----------+----------------------------+---------------------+-------------------+
# 		| Charset  | Description 					 | Default collation   | Maxlen 				 |
# 		+----------+----------------------------+---------------------+-------------------+
# 		| latin1   | cp1252 West European 		 | latin1_swedish_ci   | 1 					 |
# 		| latin2   | ISO 8859-2 Central European| latin2_general_ci   | 1 					 |
# 		| latin5   | ISO 8859-9 Turkish 		    | latin5_turkish_ci   | 1 					 |
# 		| latin7   | ISO 8859-13 Baltic 			 | latin7_general_ci   | 1 					 |
# 		+----------+----------------------------+---------------------+-------------------+
#
# SHOW_CHARACTER_SET output has these columns:
#
# 		) Charset
#
# 			The character set name
#
# 		) Description
#
# 			A description of the character set
#
# 		) Default collation
#
# 			THe default collation for the character set
#
# 		) Maxlen
#
# 			The maximum number of bytes required to store one character
#
# The filename character set is for internal use only; consequently, SHOW_CHARACTER_SET does not display it.
#
# Character set information is also available from the INFORMATION_SCHEMA CHARACTER_SETS table.
#
# See SECTION 25.2, "THE INFORMATION_SCHEMA CHARACTER_SETS TABLE"
#
# 13.7.6.4 SHOW COLLATION SYNTAX
#
# 		SHOW COLLATION
# 			[LIKE 'pattern' | WHERE expr]
#
# This statement lists collations supported by the server.
#
# By default, the output from SHOW_COLLATION includes all available collations.
#
# The LIKE clause, if present, indicates which collation names to match.
#
# The WHERE clause can be given to select rows using more general conditions,
# as discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS".
#
# For example:
#
# 		SHOW COLLATION WHERE Charset = 'latin1';
# 		+--------------------+-----------+------+--------------+--------------+------------------+
# 		| Collation 			| Charset 	| Id   | Default 		 | Compiled 	 | Sortlen 			  |
# 		+--------------------+-----------+------+--------------+--------------+------------------+
# 		| latin1_german1_ci  | latin1 	| 5 	 | 				 | Yes 			 | 	1 				  |
# 		| latin1_swedish_ci  | latin1 	| 8 	 | Yes 			 | Yes 			 | 	1 				  |
# 		| latin1_danish_ci   | latin1 	| 15 	 | 				 | Yes 			 | 	1 				  |
# 		| etc.
#
# SHOW_COLLATION output has these columns:
#
# 		) Collation
#
# 			The collation name
#
# 		) Charset
#
# 			The name of the character set with which the collation is associated
#
# 		) Id
#
# 			The collation ID
#
# 		) Default
#
# 			Whether the collation is the default for its character set
#
# 		) Compiled
#
# 			Whether hte character set is compiled into the server
#
# 		) Sortlen
#
# 			This is related to the amount of memory required to sort strings expressed in the char set
#
# To see the default collation for each character set, use the following statement.
#
# Default is a reserved word, so to use it as an identifier, it must be quoted as such:
#
# 		SHOW COLLATION WHERE `Default` = 'Yes';
# 		+--------------------------+----------+-----------+------------+------------------+-------------+
# 		| Collation 				   | Charset  | Id 		  | Default    | Compiled 			 | Sortlen 		|
# 		+--------------------------+----------+-----------+------------+------------------+-------------+
# 		| big5_chinese_ci 			| big5 	  | 1 		  | Yes 	      | Yes 				 | 1 				|
# 		etc.
#
# Collation information is also available from the INFORMATION_SCHEMA COLLATIONS table.
#
# See SECTION 25.3, "THE INFORMATION_SCHEMA COLLATIONS TABLE"
#
# 13.7.6.5 SHOW COLUMNS SYNTAX
#
# 		SHOW [EXTENDED] [FULL] {COLUMNS | FIELDS}
# 			{FROM | IN} tbl_name
# 			[{FROM | IN} db_name]
# 			[LIKE 'pattern' | WHERE expr]
#
# SHOW_COLUMNS displays information about the columns in a given table.
#
# It also works for views. SHOW_COLUMNS displays information only for those
# columns for which you have some privilege.
#
# 		SHOW COLUMNS FROM City;
# 		+-----------+------------+---------+---------+--------------+-----------------+
# 		| Field 		| Type 		 | Null 	  | Key 		| Default 		| Extra 				|
# 		+-----------+------------+---------+---------+--------------+-----------------+
# 		| ID 			| int(11) 	 | NO 	  | PRI 		| NULL 			| auto_increment 	|
# 		| Name 		| char(35) 	 | NO 	  | 			| 					| 						|
# 		| etc.
#
# An alternative to tbl_name FROM db_name syntax is db_name.tbl_name
#
# These two statements are equivalent:
#
# 		SHOW COLUMNS FROM mytable FROM mydb;
# 		SHOW COLUMNS FROM mydb.mytable;
#
# The optional EXTENDED keyword causes the output to include information
# about hidden columns that MySQL uses internally and are not accessible by users.
#
# The optional FULL keyword causes the output to include the column collation
# and comments, as well as the privileges you have for each column.
#
# The LIKE clause, if present, indicates which column names to match.
#
# The WHERE clause can be given to select rows using more general conditions,
# as discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# THe data types may differ from what you expect them to be based on a CREATE_TABLE
# statement because MySQL sometimes changes data types when you create or alter a
# table.
#
# The conditions under which this occurs are described in SECTION 13.1.20.7,
# "SILENT COLUMN SPECIFICATION CHANGES"
#
# SHOW_COLUMNS displays the following values for each table column:
#
# 		) Field
#
# 			THe name of the column
#
# 		) Type
#
# 			THe column data type
#
# 		) Collation
#
# 			THe collation for nonbinary string columns, or NULL for other columns.
#
# 			This value is displayed only if you use the FULL keyword
#
# 		) Null
#
# 			The column nullability. The value is YES if NULL values can be stored
# 			in the column, NO if not.
#
# 		) Key
#
# 			Whether the column is indexed:
#
# 				) If Key is empty, the column either is not indexed or is indexed only as a secondary
# 					column in a multiple-column, nonunique index.
#
# 				) If Key is PRI, the column is a PRIMARY KEY or is one of the columns in a multiple-column PRIMARY KEY
#
# 				) If Key is UNI, the column is the first column of a UNIQUE index. (A UNIQUE index permits multiple NULL values,
# 					but you can tell whether the column permits NULL by checking the Null field)
#
# 				) If Key is MUL, the column is the first column of a nonunique index in which multiple occurences
# 					of a given value are permitted within the column.
#
# If more than one of the Key values applies to a given column of a table, Key displays the one with the
# highest priority, in the order PRI, UNI, MUL.
#
# A UNIQUE index may be displayed as PRI if it cannot contain NULL values and there is no PRIMARY KEY
# in the table.
#
# A UNIQUE index may display as MUL if several columns form a composite UNIQUE index; although
# the combination of the columns is unique, each column can still hold multiple occurrences
# of a given value.
#
# 		) Default
#
# 			The default value for the column. This is NULL if the column has an explicit default of NULL,
# 			or if the column definition includes no DEFAULT clause.
#
# 		) Extra
#
# 			Any additional information that is available about a given column.
#
# 			The value is nonempty in these cases:
#
# 				) auto_increment for columns that have the AUTO_INCREMENT attribute
#
# 				) on update CURRENT_TIMESTAMP for TIMESTAMP or DATETIME columns that have the ON UPDATE CURRENT_TIMESTAMP attribute
#
# 				) VIRTUAL GENERATED or VIRTUAL STORED for generated columns
#
# 				) DEFAULT_GENERATED for columns that have an expression default value
#
# 		) Privileges
#
# 			THe privileges you have for the column.
#
# 			THis value is displayed only if you use the FULL keyword
#
# 		) Comment
#
# 			ANy comment included in the column definition. 
#
# 			This value is displayed only if you use the FULL keyword
#
# Table column information is also available from the INFORMATION_SCHEMA COLUMNS table.
#
# See SECTION 25.5, "THE INFORMATION_SCHEMA COLUMNS TABLE"
#
# The extended information about hidden columns is available only using
# SHOW EXTENDED COLUMNS; it cannot be obtained from the COLUMNS table.
#
# You can list a table's columns with the mysqlshow db_name tbl_name command
#
# The DESCRIBE statement provides information similar to SHOW_COLUMNS.
# See SECTION 13.8.1, "DESCRIBE SYNTAX"
#
# The SHOW_CREATE_TABLE, SHOW_TABLE_STATUS and SHOW_INDEX statements also
# provide information about tables.
#
# See SECTION 13.7.6, "SHOW SYNTAX"
#
# 13.7.6.6 SHOW CREATE DATABASE SYNTAX
#
# 		SHOW CREATE {DATABASE | SCHEMA} [IF NOT EXISTS] db_name
#
# Shows the CREATE_DATABASE statement that creates the named database.
#
# If the SHOW statement includes an IF NOT EXISTS clause, the output too includes
# such a clause.
#
# SHOW_CREATE_SCHEMA is a synonym for SHOW_CREATE_DATABASE
#
# 		SHOW CREATE DATABASE test\G
# 		************************* 1. row *******************************
# 				Database: test
# 		Create Database: CREATE DATABASE `test`
# 								/*!40100 DEFAULT CHARACTER SET utf8mb4 */
#
# 		SHOW CREATE SCHEMA test\G
# 		************************* 1. row *******************************
# 				Database: test
# 		Create Database: CREATE DATABASE `test`
# 								/*!40100 DEFAULT CHARACTER SET utf8mb4 */
#
# SHOW_CREATE_DATABASE quotes table and column names according to the value of the sql_quote_show_create option.
#
# See SECTION 5.1.8, "SERVER SYSTEM VARIABLES"
#
# 13.7.6.7 SHOW CREATE EVENT SYNTAX
#
# 		SHOW CREATE EVENT event_name
#
# THis statement displays the CREATE_EVENT statement needed to re-create a given event.
#
# it requires the EVENT privilege for the database from which the event is to be 
# shown.
#
# For example (using the same event e_daily defined and then altered in SECTION 13.7.6.18, "SHOW EVENTS SYNTAX")
#
# 		SHOW CREATE EVENT myschema..e_daily\G
# 		*********************** 1. row ***************************
# 						Event: e_daily
# 					sql_mode: ONLY_FULL_GROUP_BY, STRICT_TRANS_TABLES,
# 								 NO_ZERO_IN_DATE, NO_ZERO_DATE,
# 								 ERROR_FOR_DIVISION_BY_ZERO,
# 								 NO_ENGINE_SUBSTITUTION
# 					time_zone: SYSTEM
#				Create event: CREATE DEFINER=`jon`@`ghidora` EVENT `e_daily`
# 									ON SCHEDULE EVERY 1 DAY
# 									STARTS CURRENT_TIMESTAMP + INTERVAL 6 HOUR
# 									ON COMPLETION NOT PRESERVE
# 									ENABLE
# 									COMMENT 'Saves total number of sessions then 
# 												clears the table each day'
# 									DO BEGIN
# 										INSERT INTO site_activity.totals (time, total)
# 											SELECT CURRENT_TIMESTAMP, COUNT(*)
# 												FROM site_activity.sessions;
# 										DELETE FROM site_activity.sessions;
# 									END
# 		character_set_client: utf8mb4
# 		collation_connection: utf8mb4_0900_ai_ci
# 			Database Collation: utf8mb4_0900_ai_ci
#
# character_set_client is the session value of the character_set_client system variable
# when the event was created.
#
# collation_connection is the session value of the collation_connection system variable
# when the event was created.
#
# Database Collation is the collation of the database with which the event is associated.
#
# The output reflects the current status of the event (ENABLE) rather than the status with which
# it was created.
#
# 13.7.6.8 SHOW CREATE FUNCTION SYNTAX
#
# 		SHOW CREATE FUNCTION func_name
#
# This statement is similar to SHOW_CREATE_PROCEDURE but for stored functions.
#
# See SECTION 13.7.6.9, "SHOW CREATE PROCEDURE SYNTAX"
#
# 13.7.6.9 SHOW CREATE PROCEDURE SYNTAX
#
# 		SHOW CREATE PROCEDURE proc_name
#
# This statement is a MySQL extension.
#
# It returns the exact string that can be used to re-create the named stored procedure.
#
# A similar statement, SHOW_CREATE_FUNCTION, displays information about stored functions
# (see SECTION 13.7.6.8, "SHOW CREATE FUNCTION SYNTAX")
#
# To use either statement, you must have the global SELECT privilege.
#
# 		SHOW CREATE PROCEDURE test.simpleproc\G
# 		********************* 1. row ***************************
# 						Procedure: simpleproc
# 						sql_mode : ONLY_FULL_GROUP_BY, STRICT_TRANS_TABLES,
# 									  NO_ZERO_IN_DATE, NO_ZERO_DATE,
# 									  ERROR_FOR_DIVISION_BY_ZERO,
# 									  NO_ENGINE_SUBSTITUTION
# 			  Create Procedure: CREATE PROCEDURE `simpleproc` (OUT param1 INT)
# 									  BEGIN
# 									  SELECT COUNT(*) INTO param1 FROM t;
# 									  END
# 		 character_set_client: utf8mb4
# 		collation_connection : utf8mb4_0900_ai_ci
# 			Database Collation: utf8mb4_0900_ai_ci
#
# 		SHOW CREATE FUNCTION test.hello\G
# 		********************* 1. row ****************************
# 						Function: hello
# 						sql_mode: ONLY_FULL_GROUP_BY, STRICT_TRANS_TABLES,
# 									 NO_ZERO_IN_DATE, NO_ZERO_DATE,
# 									 ERROR_FOR_DIVISION_BY_ZERO,
# 									 NO_ENGINE_SUBSTITUTION
# 			  Create function: CREATE FUNCTION `hello`(s CHAR(20))
# 									 RETURNS char(50) CHARSET utf8mb4
# 									 RETURN CONCAT('Hello, ',s,'!')
# 		character_set_client: utf8mb4
# 		collation_connection: utf8mb4_0900_ai_ci
# 		  Database Collation: utf8mb4_0900_ai_ci
#
# character_set_client is the session value of the character_set_client system variable
# when the routine was created.
#
# collation_connection is the session value of the collation_connection system variable
# when the routine was created.
#
# Database Collation is the collation of the database with which the routine is associated.
#
# 13.7.6.10 SHOW CREATE TABLE SYNTAX
#
# 		SHOW CREATE TABLE tbl_name
#
# Shows the CREATE_TABLE statement that creates the named table.
#
# To use this statement, you must have some privilege for the table.
# This statement also works with views.
#
# 		SHOW CREATE TABLE t\G
# 		********************** 1. row **********************
# 			 	 Table: t
# 		Create Table: CREATE TABLE `t` (
# 			`id` int(11) NOT NULL AUTO_INCREMENT,
# 			`s` char(60) DEFAULT NULL,
# 			PRIMARY KEY (`id`)
# 		) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
#
# SHOW_CREATE_TABLE quotes table and column names according to the value of the sql_quote_show_create
# option.
#
# See SECTION 5.1.8, "SERVER SYSTEM VARIABLES"
#
# For information about how CREATE_TABLE statements are stored by MySQL,
# see SECTION 13.1.20.1, "CREATE TABLE STATEMENT RETENTION"
#
# 13.7.6.11 SHOW CREATE TRIGGER SYNTAX
#
# 		SHOW CREATE TRIGGER trigger_name
#
# This statement shows the CREATE_TRIGGER statement that creates the named trigger.
#
# This statement requires the TRIGGER privilege for the table associated with the trigger.
#
# 		SHOW CREATE TRIGGER ins_sum\G
# 		*************************** 1. row ******************************
# 							Trigger: ins_sum
# 						  sql_mode: ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,
# 										NO_ZERO_IN_DATE,NO_ZERO_DATE,
# 										ERROR_FOR_DIVISION_BY_ZERO,
# 										NO_ENGINE_SUBSTITUTION
# 		SQL Original Statement: CREATE DEFINER=`me`@`localhost` TRIGGER `ins_sum`
# 										BEFORE INSERT ON `account`
# 										FOR EACH ROW SET @sum = @sum + NEW.amount
# 		 character_set_client : utf8mb4
# 		  collation_connection: utf8mb4_0900_ai_ci
# 		   Database Collation : utf8mb4_0900_ai_ci
# 							Created: 2018-08-08 10:10:12.61
#
# SHOW_CREATE_TRIGGER output has these columns:
#
# 		) Trigger: The trigger name
#
# 		) sql_mode: The SQL mode in effect when the trigger executes
#
# 		) SQL Original Statement: The CREATE_TRIGGER statement that defines the trigger
#
# 		) character_set_client: The session value of the character_set_client system variable when the trigger was created
#
# 		) collation_connection: The session value of the collation_connection system variable when the trigger was created
#
# 		) Database Collation: The collation of the database with which the trigger is associated
#
# 		) Created: The date and time when the trigger was created. This is a TIMESTAMP(2) value (with a fractional part in hundredths of seconds) for triggers.
#
# Trigger information is also available from the INFORMATION_SCHEMA TRIGGERS table.
#
# See SECTION 25.33, "THE INFORMATION_SCHEMA TRIGGERS TABLE"
#
# 13.7.6.12 SHOW CREATE USER SYNTAX
#
# 		SHOW CREATE USER user
#
# This statement shows the CREATE_USER statement that creates the named user.
#
# An error occurs if the user does not exist. The statement requires the SELECT privilege
# for the mysql system database, except to see information for the current user.
#
# For the current user, the SELECT privilege for the mysql.user system table is required
# for display of the password hash in the IDENTIFIED AS clause; otherwise, the hash displays
# as <secret>
#
# To name the account, use the format described in SECTION 6.2.4, "SPECIFYING ACCOUNT NAMES"
#
# The host name part of the account name, if omitted, defaults to '%'.
#
# It is also possible to specify CURRENT_USER or CURRENT_USER() to refer to the account
# associated with the current session.
#
# 		SHOW CREATE USER 'root'@'localhost'\G
# 		********************* 1.  row **************************
# 		CREATE USER for root@localhost: CREATE USER 'root'@'localhost'
# 		IDENTIFIED WITH 'mysql_native_password'
# 		AS '*<serial>'
# 		REQUIRE NONE PASSWORD EXPIRE DEFAULT ACCOUNT UNLOCK
#
# TO display the privileges granted to an account, use the SHOW_GRANTS statement.
#
# See SECTION 13.7.6.21, "SHOW GRANTS SYNTAX"
#
# 13.7.6.13 SHOW CREATE VIEW SYNTAX
#
# 		SHOW CREATE VIEW view_name
#
# This statement shows the CREATE_VIEW statement that creates the named view.
#
# 		SHOW CREATE VIEW v\G
# 		************************* 1. row **************************
# 								View: v
# 					Create View  : CREATE ALGORITHM=UNDEFINED
# 										DEFINER=`bob`@`localhost`
# 										SQL SECURITY DEFINER VIEW
# 										`v` AS select 1 AS `a`,2 AS `b`
# 		  character_set_client: utf8mb4
# 		  collation_connection: utf8mb4_0900_ai_ci
#
# character_set_client is the session value of the character_set_client system variable
# when the view was created.
#
# collation_connection is the session value of the collation_connection system variable
# when the view was created.
#
# Use of SHOW_CREATE_VIEW requires the SHOW_VIEW privilege, and the SELECT privilege
# for the view in question.
#
# View information is also available from the INFORMATION_SCHEMA VIEWS table. See SECTION 25.35, "THE INFORMATION_SCHEMA VIEWS TABLE"
#
# MySQL lets you use different sql_mode settings to tell the server the type of SQL syntax to support.
# For example, you might use the ANSI SQL mode to ensure MySQL correctly interprets the standard
# SQL concatenation operator, the double bar (||), in your queries.
#
# If you then create a view that concatenates items, you might worry that changing the sql_mode
# setting to a value different from ANSI could cause the view to become invalid.
#
# But this is not the case. No matter how you write out a view definition, MySQL always
# stores in the same way, in a canonical form.
#
# Here is an example that shows how the server changes a double bar concatenation operator
# to a CONCAT() function:
#
# 		SET sql_mode = 'ANSI';
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		CREATE VIEW test.v AS SELECT 'a' || 'b' as col1;
# 		Query OK, 0 rows affected (0.01 sec)
#
# 		SHOW CREATE VIEW test.v\G
# 		************************** 1. row **************************************
# 								View: v
# 					  Create View: CREATE VIEW "v" AS select concat('a', 'b') AS "col1"
# 		---
# 		1 row in set (0.00 sec)
#
# The advantage of storing a view definition in canonical form is that changes made
# later to the value of sql_mode will not affect the results from the view.
#
# However an additional consequence is that comments prior to SELECT are stripped
# from the definition by the server.
#
# 13.7.6.14 SHOW DATABASES SYNTAX
#
# 		SHOW {DATABASES | SCHEMAS}
# 			[LIKE 'pattern' | WHERE expr]
#
# SHOW_DATABASES lists the databases on the MySQL server host.
#
# SHOW_SCHEMAS is a synonym for SHOW_DATABASES.. The LIKE clause, if present,
# indicates which database names to match.
#
# The WHERE clause can be given to select rows using more general conditions,
# as discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# You see only those databases for which you have some kind of privilege, unless
# you have the global SHOW_DATABASES privilege.
#
# You can also get this list using the mysqlshow command.
#
# If the server was started with the --skip-show-database option, you cannot use
# this statement at all unless you have the SHOW_DATABASES privilege.
#
# MySQL implements databases as directories in the data directory, so this statement
# simply lists directories in that location.
#
# However, the output may include names of directories that do not correspond to actual
# databases.
#
# Database information is also available from the INFORMATION_SCHEMA SCHEMATA table.
#
# See SECTION 25.23, "THE INFORMATION_SCHEMA SCHEMATA TABLE"
#
# 13.7.6.15 SHOW ENGINE SYNTAX
#
# 		SHOW ENGINE engine_name {STATUS | MUTEX}
#
# SHOW_ENGINE displays operational information about a storage engine.
# It requires the PROCESS privilege.
#
# The statement has three variants:
#
# 		SHOW ENGINE INNODB STATUS
# 		SHOW ENGINE INNODB MUTEX
# 		SHOW ENGINE PERFORMANCE_SCHEMA STATUS
#
# SHOW_ENGINE_INNODB_STATUS displays extensive information from the standard
# InnoDB Monitor about the state of the InnoDB storage engine.
#
# For information about the standard monitor and other InnoDB monitors
# that provide information about InnoDB processing, see SECTION 15.16,
# "InnoDB Monitors"
#
# SHOW_ENGINE_INNODB_MUTEX displays InnoDB mutex and rw-lock statistics.
#
# NOTE:
#
# 		InnoDB mutexes and rwlocks can also be monitored using PERFORMANCE SCHEMA tables.
#
# 		See SECTION 15.15.2, "MONITORING INNODB MUTEX WAITS USING PERFORMANCE SCHEMA"
#
# Mutex statistics collection is configured dynamically using the following options:
#
# 		) To enable the collection of mutex statisitcs, run:
#
# 			SET GLOBAL innodb_monitor_enable='latch';
#
# 		) To reset mutex statistics, run:
#
# 			SET GLOBAL innodb_monitor_reset='latch';
#
# 		) To disable the collection of mutex statistics, run:
#
# 			SET GLOBAL innodb_monitor_disable='latch';
#
# Collection of mutex statistics for SHOW_ENGINE_INNODB_MUTEX can also be enabled
# by setting innodb_monitor_enable='all', or disable by setting innodb_monitor_disable='all'
#
# SHOW_ENGINE_INNODB_MUTEX output has these columns:
#
# 		) Type
# 			
# 			Always InnoDB
#
# 		) Name
#
# 			For mutexes, the Name field reports only the mutex name.
#
# 			For rwlocks, the Name field reports the source file where the rwlock
# 			is implemented, and the line number in the file where the rwlock is created.
#
# 			The line number is specific to your version of MySQL.
#
# 		) Status
#
# 			The mutex status.
#
# 			The field reports the number of spins, waits and calls. Statistics for low-level
# 			operating system mutexes, which are implemented outside of InnoDB, are not reported.
#
# 				) spins indicates the number of spins
#
# 				) waits indicates the number of mutex waits
#
# 				) calls indicates how many times the mutex was requested
#
# 			SHOW ENGINE INNODB MUTEX skips the mutexes and rw-locks of buffer pool blocks,
# 			as the amount of output can be overwhelming on systems with a large buffer pool.
#
# 			(There is one mutex and one rw-lock in each 16K buffer pool block, and there are 
# 			65,536 blocks per gigabyte)
#
# 			SHOW ENGINE INNODB MUTEX also does not list any mutexes or rw-locks that have never
# 			been waited on (os_waits=0)
#
# 			Thus, SHOW ENGINE INNODB MUTEX only displays information about mutexes and rw-locks
# 			outside of the buffer pool that have caused at least one OS-level wait.
#
# 			Use SHOW_ENGINE_PERFORMANCE_SCHEMA_STATUS to inspect the internal operation of the
# 			Performance Schema code:
#
# 				SHOW ENGINE PERFORMANCE_SCHEMA STATUS\G
# 				---
# 				************************* 3. row ***************************************
# 					Type: performance_schema
# 					Name: events_waits_history.size
# 					Status: 76
#				************************* 4. row ***************************************
# 					Type: performance_schema
# 					Name: events_waits_history.count
# 					Status: 10000
# 				************************* 5. row ***************************************
# 					Type: performance_schema
# 					name: events_waits_history.memory
# 					Status: 760000
# 				---
# 				************************* 57. row ***************************************
# 					Type: performance_schema
# 					Name: performance_schema.memory
# 				Status : 26459600
# 				---
#
# This statement is intended to help the DBA understand the effects that different Performance
# Schema options have on memory requirements.
#
# Name values consist of two parts, which name an internal buffer and a buffer attribute,
# respectively. Interpret buffer names as follows:
#
# 		) An internal buffer that is not exposed as a table is named within parentheses.
#
# 			Examples: (pfs_cond_class).size, (pfs_mutex_class).memory
#
# 		) An internal buffer that is exposed as a table in the performance_schema database
# 			is named after the table, without parentheses.
#
# 			Examples: events_waits_history.size, mutex_instances.count
#
# 		) A value that applies to the Performance Schema as a whole begins with
# 			performance_schema. 
#
# 			Example: performance_schema.memory
#
# Buffer attributes have these meanings:
#
# 		) size is the size of the internal record used by the implementation, such as the size of a row
# 			in a table.
#
# 			size values cannot be changed
#
# 		) count is the number of internal records, such as the number of rows in a table. 
#
# 			count values can be changed using Performance Schema configuration options.
#
# 		) For a table, tbl_name.memory is the product of size and count.
#
# 			For the Performance Schema as a whole, performance_schema.memory is
# 			the sum of all the memory used (the sum of all other memory values)
#
# In some cases, there is a direct relationship between a Performance Schema configuration
# parameter and a SHOW ENGINE value.
#
# For example, events_waits_history_long.count corresponds to performance_schema_waits_history_long_size
#
# In other cases, the relationship is more complex. 
#
# For example, events_waits_history.count corresponds to performance_schema_events_waits_history_size 
# (the number of rows per thread) multiplied by performance_schema_max_thread_instances (the number of threads)
#
# SHOW ENGINE NDB STATUS
#
# If the server has the NDB storage engine enabled, SHOW ENGINE NDB STATUS displays cluster
# status information such as the number of connected data nodes, the cluster connectstring,
# and cluster binary log epochs, as well as counts of various Cluster API objects created
# by the MySQL Server when connected to the cluster.
#
# Sample output from this statement is shown here:
#
# 		SHOW ENGINE NDB STATUS;
# 		+------------+--------------------------------+--------------------------------------------+
# 		| Type 		 | Name 									 | Status 												 |
# 		+------------+--------------------------------+--------------------------------------------+
# 		| ndbcluster | connection 							 | cluster_node_id=7, 								 |
# 		  connected_host=198.51.100.103, connected_port=1186, number_of_data_nodes=4,
# 		  number_of_ready_data_nodes=3, connect_count=0
# 		| ndbcluster | NdbTransaction 					 | created=6, free=0, sizeof=212 			 	 |
# 		| ndbcluster | NdbOperation 						 | created=8, free=8, sizeof=660 				 |
# 		| ndbcluster | NdbIndexScanOperation 			 | created=1, free=1, sizeof=744 				 |
# 		| ndbcluster | NdbIndexOperation 				 | created=0, free=0, sizeof=664 				 |
# 		| ndbcluster | NdbRecAttr 							 | created=1285, free=1285, sizeof=60 			 |
# 		| ndbcluster | NdbApiSignal 						 | created=16, free=16, sizeof=136 				 |
# 		| ndbcluster | NdbLabel 							 | created=0, free=0, sizeof=196 				 |
# 		| ndbcluster | NdbBranch 							 | created=0, free=0, sizeof=24 					 |
# 		| ndbcluster | NdbSubroutine 						 | created=0, free=0, sizeof=68 					 |
# 		| ndbcluster | NdbCall 								 | created=0, free=0, sizeof=16 					 |
# 		| ndbcluster | NdbBlob 								 | created=1, free=1, sizeof=264 				 |
# 		| ndbcluster | NdbReceiver 						 | created=4, free=0, sizeof=68 					 |
# 		| ndbcluster | binlog 								 | latest_epoch=155467, latest_trans_epoch=148126,
# 		  latest_received_binlog_epoch=0, latest_handled_binlog_epoch=0,
# 		  latest_applied_binlog_epoch=0 																				 |
# 		+------------------------------------------------------------------------------------------+
# 
#
# The Status column in each of these rows provides information about the MySQL server's connection
# to the cluster and about the cluster binary log's status, respectively.
#
# The Status information is in the form of comma-delimited set of name/value pairs.
#
# The connection row's Status column contains the name/value pairs described in the following table.
#
# 		NAME 													VALUE
#
# 	cluster_node_id 								The node ID of the MySQL server in the cluster
#
# 	connected_host 								The host name or IP address of the cluster management server to which the MySQL server is connected
#
# 	connected_port 								The port used by the MySQL server to connect to the management server (connected_host)
#
# 	number_of_data_nodes 						The number of data nodes configured for the cluster (that is, the number of [ndbd] sections in
# 														the cluster config.ini file)
#
# 	number_of_ready_data_nodes 				The number of data nodes in the cluster that are actually running
#
# 	connect_count 									The number of times this mysqld has connected or reconnected to cluster data nodes
#
# The binlog row's Status column contains information relating to NDB Cluster Replication.
#
# The name/value pairs it contains are described in the following table.
#
# 		NAME 													VALUE
#
# latest_epoch 										The most recent epoch most recently run on this MySQL server (that is, the sequence
# 															number of the most recent transaction run on the server)
#
# latest_trans_epoch 								The most recent epoch processed by the cluster's data nodes
#
# latest_received_binlog_epoch 					The most recent epoch received by the binary log thread
#
# latest_handled_binlog_epoch 					The most recent epoch processed by the binary log thread (for writing to the binary log)
#
# latest_applied_binlog_epoch 					The most recent epoch actually written to the binary log
#
# See SECTION 22.6, "NDB CLUSTER REPLICATION", for more information.
#
# The remaining rows from the output of SHOW ENGINE NDB STATUS which are most likely to prove useful
# in monitoring the cluster are listed here by Name:
#
# 		) NdbTransaction: The number and size of NdbTransaction objects that have been created.
#
# 			An NdbTransaction is created each time a table schema operation (such as CREATE_TABLE or ALTER_TABLE)
# 			is performed on an NDB table.
#
# 		) NdbOperation: The number and size of NdbOperation objects that have been created.
#
# 		) NdbIndexScanOperation: The number and size of NdbIndexScanOperation objects that have been created.
#
# 		) NdbIndexOperation: The number and size of NdbIndexOperation objects that have been created.
#
# 		) NdbRecAttr: The number and size of NdbRecAttr objects that have been created.
# 		
# 			In general, one of these is created each time a data manipulation statement is performed
# 			by an SQL node.
#
# 		) NdbBlob: The number and size of NdbBlob objects that have been created. An NdbBlob is created for each
# 			new operation involving a BLOB column in an NDB table.
#
# 		) NdbReceiver: The number and size of any NdbReceiver object that have been created.
#
# 			The number in the created column is the same as the number of data nodes in the cluster
# 			to which the MySQL server was connected.
#
# 			NOTE:
#
# 				SHOW ENGINE NDB STATUS returns an empty result if no operations involving NDB tables
# 				have been performed during the current session by the MySQL client accessing
# 				the SQL node on which this statement is run.
#
# 13.7.6.16 SHOW ENGINES SYNTAX
#
# 		SHOW [STORAGE] ENGINES
#
# SHOW_ENGINES displays status information about the server's storage engines.
#
# This is particularly useful for checking whether a storage engine is supported,
# or to see what the default engine is.
#
# For information about MySQL storage engines, see CHAPTER 15, THE INNODB STORAGE ENGINE, and CHAPTER 16, ALTERNATIVE STORAGE ENGINES
#
# 		SHOW ENGINES\G
# 		*********************** 1. row ********************************
# 				Engine: ARCHIVE
# 			Support  : YES
# 			  Comment: Archive storage engine
# 		Transactions: NO
# 					 XA: NO
# 	   Savepoints  : NO
# 		************************ 2. row **********************************
# 				Engine: BLACKHOLE
# 			 Support : YES
# 			Comment  : /dev/null storage engine (anything you write to it disappears)
# 	Transactions   : NO
# 					 XA: NO
# 		Savepoints  : NO
# 		************************* 3. row ***********************************
# 				Engine: MRG_MYISAM
# 			Support  : YES
# 			Comment  : Collection of identical MyISAM tables
# 		Transactions: NO
# 					XA : NO
# 		Savepoints  : NO
# 		************************* 4. row ************************************
# 				Engine: FEDERATED
# 			Support  : NO
# 			Comment  : Federated MySQL storage engine
# 		Transactions: NULL
# 					XA : NULL
# 		 Savepoints : NULL
# 		************************** 5. row ************************************
# 				Engine: MyISAM
# 			Support  : YES
# 			  Comment: MyISAM storage engine
# 		Transactions: NO
# 					XA : NO
# 		Savepoints  : NO
# 		************************** 6. row **************************************
# 				Engine: PERFORMANCE_SCHEMA
# 			  Support: YES
# 			  Comment: Performance Schema
# 		Transactions: NO
# 					XA : NO
# 		  Savepoints: NO
# 		************************** 7. row **************************************
# 				Engine: InnoDB
# 			  Support: DEFAULT
# 			  Comment: Supports transactions, row-level locking, and foreign keys
# 		Transactions: YES
# 					XA : YES
# 		Savepoints  : YES
# 		************************** 8. row ***************************************
# 				Engine: MEMORY
# 			Support  : YES
# 			  Comment: Hash based, stored in memory, useful for temporary tables
# 		Transactions: NO
# 					XA : NO
# 	    Savepoints : NO
# 		************************** 9. row ***************************************
# 				Engine: CSV
# 			  Support: YES
# 			  Comment: CSV storage engine
# 		Transactions: NO
# 					XA : NO
# 		  Savepoints: NO
#
# The output from SHOW_ENGINES may vary according to the MySQL version used and other factors.
#
# SHOW_ENGINES output has these columns:
#
# 		) Engine
#
# 			The name of the storage engine
#
# 		) Support
#
# 			The server's level of support for the storage engine, as shown in the following table.
#
# 			VALUE 		MEANING
#
# 			YES 			The engine is supported and is active
#
# 			DEFAULT 		Like YES, plus this is the default engine
#
# 			NO 			The engine is not supported
#
# 			DISABLED 	The engine is supported but has been disabled
#
# 			A value of NO means that the server was compiled without support for the engine,
# 			so it cannot be enabled at runtime.
#
# 			A value of DISABLED occurs either because the server was started with an option that
# 			disables the engine, or because not all options required to enable it were given.
#
# 			In the latter case, the error log should contain a reason indicating why the option
# 			is disabled.
#
# 			See SECTION 5.4.2, "THE ERROR LOG"
#
# 			You might also see DISABLED for a storage engine if the server was compiled to support
# 			it, but was started with a --skip-engine_name option.
#
# 			For the NDB storage engine, DISABLED means the server was compiled with support
# 			for NDB Cluster, but was not started with the --ndbcluster option
#
# 			All MySQL servers support MyISAM tables. It is not possible to disable MyISAM
#
# 		) Comment
#
# 			A brief desription of the storage engine
#
# 		) Transactions
#
# 			Whether the storage engine supports transactions
#
# 		) XA
#
# 			Whether the storage engine supports XA transactions
#
# 		) Savepoints
#
# 			Whether the storage engine supports savepoints
#
# Storage engine information is also available from the INFORMATION_SCHEMA ENGINES table.
#
# See SECTION 25.8, "THE INFORMATION_SCHEMA ENGINES TABLE"
#
# 13.7.6.17 SHOW ERRORS SYNTAX
#
# 		SHOW ERRORS [LIMIT [offset,] row_count]
# 		SHOW COUNT(*) ERRORS
#
# SHOW_ERRORS is a diagnostic statement that is similar to SHOW_WARNINGS, except that
# it displays information only for errors, rather than for errors, warnings, and notes.
#
# The LIMIT clause has the same syntax as for the SELECT statement. See SECTION 13.2.10, "SELECT SYNTAX"
#
# The SHOW_COUNT(*)_ERRORS statement displays the number of errors. You can also retrieve this
# number from the error_count variable:
#
# 		SHOW COUNT(*) ERRORS;
# 		SELECT @@error_count;
#
# SHOW_ERRORS and error_count apply only to errors, not warnings or notes.
#
# In other respects, they are similar to SHOW_WARNINGS and warning_count.
#
# In particular, SHOW_ERRORS cannot display information for more than max_error_count
# messages, and error_count can exceed the value of max_error_count if the number
# of errors exceeds max_error_count
#
# For more information, see SECTION 13.7.6.40, "SHOW WARNINGS SYNTAX"
#
# 13.7.6.18 SHOW EVENTS SYNTAX
#
# SHOW EVENTS
# 		[{FROM | IN} schema_name]
# 		[LIKE 'pattern' | WHERE expr]
#
# This statement displays information about Event Manager events, which are discussed
# in SECTION 24.4, "USING THE EVENT SCHEDULER"
#
# It requires the EVENT privilege for the database from which the events are to be shown.
#
# In its simplest form, SHOW_EVENTS lists all of the events in the current schema:
#
# 		SELECT CURRENT_USER(), SCHEMA();
# 		+-----------------+-----------+
# 		| CURRENT_USER()  | SCHEMA()  |
# 		+-----------------+-----------+
# 		| jon@ghidora     | myschema  |
# 		+-----------------+-----------+
# 		1 row in set (0.00 sec)
#
# 		SHOW EVENTS\G
# 		*********************** 1. row ***************************************
# 							Db: myschema
# 						 Name: e_daily
# 					Definer : jon@ghidora
# 				Time zone  : SYSTEM
# 					  Type  : RECURRING
# 			Execute at    : NULL
# 			Interval value: 1
# 			Interval field: DAY
# 					Starts  : 2018-08-08 11:06:34
# 					  Ends  : NULL
# 					Status  : ENABLED
# 				Originator : 1
# 	character_set_client: utf8mb4
# 	collation_connection: utf8mb4_0900_ai_ci
#   Database Collation : utf8mb4_0900_ai_ci
#
# To see events for a specific schema, use the FROM clause. For example, to see events
# for the test schema, use the following statement:
#
# 		SHOW EVENTS FROM test;
#
# The LIKE clause, if present, indicates which event names to match.
#
# The WHERE clause can be given to select rows using more general conditions,
# as discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# SHOW_EVENTS output has these columns:
#
# 		) Db
# 			
# 			The name of the schema (database) to which the event belongs
#
# 		) Name
#
# 			The name of the event
#
# 		) Definer
#
# 			The account of the user who created the event, in 'user_name'@'host_name' format.
#
# 		) Time zone
#
# 			The event time zone, which is the time zone used for scheduling the event and that
# 			is in effect within the event as it executes.
#
# 			The default value is SYSTEM
#
# 		) Type
#
# 			The event repetition type, either ONE TIME (transient) or RECURRING (repeating)
#
# 		) Execute At
#
# 			For a one-time event, this is the DATETIME value specified in the AT clause of the
# 			CREATE_EVENT statement used to create the event, or of the last ALTER_EVENT
# 			statement that modified the event.
#
# 			The value shown in this column reflects the addition or subtraction of any
# 			INTERVAL value included in the event's AT clause.
#
# 			For example, if an event is created using ON SCHEDULE AT CURRENT_TIMESTAMP + '1:6' DAY_HOUR,
# 			and the event was created at 2018-02-09 14:05:30, the value shown in this column
# 			would be '2018-02-10 20:05:30'
#
# 			If the event's timing is determined by an EVERY clause instead of an AT clause
# 			(that is, if the event is recurring), the value of this column is NULL
#
# 		) Interval Value
#
# 			For a recurring event, the number of intervals to wait between event executions.
#
# 			For a transient event, the value of this column is always NULL
#
# 		) Interval Field
#
# 			The time units used for the interval which a recurring event waits before repeating.
#
# 			For a transient event, the value of this column is always NULL
#
# 		) Starts
#
# 			The start date and time for a recurring event.
#
# 			This is displayed as a DATETIME value, and is NULL if no start date and
# 			time are defined for the event.
#
# 			For a transient event, this column is always NULL. For a recurring event
# 			whose definition includes a STARTS clause, this column contains the corresponding
# 			DATETIME value.
#
# 			As with the Execute At column, this value resolves any expressions used.
#
# 			If there is no STARTS clause affecting the timing of the event, this column is NULL
#
# 		) ENDS
#
# 			For a recurring event whose definition includes a ENDS clause, this column contains the
# 			corresponding DATETIME value.
#
# 			As with the Execute At column, this value resolves any expressions used.
#
# 			If there is no ENDS clause affecting the timing of the event, this column is NULL
#
# 		) Status
#
# 			The event status. One of ENABLED, DISABLED or SLAVESIDE_DISABLED.
#
# 			SLAVESIDE_DISABLED indicates that hte creation of the event occurred
# 			on another MySQL server acting as a replication master and replicated
# 			to the current MySQL server which is acting as a slave, but the event
# 			is not presently being executed on the slave.
#
# 			For more information, see SECTION 17.4.1.16, "REPLICATION OF INVOKED FEATURES"
# 			information
#
# 		) Originator
#
# 			The server ID of the MySQL server on which the event was created; used in replication.
#
# 			The default value is 0
#
# 		) character_set_client
#
# 			The session value of the character_set_client system variable when the event
# 			was created.
#
# 		) collation_connection
#
# 			The session value of the collation_connection system variable when the event
# 			was created.
#
# 		) Database Collation
#
# 			The collation of the database with which the event is associated
#
# For more information about SLAVESIDE_DISABLED and the Originator column, see SECTION 17.4.1.16,
# "REPLICATION OF INVOKED FEATURES"
#
# Times displayed by SHOW_EVENTS are given in the event time zone, as discussed in SECTION 24.4.4, "EVENT METADATA"
#
# Event information is also available from the INFORMATION_SCHEMA EVENTS table. See SECTION 25.9, "THE INFORMATION_SCHEMA EVENTS TABLE"
#
# The event action statement is not shown in the output of SHOW_EVENTS.
#
# Use SHOW_CREATE_EVENT or the INFORMATION_SCHEMA EVENTS table.
#
# 13.7.6.19 SHOW FUNCTION CODE SYNTAX
#
# 		SHOW FUNCTION CODE func_name
#
# This statement is similar to SHOW_PROCEDURE_CODE but for stored functions.
#
# See SECTION 13.7.6.27, "SHOW PROCEDURE CODE SYNTAX"
#
# 13.7.6.20 SHOW FUNCTION STATUS SYNTAX
#
# 		SHOW FUNCTION STATUS
# 			[LIKE 'pattern' | WHERE expr]
#
# This statement is similar to SHOW_PROCEDURE_STATUS but for stored functions.
#
# See SECTION 13.7.6.28, "SHOW PROCEDURE STATUS SYNTAX"
#
# 13.7.6.21 SHOW GRANTS SYNTAX
#
# 		SHOW GRANTS
# 			[FOR user_or_role
# 				[USING role [, role] ---]]
#
# 		user_or_role: {
# 			user
# 		 | role
# 		}
#
# This statement displays the privileges and roles that are assigned to a MySQL
# user account or role, in the form of GRANT statements that must be executed
# to duplicate the privilege and role assignments.
#
# NOTE:
#
# 		To display nonprivilege information for MySQL accounts, use the SHOW_CREATE_USER
# 		statement.
#
# 		See SECTION 13.7.6.12, "SHOW CREATE USER SYNTAX"
#
# SHOW_GRANTS requires the SELECT privilege for the mysql system database, except to
# display privileges and roles for the current user.
#
# To name the account or role for SHOW_GRANTS, use the same format as for the  GRANT
# statement; for example, 'jeffrey'@'localhost':
#
# 		SHOW GRANTS FOR 'jeffrey'@'localhost';
# 		+-----------------------------------------------------------------+
# 		| Grants for jeffrey@localhost 												|
# 		+-----------------------------------------------------------------+
# 		| GRANT USAGE ON *.* TO `jeffrey`@`localhost`   						|
# 		| GRANT SELECT, INSERT, UPDATE ON `db1`.* TO `jeffrey`@`localhost`|
# 		+-----------------------------------------------------------------+
#
# The host part, if omitted, defaults to '%'
#
# For additional information about specifying account and role names,
# See SECTION 6.2.4, "SPECIFYING ACCOUNT NAMES", and SECTION 6.2.5, "SPECIFYING ROLE NAMES"
#
# To display the privileges granted to the current user (the account you are using to connect
# to the server), you can use any of the following statements:
#
# 		SHOW GRANTS;
# 		SHOW GRANTS FOR CURRENT_USER;
# 		SHOW GRANTS FOR CURRENT_USER();
#
# If SHOW GRANTS FOR CURRENT_USER (or any of the equivalent syntaxes) is used in
# definer context, such as within a stored procedure that executes with definer
# rather than invoker privileges, the grants displayed are those of the definer
# and not hte invoker.
#
# In MySQL 8.0 compared to previous series, SHOW_GRANTS no longer displays ALL_PRIVILEGES
# in its global-privileges output because the meaning of ALL_PRIVILEGES at the
# global level varies depending on which dynamic privileges are defined.
#
# Instead, SHOW_GRANTS explicitly lists each granted global privilege:
#
# 		SHOW GRANTS FOR 'root'@'localhost';
# 		+-------------------------------------------------------+
# 		| Grants for root@localhost 								     |
# 		+-------------------------------------------------------+
# 		| GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP,   |
# 		| RELOAD, SHUTDOWN, PROCESS, FILE, REFERENCES, INDEX,   |
# 		| ALTER, SHOW DATABASES, SUPER, CREATE TEMPORARY TABLES,|
# 		| LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION  |
# 		| CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, 		  |
# 		| ALTER ROUTINE, CREATE USER, EVENT, TRIGGER, CREATE    |
# 		| TABLESPACE, CREATE ROLE, DROP ROLE ON *.* TO  		  |
# 		| `root`@`localhost` WITH GRANT  							  |
# 		| OPTION  															  |
# 		| GRANT PROXY ON ''@'' TO 'root'@'localhost' WITH       |
# 		|  GRANT OPTION       											  |
# 		+-------------------------------------------------------+
#
# Applications that process SHOW_GRANTS output should be adjusted accordingly.
#
# At the global level, GRANT_OPTION applies to all granted static global
# privileges if granted for any of them, but applies individually to granted
# dynamic privileges.
#
# SHOW_GRANTS displays global privileges this way:
#
# 		) One line listing all granted static privileges, if there are any, including WITH GRANT OPTION if appropriate
#
# 		) One line listing all granted dynamic privileges for which GRANT_OPTION is granted, if there are any,
# 			including WITH GRANT OPTION
#
# 		) One line listing all granted dynamic privileges for which GRANT_OPTION is not granted, if there
# 			are any, without WITH GRANT OPTION
#
# With the optional USING clause, SHOW_GRANTS enables you to examine the privileges associated
# with roles for the user.
#
# Each role named in the USING clause must be granted to the user.
#
# Suppose that user u1 is assigned roles r1 and r2, as follows:
#
# 		CREATE ROLE 'r1', 'r2';
# 		GRANT SELECT ON db1.* TO 'r1';
# 		GRANT INSERT, UPDATE, DELETE ON db1.* TO 'r2';
# 		CREATE USER 'u1'@'localhost' IDENTIFIED BY 'u1pass';
# 		GRANT 'r1', 'r2' TO 'u1'@'localhost';
#
# SHOW_GRANTS without USING shows the granted roles:
#
# 		SHOW GRANTS FOR 'u1'@'localhost';
# 		+-----------------------------------------------+
# 		| Grants for u1@localhost 								|
# 		+-----------------------------------------------+
# 		| GRANT USAGE ON *.* TO `u1`@`localhost`  		|
# 		| GRANT `r1`@`%`, `r2`@`%` TO `u1`@`localhost`  |
# 		+-----------------------------------------------+
#
# Adding a USING clause causes the statement to also display the privileges
# associated with each role named in the clause:
#
# 		SHOW GRANTS FOR 'u1'@'localhost' USING 'r1';
# 		+--------------------------------------------+
# 		| Grants for u1@localhost 					 		|
# 		+--------------------------------------------+
# 		| GRANT USAGE ON *.* TO `u1`@`localhost`		|
# 		| GRANT SELECT ON `db1`.* TO `u1`@`localhost`|
# 		| GRANT `r1`@`%`,`r2`@`%` TO `u1`@`localhost`|
# 		+--------------------------------------------+
# 		SHOW GRANTS FOR 'u1'@'localhost' USING 'r2';
# 		+------------------------------------------------------------+
# 		| Grants for u1@localhost 												 |
# 		+------------------------------------------------------------+
# 		| GRANT USAGE ON *.* TO `u1`@`localhost` 							 |
# 		| GRANT INSERT, UPDATE, DELETE ON `db1`.* TO `u1`@`localhost`|
# 		| GRANT `r1`@`%`, `r2`@`%` TO `u1`@`localhost`					 |
# 		+------------------------------------------------------------+
# 		SHOW GRANTS FOR 'u1'@'localhost' USING 'r1', 'r2';
# 		+--------------------------------------------------------------------+
# 		| Grants for u1@localhost 												 			|
# 		+--------------------------------------------------------------------+
# 		| GRANT USAGE ON *.* TO `u1`@`localhost` 							 			|
# 		| GRANT SELECT, INSERT, UPDATE, DELETE ON `db1`.* TO `u1`@`localhost`|
# 		| GRANT `r1`@`%`,`r2`@`%` TO `u1`@`localhost`								|
# 		+--------------------------------------------------------------------+
#
# NOTE:
#
# 		A privilege granted to an account is always in effect, but a role is not.
#
# 		The active roles for an account can differ across and within sessions,
# 		depending on the value of the activate_all_roles_on_login system
# 		variable, the account default roles, and whether SET_ROLE has been executed
# 		within a session.
#
# SHOW_GRANTS does not display privileges that are available to the named account
# but are granted to a different account.
#
# For example, if an anonymous account exists, the named account might be able to
# use its privileges, but SHOW_GRANTS does not display them.
#
# SHOW_GRANTS displays mandatory roles named in the mandatory_roles system variable
# value as follows:
#
# 		) SHOW_GRANTS without a FOR clause displays privileges for the current user,
# 			and includes mandatory roles
#
# 		) SHOW_GRANTS_FOR_user displays privileges for the name used, and does not include
# 			mandatory roles.
#
# This behavior is for the benefit of applications that use the output of SHOW_GRANTS_FOR_user
# to determine which privileges are granted explicitly to the named user.
#
# Were that output to include mandatory roles, it would be difficult to distinguish roles
# granted explicitly to the user from mandatory roles.
#
# For the current user, applications can determine privileges with or without mandatory
# roles by using SHOW_GRANTS or SHOW_GRANTS_FOR_CURRENT_USER, respectively.
#
# 13.7.6.22 SHOW INDEX SYNTAX
#
# 		SHOW [EXTENDED] {INDEX | INDEXES | KEYS}
# 			{FROM | IN} tbl_name
# 			[{FROM | IN} db_name]
# 			[WHERE expr]
#
# SHOW_INDEX returns table index information.
#
# The format resembles that of the SQLStatistics call in ODBC.
#
# This statement requires some privilege for any column in the table.
#
# 		SHOW INDEX FROM City\G
# 		*********************** 1. row ******************************
# 					Table: city
# 			Non_unique : 0
# 				Key_name: PRIMARY
# 		Seq_in_index  : 1
# 		Column_name   : ID
# 			Collation  : A
# 		Cardinality   : 4188
# 			Sub_part   : NULL
# 			  Packed   : NULL
# 				NULL    : 
# 		Index_type    : BTREE
# 				Comment : 
# 		Index_comment :
# 			Visible    : YES
# 		  Expression  : NULL
# 		*********************** 2. row *********************************
# 					Table: city
# 			Non_unique : 1
# 				Key_name: CountryCode
# 			Seq_in_index: 1
# 			Column_name: CountryCode
# 			 Collation : A
# 			Cardinality: 232
# 				Sub_part: NULL
# 				 Packed : NULL
# 					Null :
# 			Index_type : BTREE
# 			  Comment  :
# 		Index_comment :
# 			Visible    : YES
# 			Expression : NULL
#
# An alternative to tbl_name FROM db_name syntax is db_name.tbl_name
#
# These two statements are equivalent:
#
# 		SHOW INDEX FROM mytable FROM mydb;
# 		SHOW INDEX FROM mydb.mytable;
#
# The optional EXTENDED keyword causes the output to include information about
# hidden indexes that MySQL uses internally and are not accessible by users.
#
# The WHERE clause can be given to select rows using more general conditions,
# as discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# SHOW_INDEX returns the following fields:
#
# 		) Table
#
# 			The name of the table
#
# 		) Non_unique
#
# 			0 if the index cannot contain duplicates, 1 if it can
#
# 		) Key_name
#
# 			The name of the index. If the index is the primary key, the name
# 			is always PRIMARY
#
# 		) Seq_in_index
# 
# 			The column sequence number in the index, starting with 1
#
# 		) Column_name
#
# 			The column name. See also the description for the Expression column
#
# 		) Collation
#
# 			How the column is sorted in the index. This can have values A (ascending), D (descending)
# 			or NULL (not sorted)
#
# 		) Cardinality
#
# 			An estimate of the number of unique values in the index.
#
# 			To update this number, run ANALYZE_TABLE or (for MyISAM tables) myisamchk -a
#
# 			Cardinality is counted based on statistics stored as integers, so the value is not
# 			necessarily exact even for small tables.
#
# 			The higher hte cardinality, the greater the chance that MySQL uses the index when doing joins
#
# 		) Sub_part
#
# 			The index prefix.
#
# 			That is, the number of indexed characters if the column is only partly indexed,
# 			NULL if the entire column is indexed.
#
# 			NOTE:
#
# 				Prefix limits are measured in bytes. However, prefix lengths for index
# 				specifications in CREATE_TABLE, ALTER_TABLE, and CREATE_INDEX statements
# 				are interpreted as number of characters for nonbinary string types
# 				(CHAR, VARCHAR, TEXT) and number of bytes for binary string types
# 				(BINARY, VARBINARY, BLOB)
#
# 				Take this into account when specifying a prefix length for a nonbinary string
# 				column that uses a multibyte character set.
#
# 			For additional information about index prefixes, see SECTION 8.3.5, "COLUMN INDEXES",
# 			and SECTION 13.1.15, "CREATE INDEX SYNTAX"
#
# 		) PACKED
#
# 			Indicates how the key is packed. NULL if it is not
#
# 		) NULL
#
# 			Contains YES if the column may contain NULL values and '' if not
#
# 		) INDEX_TYPE
#
# 			The index method used (BTREE, FULLTEXT, HASH, RTREE)
#
# 		) COMMENT
#
# 			Information about the index not described in its own column, such as disabled if the index is disabled.
#
# 		) INDEX_COMMENT
#
# 			Any comment provided for the index with a COMMENT attribute when the index was created.
#
# 		) VISIBLE
#
# 			Whether the index is visible to the optimizer. See SECTION 8.3.12, "INVISIBLE INDEXES"
#
# 		) EXPRESSION
#
# 			MySQL 8.0.13 and higher supports functional key parts (see FUNCTIONAL KEY PARTS),
# 			which affects both the Column_name and Expression columns:
#
# 				) For a nonfunctional key part, Column_name indicates the column indexed
# 					by the key part and Expression is NULL
#
# 				) For a functional key part, Column_name column is NULL and Expression indicates
# 					the expression for the key part.
#
# Information about table indexes is also available from the INFORMATION_SCHEMA STATISTICS
# table.
#
# See SECTION 25.25, "THE INFORMATION_SCHEMA STATISTICS TABLE"
#
# The extended information about hidden indexes is available only using
# SHOW EXTENDED INDEX; it cannot be obtained from the STATISTICS table.
#
# You can list a table's indexes with the mysqlshow -k db_name tbl_name command
#
# 13.7.6.23 SHOW MASTER STATUS SYNTAX
#
# 		SHOW MASTER STATUS
#
# This statement provides status information about the binary log files of the master.
#
# It requires either the SUPER or REPLICATION_CLIENT privilege.
#
# Example:
#
# 		SHOW MASTER STATUS\G
# 		********************** 1. row **************************
# 						File: master-bin.000002
# 				Position  : 1307
# 			Binlog_Do_DB : test
# 		Binlog_Ignore_DB: manual, mysql
# 	  Executed_Gtid_Set: <serial>
# 		1 row in set (0.00 sec)
#
# When global transaction IDs are in use, Executed_Gtid_Set shows the set of
# GTIDs for transactions that have been executed on the master.
#
# This is the same as the value for the gtid_executed system variable
# on this server, as well as the value for Executed_Gtid_Set in the
# output of SHOW_SLAVE_STATUS on this server.
#
# 13.7.6.24 SHOW OPEN TABLES SYNTAX
#
# 		SHOW OPEN TABLES
# 			[{FROM | IN} db_name]
# 			[LIKE 'pattern' | WHERE expr]
#
# SHOW_OPEN_TABLES lists the non-TEMPORARY tables that are currently open in the
# table cache.
#
# See SECTION 8.4.3.1, "HOW MYSQL OPENS AND CLOSES TABLES"
#
# The FROM clause, if present, restricts the tables shown to those
# present in the db_name database.
#
# The LIKE clause, if present, indicates which table names to match.
#
# The WHERE clause can be given to select rows using more general conditions,
# as discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# SHOW_OPEN_TABLES output has these columns:
#
# 		) Database
#
# 			The database containing the table
#
# 		) Table
#
# 			The table name
#
# 		) In_use
#
# 			The number of table locks or lock requests there are for the table.
#
# 			For example, if one client acquires a lock for a table using:
#
# 				LOCK TABLE t1 WRITE
#
# 			In_use will be 1.
#
# 			If another client issues LOCK TABLE t1 WRITE while the table remains
# 			locked, the client will block waiting for the lock, but the lock request
# 			causes In_use to be 2.
#
# 			If the count is zero, the table is open but not currently being used.
#
# 			In_use is also increased by the HANDLER_---_OPEN statement and decreases
# 			by HANDLER_---_CLOSE
#
# 		) Name_locked
#
# 			Whether the table name is locked. Name locking is used for operations
# 			such as dropping or renaming tables.
#
# If you have no privileges for a table, it does not show up in the ouput from
# SHOW_OPEN_TABLES
#
# 13.7.6.25 SHOW PLUGINS SYNTAX
#
# 		SHOW PLUGINS
#
# SHOW_PLUGINS displays information about server plugins
#
# Example of SHOW_PLUGINS output:
#
# 		SHOW PLUGINS\G
# 		********************* 1. row ************************
# 			Name: binlog
# 		 Status: ACTIVE
# 		  Type : STORAGE ENGINE
# 		Library: NULL
# 		License: GPL
# 		********************** 2. row ************************
# 			Name: CSV
# 		 Status: ACTIVE
# 		  Type : STORAGE ENGINE
# 		Library: NULL
# 		License: GPL
#		*********************** 3. row ************************
# 			Name: MEMORY
# 		 Status: ACTIVE
# 		  Type : STORAGE ENGINE
# 		Library: NULL
# 		License: GPL
# 		*********************** 4. row *************************
# 			Name: MyISAM
# 		  Status: ACTIVE
# 		Type   : STORAGE ENGINE
# 		Library: NULL
# 		License: GPL
# 		---
#
# SHOW_PLUGINS output has these columns:
#
# 		) Name
#
# 			The name used to refer to the plugin in statements such as INSTALL_PLUGIN
# 			and UNINSTALL_PLUGIN
#
# 		) Status
#
# 			The plugin status, one of ACTIVE, INACTIVE, DISABLED, DELETING or DELETED.
#
# 		) Type
#
# 			The type of plugin, such as STORAGE ENGINE, INFORMATION_SCHEMA, or AUTHENTICATION
#
# 		) Library
#
# 			The name of the plugin shared library file.
#
# 			This is the name used to refer to the plugin file in statements such as
# 			INSTALL_PLUGIN and UNINSTALL_PLUGIN
#
# 			This file is located in the direcotry named by the plugin_dir system variable.
#
# 			If the library name is NULL, the plugin is compiled in and cannot be uninstalled
# 			with UNINSTALL_PLUGIN
#
# 		) License
#
# 			How the plugin is licensed; for example, GPL
#
# For plugins installed with INSTALL_PLUGIN, the Name and Library values are also
# registered in the mysql.plugin system table.
#
# For information about plugin data structures that form the basis of the information
# displayed by SHOW_PLUGINS, see SECTION 29.2, "THE MYSQL PLUGIN API"
#
# Plugin information is also available from the INFORMATION_SCHEMA.PLUGINS table
#
# See SECTION 25.17, "THE INFORMATION_SCHEMA PLUGINS TABLE"
#
# 13.7.6.26 SHOW PRIVILEGES SYNTAX
#
# 		SHOW PRIVILEGES
#
# SHOW_PRIVILEGES shows the list of system privileges that the MySQL server supports.
#
# The privileges displayed include all static privileges, and all currently registered
# dynamic privileges.
#
# 		SHOW PRIVILEGES\G
# 		********************** 1. row *************************
# 		Privilege: Alter
# 		Context: Tables
# 		Comment: To alter the table
# 		********************** 2. row **************************
# 		Privilege: Alter routine
# 		Context: Functions, Procedures
# 		Comment: To alter or drop stored functions/procedures
# 		********************** 3. row ***************************
# 		Privilege: Create
# 		Context: Databases, Tables, Indexes
# 		Comment: To create new databases and tables
# 		********************** 4. row ****************************
# 		Privilege: Create routine
# 		Context: Databases
# 		Comment: To use CREATE FUNCTION/PROCEDURE
# 		********************** 5. row *****************************
# 		Privilege: Create temporary tables
# 		Context: Databases
# 		Comment: To use CREATE TEMPORARY TABLE
# 		---
#
# Privileges belonging to a specific user are displayed by the SHOW_GRANTS statement.
#
# See SECTION 13.7.6.21, "SHOW GRANTS SYNTAX", for more information.
#
# 13.7.6.27 SHOW PROCEDURE CODE SYNTAX
#
# 		SHOW PROCEDURE CODE proc_name
#
# this statement is a MySQL extension that is available only for servers that
# have been built with debugging support.
#
# It displays a representation of the internal implementation of the named
# stored procedure.
#
# A similar statement, SHOW_FUNCTION_CODE, displays information about stored
# functions (see SECTION 13.7.6.19, "SHOW FUNCTION CODE SYNTAX")
#
# To use either statement, you must have the global SELECT privilege.
#
# If the named routine is available, each statement produces a result set.
# Each row in the result set corresponds to one "instruction" in the routine.
#
# The first column is Pos, which is an ordinal number beginning with 0.
#
# The second column is Instruction, which contains an SQL statement (usually
# changed from the original source), or a directive which has meaning only to
# the stored-routine handler.
#
# 		DELIMITER //
# 		CREATE PROCEDURE p1 ()
# 		BEGIN
# 			DECLARE fanta INT DEFAULT 55;
# 			DROP TABLE t2;
# 			LOOP
# 				INSERT INTO t3 VALUES (fanta);
# 				END LOOP;
# 			END//
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		SHOW PROCEDURE CODE p1//
# 		+------+----------------------------------------+
# 		| Pos  | Instruction 								   |
# 		+------+----------------------------------------+
# 		| 0    | set fanta@0 55 							   |
# 		| 1    | stmt 9 "DROP TABLE t2" 				      |
# 		| 2    | stmt 5 "INSERT INTO t3 VALUES (fanta)" |
# 		| 3  	 | jump 2 											|
# 		+------+----------------------------------------+
# 		4 rows in set (0.00 sec)
#
# In this example, the nonexecutable BEGIN and END statements have disappeared, and for
# the DECLARE variable_name statement, only the executable part appears (the part where
# the default is assigned)
#
# For each statement that is taken from source, there is a code word stmt followed
# by a type (9 means DROP, 5 means INSERT, and so on)
#
# Tthe final row contains an instruction jump 2, meaning GOTO instruction #2
#
# 13.7.6.28 SHOW PROCEDURE STATUS SYNTAX
#
# 		SHOW PROCEDURE STATUS
# 			[LIKE 'pattern' | WHERE expr]
#
# This statement is a MySQL extension. It returns characteristics of a stored procedure,
# such as the database, name, type, creator, creation and modification dates, and
# character set information.
#
# A similar statement, SHOW_FUNCTION_STATUS, displays information about stored functions
# (see SECTION 13.7.6.20, "SHOW FUNCTION STATUS SYNTAX")
#
# The LIKE clause, if present, indicates which procedure or function names to match.
#
# The WHERE clause can be given to select rows using more general conditions, as
# discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# 		SHOW PROCEDURE STATUS LIKE 'sp1'\G
# 		************************** 1. row *****************************
# 								Db: test
# 							Name : sp1
# 							 Type: PROCEDURE
# 						 Definer: testuser@localhost
# 					  Modified : 2018-08-08 13:54:11
# 					   Created : 2018-08-08 13:54:11
# 				Security_type : DEFINER
# 						Comment : 
# 		character_set_client: utf8mb4
# 		collation_connection: utf8mb4_0900_ai_ci
# 		  Database Collation: utf8mb4_0900_ai_ci
#
# character_set_client is the session value of the character_set_client system variable
# when the routine was created.
#
# collation_connection is the session value of the collation_connection system variable
# when the routine was created.
#
# Database Collation is the collation of the database with which the routine is associated.
#
# Stored routine information is also available from the INFORMATION_SCHEMA PARAMETERS
# and ROUTINES tables.
#
# See SECTION 25.15, "THE INFORMATION_SCHEMA PARAMETERS TABLE", and SECTION 25.22, "THE INFORMATION_SCHEMA ROUTINES TABLE"
#
# 13.7.6.29 SHOW PROCESSLIST SYNTAX
#
# 		SHOW [FULL] PROCESSLIST
#
# SHOW_PROCESSLIST shows which threads are running. If you have the PROCESS privilege,
# you can see all threads.
#
# Otherwise, you can see only your own threads (that is, threads associated with the
# MySQL account that you are using)
#
# If you do not use the FULL keyword, only the first 100 characters of each statement
# are shown in the Info field.
#
# The SHOW_PROCESSLIST statement is very useful if you get the "too many connections"
# error message and want to find out what is going on.
#
# MySQL reserves one extra connection to be used by accounts that have the CONNECTION_ADMIN
# or SUPER privilege, to ensure that administrators should always be able to connect
# and check the system (assuming that you are not giving this privilege to all your users)
#
# Threads can be killed with the KILL statement. See SECTION 13.7.7.4, "KILL SYNTAX"
#
# Example of SHOW_PROCESSLIST output:
#
# 		SHOW FULL PROCESSLIST\G
# 		********************** 1. row *******************************
# 		Id: 1
# 		User: system user
# 		Host:
# 		db: NULL
# 		Command: Connect
# 		Time: 1030455
# 		State: Waiting for master to send event
# 		Info: NULL
# 		********************* 2. row *********************************
# 		Id: 2
# 		User: system user
# 		Host:
# 		db: NULL
# 		Command: Connect
# 		Time: 1004
# 		State: Has read all relay log; waiting for the slave I/O thread to update it
# 		Info: NULL
# 		********************** 3. row *********************************
# 		Id: 3112
# 		User: replikator
# 		Host: artemis:2204
# 		db: NULL
# 		Command: Binlog Dump
# 		Time: 2144
# 		State: Has sent all binlog to slave; waiting for binlog to be updated
# 		Info: NULL
# 		********************* 4. row ***********************************
# 		Id: 3113
# 		User: replikator
# 		Host: iconnect2:45781
# 		db: NULL
# 		Command: Binlog Dump
# 		Time: 2086
# 		State: Has sent all binlog to slave; waiting for binlog to be updated
# 		Info: NULL
# 		******************** 5. row ************************************
# 		Id: 3123
# 		User: stefan
# 		Host: localhost
# 		db: apollon
# 		Command: Query
# 		Time: 0
# 		State: NULL
# 		Info: SHOW FULL PROCESSLIST
# 		5 rows in set (0.00 sec)
#
# SHOW_PROCESSLIST output has these columns:
#
# 		) Id
#
# 			The connection identifier.
#
# 			This is the same type of value displayed in the ID column of the
# 			INFORMATION_SCHEMA PROCESSLIST table, the PROCESSLIST_ID column of
# 			the Performance Schema threads table, and returned by the CONNECTION_ID() function.
#
# 		) User
#
# 			THe MySQL user who issued the statement. A value of system user refers to a nonclient
# 			thread spawned by the server to handle tasks internally.
#
# 			This could be the I/O or SQL thread used on replication slaves or a delayed-row handler.
#
# 			For system user, there is no host specified in the Host column.
#
# 			unauthenticated user refers to a thread that has become associated with a client connection
# 			but for which authentication of the client user has not yet been done.
#
# 			event_scheduler refers ot the thread that monitors scheduled events
# 			(see SECTION 24.4, "USING THE EVENT SCHEDULER")
#
# 		) Host
#
# 			The host name of the client issuing the statement (except for system user, for which
# 			there is no host)
#
# 			The host name for TCP/IP connections is reported in host_name:client_port format
# 			to make it easier to determine which client is doing what.
#
# 		) db
#
# 			The default database, if one is selected; otherwise NULL
#
# 		) Command
#
# 			The type of command the thread is executing.
#
# 			For descriptions for thread commands, see SECTION 8.14, "EXAMINING THREAD INFORMATION"
#
# 			The value of this column corresponds to the COM_xxx commands of the client/server
# 			protocol and Com_xxx status variables.
#
# 			See SECTION 5.1.10, "SERVER STATUS VARIABLES"
#
# 		) Time
#
# 			The time in seconds that the thread has been in its current state.
#
# 			For a slave SQL thread, the value is the number of seconds between
# 			the timestamp of the last replicated event and the real time of the slave
# 			machine.
#
# 			See SECTION 17.2.2, "REPLICATION IMPLEMENTATION DETAILS"
#
# 		) State
#
# 			An action, event or state that indicates what the thread is doing.
#
# 			Descriptions for State values can be found at SECTION 8.14, "EXAMINING THREAD INFORMATION"
#
# 			Most states correspond to very quick operations. If a thread stays in a given state
# 			for many seconds, there might be a problem that needs to be investigated.
#
# 			For the SHOW_PROCESSLIST statement, the value of State is NULL
#
# 		) Info
#
# 			The statement the thread is executing, or NULL if it is not executing any statement.
#
# 			The statement might be the one sent ot the server, or an innermost statement if the
# 			statement executes other statements.
#
# 			For example, if a CALL statement executes a stored procedure that is executing
# 			a SELECT statement, the Info value shows the SELECT statement.
#
# Process information is also available from the mysqladmin processlist command, the
# INFORMATION_SCHEMA PROCESSLIST table, and the Performance Schema threads table
#
# (see SECTION 4.5.2, "MYSQLADMIN -- CLIENT FOR ADMINISTERING A MYSQL SERVER", SECTION 25.18, "THE INFORMATION_SCHEMA PROCESSLIST TABLE",
# and SECTION 26.12.17.5, "THE THREADS TABLE")
#
# In contrast to the INFORMATION_SCHEMA PROCESSLIST table and SHOW_PROCESSLIST statement,
# which have negative performance consequences because they require a mutex, access to 
# threads does not require a mutex and has minimal impact on the server performance.
#
# The threads table also shows information about background threads, which the PROCESSLIST
# table and SHOW_PROCESSLIST do not.
#
# This means that threads can be used to monitor activity the other thread information
# sources cannot.
#
# 13.7.6.30 SHOW PROFILE SYNTAX
#
# 		SHOW PROFILE [type [, type] --- ]
# 			[FOR QUERY n]
# 			[LIMIT row_count [OFFSET offset]]
#
# 		type: {
# 			ALL
# 		 | BLOCK IO
# 		 | CONTEXT SWITCHES
#      | CPU
#      | IPC
# 	    | MEMORY
# 		 | PAGE FAULTS
# 		 | SOURCE
# 		 | SWAPS
# 		}
#
# The SHOW_PROFILE and SHOW_PROFILES statements display profiling information that indicates
# resource usage for statements executed during the course of the current session.
#
# NOTE:
#
# 		The SHOW_PROFILE and SHOW_PROFILES statements are deprecated and will be removed in a future
# 		MySQL release.
#
# 		Use the PERFORMANCE SCHEMA instead; see SECTION 26.19.1, "QUERY PROFILING USING PERFORMANCE SCHEMA"
#
# To control profiling, use the profiling session variable, which has a default value of 0 (OFF)
#
# Enable profiling by setting profiling to 1 or ON:
#
# 		SET profiling = 1;
#
# SHOW_PROFILES displays a list of the most recent statements sent to the server.
#
# The size of the list is controlled by the profiling_history_size session variable,
# which has a default value of 15.
#
# The maximum value is 100.
#
# Setting the value to 0 has the practical effect of disabling profiling.
#
# All statements are profiled except SHOW_PROFILE and SHOW_PROFILES, so you will find
# neither of those statements in the profile list.
#
# Malformed statements are profiled. For example, SHOW PROFILING is an illegal statement,
# and a syntax error occurs if you try to execute it, but it will show up in the
# profiling list.
#
# SHOW_PROFILE displays detailed information about a single statement.
#
# Without the FOR QUERY n clause, the output pertains to the most recently
# executed statement.
#
# If FOR QUERY n is included, SHOW_PROFILE displays information for statement n.
#
# The values of n correspond to the Query_ID values displayed by SHOW_PROFILES.
#
# The LIMIT row_count clause may be given to limit the output to row_count rows.
#
# If LIMIT is given, OFFSET offset may be added to begin the output offset rows
# into the full set of rows.
#
# By default, SHOW_PROFILE displays Status and Duration columns.
#
# The Status values are like the State values displayed by SHOW_PROCESSLIST,
# although there might be some minor differences in interpretation for the two
# statements for some status values (see SECTION 8.14, "EXAMINING THREAD INFORMATION")
#
# Optional type values may be specified to display specific additional types of information:
#
# 		) ALL displays all information
#
# 		) BLOCK IO displays counts for block input and output operations
#
# 		) CONTEXT SWITCHES displays counts for voluntary and involuntary context switches
#
# 		) CPU displays user and system CPU usage times
#
# 		) IPC displays counts for messages sent and received
#
# 		) MEMORY is not currently implemented
#
# 		) PAGE FAULTS displays counts for major and minor page faults
#
# 		) SOURCE displays the names of functions from the source code, together with the name
# 			and line number of the file in which the function occurs
#
# 		) SWAPS displays swap counts
#
# Profiling is enabled per session.
#
# When a session ends, its profiling information is lost.
#
# SELECT @@profiling;
# +-----------------------+
# | @@profiling 			  |
# +-----------------------+
# | 			0 				  |
# +-----------------------+
# 1 row in set (0.00 sec)
#
# SET profiling = 1;
# Query OK, 0 rows affected (0.00 sec)
#
# DROP TABLE IF EXISTS t1;
# Query OK, 0 rows affected, 1 warning (0.00 sec)
#
# CREATE TABLE T1 (id INT);
# Query OK, 0 rows affected (0.01 sec)
#
# SHOW PROFILES;
# +-----------+-------------+-----------------------------------+
# | Query_ID  | Duration 	 | Query 									 |
# +-----------+-------------+-----------------------------------+
# | 0 		  | 0.000088    | SET PROFILING = 1 					 |
# | 1 		  | 0.000136    | DROP TABLE IF EXISTS t1 			 |
# | 2 		  | 0.011947    | CREATE TABLE t1 (id INT) 			 |
# +-----------+-------------+-----------------------------------+
# 3 rows in set (0.00 sec)
#
# SHOW PROFILE;
# +------------------------------+---------------+
# | Status 								| Duration 		 |
# +------------------------------+---------------+
# | checking permissions 			| 0.000040 		 |
# | creating table 					| etc. 			 |
# | After create 						| etc. 			 |
# | query end 							| 0.000375 		 |
# | freeing items 				   | 0.000089 		 |
# | logging slow query 				| 0.000019 		 |
# | cleaning up 						| 0.000005 		 |
# +------------------------------+---------------+
# 7 rows in set (0.00 sec)
#
# SHOW PROFILE FOR QUERY 1;
# +--------------------------+----------------+
# | Status 						  | Duration 		 |
# +--------------------------+----------------+
# | query end  				  | 0.000107 		 |
# | freeing items 			  | 0.000008 		 |
# | logging slow query 		  | 0.000015 		 |
# | cleaning up 				  | 0.000006 		 |
# +--------------------------+----------------+
# 4 rows in set (0.00 sec)
#
# SHOW PROFILE CPU FOR QUERY 2;
# +--------------------------+----------------+-------------+-----------------+
# | Status 						  | 	Duration 	 | CPU_user 	| CPU_system 		|
# +--------------------------+----------------+-------------+-----------------+
# | checking permissions 	  | 0.000040 		 | 0.000038 	| 0.000002 			|
# | creating table 			  | 0.000056 		 | 0.000028 	| 0.000028 			|
# | After create 				  | 0.011363 		 | 0.000217 	| 0.001571 			|
# | query end 					  | 0.000375 		 | 0.000013 	| 0.000028 			|
# | freeing items 			  | 0.000089 		 | 0.000010 	| 0.000014 			|
# | logging slow query 		  | 0.000019 		 | 0.000009 	| 0.000010 			|
# | cleaning up 				  | 0.000005 		 | 0.000003 	| 0.000002 			|
# +--------------------------+----------------+-------------+-----------------+
# 7 rows in set (0.00 sec)
#
# NOTE:
#
# 		Profiling is only partially functional on some architechtures.
#
# 		For values that depend on the getrusage() system call, NULL is returned
# 		on systems such as Windows that do not support the call.
#
# 		In addition, profiling is per process and not per thread.
#
# 		This means that activity on threads within the server other than your
# 		own may affect the timing information that you see
#
# Profiling information is also available from the INFORMATION_SCHEMA
# PROFILING table.
#
# See SECTION 25.19, "THE INFORMATION_SCHEMA PROFILING TABLE"
#
# For example, the following queries are equivalent:
#
# 		SHOW PROFILE FOR QUERY 2;
#
# 		SELECT STATE, FORMAT(DURATION, 6) AS DURATION
# 		FROM INFORMATION_SCHEMA.PROFILING
# 		WHERE QUERY_ID = 2 ORDER BY SEQ;
#
# 13.7.6.31 SHOW PROFILES SYNTAX
#
# 		SHOW PROFILES
#
# the SHOW_PROFILES statement, together with SHOW_PROFILE, displays
# profiling information that indicates resource usage for statements executed
# during the course of the current session.
#
# For more information, see SECTION 13.7.6.30, "SHOW PROFILE SYNTAX"
#
# NOTE:
#
# 		The SHOW_PROFILE and SHOW_PROFILES statements are deprecated and will be
# 		removed in a future MySQL release.
#
# 		Use the Performance Schema instead; see SECTION 26.19.1, "QUERY PROFILING USING PERFORMANCE SCHEMA"
#
# 13.7.6.32 SHOW RELAYLOG EVENTS SYNTAX
#
# 		SHOW RELAYLOG EVENTS
# 			[IN 'log_name']
# 			[FROM pos]
# 			[LIMIT [offset,] row_count]
# 			[channel_option]
#
# 		channel_option:
# 			FOR CHANNEL channel
#
# Shows the events in the relay log of a replication slave.
#
# If you do not specify 'log_name', the first relay log is displayed.
#
# This statement has no effect on the master.
#
# The LIMIT clause has the same syntax as for the SELECT statement.
# See SECTION 13.2.10, "SELECT SYNTAX"
#
# NOTE:
#
# 		Issuing a SHOW_RELAYLOG_EVENTS with no LIMIT clause could start
# 		a very time- and resource-consuming process because the server returns
# 		to the client the complete contents of the relay log
#
# 		(including all statements modifying data that have been received by the slave)
#
# The optional FOR CHANNEL channel clause enables you to name which replication channel
# the statement applies to.
#
# Providing a FOR CHANNEL channel clause applies the statement to a specific replication
# channel.
#
# If no channel is named and no extra channels exist, the statement applies to the default channel.
#
# When using multiple replication channels, if a SHOW_RELAYLOG_EVENTS statement does not have
# a channel defined using a FOR CHANNEL channel clause an error is generated.
#
# See SECTION 17.2.3, "REPLICATION CHANNELS" for more information
#
# SHOW_RELAYLOG_EVENTS displays the following fields for each event in the relay log:
#
# 		) Log_name
#
# 			the name of the file that is being listed
#
# 		) Pos
#
# 			The position at which the event occurs
#
# 		) Event_type
#
# 			An identifier that describes the event type
#
# 		) Server_id
#
# 			The server ID of hte server on which the event originated
#
# 		) End_log_pos
#
# 			The value of End_log_pos for htis event in the master's binary log
#
# 		) Info
#
# 			More detailed information about the event type.
#
# 			The format of this information depends on the event type.
#
# NOTE:
#
# 		Some events relating to the setting of user and system variables are not included
# 		in the output from SHOW_RELAYLOG_EVENTS
#
# 		To get complete coverage of events within a relay log, use mysqlbinlog
#
# 13.7.6.33 SHOW SLAVE HOSTS SYNTAX
#
# 		SHOW SLAVE HOSTS
#
# Displaying a list of replication slaves currently registered with the master
#
# SHOW SLAVE HOSTS should be executed on a server that acts as a replication master.
#
# The statement displays information about servers that are or have been connected
# as replication slaves, with each row of the result corresponding to one slave server,
# as shown here:
#
# 		SHOW SLAVE HOSTS;
# 		+------------+-------------+-------+---------------+------------------------------+
# 		| Server_id  | Host 			| Port  | Master_id 		| Slave_UUID 						 |
# 		+------------+-------------+-------+---------------+------------------------------+
# 		| <id> 	    | iconnect2 	| 3306  | <id> 			| <serial> 							 |
# 		| <id> 		 | athena 		| 3306  | <id> 			| <serial> 							 |
# 		+------------+-------------+-------+---------------+------------------------------+
#
# 		) Server_id: The unique server ID of the slave server, as configured in the slave server's
# 			option file, or on the command line with --server-id=value
#
# 		) Host: The host name of the slave server as specified on the slave with the --report-host
# 			option.
#
# 			THis can differ from the machine name as configured in the operating system
#
# 		) User: The slave server uses name as, specified on the slave with the --report-user option
#
# 			Statement output includes this column only if the master server is started with the
# 			--show-slave-auth-info option
#
# 		) Password: The slave server password as, specified on the slave with --report-password option
#
# 			Statement output includes this column only if the master server is started with the
# 			--show-slave-auth-info option
#
# 		) Port: The port on the master to which the slave server is listening, as specified
# 			on the slave with the --report-port option
#
# 			A zero in this column means that the slave port (--report-port) was not set
#
# 		) Master_id: The unique server ID of the master server that the slave server is
# 			replicating from.
#
# 			This is the server ID of the server on which SHOW SLAVE HOSTS is executed,
# 			so this same value is listed for each row in the result.
#
# 		) Slave_UUID: The globally unique Id of this slave, as generated on the slave
# 			and found in the slave's auto.cnf file
#
# 13.7.6.34 SHOW SLAVE STATUS SYNTAX
#
# 		SHOW SLAVE STATUS [FOR CHANNEL channel]
#
# This statement provides status information on essential parameters of the slave threads.
#
# It requires either the SUPER or REPLICATION_CLIENT privilege.
#
# SHOW SLAVE STATUS is nonblocking.
#
# When run concurrently with STOP_SLAVE, SHOW SLAVE STATUS returns without waiting for
# STOP SLAVE to finish shutting down the slave SQL thread or slave I/O thread (or both)
#
# This permits use in monitoring and other applications where getting an immediate
# response from SHOW SLAVE STATUS more important than ensuring that it returned
# the latest data.
#
# If you issue this statement using the mysql client, you can use a \G statement terminator
# rather than a semicolon to obtain a more readable vertical layout:
#
# 		SHOW SLAVE STATUS\G
# 		******************************** 1. row ***********************************
# 								Slave_IO_State: Waiting for master to send event
# 								 Master_Host  : localhost
# 								   Master_User: repl
# 								   Master_Port: 13000
# 								 Connect_Retry: 60
# 							  Master_Log_File: master-bin.000002
# 						 Read_Master_Log_Pos: 1307
# 								Relay_Log_File: slave-relay-bin.000003
# 								 Relay_Log_Pos: 1508
# 					  Relay_Master_Log_File: master-bin.000002
# 						Slave_IO_Running    : Yes
# 							SLave_SQL_Running: Yes
# 							  Replicate_Do_DB: 
# 						Replicate_Ignore_DB : 
# 						  Replicate_Do_Table:
# 					 Replicate_Ignore_Table: 		
#  				Replicate_Wild_Do_Table: 
# 			  Replicate_Wild_Ignore_Table:
# 								Last_Errno    : 0
# 									Last_Error : 
# 								Skip_Counter  : 0
# 						Exec_Master_Log_Pos : 1307
# 							Relay_Log_Space  : 1858
# 							Until_Condition  : None
# 							 Until_Log_File  : 
# 							   Until_Log_Pos : 0
# 						Master_SSL_Allowed  : No
# 						Master_SSL_CA_File  : 
# 						Master_SSL_CA_Path  :
# 							Master_SSL_Cert  :
# 						Master_SSL_Cipher   : 
# 					Seconds_Behind_Master  : 0
# 			Master_SSL_Verify_Server_Cert: No
# 						Last_IO_Errno 		  : 0
# 						Last_IO_Error 		  :
# 						Last_SQL_Errno 	  : 0
# 						Last_SQL_Error 	  : 
# 			Replicate_Ignore_Server_Ids  : 
# 					Master_Server_Id  	  : 1
# 							Master_UUID 	  : <serial>
# 					Master_Info_File 		  : /var/mysqld.2/data/master.info
# 								SQL_Delay 	  : 0
# 					SQL_Remaining_Delay 	  : NULL
# 					Slave_SQL_Running_State: Reading event from the relay log
# 						Master_Retry_Count  : 10
# 						Master_Bind 		  : 
# 					Last_IO_Error_Timestamp:
# 				 Last_SQL_Error_Timestamp : 
# 						Master_SSL_Crl 	  : 
# 						Master_SSL_Crlpath  :
# 						Retrieved_Gtid_Set  : <serial>
# 						Executed_Gtid_Set   : <serial>
# 							Auto_Position    : 1
# 						Replicate_Rewrite_DB:
# 							Channel_name 	  :
# 						Master_TLS_Version  : TLSv1.2
# 					Master_public_key_path : public_key.pem
# 					  Get_master_public_key: 0
#
# The Performance Schema provides tables that expose replication information.
#
# This is similar to the information available from the SHOW_SLAVE_STATUS statement,
# but represented in table form.
#
# For details, see SECTION 26.12.11, "PERFORMANCE SCHEMA REPLICATION TABLES"
#
# The following list describes the fields returned by SHOW_SLAVE_STATUS
#
# For additional information about interpreting their meanings, see SECTION 17.1.7.1,
# "CHECKING REPLICATION STATUS"
#
# 		) Slave_IO_State
#
# 			A copy of the State field of the SHOW_PROCESSLIST output for the slave I/O thread.
#
# 			This tells you what the thread is doing: trying to connect to the master, waiting
# 			for events from the master, reconnecting to the master, and so on.
#
# 			For a listing of possible states, see SECTION 8.14.4, "REPLICATION SLAVE Ì/O THREAD STATES"
#
# 		) Master_Host
#
# 			The master host that the slave is connected to
#
# 		) Master_User
#
# 			The user name of the account used to connect to the master
#
# 		) Master_Port
#
# 			The port used to connect to the master
#
# 		) Connect_Retry
#
# 			The number of seconds between connect retries (default 60)
#
# 			This can be set with the CHANGE_MASTER_TO statement
#
# 		) Master_Log_File
#
# 			The name of the master binary log file from which the I/O thread is currently reading
#
# 		) Read_Master_Log_Pos
#
# 			The position in the current master binary log file up to which the I/O thread has read
#
# 		) Relay_Log_File
#
# 			The name of the relay log file from which the SQL thread is currently reading and executing
#
# 		) Relay_Log_Pos
#
# 			The position in the current relay log file up to which the SQL thread has read and executed.
#
# 		) Relay_Master_Log_File
#
# 			The name of the master binary log file containing the most recent event executed by the SQL thread
#
# 		) Slave_IO_Running
#
# 			Whether the I/O thread is started and has connected successfully to the master.
#
# 			Internally, the state of this thread is represented by one of the following three values:
#
# 				) MYSQL_SLAVE_NOT_RUN. The slave I/O thread is not running. For this state, Slave_IO_Running is No
#
# 				) MYSQL_SLAVE_RUN_NOT_CONNECT. The slave I/O thread is running, but is not connected to a replication master.
# 
# 					For this state, Slave_IO_Running is Connecting
#
# 				) MYSQL_SLAVE_RUN_CONNECT.
#
# 					The slave I/O thread is running, and is connected to a replication master.
#
# 					For this state, Slave_IO_Running is Yes
#
# 			The value of the Slave_running system status variable corresponds with this value
#
# 		) Slave_SQL_Running
#
# 			Whether the SQL thread is started
#
# 		) Replicate_Do_DB, Replicate_Ignore_DB
#
# 			The names of any databases that were specified with the --replicate-do-db and
# 			--replicate-ignore-db options, or the CHANGE_REPLICATION_FILTER statement.
#
# 			If the FOR CHANNEL clause was used, the channel specific replication filters are shown.
#
# 			Otherwise, the replication filters for every replication channel are shown.
#
# 		) Replicate_Do_Table, Replicate_Ignore_Table, Replicate_Wild_Do_Table, Replicate_Wild_Ignore_Table
#
# 			The names of any tables that were specified with the --replicate-do-table, --replicate-ignore-table,
# 			--replicate-wild-do-table and --replicate-wild-ignore-table options, or the CHANGE_REPLICATION_FILTER
# 			statement.
#
# 			If the FOR CHANNEL clause was used, the channel specific replication filters are shown.
#
# 			Otherwise, the replication filters for every replication channel are shown.
#
# 		) Last_Errno, Last_Error
#
# 			These columns are aliases for Last_SQL_Errno and Last_SQL_Error
#
# 			Issuing RESET_MASTER or RESET_SLAVE resets the values shown in these columns.
#
# 			NOTE:
#
#				When the slave SQL thread receives an error, it reports the error first, then
# 				stops the SQL thread.
#
# 				This means that there is a small window of time during which SHOW_SLAVE_STATUS
# 				shows a nonzero value for Last_SQL_Errno even though Slave_SQL_Running still displays Yes.
#
# 		) Skip_Counter
#
# 			The current value of the sql_slave_skip_counter system variable.
#
# 			See SECTION 13.4.2.5, "SET GLOBAL SQL_SLAVE_SKIP_COUNTER SYNTAX"
#
# 		) Exec_Master_Log_Pos
#
# 			The position in the current master binary log file to which the SQL thread has read
# 			and executed, marking the start of the next transaction or event to be processed.
#
# 			You can use this value with the CHANGE_MASTER_TO statement's MASTER_LOG_POS option
# 			when starting a new slave from an existing slave, so that the new slave reads
# 			from this point.
#
# 			The coordinates given by (Relay_Master_Log_File, Exec_Master_Log_Pos) in the
# 			master's binary log correspond to the coordinates given by (Relay_Log_File, Relay_Log_Pos)
# 			in the relay log.
#
# 			Inconsistencies in the sequence of transactions from the relay log which have been 
# 			executed can cause this value to be a "low-water mark".
#
# 			In other words, transactions appearing before the position are guaranteed to have committed,
# 			but transactions after the position may have committed or not.
#
# 			If these gaps need to be corrected, use START_SLAVE_UNTIL_SQL_AFTER_MTS_GAPS
#
# 			See SECTION 17.4.1.34, "REPLICATION AND TRANSACTION INCONSISTENCIES" for more information
#
# 		) Relay_Log_Space
#
# 			The total combined size of all existing relay log files
#
# 		) Until_Condition, Until_Log_File, Until_Log_Pos
#
# 			The values specified in the UNTIL clause of the START_SLAVE statement.
#
# 			Until_Condition has these values:
#
# 				) None if no UNTIL clause was specified
#
# 				) Master if the slave is reading until a given position in the master's binary log
#
# 				) Relay if the slave is reading until a given position in its relay log
#
# 				) SQL_BEFORE_GTIDS if the slave SQL thread is processing transactions until it has reached
# 					the first transaction whose GTID is listed in the gtid_set
#
# 				) SQL_AFTER_GTIDS if the slave threads are processing all transactions until the last transaction
# 					in the gtid_set has been processed by both threads.
#
# 				) SQL_AFTER_MTS_GAPS if a multithreaded slave's SQL threads are running until no more gaps are found in the relay log
#
# 			Until_Log_File and Until_Log_Pos indicate the log file name and position that define the coordinates
# 			at which the SQL thread stops executing.
#
# 			For more information on UNTIL clauses, see SECTION 13.4.2.6, "START SLAVE SYNTAX"
#
# 		) Master_SSL_Allowed, Master_SSL_CA_File, Master_SSL_CA_Path, Master_SSL_Cert, Master_SSL_Cipher,
# 			Master_SSL_CRL_File, Master_SSL_CRL_Path, Master_SSL_Key, Master_SSL_Verify_Server_Cert
#
# 			These fields show the SSL parameters used by the slave to connect to the master, if any.
#
# 			Master_SSL_Allowed has these values:
#
# 				) Yes if an SSL connection to the master is permitted
#
# 				) No if an SSL connection to the master is not permitted
#
# 				) Ignored if an SSL connection is permitted but the slave server does not have SSL support enabled
#
# 			The values of the other SSL-related fields correspond to the values of the MASTER_SSL_CA,
# 			MASTER_SSL_CAPATH, MASTER_SSL_CERT, MASTER_SSL_CIPHER, MASTER_SSL_CRL, MASTER_SSL_CRLPATH,
# 			MASTER_SSL_KEY and MASTER_SSL_VERIFY_SERVER_CERT options to the CHANGE_MASTER_TO statement.
#
# 			See SECTION 13.4.2.1, "CHANGE MASTER TO SYNTAX"
#
# 		) Seconds_Behind_Master
#
# 			This field is an indication of how "late" the slave is:
#
# 				) When the slave is actively processing updates, this field shows the difference between the
# 					current timestamp on the slave and the original timestamp logged on the master for the
# 					event currently being processed on the slave.
#
# 				) When no event is currently being processed on the slave, this value is 0
#
# 			In essence, this field measures the time difference in seconds between the slave SQL thread
# 			and the slave I/O thread.
#
# 			If the network connection between master and slave is fast, the slave I/O thread is very close
# 			to the master, so this field is a good approximation of how late the slave SQL thread is compared
# 			to the master.
#
# 			If the network is slow, this is not a good approximation; the slave SQL thread may quite often
# 			be caught up with the slow-reading slave I/O thread, so Seconds_Behind_Master often shows
# 			a value of 0, even if the I/O thread is late compared to the master.
#
# 			In other words, this column is useful only for fast networks.
#
# 			This time difference computation works even if the master and slave do not have identical
# 			clock times, provided that the difference, computed when the slave I/O thread strats,
# 			remains constant from then on.
#
# 			Any changes - including NTP updates - can lead to clock skews that can make calculation
# 			of Seconds_Behind_Master less reliable.
#
# 			In MySQL 8.0, this field is NULL (undefined or unknown) if the slave SQL thread is not running,
# 			or if the SQL thread has consumed all of the relay log and the slave I/O thread is not running.
#
# 			(In older versions of MySQL, this field was NULL if the slave SQL thread or the slave I/O thread
# 			was not running or was not connected to the master)
#
# 			If the I/O thread is running but the relay log is exhausted, Seconds_Behind_Master is set to 0
#
# 			The value of Seconds_Behind_Master is based on the timestamps stored in events, which are preserved
# 			through replication.
#
# 			This means that if a master M1 is itself a slave of M0, any event from M1's binary log that originates
# 			from M0's binary log has M0's timestamp for that event.
#
# 			This enables MySQL to replicate TIMESTAMP successfully.
#
# 			However, the problem for Seconds_Behind_Master is that if M1 also receives direct
# 			updates from clients, the Seconds_Behind_Master value randomly fluctuates because
# 			sometimes the last event from M1 originates from M0 and sometimes is the result
# 			of a direct update on M1.
#
# 			When using a multithreaded slave, you should keep in mind that this value is based on
# 			Exec_Master_Log_Pos, and so may not reflect the position of the most recently
# 			committed transaction.
#
# 		) Last_IO_Errno, Last_IO_Error
#
# 			The error number and error message of the most recent error that caused the I/O
# 			thread to stop.
#
# 			An error number of 0 and message of the empty string mean "no error"
#
# 			If the Last_IO_Error value is not empty, the error values also appear in the
# 			slave's error log.
#
# 			I/O error information includes a timestamp showing when the most recent I/O
# 			thread error occurred.
#
# 			This timestamp uses the format YYMMDD HH:MM:SS, and appears in the Last_IO_Error_Timestamp
# 			column.
#
# 			Issuing RESET_MASTER or RESET_SLAVE resets the values shown in these columns.
#
# 		) Last_SQL_Errno, Last_SQL_Error
#
# 			The error number and error message of the most recent error that caused the SQL thread
# 			to stop.
#
# 			An error number of 0 and message of the empty string mean "no error"
#
# 			If the Last_SQL_Error value is not empty, the error values also appear in the
# 			slave's error log.
#
# 			If the slave is multithreaded, the SQL thread is the coordinator for worker threads.
#
# 			In this case, the Last_SQL_Error field shows exactly what the Last_Error_Message
# 			column in the Performance Schema replication_applier_status_by_coordinator table shows.
#
# 			The field value is modified to suggest that there may be more failures in the other
# 			worker threads which can be seen in the replication_applier_status_by_worker table that
# 			shows each worker thread's status.
#
# 			If that table is not available, the slave error log can be used.
#
# 			The log or the replication_applier_status_by_worker table should also be used
# 			to learn more about the failure shown by SHOW_SLAVE_STATUS or the coordinator table.
#
# 			SQL error information includes a timestamp showing when the most recent SQL thread error occurred.
#
# 			This timestamp uses the format YYMMDD HH:MM:SS, and appears in the Last_SQL_Error_Timestamp column
#
# 			Issuing RESET_MASTER or RESET_SLAVE resets the values shown in these columns.
#
# 			In MySQL 8.0, all error codes and messages displayed in the Last_SQL_Errno and Last_SQL_Error
# 			columns correspond to error values listed in SECTION B.3, "SERVER ERROR MESSAGE REFERENCE"
#
# 			This was not always true in previous versions. (Bug #11760365, Bug #52768)
#
# 		) Replicate_Ignore_Server_Ids
#
# 			Any server IDs that have been specified using the IGNORE_SERVER_IDS option of the
# 			CHANGE_MASTER_TO statement, so that the slave ignores events from these servers.
#
# 			This option is used in a circular or other multi-master replication setup when
# 			one of the servers is removed.
#
# 			If any server IDs have been set in this way, a comma-delimited list of one
# 			or more numbers is shown. If no server IDs have been set, the field is blank.
#
# 			NOTE:
#
# 				The Ignored_server_ids value in the slave_master_info table also shows the
# 				server IDs to be ignored, but as a space-delimited list, preceded by the total
# 				number of server IDs to be ignored.
#
# 				For example, if a CHANGE_MASTER_TO statement containing the IGNORE_SERVER_IDS = (2,6,9)
# 				option has been issued to tell a slave to ignore masters having the server ID
# 				2, 6 or 9, that information appears as shown here:
#
# 					Replicate_Ignore_Server_Ids: 2, 6, 9
# 
# 					Ignored_server_ids: 3, 2, 6, 9
#
# 			Replicate_Ignore_Server_Ids filtering is performed by the I/O thread, rather than by the
# 			SQL thread, which means that events which are filtered out are not written to the relay log.
#
# 			This differs from the filtering actions taken by server options such --replicate-do-table,
# 			which apply to the SQL thread.
#
# 			NOTE:
#
# 				From MySQL 8.0.3, a deprecation warning is issued if SET GTID_MODE=ON is issued
# 				when any channel has existing server IDs set with IGNORE_SERVER_IDS.
#
# 				Before starting GTID-based replication, use SHOW_SLAVE_STATUS to check
# 				for and clear all ignored server ID lists on the servers involved.
#
# 				You can clear a list by issuing a CHANGE_MASTER_TO statement containing
# 				the IGNORE_SERVER_IDS option with an empty list.
#
# 		) Master_Server_Id
#
# 			The server_id value from the master.
#
# 		) Master_UUID
#
# 			THe server_uuid value from the master
#
# 		) Master_Info_File
#
# 			The location of the master.info file, if a file rather than a table is used for
# 			the slave's master info repository
#
# 		) SQL_Delay
#
# 			The number of seconds that the slave must lag the master
#
# 		) SQL_Remaining_Delay
#
# 			When Slave_SQL_Running_State is Waiting until MASTER_DELAY seconds after master
# 			executed event, this field contains the number of delay seconds remaining.
#
# 			At other times, this field is NULL
#
# 		) Slave_SQL_Running_State
#
# 			The state of the SQL thread (analogous to Slave_IO_State)
#
# 			The value is identical to the State value of the SQL thread as displayed
# 			by SHOW_PROCESSLIST.
#
# 			SECTION 8.14.5, "REPLICATION SLAVE SQL THREAD STATES", provides a listing
# 			of possible states
#
# 		) Master_Retry_Count
#
# 			The number of times the slave can attempt to reconnect to the master in the event
# 			of a lost connection.
#
# 			This value can be set using the MASTER_RETRY_COUNT option of the
# 			CHANGE_MASTER_TO statement (preferred) or the older --master-retry-count server
# 			option (still supported for backward compatibility)
#
# 		) Master_Bind
#
# 			The network interface that the slave is bound to, if any.
#
# 			This is set using the MASTER_BIND option for the CHANGE_MASTER_TO statement
#
# 		) Last_IO_Error_Timestamp
#
# 			A timestamp in YYMMDD HH:MM:SS format that shows when the most recent I/O error took place
#
# 		) Last_SQL_Error_Timestamp
#
# 			A timestamp in YYMMDD HH:MM:SS format that shows when the last SQL error occurred.
#
# 		) Retrieved_Gtid_Set
#
# 			The set of global transaction IDs corresponding to all transactions received by this slave.
#
# 			Empty if GTIDs are not in use. See GTID Sets for more information.
#
# 			This is the set of all GTIDs that exist or have existed in the relay logs.
#
# 			Each GTID is added as soon as the Gtid_log_event is received.
#
# 			This can cause partially transmitted transactions to have their GTIDs included in the set.
#
# 			When all relay logs are lost due to executing RESET_SLAVE or CHANGE_MASTER_TO, or due
# 			to the effects of the --relay-log-recovery option, the set is cleared.
#
# 			When relay_log_purge = 1, the newest relay log is always kept, and the set is not cleared.
#
# 		) Executed_Gtid_Set
#
# 			The set of global transaction IDs written in the binary log.
#
# 			This is the same as the value for the global gtid_executed system variable
# 			on this server, as well as the value for Executed_Gtid_Set in the output of
# 			SHOW_MASTER_STATUS on this server.
#
# 			Empty if GTIDs are not in use.
#
# 			See GTID SETS for more information.
#
# 		) Auto_Position
#
# 			1 if autopositioning is in use; otherwise 0.
#
# 		) Replicate_Rewrite_DB
#
# 			The Replicate_Rewrite_DB value displays any replication filtering rules that were
# 			specified.
#
# 			For example, if the following replication filter rule was set:
#
# 				CHANGE REPLICATION FILTER REPLICATE_REWRITE_DB=((db1,db2), (db3,db4));
#
# 			the Replicate_Rewrite_DB value displays:
#
# 				Replicate_Rewrite_DB: (db1,db2), (db3,db4)
#
# 			For more information, see SECTION 13.4.2.2, "CHANGE REPLICATION FILTER SYNTAX"
#
# 		) Channel_name
#
# 			The replication channel which is being displayed.
#
# 			There is always a default replication channel, and more replication channels
# 			can be added.
#
# 			See SECTION 17.2.3, "REPLICATION CHANNELS" for more information.
#
#		) Master_TLS_Version
#
# 		 	The TLS version used on the master.
#
# 			For TLS version information, see SECTION 6.4.6, "ENCRYPTED CONNECTION PROTOCOLS AND CIPHERS"
#
# 		) Master_public_key_path
#
# 			The path name to a file containing a slave-side copy of the public key required by the master
# 			for RSA key pair-based password exchange.
#
# 			The file must be in PEM format.
#
# 			This column applies to slaves that authenticate with the sha256_password
# 			or caching_sha2_password authentication plugin.
#
# 			If Master_public_key_path is given and specifies a valid public key file, it takes
# 			precedence over Get_master_public_key
#
# 		) Get_master_public_key
#
# 			Whether to request from the master the public key required for RSA key pair-based
# 			password exchange.
#
# 			This column applies to slaves that authenticate with the caching_sha2_password
# 			authentication plugin.
#
# 			For that plugin, the master does not send the public key unless requested.
#
# 			If Master_public_key_path is given and specifies a valid public key file,
# 			it takes precedence over Get_master_public_key
#
# 13.7.6.35 SHOW STATUS SYNTAX
#
# 		SHOW [GLOBAL | SESSION] STATUS
# 			[LIKE 'pattern' | WHERE expr]
#
# SHOW_STATUS provided server status information (see SECTION 5.1.10, "SERVER STATUS VARIABLES")
#
# This statement does not require any privilege. It requires only the ability to
# connect to the server.
#
# Status variable information is also available from these sources:
#
# 		) Performance Schema tables. See SECTION 26.12.14, "PERFORMANCE SCHEMA STATUS VARIABLE TABLES"
#
# 		) The mysqladmin extended-status command. See SECTION 4.5.2, "mysqladmin -- CLIENT FOR ADMINISTERING A MYSQL SERVER"
#
# For SHOW_STATUS, a LIKE clause, if present, indicates which variable names to match.
#
# A WHERE clause can be given to select rows using more general conditions, as discussed
# in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# SHOW_STATUS accepts an optional GLOBAL or SESSION variable scope modifier:
#
# 		) With a GLOBAL modifier, the statement displays the global status values.
#
# 			A global status variable may represent status for some aspect of the server
# 			itself (for example, Aborted_connects), or the aggregated status over all
# 			connections to MySQL (for example, Bytes_received and Bytes_sent)
#
# 			If a variable has no global value, the session value is displayed
#
# 		) With a SESSION modifier, the statement displays the status variable values for the
# 			current connection.
#
# 			If a variable has no session value, the global value is displayed.
#
# 			LOCAL is a synonym for SESSION
#
# 		) If no modifier is present, the default is SESSION
#
# The scope for each status variable is listed at SECTION 5.1.10, "SERVER STATUS VARIABLES"
#
# Each invocation of the SHOW_STATUS statement uses an internal temporary table and increments
# the global Created_tmp_tables value.
#
# Partial output is shown here.
#
# The list of names and value may differ for your server.
#
# The meaning of each variable is given in SECTION 5.1.10, "SERVER STATUS VARIABLES"
#
# 		SHOW STATUS;
# 		+---------------------------------------+--------------+
# 		| Variable_name 								 | Value 		 |
# 		+---------------------------------------+--------------+
# 		| Aborted_clients 							 | 0 				 |
# 		| Aborted_connects 							 | 0 				 |
# 		| Bytes_received 								 | 155372598 	 |
# 		| Bytes_sent 									 | 1176560426   |
# 		| Connections 									 | 30023 		 |
# 		| Created_tmp_disk_tables 					 | 0 				 |
# 		| Created_tmp_tables 						 | 8340 			 |
# 		| Created_tmp_files 							 | 60 			 |
# 		---
# 		| Open_tables 									 | 1 				 |
# 		| Open_files 									 | 2 				 |
# 		| Open_streams 								 | 0 				 |
# 		| Opened_tables 								 | 44600 		 |
# 		| Questions 									 | 2026873 		 |
# 		---
# 		| Table_locks_immediate 					 | 1920382 		 |
# 		| Table_locks_waited 						 | 0 				 |
# 		| Threads_cached 								 | 0 				 |
# 		| Threads_created 							 | 30022 		 |
# 		| Threads_connected 							 | 1 				 |
# 		| Threads_running 							 | 1 				 |
# 		| Uptime 										 | 80380 		 |
# 		+---------------------------------------+--------------+
#
# With a LIKE clause, the statement displays only rows for those variables with names
# that match the pattern:
#
# 		SHOW STATUS LIKE 'Key%';
# 		+---------------------+-----------+
# 		| Variable_name 		 | Value 	 |
# 		+---------------------+-----------+
# 		| Key_blocks_used 	 | 14955 	 |
# 		| Key_read_requests   | 96854827  |
# 		| Key_reads 			 | 162040 	 |
# 		| Key_write_requests  | 7589728   |
# 		| Key_writes 			 | 3813196   |
# 		+---------------------+-----------+
#
# 13.7.6.36 SHOW TABLE STATUS SYNTAX
#
# 		SHOW TABLE STATUS
# 			[{FROM | IN} db_name]
# 			[LIKE 'pattern' | WHERE expr]
#
# SHOW_TABLE_STATUS works likes SHOW_TABLES, but provides a lot of information about each
# non-TEMPORARY table.
#
# You can also get this list using the mysqlshow --status db_name command.
#
# The LIKE clause, if present, indicates which table names to match.
#
# The WHERE clause can be given to select rows using more general conditions,
# as discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# This statement also displays information about views.
#
# SHOW_TABLE_STATUS output has these columns:
#
# 		) Name
#
# 			The name of the table
#
# 		) Engine
#
# 			The storage engine for the table.
#
# 			See CHAPTER 15, THE INNODB STORAGE ENGINE, and CHAPTER 16, ALTERNATIVE STORAGE ENGINES
#
# 			For partitioned tables, Engine shows the name of the storage engine used by all partitions
#
# 		) Version
#
# 			This column is unused. With the removal of .frm files in MySQL 8.0, this column now
# 			reports a hardcoded value of 10, which is the last .frm file version used in MySQL
# 			5.7
#
# 		) Row_format
#
# 			The row-storage format (Fixed, Dynamic, Compressed, Redundant, Compact)
#
# 			For MyISAM tables, Dynamic corresponds to what myisamchk -dvv reports as Packed.
#
# 		) Rows
#
# 			The number of rows.
#
# 			Some storage engines, such as MyISAM, store the exact count.
#
# 			For other storage engines, such as InnoDB, this value is an approximation
# 			and may vary from the actual value by as much as 40% to 50%.
#
# 			In such cases, use SELECT COUNT(*) to obtain an accurate count.
#
# 			The Rows value is NULL for INFORMATION_SCHEMA tables.
#
# 			For InnoDB tables, the row count is only a rough estimate used in SQL optimization.
#
# 			(This is also true if the InnoDB table is partitioned)
#
# 		) Avg_row_length
#
# 			The average row length
#
# 		) Data_length
#
# 			For MyISAM, Data_length is the length of the data file, in bytes.
#
# 			For InnoDB, Data_length is the approximate amount of memory allocated
# 			for the clustered index, in bytes.
#
# 			Specifically, it is the clustered index size, in pages, multiplied by
# 			the InnoDB page size.
#
# 			Refer to the notes at the end of this section for information regarding
# 			other storage engines.
#
# 		) Max_data_length
#
# 			For MyISAM, Max_data_length is maximum length of the data file.
#
# 			This is the total number of bytes of data that can be stored in the table,
# 			given the data pointer size used.
#
# 			Unused for InnoDB.
#
# 			Refer to the notes at the end of this section for information regarding other
# 			storage engines.
#
# 		) Index_length
#
# 			For MyISAM, Index_length is the length of the index file, in bytes.
#
# 			For InnoDB, Index_length is the approximate amount of memory allocated for
# 			non-clustered indexes, in bytes.
#
# 			Specifically, it is the sum of non-clustered index sizes, in pages,
# 			multiplied by the InnoDB page size.
#
# 			Refer to the notes at the end of this section for information regarding other
# 			storage engines.
#
# 		) Data_free
#
# 			The number of allocated but unused bytes.
#
# 			InnoDB tables report the free space of the tablespace to which the table belongs.
#
# 			For a table located in the shared tablespace, this is the free space of the shared
# 			tablespace.
#
# 			If you are using multiple tablespaces and the table has its own tablespace,
# 			the free space is for only that table.
#
# 			Free space means the number of bytes in completely free extents minus a safety
# 			margin.
#
# 			Even if free space displays as 0, it may be possible to insert rows as long
# 			as new extents need not be allocated.
#
# 			For NDB Cluster, Data_free shows the space allocated on disk for, but not
# 			used by, a Disk Data table or fragment on disk.
#
# 			(In-memory data resource usage is reported by the Data_length column)
#
# 			For partitioned tables, this value is only an estimate and may not be absolutely
# 			correct.
#
# 			A more accurate model of obtaining this information in such cases is to query
# 			the INFORMATION_SCHEMA PARTITIONS table, as shown in this example:
#
# 				SELECT SUM(DATA_FREE)
# 					FROM INFORMATION_SCHEMA.PARTITIONS
# 					WHERE TABLE_SCHEMA = 'mydb'
# 					AND TABLE_NAME = 'mytable';
#
# 			For more information, see SECTION 25.16, "THE INFORMATION_SCHEMA PARTITIONS TABLE"
#
# 		) Auto_increment
#
# 			The next AUTO_INCREMENT value
#
# 		) Create_time
#
# 			When the table was created.
#
# 		) Update_time
#
# 			When the data file was last updated. For some storage engines, this value is NULL.
#
# 			For example, InnoDB stores multiple tables in its system tablespace and the data
# 			file timestamp does not apply.
#
# 			Even with file-per-table mode with each InnoDB table in a separate .ibd file,
# 			change buffering can delay the write to the data file, so the file modification
# 			time is different from the time of the last insert, update, or delete.
#
# 			For MyISAM, the data file timestamp is used; however, on Windows the timestamp
# 			is not updated by updates, so the value is inaccurate.
#
# 			Update_time displays a timestamp value for the last UPDATE, INSERT, or DELETE
# 			performed on InnoDB tables that are not partitioned.
#
# 			For MVCC, the timestamp value reflects the COMMIT time, which is considered
# 			the last update time.
#
# 			Timestamps are not persisted when the server is restarted or when the table is evicted
# 			from the InnoDB data dictionary cache.
#
# 		) Check_time
#
# 			When the table was last checked. Not all storage engines update this time, in which case,
# 			the value is always NULL
#
# 			For partitioned InnoDB tables, Check_time is always NULL
#
# 		) Collation
#
# 			The table default collation. The output does not explicitly list the table default character
# 			set, but the collation name begins with the character set name
#
# 		) Checksum
#
# 			The live checksum value, if any
#
# 		) Create_options
#
# 			Extra options used with CREATE_TABLE.
#
# 			The original options from when CREATE_TABLE was executed are retained
# 			and the options reported here may differ from the active table settings and options.
#
# 			For InnoDB tables, the actual ROW_FORMAT and KEY_BLOCK_SIZE options are reported.
#
# 			Prior to MySQL 8.0, Create_options reports the originally supplied ROW_FORMAT
# 			and KEY_BLOCK_SIZE.
#
# 			For more information, see SECTION 13.1.20, "CREATE TABLE SYNTAX"
#
# 			Create_options shows partitioned if the table is partitioned.
#
# 			It also shows the ENCRYPTION option specified when creating or altering
# 			a file-per-table tablespace.
#
# 			This column does not show the encryption option specified when creating
# 			or altering a general tablespace.
#
# 			The ENCRYPTION column of the INNODB_TABLESPACES table is applicable
# 			to both file-per-table and general tablespaces.
#
# 		) Comment
#
# 			The comment used when creating the table (or information as to why MySQL could not
# 			access the table information)
#
# NOTES
#
# 		) For NDB tables, the output of this statement shows appropriate values for the Avg_row_length
# 			and Data_length columns, with the exception that BLOB columns are not taken into account.
#
# 		) For NDB tables, Data_length includes data stored in main memory only; the Max_data_length
# 			and Data_free columns apply to Disk Data
#
# 		) For NDB Cluster Disk Data tables, Max_data_length shows the space allocated for the disk part
# 			of a Disk Data table or fragment.
#
# 			(In-memory data resource usage is reported by the Data_length column)
#
# 		) For MEMORY tables, the Data_length, Max_data_length, and Index_length values
# 			approximate the actual amount of allocated memory.
#
# 			The allocation algorithm reserves memory in large amounts to reduce the number
# 			of allocation operations.
#
# 		) For views, most columns displayed by SHOW_TABLE_STATUS are 0 or NULL except
# 			that Name indicates the view name, Create_time indicates the creation time,
# 			and Comment says VIEW.
#
# Table information is also available from the INFORMATION_SCHEMA TABLES table.
#
# See SECTION 25.29, "THE INFORMATION_SCHEMA TABLES TABLE"
#
# 13.7.6.37 SHOW TABLES SYNTAX
#
# 		SHOW [EXTENDED] [FULL] TABLES
# 			[{FROM | IN} db_name]
# 			[LIKE 'pattern' | WHERE expr]
#
# SHOW_TABLES lists the non-TEMPORARY tables in a given database.
#
# You can also get this list using the mysqlshow db_name command.
#
# The LIKE clause, if present, indicates which table names to match:
#
# The WHERE clause can be given to select rows using more general conditions,
# as discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# Matching performed by the LIKE clause is dependent on the setting of the lower_case_table_names
# system variable.
#
# The optional EXTENDED modifier causes SHOW_TABLES to list hidden tables created by failed
# ALTER_TABLE statements.
#
# These temporary tables have names beginning with #sql and can be dropped using
# DROP_TABLE
#
# This statement also lists any views in the database. The optional FULL modifier
# causes SHOW_TABLES to display a second output column with values of BASE TABLE
# for a table, VIEW for a view, or SYSTEM VIEW for an INFORMATION_SCHEMA table
#
# If you have no privileges for a base table or view, it does not show up in the
# output from SHOW_TABLES or mysqlshow db_name
#
# Table information is also available from the INFORMATION_SCHEMA TABLES table.
#
# See SECTION 25.29, "THE INFORMATION_SCHEMA TABLES TABLE"
#
# 13.7.6.38 SHOW TRIGGERS SYNTAX
#
# 		SHOW TRIGGERS
# 			[{FROM | IN} db_name]
# 			[LIKE 'pattern' | WHERE expr]
#
# SHOW_TRIGGERS lists the triggers currently defined for tables in a database
# (The default database unless a FROM clause is given)
#
# This statement returns results only for databases and tables for which you have
# the TRIGGER privilege.
#
# The LIKE clause, if present, indicates which table names (not trigger names)
# to match and causes the statement to display triggers for those tables.
#
# The WHERE clause can be given to select rows using more general conditions,
# as dicussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# For the ins_sum trigger defined in SECTION 24.3, "USING TRIGGERS",
# the output of SHOW_TRIGGERS is as shown here:
#
# 		SHOW TRIGGERS LIKE 'acc%'\G
# 		*********************** 1. row ********************************
# 						Trigger: ins_sum
# 						 Event : INSERT
# 					    Table : account
# 					Statement : SET @sum = @sum + NEW.amount
# 						Timing : BEFORE
# 						Created: 2018-08-08 10:10:12.61
# 					sql_mode  : ONLY_FULL_GROUP_BY, STRICT_TRANS_TABLES,
# 									NO_ZERO_IN_DATE, NO_ZERO_DATE,
# 									ERROR_FOR_DIVISION_BY_ZERO,
# 									NO_ENGINE_SUBSTITUTION
# 						Definer: me@localhost
# 	  character_set_client: utf8mb4
# 	  collation_connection: utf8mb4_0900_ai_ci
# 		Database Collation : utf8mb4_0900_ai_ci
#
# SHOW_TRIGGERS output has these columns:
#
# 		) Trigger
#
# 			The name of the trigger
#
# 		) Event
#
# 			The trigger event. This is the type of operation on the associated table for which
# 			the trigger activates.
#
# 			The value is INSERT (a row was inserted), DELETE (a row was deleted), or
# 			UPDATE (a row was modified)
#
# 		) Table
#
# 			The table for which the trigger is defined
#
# 		) Statement
#
# 			The trigger body; that is, the statement executed when the trigger activates.
#
# 		) Timing
#
# 			Whether hte trigger activates before or after the triggering event.
#
# 			The value is BEFORE or AFTER.
#
# 		) Created
#
# 			The date and time when the trigger was created. This is a TIMESTAMP(2) value
# 			(with a fractional part in hundreths of seconds) for triggers.
#
# 		) sql_mode
#
# 			The SQL mode in effect when the trigger was created, and under which the
# 			trigger executes.
#
# 			For the permitted values, see SECTION 5.1.11, "SERVER SQL MODES"
#
# 		) Definer
#
# 			The account of the user who created the trigger, in 'user_name'@'host_name' format
#
# 		) character_set_client
#
# 			The session value of the character_set_client system variable when the trigger was created.
#
# 		) collation_connection
#
# 			The session value of the collation_connection system variable when the trigger was created.
#
# 		) Database Collation
#
# 			The collation of the database with which the trigger is associated
#
# Trigger information is also available from the INFORMATION_SCHEMA TRIGGERS table.
#
# See SECTION 25.33, "THE INFORMATION_SCHEMA TRIGGERS TABLE"
#
# 13.7.6.39 SHOW VARIABLES SYNTAX
#
# 		SHOW [GLOBAL | SESSION] VARIABLES
# 			[LIKE 'pattern' | WHERE expr]
#
# SHOW_VARIABLES shows the values of MySQL system variables (see SECTION 5.1.8, "SERVER SYSTEM VARIABLES")
#
# This statement does not require any privilege. It requires only the ability to connect to the server.
#
# System variable information is also available from these sources:
#
# 		) Performance Schema tables. See SECTION 26.12.13, "PERFORMANCE SCHEMA SYSTEM VARIABLE TABLES"
#
# 		) The mysqladmin variables command. See SECTION 4.5.2, "mysqladmin -- Client for Administering a MySQL Server"
#
# For SHOW_VARIABLES, a LIKE clause, if present, indicates which variable names to match.
#
# A WHERE clause can be given to select rows using more general conditions, as
# discussed in SECTION 25.41, "EXTENSIONS TO SHOW STATEMENTS"
#
# SHOW_VARIABLES accepts an optional GLOBAL or SESSION variable scope modifier:
#
# 		) With a GLOBAL modifier, the statement displays global system variable values.
#
# 			These are the values used to initialize the corresponding session variables
# 			for new connections to MySQL.
#
# 			If a variable has no global value, no value is displayed
#
# 		) With a SESSION modifier, the statement displays the system variable values that are
# 			in effect for the current connection.
#
# 			If a variable has no session value, the global value is displayed.
#
# 			LOCAL is a synonym for SESSION
#
# 		) If no modifier is present, the default is SESSION
#
# The scope for each system variable is listed at SECTION 5.1.8, "SERVER SYSTEM VARIABLES"
#
# SHOW_VARIABLES is subject to a version-dependent display-width limit.
#
# For variables with very long values that are not completely displayed, use SELECT
# as a workaround.
#
# For example:
#
# 		SELECT @@GLOBAL.innodb_data_file_path;
#
# Most system variables can be set at server startup (read-only variables such as
# version_comment are exceptions)
#
# Many can be changed at runtime with the SET statement.
#
# See SECTION 5.1.9, "USING SYSTEM VARIABLES", and SECTION 13.7.5.1, "SET SYNTAX FOR VARIABLE ASSIGNMENT"
#
# Partial output is shown here.
#
# The list of names and values may differ for your server.
#
# SECTION 5.1.8, "SERVER SYSTEM VARIABLES", describes the meaning of each variable,
# and SECTION 5.1.1, "CONFIGURING THE SERVER", provides information about tuning them.
#
# 		SHOW VARIABLES;
# 		+--------------------------------------------+--------------------------+
# 		| Variable_name 							 			| Value 						   |
# 		+--------------------------------------------+--------------------------+
# 		| activate_all_roles_on_login 		 			| OFF 							|
# 		| auto_generate_certs 					 			| ON 							   |
# 		| auto_increment_increment 			 			| 1 								|
# 		| auto_increment_offset 				 			| 1 								|
# 		| autocommit 								 			| ON 							   |
# 		| automatic_sp_privileges 				 			| ON 							   |
# 		| avoid_temporal_upgrade 				 			| OFF 							|
# 		| back_log 									 			| 151 							|
# 		| basedir 									 			| /usr/ 						   |
# 		| big_tables 								 			| OFF 							|
# 		| bind_address 							 			| * 								|
# 		| binlog_cache_size 						 			| 32768 						   |
# 		| binlog_checksum 						 			| CRC32 						   |
# 		| binlog_direct_non_transactional_updates 	| OFF 							|
# 		| binlog_error_action 								| ABORT_SERVER 				|
# 		| binlog_expire_logs_seconds 						| 2592000 						|
# 		| binlog_format 										| ROW 							|
# 		| binlog_group_commit_sync_delay 				| 0 								|
# 		| binlog_group_commit_sync_no_delay_count 	| 0 								|
# 		| binlog_gtid_simple_recovery 					| ON 								|
# 		| binlog_max_flush_queue_time 					| 0 								|
# 		| binlog_order_commits 								| ON 								|
# 		| binlog_row_image 									| FULL 							|
# 		| binlog_row_metadata 								| MINIMAL 						|
# 		| binlog_row_value_options 						| 									|
# 		| binlog_rows_query_log_events 					| OFF 							|
# 		| binlog_stmt_cache_size 							| 32768 							|
# 		| binlog_transaction_dependency_history_size | 25000 							|
# 		| binlog_transaction_dependency_tracking 	   | COMMIT_ORDER 				|
# 		| block_encryption_mode 							| aes-128-ecb 					|
# 		| bulk_insert_buffer_size 							| 8388608 						|
# 		---
# 		---
# 		---
# 		| max_allowed_packet 								| 67108864 						|
# 		| max_binlog_cache_size 							| 18446744073709547520 		|
# 		| max_binlog_size 									| 1073741824 					|
# 		| max_binlog_stmt_cache_size 						| 1844674073709547520 		|
# 		| max_connect_errors 								| 100 							|
# 		| max_connections 									| 151 							|
# 		| max_delayed_threads 								| 20 								|
# 		| max_digest_length 									| 1024 							|
# 		| max_error_count 									| 1024 							|
# 		| max_execution_time 								| 0 								|
# 		| max_heap_table_size 								| 16777216 						|
# 		| max_insert_delayed_threads 						| 20 								|
# 		| max_join_size 										| 18446744073709551615 		|
# 		---
# 		---
# 		---
# 		| thread_handling 									| one-thread-per-connection|
# 		| thread_stack 										| 286720 						|
# 		| time_zone 											| SYSTEM 						|
# 		| timestamp 											| 1530906638.765316 			|
# 		| tls_version 											| TLSv1, TLSv1.1,TLSv1.2   |
# 		| tmp_table_size 										| 16777216 						|
# 		| tmpdir 												| /tmp 							|
# 		| transaction_alloc_block_size 					| 8192 							|
# 		| transaction_allow_batching 						| OFF 							|
# 		| transaction_isolation 							| REPEATABLE-READ 			|
# 		| transaction_prealloc_size 						| 4096 							|
# 		| transaction_read_only 							| OFF 							|
# 		| transaction_write_set_extraction 			 	| XXHASH64 						|
# 		| unique_checks 										| ON 								|
# 		| updatable_views_with_limit 						| YES 							|
# 		| version 												| 8.0.12 						|
# 		| version_comment 									| MySQL Community Server - GPL |
# 		| version_compile_machine 							| x86_64 						|
# 		| version_compile_os 								| Linux 							|
# 		| version_compile_zlib 								| 1.2.11 						|
# 		| wait_timeout 										| 28800 							|
# 		| warning_count 										| 0 								|
# 		| windowing_use_high_precision 					| ON 								|
# 		+--------------------------------------------+--------------------------+
#
# With a LIKE clause, the statement displays only rows for those variables with names that
# match the pattern.
#
# To obtain the row for a specific variable, use a LIKE clause as shown:
#
# 		SHOW VARIABLES LIKE 'max_join_size';
# 		SHOW SESSION VARIABLES LIKE 'max_join_size';
#
# To get a list of variables whose name match a pattern, use the % wildcard character
# in a LIKE clause:
#
# 		SHOW VARIABLES LIKE '%size%';
# 		SHOW GLOBAL VARIABLES LIKE '%size%';
#
# Wildcard characters can be used in any position within the pattern to be matched.
#
# Strictly speaking, because _ is a wildcard that matches any single character,
# you should escape it as \_ to match it literally.
#
# In practice, this is rarely necessary.
#
# 13.7.6.40 SHOW WARNINGS SYNTAX
#
# 		SHOW WARNINGS [LIMIT [offset,] row_count]
# 		SHOW COUNT(*) WARNINGS
#
# SHOW_WARNINGS is a diagnostic statement that displays information about the conditions
# (errors, warnings and notes) resulting from executing a statement in the current session.
#
# Warnings are generated for DML statements such as INSERT, UPDATE, and LOAD_DATA_INFILE
# as well as DDL statements such as CREATE_TABLE and ALTER_TABLE
#
# The LIMIT clause has the same syntax as for the SELECT statement. See SECTION 13.2.10, "SELECT SYNTAX"
#
# SHOW_WARNINGS is also used following EXPLAIN, to display the extended information generated
# by EXPLAIN.
#
# See SECTION 8.8.3, "EXTENDED EXPLAIN OUTPUT FORMAT"
#
# SHOW_WARNINGS displays information about the conditions resulting from execution
# of the most recent nondiagnostic statement in the current session.
#
# If the most recent statement resulted in an error during parsing, SHOW_WARNINGS
# shows the resulting conditions, regardless of statement type (diagnostic or nondiagnostic)
#
# The SHOW_COUNT(*)_WARNINGS diagnostic statement displays the total number of errors,
# warnings, and notes.
#
# You can also retrieve this number from the warning_count system variable:
#
# 		SHOW COUNT(*) WARNINGS;
# 		SELECT @@warning_count;
#
# A difference in these statements is that the first is a diagnostic statement that
# does not clear the message list.
#
# The second, because it is a SELECT statement is considered nondiagnostic and does
# clear the message list.
#
# A related diagnostic statement, SHOW_ERRORS, shows only error conditions (it excludes
# warnings and notes), and SHOW_COUNT(*)_ERRORS statement displays the total number
# of errors.
#
# See SECTION 13.7.6.17, "SHOW ERRORS SYNTAX"
#
# GET_DIAGNOSTICS can be used to examine information for individual conditions.
#
# See SECTION 13.6.7.3, "GET DIAGNOSTICS SYNTAX"
#
# Here is a simple example that shows data-conversion warnings for INSERT.
#
# The example assumes that strict SQL mode is disabled. With strict mode
# enabled, the warnings would become errors and terminate the INSERT.
#
# 		CREATE TABLE t1 (a TINYINT NOT NULL, b CHAR(4));
# 		Query OK, 0 rows affected (0.05 sec)
#
# 		INSERT INTO t1 VALUES(10,'mysql'), (NULL,'test'), (300, 'xyz');
# 		Query OK, 3 rows affected, 3 warnings (0.00 sec)
# 		Records: 3 Duplicates: 0 Warnings: 3
#
# 		SHOW WARNINGS\G
# 		************************ 1. row ******************************
# 			Level: Warning
# 			Code : 1265
# 		Message : Data truncated for column 'b' at row 1
# 		************************ 2. row ******************************
# 			Level: Warning
# 			Code : 1048
# 		Message : Column 'a' cannot be null
# 		************************ 3. row ******************************
# 			Level: Warning
# 			Code : 1264
# 		Message : Out of range value for column 'a' at row 3
# 		3 rows in set (0.00 sec)
#
# The max_error_count system variable controls the maximum number of error, warning,
# and note messages for which the server stores information, and thus the number
# of messages that SHOW_WARNINGS displays.
#
# To change the number of messages the server can store, change the value of max_error_count
#
# max_error_count controls only how many messages are stored, not how many are counted.
#
# The value of warning_count is not limited by max_error_count, even if the number of
# messages generated exceeds max_error_count.
#
# The following example demonstrates this.
#
# The ALTER_TABLE statement produces three warning messages (strict SQL mode is disabled
# for the example to prevent an error from occuring after a single conversion issue).
#
# Only one message is stored and displayed because max_error_count has been set to 1,
# but all three are counted (as shown by the value of warning_count):
#
# 		SHOW VARIABLES LIKE 'max_error_count';
# 		+-----------------+-----------+
# 		| Variable_name   | Value 		|
# 		+-----------------+-----------+
# 		| max_error_count | 1024 		|
# 		+-----------------+-----------+
# 		1 row in set (0.00 sec)
#
# 		SET max_error_count=1, sql_mode = '';
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		ALTER TABLE t1 MODIFY b CHAR;
# 		Query OK, 3 rows affected, 3 warnings (0.00 sec)
# 		Records: 3 Duplicates: 0 Warnings: 3
#
# 		SHOW WARNINGS;
# 		+---------+-------+----------------------------------------+
# 		| Level   | Code  | Message 										  |
# 		+---------+-------+----------------------------------------+
# 		| Warning | 1263  | Data truncated for column 'b' at row 1 |
# 		+---------+-------+----------------------------------------+
# 		1 row in set (0.00 sec)
#
# 		SELECT @@warning_count;
# 		+------------------------+
# 		| @@warning_count 		 |
# 		+------------------------+
# 		| 			3 					 |
# 		+------------------------+
# 		1 row in set (0.01 sec)
#
# To disable message storage, set max_error_count to 0.
#
# In this case, warning_count still indicates how many warnings occurred, but messages
# are not stored and cannot be displayed.
#
# The sql_notes system variable controls whether note messages increment warning_count
# and whether the server stores them.
#
# By default, sql_notes is 1, but if set to 0, notes do not increment warning_count and
# the server does not store them:
#
# 		SET sql_notes = 1;
# 		DROP TABLE IF EXISTS test.no_such_table;
# 		Query OK, 0 rows affected, 1 warning (0.00 sec)
# 		SHOW WARNINGS;
# 		+-------+--------+------------------------------------+
# 		| Level | Code   | Message 							 		|
# 		+-------+--------+------------------------------------+
# 		| Note  | 1051   | Unknown table 'test.no_such_table' |
# 		+-------+--------+------------------------------------+
# 		1 row in set (0.00 sec)
#
# 		SET sql_notes = 0;
# 		DROP TABLE IF EXISTS test.no_such_table;
# 		Query OK, 0 rows affected (0.00 sec)
# 		SHOW WARNINGS;
# 		Empty set (0.00 sec)
#
# The MySQL server sends to each client a count indicating the total number of errors,
# warnings and notes resulting from the most recent statement executed by that client.
#
# From the C API, this value can be obtained by calling mysql_warning_count()
#
# See SECTION 28.7.7.82, "MYSQL_WARNING_COUNT()"
#
# In the mysql client, you can enable and disable automatic warnings display using the
# warnings and nowarning commands, respectively, or their shortcuts, \W and \w
# (see SECTION 4.5.1.2, "MYSQL CLIENT COMMANDS").
#
# For example:
#
# 		\W
# 		Show warnings enabled
# 		SELECT 1/0;
# 		+---------+
# 		| 1/0     |
# 		+---------+
# 		| NULL 	 |
# 		+---------+
# 		1 row in set, 1 warning (0.03 sec)
#
# 		Warning (Code 1365): Division by 0
# 		\w
# 		Show warnings disabled
#
# 13.7.7 OTHER ADMINISTRATIVE STATEMENTS
#
# 13.7.7.1 BINLOG SYNTAX
# 13.7.7.2 CACHE INDEX SYNTAX
#
# 13.7.7.3 FLUSH SYNTAX
# 13.7.7.4 KILL SYNTAX
#
# 13.7.7.5 LOAD INDEX INTO CACHE SYNTAX
# 13.7.7.6 RESET SYNTAX
#
# 13.7.7.7 RESET PERSIST SYNTAX
# 13.7.7.8 RESTART SYNTAX
#
# 13.7.7.9 SHUTDOWN SYNTAX
#
# 13.7.7.1 BINLOG SYNTAX
#
# 		BINLOG 'str'
#
# BINLOG is an internal-use statement. It is generated by the mysqlbinlog program
# as the printable representation of certain events in binary log files.
#
# (See SECTION 4.6.8, "MYSQLBINLOG -- UTILITY FOR PROCESSING BINARY LOG FILES")
#
# The 'str' value is base 64-encoded string that the server decodes to determine
# the data change indicated by the corresponding event.
#
# This statement requires the BINLOG_ADMIN or SUPER privilege.
#
# This statement can execute only format description events and row events.
#
# 13.7.7.2 CACHE INDEX SYNTAX
#
# 		CACHE INDEX
# 			tbl_index_list [, tbl_index_list]
# 			[PARTITION (partition_list | ALL)]
# 			IN key_cache_name
#
# 		tbl_index_list:
# 			tbl_name [[INDEX|KEY] (index_name[, index_name] ---)]
#
# 		partition_list:
# 			partition_name[, partition_name][, ---]
#
# The CACHE_INDEX statement assigns table indexes to a specific key cache.
#
# It is used only for MyISAM tables. After the indexes have been assigned,
# they can be preloaded into the cache if desired with LOAD_INDEX_INTO_CACHE
#
# The following statement assigns indexes from the tables t1, t2, and t3 to the
# key cache named hot_cache:
#
# 		CACHE INDEX t1, t2, t3 IN hot_cache;
# 		+----------+------------------------+----------+------------+
# 		| Table 	  | Op 							| Msg_type | Msg_text 	|
# 		+----------+------------------------+----------+------------+
# 		| test.t1  | assign_to_keycache 		| status   | OK 			|
# 		| test.t2  | assign_to_keycache 		| status   | OK 			|
# 		| test.t3  | assign_to_keycache 		| status   | OK 			|
# 		+----------+------------------------+----------+------------+
#
# The syntax of CACHE_INDEX enables you to specify that only particular indexes from
# a table should be assigned to the cache.
#
# The current implementation assigns all the table's indexes to the cache, so there
# is no reason to specify anything other than the table name.
#
# The key cache referred to in a CACHE_INDEX statement can be created by setting its
# size with a parameter setting statement or in the server parameter settings.
#
# For example:
#
# 		SET GLOBAL keycache1.key_buffer_size=128*1024;
#
# Key cache parameters can be accessed as members of a structured system variable.
# See SECTION 5.1.9.5, "STRUCTURED SYSTEM VARIABLES"
#
# A key cache must exist before you can assign indexes to it:
#
# 		CACHE INDEX t1 IN non_existent_cache;
# 		ERROR 1284 (HY000): Unknown key cache 'non_existent_cache'
#
# By default, table indexes are assigned to the main (default) key cache created at the
# server startup.
#
# When a key cache is destroyed, all indexes assigned to it become assigned to the
# default key cache again.
#
# Index assignment affects the server globally: If one client assigns an index to a given cache,
# This cache is used for all queries involving the index, no matter which client issues
# the queries.
#
# In MySQL 8.0, this statement is also supported for partitioned MyISAM tables.
#
# You can assign one or more indexes for one, several, or all partitions to a given
# key cache.
#
# For example, you can do the following:
#
# 		CREATE TABLE pt (c1 INT, c2 VARCHAR(50), INDEX i(c1))
# 			ENGINE=MyISAM
# 			PARTITION BY HASH(c1)
# 			PARTITIONS 4;
#
# 		SET GLOBAL kc_fast.key_buffer_size = 128 * 1024;
# 		SET GLOBAL kc_slow.key_buffer_size = 128 * 1024;
#
# 		CACHE INDEX pt PARTITION (p0) IN kc_fast;
# 		CACHE INDEX pt PARTITION (p1, p3) IN kc_slow;
#
# The previous set of statements performs the following actions:
#
# 		) Creates a partitioned table with 4 partitions; these partitions are automatically named p0, ---, p3;
# 			This table has an index named i on column c1
#
# 		) Creates 2 key caches named kc_fast and kc_slow
#
# 		) Assigns the index for partition p0 to the kc_fast key cache and the index
# 			for partitions p1 and p3 to the kc_slow key cache;
#
# 			The index for the remaining partition (p2) uses the servers
# 			default key cache
#
# If you wish instead to assign the indexes for all partitions in table pt to a single
# key cache named kc_all, you can use either one of the following 2 statements:
#
# 		CACHE INDEX pt PARTITION (ALL) IN kc_all;
#
# 		CACHE INDEX pt IN kc_all;
#
# The two statements just shown are equivalent, and issuing either one of them
# has exactly the same effect.
#
# In other words, if you wish to assign indexes for all partitions of a partitioned
# table to the same key cache, then the PARTITION (ALL) clause is optional.
#
# When assigning indexes for multiple partitions to a key cache, the partitions
# do not have to be contiguous, and you are not required to list their names
# in any particular order.
#
# Indexes for any partitions that are not explicitly assigned to a key cache automatically
# use the server's default key cache.
#
# In MySQL 8.0, index preloading is also supported for partitioned MyISAM tables.
#
# For more information, see SECTION 13.7.7.5, "LOAD INDEX INTO CACHE SYNTAX"
#
# 13.7.7.3 FLUSH SYNTAX
#
# 		FLUSH [NO_WRITE_TO_BINLOG | LOCAL] {
# 			flush_option [, flush_option] ---
# 		 | tables_option
# 		}
#
# 		flush_option: {
# 			BINARY LOGS
# 		 | ENGINE LOGS
# 		 | ERROR LOGS
# 		 | GENERAL LOGS
# 		 | HOSTS
# 		 | LOGS
# 		 | PRIVILEGES
# 		 | OPTIMIZER_COSTS
# 		 | RELAY LOGS [FOR CHANNEL channel]
# 		 | SLOW LOGS
# 		 | STATUS
# 		 | USER_RESOURCES
# 		}
#
# 		tables_option: {
# 			TABLES
# 		 | TABLES tbl_name [, tbl_name] ---
# 		 | TABLES WITH READ LOCK
# 		 | TABLES tbl_name [, tbl_name] --- WITH READ LOCK
# 		 | TABLES tbl_name [, tbl_name] --- FOR EXPORT
# 		}
#
# The FLUSH statement has several variant forms that clear or reload various internal caches,
# flush tables, or acquire locks.
#
# To execute FLUSH, you must have the RELOAD privilege.
#
# Specific flush options might require additional privileges, as described later.
#
# NOTE:
#
# 		It is not possible to issue FLUSH statements within stored functions or triggers.
#
# 		However, you may use FLUSH in stored procedures, so long as these are not called
# 		from stored functions or triggers.
#
# 		See SECTION C.1, "RESTRICTIONS ON STORED PROGRAMS"
#
# By default, the server writes FLUSH statements to the binary log so that they replicate
# to replication slaves.
#
# To suppress logging, specify the optional NO_WRITE_TO_BINLOG keyword or its alias LOCAL.
#
# NOTE:
#
# 		FLUSH_LOGS, FLUSH_BINARY_LOGS, FLUSH_TABLES_WITH_READ_LOCK (with or without a table list),
# 		and FLUSH_TABLES_tbl_name_---_FOR_EXPORT are not written to the binary log in any case
# 		because they would cause problems if replicated to a slave.
#
# The FLUSH statement causes an implicit commit. See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# The mysqladmin utility provides a command-line interface to some flush operations, using commands such
# as flush-hosts, flush-logs, flush-privileges, flush-status, and flush-tables.
#
# See SECTION 4.5.2, "MYSQLADMIN -- CLIENT FOR ADMINISTERING A MYSQL SERVER"
#
# Sending a SIGHUP signal to the server causes several flush operations to occur that are similar
# to various forms of the FLUSH statement.
#
# See SECTION 5.1.16, "SERVER RESPONSE TO SIGNALS"
#
# The RESET statement is similar to FLUSH
#
# See SECTION 13.7.7.6, "RESET SYNTAX", for information about using the RESET statement with replication.
#
# The following list describes the permitted FLUSH statement flush_option values.
#
# For descriptions of FLUSH_TABLES variants, see FLUSH TABLES SYNTAX
#
# 		) FLUSH_BINARY_LOGS
#
# 			Closes and reopens any binary log file to which the server is writing.
#
# 			If binary logging is enabled, the sequence number of the binary log file is
# 			incremented by one relative to the previous file.
#
# 		) FLUSH_ENGINE_LOGS
#
# 			Closes and reopens any flushable logs for installed storage engines.
#
# 			This causes InnoDB to flush its logs to disk.
#
# 		) FLUSH_ERROR_LOGS
#
# 			Closes and reopens any error log file to which the server is writing
#
# 		) FLUSH_GENERAL_LOGS
#
# 			Closes and reopens any general query log file to which the server is writing.
#
# 		) FLUSH_HOSTS
#
# 			Empties the host cache and the Performance Schema host_cache table that exposes
# 			the cache contents, and unblocks any blocked hosts.
#
# 			See SECTION 8.12.4.2, "DNS LOOKUP OPTIMIZATION AND THE HOST CACHE"
#
# 			Flush the host cache if some of your host change IP address or if the error message
# 			Host 'host_name' is blocked occurs for connections from legitimate hosts.
#
# 			(See SECTION B.6.2.5, "HOST 'host_name' IS BLOCKED")
#
# 			When more than max_connect_errors errors occur successively for a given host
# 			while connecting to the MySQL server, MySQL assumes that something is wrong
# 			and blocks the host from further connection requests.
#
# 			Flushing the host cache enables further connection attempts from the host.
#
# 			The default value of max_connect_errors is 100
#
# 			To avoid this error message, start the server with max_connect_errors
# 			set to a large value.
#
# 		) FLUSH_LOGS
#
# 			Closes and reopens any log file to which the server is writing.
#
# 			If binary logging is enabled, the sequence number of the binary log file
# 			is incremented by one relative to the previous file.
#
# 			If relay logging is enabled, the sequence number of the relay log file
# 			is incremented by one relative to the previous file.
#
# 			FLUSH_LOGS has no effect on tables used for the general query log or
# 			for the slow query log (see SECTION 5.4.1, "SELECTING GENERAL QUERY LOG AND SLOW QUERY LOG OUTPUT DESTINATIONS")
#
# 		) FLUSH_OPTIMIZER_COSTS
#
# 			Rereads the cost model tables so that the optimizer starts using the current cost estimates stored
# 			in them.
#
# 			The server writes a warning to the error log for any unrecognized entries.
#
# 			(For information about these tables, see SECTION 8.9.5, "THE OPTIMIZER COST MODEL")
#
# 			This operation affects only sessions that begin subsequent to the flush.
#
# 			Existing sessions continue to use the cost estimates that were current when they began.
#
# 		) FLUSH_PRIVILEGES
#
# 			Reloads the privileges from the grant tables in the mysql system database, and clears
# 			the in-memory cache used by the caching_sha2_password authentication plugin.
#
# 			As part of this operation, the server reads the global_grants table containing
# 			dynamic privilege assignments and registers any unregistered privileges found there.
#
# 			The server cache information in memory as a result of GRANT, CREATE_USER, CREATE_SERVER,
# 			and INSTALL_PLUGIN statements.
#
# 			This memory is not released by the corresponding REVOKE, DROP_USER, DROP_SERVER and
# 			UNINSTALL_PLUGIN statements, so for a server that executes many instances of the statements
# 			that cause caching, there will be an increase in memory use.
#
# 			This cached memory can be freed with FLUSH_PRIVILEGES
#
# 		) FLUSH_RELAY_LOGS_[FOR_CHANNEL_channel]
#
# 			Closes and reopens any relay log file to which the server is writing.
#
# 			If relay logging is enabled, the sequence number of the relay log file is
# 			incremented by one relative to the previous file.
#
# 			The FOR CHANNEL channel clause enables you to name which replication channel the
# 			statement applies to.
#
# 			Execute FLUSH_RELAY_LOGS_FOR_CHANNEL_channel to flush the relay log for a specific
# 			replication channel.
#
# 			If no channel is named and no extra replication channels exist, the statement
# 			applies to the default channel.
#
# 			If no channel is named and multiple replication channels exist, the statement
# 			applies to all replication channels.
#
# 			For more information, see SECTION 17.2.3, "REPLICATION CHANNELS"
#
# 		) FLUSH_SLOW_LOGS
#
# 			Closes and reopens any slow query log file to which the server is writing.
#
# 		) FLUSH_STATUS
#
# 			This option adds the session status from all active sessions to the global
# 			status variables, resets the status of all active sessions, and resets account,
# 			host, and user status values aggregated from disconnected sessions.
#
# 			See SECTION 26.12.14, "PERFORMANCE SCHEMA STATUS VARIABLE TABLES"
#
# 			This information may be of use when debugging a query. See SECTION 1.7, "HOW TO REPORT BUGS OR PROBLEMS"
#
# 		) FLUSH_USER_RESOURCES
#
# 			Resets all per-hour user resources to zero.
#
# 			This enables clients that have reached their hourly connection, query, or update
# 			limits to resume activity immediately.
#
# 			FLUSH_USER_RESOURCES does not apply to the limit on maximum simultaneous connections
# 			that is controlled by the max_user_connections system variable.
#
# 			See SECTION 6.3.6, "SETTING ACCOUNT RESOURCE LIMITS"
#
# FLUSH TABLES SYNTAX
#
# 	FLUSH_TABLES flushes tables, and depending on the variant used, acquires locks.
#
# Any TABLES variant used in a FLUSH statement must be the only option used.
#
# FLUSH_TABLE is a synonym for FLUSH_TABLES
#
# NOTE:
#
# 		The descriptions here that indicate tables are flushed by closing them apply differently
# 		for InnoDB, which flushes table contents to disk but leaves them open.
#
# 		This still permits table files to be copied while the tables are open, as long as other
# 		activity does not modify them.
#
# 		) FLUSH_TABLES
#
# 			Closes all open tables,, forces all tables in use to be closed, and flushes the prepared
# 			statement cache.
#
# 			For information about prepared statement caching, see SECTION 8.10.3, "CACHING OF PREPARED STATEMENTS AND STORED PROGRAMS"
#
# 			FLUSH_TABLES is not permitted when there is an active LOCK_TABLES_---_READ 
#
# 			To flush and lock tables, use FLUSH_TABLES_tbl_name_---_WITH_READ_LOCK instead.
#
# 		) FLUSH_TABLES_tbl_name_[, tbl_name] ---
#
# 			With a list of one or more comma-separated table names, this statement is like FLUSH_TABLES
# 			with no names except that the server flushes only the named tables.
#
# 			If a named table does not exist, no error occurs.
#
# 		) FLUSH_TABLES_WITH_READ_LOCK
#
# 			Closes all open tables and locks all tables for all databases with a global read lock.
#
# 			This is a very convenient way to get backups if you have a file system such as
# 			Veritas or ZFS that can take snapshots in time.
#
# 			Use UNLOCK_TABLES to release the lock.
#
# 			FLUSH_TABLES_WITH_READ_LOCK acquires a global read lock rather  than table locks, so it is
# 			not subject to the same behavior as LOCK_TABLES and UNLOCK_TABLES with respect to table
# 			locking and implicit commits:
#
# 				) UNLOCK_TABLES implicitly commits any active transaction only if any tables currently
# 					have been locked with LOCK_TABLES.
#
# 					The commit does not occur for UNLOCK_TABLES following FLUSH_TABLES_WITH_READ_LOCK
# 					because the latter statement does not acquire table locks.
#
# 				) Beginning a transaction causes table locks acquired with LOCK_TABLES to be released,
# 					as though you had executed UNLOCK_TABLES
#
# 					Beginning a transaction does not release a global read lock acquired with
# 					FLUSH_TABLES_WITH_READ_LOCK
#
# 			FLUSH_TABLES_WITH_READ_LOCK does not prevent the server from inserting rows into the log tables
# 			(see SECTION 5.4.1, "SELECTING GENERAL QUERY LOG AND SLOW QUERY LOG OUTPUT DESTINATIONS")
#
# 		) FLUSH_TABLES_tbl_name [, tbl_name] --- WITH_READ_LOCK
#
# 			This statement flushes and acquires read locks for the named tables.
#
# 			The statement first acquires exclusive metadata locks for the tables, so it waits
# 			for transactions that have those tables open to complete.
#
# 			Then the statement flushes the tables from the table cache, reopens the tables, acquires
# 			table locks (like LOCK_TABLES_---_READ), and downgrades the metadata locks from exclusive
# 			to shared.
#
# 			After the statement acquires locks and downgrades the metadata locks, other sessions can read
# 			but not modify the tables.
#
# 			Because this statement acquires table locks, you must have the LOCK_TABLES privilege for each
# 			table, in addition to the RELOAD privilege that is required to use any FLUSH Statement.
#
# 			This statement applies only to existing base (non-TEMPORARY) tables.
#
# 			If a name refers to a base table, that table is used.
#
# 			If it refers to a TEMPORARY table, it is ignored.
#
# 			If a name applies to a view, an ER_WRONG_OBJECT error occurs.
#
# 			Otherwise, an ER_NO_SUCH_TABLE error occurs.
#
# 			Use UNLOCK_TABLES to release the locks, LOCK_TABLES to release the locks
# 			and acquire other locks, or START_TRANSACTION to release the locks and begin
# 			a new transaction.
#
# 			This FLUSH_TABLES variant enables tables to be flushed and locked in a single operation.
#
# 			It provides a workaround for the restriction that FLUSH_TABLES is not permitted
# 			when there is an active LOCK_TABLES_---_READ
#
# 			This statement does not perform an implicit UNLOCK_TABLES, so an error results if
# 			you use the statement while there is any active LOCK_TABLES or use it a second
# 			time without first releasing the locks acquired.
#
# 			If a flushed table was opened with HANDLER, the handler is implicitly flushed and
# 			loses its position.
#
# 		) FLUSH_TABLES_tbl_name_[, tbl_name]_---_FOR_EXPORT
#
# 			This FLUSH_TABLES variant applies to InnoDB tables.
#
# 			It ensures that changes to the named tables have been flushed to disk so that
# 			binary table copies can be made while the server is running.
#
# 			The statement works like this:
#
# 				a. It acquires shared metadata locks for the named tables.
#
# 					The statement blocks as long as other sessions have active transactions
# 					that have modified those tables or hold table locks for them.
#
# 					When the locks have been acquired, the statement blocks transactions
# 					that attempt to update the tables, while permitting read-only operations
# 					to continue.
#
# 				b. It checks whether all storage engines for the tables support FOR EXPORT.
#
# 					If any do not, an ER_ILLGAL_HA error occurs and the statement fails.
#
# 				c. The statement notifies the storage engine for each table to make the table ready
# 					for export.
#
# 					The storage engine must ensure that any pending changes are written to disk.
#
# 				d. THe statement puts the session in lock-tables mode so that the metadata locks acquired
# 					earlier are not released when the FOR EXPORT statement completes.
#
# The FLUSH_TABLES_---_FOR_EXPORT statement requires that you have the SELECT privilege for each table.
#
# Because this statement acquires table locks, you must also have the LOCK_TABLES privilege for each
# table, in addition to the RELOAD privilege that is required to use any FLUSH statement.
#
# This statement applies only to existing base (non-TEMPORARY) tables.
#
# If a name refers to a base table, that table is used. If it refers to a TEMPORARY table,
# it is ignored.
#
# if a name applies to a view, an ER_WRONG_OBJECT error occurs. Otherwise, an ER_NO_SUCH_TABLE error occurs.
#
# InnoDB supports FOR EXPORT for tables that have their own .ibd file file (that is, tables created with the
# innodb_file_per_table setting enabled)
#
# InnoDB ensures when notidifed by the FOR EXPORT statement that any changes have been flushed to disk.
#
# This permits a binary copy of table contents to be made while the FOR EXPORT statement is in effect
# because the .ibd file is transaction consistent and can be copied while the server is running.
#
# FOR EXPORT does not apply to InnoDB system tablespace files, or to InnoDB tables that have FULLTEXT
# indexes.
#
# FLUSH_TABLES_---_FOR_EXPORT is supported for partitioned InnoDB tables.
#
# When notified by FOR EXPORT, InnoDB writes to disk certain kinds of data that is normally
# held in memory or in separate disk buffers outside the tablespace files.
#
# For each table, InnoDB also produces a file named table_name.cfg in the same database
# directory as the table.
#
# The .cfg contains metadata needed to reimport the tablespace files later, into the same or
# different server.
#
# When the FOR EXPORT statement completes, InnoDB will have flushed all dirty pages to the table
# data files.
#
# ANy change buffer entries are merged prior to flushing.
#
# At this point, the tables are locked and quiescent: The tables are in a transactionally
# consistent state on disk and you can copy the .ibd tablespace files along with the corresponding
# .cfg files to get a consistent snapshot of those tables.
#
# For the procedure to reimport the copied table data into a MySQL instance, see SECTION 15.6.3.7, "COPYING TABLESPACES TO ANOTHER INSTANCE"
#
# After you are done with the tables, use UNLOCK_TABLES to release the locks, LOCK_TABLES to release
# the locks and acquire other locks, or START_TRANSACTION to release the locks and begin a new 
# transaction.
#
# While any of these statements is in effect within the session, attempts to use FLUSH_TABLES_---_FOR_EXPORT
# produce an error:
#
# 		FLUSH TABLES --- WITH READ LOCK
# 		FLUSH TABLES --- FOR EXPORT
# 		LOCK TABLES --- READ
# 		LOCK TABLES --- WRITE
#
# While FLUSH_TABLES_---_FOR_EXPORT is in effect within the session, attempts to use any of these
# statements produce an error:
#
# 		FLUSH TABLES WITH READ LOCK
# 		FLUSH TABLES --- WITH READ LOCK
# 		FLUSH TABLES --- FOR EXPORT
#
# 13.7.7.4 KILL SYNTAX
#
# 		KILL [CONNECTION | QUERY] processlist_id
#
# Each connection to mysqld runs in a separate thread. You can kill a thread with the KILL processlist_id statement.
#
# Thread processlist identifiers can be determined from the ID column of the INFORMATION_SCHEMA PROCESSLIST table,
# the Id column of SHOW_PROCESSLIST output, and the PROCESSLIST_ID column of the Performance Schema threads table.
#
# The value for the current thread is returned by the CONNECTION_ID() function.
#
# KILL permits an optional CONNECTION or QUERY modifier:
#
# 		) KILL_CONNECTION is the same as KILL with no modifier:
#
# 			It terminates the connection associated with the given processlist_id, after terminating
# 			any statement the connection is executing.
#
# 		) KILL_QUERY terminates the statement the connection is currently executing, but leaves the connection
# 			itself intact.
#
# If you have the PROCESS privilege, you can see all threads.
#
# If you have the CONNECTION_ADMIN or SUPER privilege, you can kill all threads and statements.
#
# Otherwise, you can see and kill only your own threads and statements.
#
# You can also use the mysqladmin processlist and mysqladmin kill commands to examine and kill threads.
#
# When you use a KILL, a thread-specific kill flag is set for the thread.
#
# In most cases, it might take some time for the thread to die because the kill flag
# is checked only at specific intervals:
#
# 		) During SELECT operations, for ORDER BY and GROUP BY loops, the flag is checked after reading
# 			a block of rows.
#
# 			If the kill flag is set, the statement is aborted.
#
# 		) ALTER_TABLE operations that make a table copy check the kill flag periodically for each few copied
# 			rows read from the original table.
#
# 			If the kill flag was set, the statement is aborted and the temporary table is deleted.
#
# 			The KILL statement returns without waiting for confirmation, but the kill flag check aborts the
# 			operation within a reasonably small amount of time.
#
# 			Aborting the operation to perform any necessary cleanup also takes some time.
#
# 		) During UPDATE or DELETE operations, the kill flag is checked after each block read
# 			and after each updated or deleted row.
#
# 			If the kill flag is set, the statement is aborted.
#
# 			If you are not using transactions, the changes are not rolled back.
#
# 		) GET_LOCK() aborts and returns NULL
#
# 		) If the thread is in the table lock handler (state: Locked), the table lock is quickly aborted
#
# 		) If the thread is waiting for free disk space in a write cell, the write is aborted with a 
# 			"disk full" error message.
#
# WARNING:
#
# 		Killing a REPAIR_TABLE or OPTIMIZE_TABLE operation on a MyISAM table results in a table that is
# 		corrupted and unusable.
#
# 		Any reads or writes to such a table fail until you optimize or repair it again (without interruption)
#
# 13.7.7.5 LOAD INDEX INTO CACHE SYNTAX
#
# 		LOAD INDEX INTO CACHE
# 			tbl_index_list [, tbl_index_list] ---
#
# 		tbl_index_list:
# 			tbl_name
# 				[PARTITION (partition_list | ALL)]
# 				[[INDEX|KEY] (index_name[, index_name] ---)]
# 				[IGNORE LEAVES]
#
# 		partition_list:
# 			partition_name[, partition_name][, ---]
#
# The LOAD_INDEX_INTO_CACHE statement preloads a table index into the key cache to which
# it has been assigned by an explicit CACHE_INDEX statement, or into the default key cache
# otherwise.
#
# LOAD_INDEX_INTO_CACHE is used only for MyISAM tables. In MySQL 8.0, it is also supported
# for partitioned MyISAM tables; in addition, indexes on partitioned tables can be
# preloaded for one, several or all partitions.
#
# The IGNORE LEAVES modifier causes only blocks for the nonleaf nodes of the index to be preloaded.
#
# IGNORE LEAVES is also supported for partitioned MyISAM tables.
#
# The following statement preloads nodes (index blocks) of indexes for the tables t1 and t2:
#
# 		LOAD INDEX INTO CACHE t1, t2 IGNORE LEAVES;
# 		+-----------+--------------------+----------+-------------+
# 		| Table 		| Op 						| Msg_type | Msg_text 	 |
# 		+-----------+--------------------+----------+-------------+
# 		| test.t1   | preload_keys 	   | status   | OK 			 |
# 		| test.t2   | preload_keys 		| status   | OK 			 |
# 		+-----------+--------------------+----------+-------------+
#
# This statement preloads all index blocks from t1.
#
# It preloads only blocks for the nonleaf nodes from t2.
#
# The syntax of LOAD_INDEX_INTO_CACHE enables you to specify that only particular
# indexes from a table should be preloaded.
#
# The current implementation preloads all the table's indexes into the cache,
# so there is no reason to specify anything other than the table name.
#
# It is possible to preload indexes on specific partitions of partitioned MyISAM tables.
#
# For example, of the following 2 statements, the first preloads indexes for partition
# p0 of a partitioned table pt, while the second preloads the indexes for partitions
# p1 and p3 of the same table:
#
# 		LOAD INDEX INTO CACHE pt PARTITION (p0);
# 		LOAD INDEX INTO CACHE pt PARTITION (p1, p3);
#
# To preload the indexes for all partitions in table pt, you can use either one of the
# following 2 statements:
#
# 		LOAD INDEX INTO CACHE pt PARTITION (ALL);
# 
# 		LOAD INDEX INTO CACHE pt;
#
# The two statements just shown are equivalent, and issuing either one of them has exactly
# the same effect.
#
# In other words, if you wish to preload indexes for all partitions of a partitioned table,
# then the PARTITION (ALL) clause is optional.
#
# When preloading indexes for multiple partitions, the partitions do not have to be contiguous,
# and you are not required to list their names in any particular order.
#
# LOAD_INDEX_INTO_CACHE_---_IGNORE_LEAVES fails unless all indexes in a table have the
# same block size.
#
# You can determine index block size for a table by using myisamchk -dv and checking
# the Blocksize column.
#
# 13.7.7.6 RESET SYNTAX
#
# 		RESET reset_option [, reset_option] ---
#
# 		reset_option: {
# 			MASTER
# 		 | SLAVE
# 		}
#
# The RESET statement is used to clear the state of various server operations.
#
# You must have the RELOAD privilege to execute RESET.
#
# For information about the RESET_PERSIST statement that removes persisted global
# system variables, see SECTION 13.7.7.7, "RESET PERSIST SYNTAX"
#
# RESET acts as a stronger version of the FLUSH statement. See SECTION 13.7.7.3, "FLUSH SYNTAX"
#
# The RESET statement causes an implicit commit. See SECTION 13.3.3, "STATEMENTS THAT CAUSE AN IMPLICIT COMMIT"
#
# The following list describes the permitted RESET statement reset_option values:
#
# 		) RESET MASTER
#
# 			Deletes all binary logs listed in the index file, resets the binary log index file
# 			to be empty and creates a new binary log file
#
# 		) RESET SLAVE
#
# 			Makes the slave forget its replication position in the master binary logs.
#
# 			Also resets the relay log by deleting any existing relay log files and beginning
# 			a new one.
#
# 13.7.7.7 RESET PERSIST SYNTAX
#
# 		RESET PERSIST [[IF EXISTS] system_var_name]
#
# RESET_PERSIST removes persisted global system variable settings from the mysqld-auto.cnf option file
# in the data directory.
#
# Removing a persisted system variable causes the variable no longer to be initialized from mysqld-auto.cnf
# at server startup.
#
# For more information about persisting system variables and the mysqld-auto.cnf file, see SECTION 5.1.9.3, "PERSISTED SYSTEM VARIABLES"
#
# THe privileges required for RESET_PERSIST depend on the type of system variable to be removed:
#
# 		) For dynamic system variables, this statement requires the SYSTEM_VARIABLES_ADMIN or SUPER privilege
#
# 		) For read-only system variables, this statement requires the SYSTEM_VARIABLES_ADMIN and PERSIST_RO_VARIABLES_ADMIN privileges
#
# See SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES"
#
# Depending on whether the variable name and IF EXISTS clauses are present, the RESET_PERSIST
# statement has these forms:
#
# 		) To remove all persisted variables from mysqld-auto.cnf, use RESET_PERSIST without naming any system variable:
#
# 			RESET PERSIST;
#
# 			You must have privileges for removing both dynamic and read-only system variables if mysqld-auto.cnf
# 			contains both kinds of variables.
#
# 		) To remove a specific persisted variable from mysqld-auto.cnf, name it in the statement:
#
# 			RESET PERSIST system_var_name;
#
# 			This includes plugin system variables, even if the plugin is not currently installed.
#
# 			If the variable is not present in the file, an error occurs.
#
# 		) To remove a specific persisted variable from mysqld-auto.cnf, but produces a warning rather than an error
# 			if the variable is not present in the file, add an IF EXISTS clause to the previous syntax:
#
# 			RESET PERSIST IF EXISTS system_var_name;
#
# RESET_PERSIST is not affected by the value of the persisted_globals_load system variable.
#
# RESET_PERSIST affects the contents of the Performance Schema persisted_variables table because the tables
# contents correspond to the contents of the mysqld-auto.cnf file.
#
# On the other hand, because RESET_PERSIST does not change variable values, it has no effect on the contents
# of the Performance Schema variables_info table until the server is restarted.
#
# For information about RESET statement variants that clear the state of other server operations,
# see SECTION 13.7.7.6, "RESET SYNTAX"
#
# 13.7.7.8 RESTART SYNTAX
#
# 		RESTART
#
# This statement stops and restarts the MySQL server. It requires the SHUTDOWN privilege.
#
# One use for RESTART is when it is not possible or convenient to gain command-line access
# to the MySQL server on the server host to restart it.
#
# For example, SET_PERSIST_ONLY can be used at runtime to make configuration changes to
# system variables that can be set only at server startup, but the server must still be 
# restarted for those changes to take effect.
#
# The RESTART statement provides a way to do so from within client sessions, without requiring
# command-line access on the server host.
#
# NOTE:
#
# 		After executing a RESTART statement, the client can expect the current connection to be lost.
#
# 		If auto-reconnect is enabled, the connection will be reestablished after the server
# 		restarts.
#
# 		Otherwise, the connection must be reestablished manually.
#
# A successful RESTART operation requires mysqld to be running in an environment that has a monitoring
# process available to detect a server shutdown performed for restart purposes:
#
# 		) In the presence of a monitoring process, RESTART causes mysqld to terminate such that the monitoring
# 			process can determine that it hsould start a new mysqld instance
#
# 		) If no monitoring process is present, RESTART fails with an error
#
# These platforms provide the necessary monitoring support for the RESTART statement:
#
# 		) Windows, when mysqld is started as a Windows service or standalone.
#
# 			(mysqld forks, and one process acts as a monitor to the other, which acts as
# 			the server)
#
# 		) Unix and Unix-like systems that use systemd or mysqld_safe to manage mysqld
#
# On Windows, the forking used to implement RESTART makes determining the server process to attach
# to for debugging more difficult.
#
# To alleviate this, starting the server with --gdb suppresses forking, in addition to its other
# actions done to set up a debugging environment.
#
# In non-debug settings, --no-monitor may be used for the sole purpose of suppressing
# forking at the monitor process.
#
# For a server started with either --gdb or --no-monitor, executing RESTART causes the server
# to simply exit without restarting.
#
# 13.7.7.9 SHUTDOWN SYNTAX
#
# 		SHUTDOWN
#
# THis statement stops the MySQL server. It requires the SHUTDOWN privilege.
#
# SHUTDOWN provides an SQL-level interface to the same functionality available
# using the mysqladmin shutdown command.
#
# 13.8 UTILITY STATEMENTS
#
# 13.8.1 DESCRIBE SYNTAX
#
# 		The DESCRIBE and EXPLAIN statements are synonyms, used either to obtain information about
# 		table structure or query execution plans.
#
# 		For more information, see SECTION 13.7.6.5, "SHOW COLUMNS SYNTAX" and SECTION 13.8.2, "EXPLAIN SYNTAX"
#
# 13.8.2 EXPLAIN SYNTAX
#
# 		{EXPLAIN | DESCRIBE | DESC}
# 			tbl_name [col_name | wild]
#
# 		{EXPLAIN | DESCRIBE | DESC}
# 			[explain_type]
# 			{explainable_stmt | FOR CONNECTION connection_id}
#
# 		explain_type: {
# 			FORMAT = format_name
# 		}
#
# 		format_name: {
# 			TRADITIONAL
# 		 | JSON
# 		}
#
# 		explainable_stmt: {
# 			SELECT statement
# 		 | DELETE statement
# 		 | INSERT statement
# 		 | REPLACE statement
# 		 | UPDATE statement
# 		}
#
# The DESCRIBE and EXPLAIN statements are synonyms. In practice, the DESCRIBE keyword is more often
# used to obtain information about table structure, whereas EXPLAIN is used to obtain a query
# execution plan (that is, an explanation of how MySQL would execute a query)
#
# The following discussion uses the DESCRIBE and EXPLAIN keywords in accordance with those uses,
# but the MySQL parser treats them as complete synonyms.
#
# 		) OBTAINING TABLE STRUCTURE INFORMATION
#
# 		) OBTAINING EXECUTION PLAN INFORMATION
#
# OBTAINING TABLE STRUCTURE INFORMATION
#
# DESCRIBE provides information about the columns in a table:
#
# 		DESCRIBE City;
# 		+-------------------+-------------------+---------+---------+-------------------+--------------------------+
# 		| Field 				  | Type 				 | Null    | Key 		| Default 			  | Extra 						  |
# 		+-------------------+-------------------+---------+---------+-------------------+--------------------------+
# 		| Id 					  | int(11) 			 | NO 	  | PRI 	   | NULL 				  | auto_increment 			  |
# 		| Name 				  | char(35) 			 | NO 	  | 			| 						  | 								  |
# 		| Country 			  | char(3) 			 | NO 	  | UNI 	   | 						  | 								  |
# 		| District 			  | char(20) 			 | YES 	  | MUL     | 						  | 								  |
# 		| Population 		  | int(11) 			 | NO 	  | 		   | 0 					  | 								  |
# 		+-------------------+-------------------+---------+---------+-------------------+--------------------------+
#
# DESCRIBE is a shortcut for SHOW_COLUMNS
#
# These statements also display information for views. The description for SHOW_COLUMNS provides more information
# about the output columns.
#
# See SECTION 13.7.6.5, "SHOW COLUMNS SYNTAX"
#
# By default, DESCRIBE displays information about all columns in the table.
#
# col_name, if given, is the name of a column in the table. In this case, the statement displays
# information only for the named column.
#
# wild, if given, is a pattern string. It can contain the SQL % and _ wildcard characters.
#
# In this case, the statement displays output only for the columns with names matching the string.
#
# There is no need to enclose the string within quotation marks unless it contains spaces or
# other special characters.
#
# The DESCRIBE statement is provided for compatibility with Oracle
#
# The SHOW_CREATE_TABLE, SHOW_TABLE_STATUS and SHOW_INDEX statements also provide information
# about tables.
#
# See SECTION 13.7.6, "SHOW SYNTAX"
#
# OBTAINING EXECUTION PLAN INFORMATION
#
# The EXPLAIN statement provides information about how MySQL executes statements:
#
# 		) EXPLAIN works with SELECT, DELETE, INSERT, REPLACE and UPDATE statements.
#
# 		) When EXPLAIN is used with an explainable statement, MySQL displays information from the
# 			optimizer about the statement execution plan.
#
# 			That is, MySQL explains how it would process the statement, including information about how
# 			tables are joined and in which order.
#
# 			FOr information about using EXPLAIN to obtain execution plan information.
#
# 			See SECTION 8.8.2, "EXPLAIN OUTPUT FORMAT"
#
# 		) When EXPLAIN is used with FOR CONNECTION connection_id rather than an explainable
# 			statement, it displays the execution for the statement executing in the named connection.
#
# 			See SECTION 8.8.4, "OBTAINING EXECUTION PLAN INFORMATION FOR A NAMED CONNECTION"
#
# 		) For explainable statements, EXPLAIN produces additional execution plan information that
# 			can be displayed using SHOW_WARNINGS.
#
# 			See SECTION 8.8.3, "EXTENDED EXPLAIN OUTPUT FORMAT"
#
# 		) EXPLAIN is useful for examining queries involving partitioned tables. See SECTION 23.3.5, "OBTAINING INFORMATION ABOUT PARTITIONS"
#
# 		) The FORMAT option can be used to select the output format.
#
# 			TRADITIONAL presents the output in tabular format.
#
# 			THis is the default if no FORMAT option is present. JSON format displays the information
# 			in JSON format.
#
# EXPLAIN requires the SELECT privilege for any tables or views accessed, including any underlying
# tables of views.
#
# For views, EXPLAIN also requires the SHOW_VIEW privilege.
#
# With the help of EXPLAIN, you can see where you should add indexes to tables so that the statement
# executes faster by using indexes to find rows.
#
# You can also use EXPLAIN to check whether the optimizer joins the tables in an optimal order.
#
# To give a hint to the optimizer to use a join order corresponding to the order in which the
# tables are named in a SELECT statement, begin the statement with SELECT STRAIGHT_JOIN rather
# than just SELECT.
#
# (See SECTION 13.2.10, "SELECT SYNTAX")
#
# The optimizer trace may sometimes provide information complementary to that of EXPLAIN
#
# However, the optimizer trace format and content are subject to change between versions.
#
# For details, see MYSQL INTERNALS: TRACING THE OPTIMIZER
#
# If you have a problem with indexes not being used when you believe that they should be,
# run ANALYZE_TABLE to update table statistics, such as cardinality of keys, that can
# affect the choices the optimizer makes.
#
# See SECTION 13.7.3.1, "ANALYZE TABLE SYNTAX"
#
# NOTE:
#
# 		MYSQL Workbench has a Visual Explain capability that provides a visual representation
# 		of EXPLAIN output.
#
# 		See TUTORIAL: USING EXPLAIN TO IMPROVE QUERY PERFORMANCE
#
# 13.8.3 HELP SYNTAX
#
# 		HELP 'search_string'
#
# The HELP statement returns online information from the MySQL Reference manual.
#
# Its proper operation requires that the help tables in the mysql database be
# initialized with help topic information (see SECTION 5.1.15, "SERVER-SIDE HELP")
#
# The HELP statement searches the help tables for the given search string and displays the result of the search.
#
# The search string is not case-sensitive.
#
# The search string can contain the wildcard characters % and _
#
# These have the same meaning as for pattern-matching operations performed with the LIKE
# operator.
#
# For example, HELP 'rep%' returns a list of topics that begin with rep.
#
# The HELP statement understands several types of search strings:
#
# 		) At the most general level, use contents to retrieve a list of the top-level help categories:
#
# 			HELP 'contents'
#
# 		) For a list of topics in a given help category, such as Data Types, use the category name:
#
# 			HELP 'data types'
#
# 		) For help on a specific help topic, such as the ASCII() function or the CREATE_TABLE statement,
# 			use the associated keyword or keywords:
#
# 			HELP 'ascii'
# 			HELP 'create table'
#
# In other words, the search string matches a category, many topics or a single topic.
#
# You cannot necessarily tell in advance whether a given search string will return a list
# of items or the help information for a single help topic.
#
# However, you can tell what kind of response HELP returned by examining the number of rows
# and columns in the result set.
#
# The following descriptions indicate the forms that the result set can take.
#
# Output for the example statements is shown using the familiar "tabular" or "vertical"
# format that you see when using the mysql client, but note that mysql itself reformats
# HELP result sets in a different way.
#
# 		) Empty result set
#
# 			No match could be found for the search string.
#
# 		) Result set containing a single row with three columns
#
# 			THis means that the search string yielded a hit for the help topic.
#
# 			The result has three columns:
#
# 				) name: The topic name
#
# 				) description: Descriptive help text for the topic
#
# 				) example: Usage example or examples. This column might be blank
#
# 			Example: HELP 'replace'
#
# 			Yields:
#
# 				name: REPLACE
# 				description: Syntax:
# 				REPLACE(str,from_str,to_str)
#
# 				Returns the string str with all occurrences of the string from_str
# 				replaced by the string to_str. REPLACE() performs a case-sensitive
# 				match when searching for from_str.
#
# 				example: mysql> SELECT REPLACE('www.mysql.com', 'w', 'WW');
# 						-> 'WWWWWW.mysql.com'
#
# 		) Result set containing multiple rows with two columns
#
# 			This means that the search string matched many help topics.
#
# 			The result set indicates the help topic names:
#
# 				) name: THe help topic name.
#
# 				) is_it_category: Y if the name represents a help category, N if it does not.
#
# 					If it does not, the name value when specified as the argument to the HELP
# 					statement should yield a single-row result set containing a description for the named item.
#
# 			Example: HELP 'status'
#
# 			Yields:
#
# 				+--------------------------------------+----------------+
# 				| name 											| is_it_category |
# 				+--------------------------------------+----------------+
# 				| SHOW 										   | N 				  |
# 				| SHOW ENGINE 									| N 				  |
# 				| SHOW MASTER STATUS 						| N 				  |
# 				| SHOW PROCEDURE STATUS 					| N 				  |
# 				| SHOW SLAVE STATUS 							| N 				  |
# 				| SHOW STATUS 									| N 				  |
# 				| SHOW TABLE STATUS 							| N 				  |
# 				+--------------------------------------+----------------+
#
# 		) Result set containing multiple rows with three columns
#
# 			This means the search string matches a category. The result set contains category entries:
#
# 				) source_category_name: The help category name
#
# 				) name: The category or topic name
#
# 				) is_it_category: Y if the name represents a help category, N if it does not.
#
# 					If it does not, the name value when specified as the argument to the HELP statement
# 					should yield a single-row result set containing a description for the named item.
#
# 			Example: HELP 'functions'
#
# 			Yields:
#
# 				+-----------------------------------------+----------------------------------+------------------------+
# 				| source_category_name 							| name 									  | is_it_category 		   |
# 				+-----------------------------------------+----------------------------------+------------------------+
# 				| Functions 										| CREATE FUNCTION 					  | N 							|
# 				| Functions 										| DROP FUNCTION 						  | N 							|
# 				| Functions 										| Bit Functions 						  | Y 							|
# 				| Functions 										| Comparison operators 				  | Y 							|
# 				| Functions 										| Control flow functions 			  | Y 							|
# 				| Functions 										| Date and Time Functions 			  | Y 							|
# 				| Functions 										| Encryption Functions 				  | Y 							|
# 				| Functions 										| Information Functions 			  | Y 							|
# 				| Functions 										| Logical operators 					  | Y 							|
# 				| Functions 										| Miscellaneous Functions 			  | Y 							|
# 				| Functions 										| Numeric Functions 					  | Y 							|
# 				| Functions 										| String Functions 					  | Y 							|
# 				+-----------------------------------------+----------------------------------+------------------------+
#
# 
# 13.8.4 USE SYNTAX
#
# 		USE db_name
#
# The USE db_name statement tells MySQL to use the db_name database as the default (current) database
# for subsequent statements.
#
# The database remains the default until the end of the session or another USE statement is issued:
#
# 		USE db1;
# 		SELECT COUNT(*) FROM mytable; #selects from db1.mytable
# 		USE db2;
# 		SELECT COUNT(*) FROM mytable; #selects from db2.mytable
#
# The database name must be specified on a single line. Newlines in database names are not supported.
#
# Making a particular database the default by means of the USE statement does not preclude accessing
# tables in other databases.
#
# The following example accesses the author table from the db1 database and the editor
# table from the db2 database:
#
# 		USE db1;
# 		SELECT author_name.editor_name FROM author,db2.editor
# 			WHERE author.editor_id = db2.editor.editor_id;
#
# CHAPTER 14 MYSQL DATA DICTIONARY
#
# TABLE OF CONTENTS
#
# 14.1 DATA DICTIONARY SCHEMA
# 14.2 REMOVAL OF FILE-BASED METADATA STORAGE
# 14.3 TRANSACTIONAL STORAGE OF DICTIONARY DATA
# 14.4 DICTIONARY OBJECT CACHE
#
# 14.5 INFORMATION_SCHEMA AND DATA DICTIONARY INTEGRATION
# 14.6 SERIALIZED DICTIONARY INFORMATION (SDI)
# 14.7 DATA DICTIONARY USAGE DIFFERENCES
# 14.8 DATA DICTIONARY LIMITATIONS
#
# MySQL Server incorporates a transactional data dictionary that stores information about database
# objects.
#
# In previous MySQL releases, dictionary data was stored in metadata files, nontransactional tables,
# and storage engine-specific data dictionaries.
#
# This chapter describes the main features, benefits, usage differences, and limitations of the data
# dictionary.
#
# For other implications of the data dictionary feature, refer to the "Data Dictionary notes" section
# in the MySQL 8.0 RELEASE NOTES.
#
# Benefits of the MySQL data dictionary include:
#
# 		) Simplicity of a centralized data dictionary schema that uniformly stores dictionary data. See SECTION 14.1, "DATA DICTIONARY SCHEMA"
#
# 		) Removal of file-based metadata storage. See SECTION 14.2,, "REMOVAL OF FILE-BASED METADATA STORAGE"
#
# 		) Transactional, crash-safe storage of dictionary data. See SECTION 14.3, "TRANSACTIONAL STORAGE OF DICTIONARY DATA"
#
# 		) Uniform and centralized caching for dictionary objects. See SECTION 14.4, "DICTIONARY OBJECT CACHE"
#
# 		) A simpler and improved implementation for some INFORMATION_SCHEMA tables. See SECTION 14.5, "INFORMATION_SCHEMA AND DATA DICTIONARY INTEGRATION"
#
# 		) Atomic DDL. See SECTION 13.1.1, "ATOMIC DATA DEFINITION STATEMENT SUPPORT"
#
# IMPORTANT:
#
# 		A data dictionary-enabled server entails some general operational differences compared to a server
# 		that does not have a data dictionary; see SECTION 14.7, "DATA DICTIONARY USAGE DIFFERENCES"
#
# 		Also, for upgrades to MySQL 8.0, the upgrade procedure differs somewhat from previous MySQL
# 		releases and requires that you verify the upgrade readiness of your installation by checking
# 		specific prerequisites.
#
# 		For more information, see SECTION 2.11.1, "UPGRADING MYSQL", particularly SECTION 2.11.1.4,
# 		"PREPARING YOUR INSTALLATION FOR UPGRADE"
#
# 14.1 DATA DICTIONARY SCHEMA
#
# Data dictionary tables are protected and may only be accessed in debug builds of MySQL.
#
# However, MySQL supports access to data stored in data dictionary tables through INFORMATION_SCHEMA
# tables and SHOW statements.
#
# For an overview of the tables that comprise the data dictionary, see DATA DICTIONARY TABLES.
#
# MySQL system tables still exist in MySQL 8.0 and can be viewed by issuing a SHOW_TABLES
# statement on the mysql system database.
#
# Generally, the difference between MySQL system tables and data dictionary tables is that system
# tables contain auxiliary data such as time zone and help information, whereas data dictionary
# tables contain data required to execute SQL queries.
#
# MySQL system tables and data dictionary tables also differ in how they are upgraded.
#
# Upgrading MySQL system tables requires running mysql_upgrade. Data dictionary upgrades
# are managed by the MySQL server.
#
# See How The Data Dictionary is Upgraded.
#
# HOW THE DATA DICTIONARY IS UPGRADED
#
# New versions of MYSQL may include changes to data dictionary table definitions.
#
# Such changes are present in newly installed versions of MySQL, but when performing an
# in-place upgrade of MySQL binaries, changes are applied when the MySQL server is restarted
# using the new binaries.
#
# At startup, the data dictionary version of the server is compared to the version information
# stored in the data dictionary to determine if data dictionary tables should be upgraded.
#
# If an upgrade is necessary and supported, the server creates data dictionary tables with
# updated definitions, copies persisted metadata to the new tables, atomically replaces
# the old tables with the new ones, and reinitializes the data dictionary.
#
# If an upgrade is not necessary, startup continues without updating the data dictionary
# tables.
#
# Upgrade of data dictionary tables is an atomic operation, which means that all of the
# data dictionary tables are upgraded as necessary or the operation fails.
#
# If the upgrade operation fails, server startup fails with an error.
#
# In this case, the old server binaries can be used with the old data directory to start
# the server.
#
# When the new server binaries are used again to start the server, the data dictionary upgrade
# is reattempted.
#
# Generally, after data dictionary tables are successfully upgraded, it is not possible to restart
# the server using the old server binaries.
#
# As a result, downgrading MySQL server binaries to a previous MySQL version is not supported
# after data dictionary tables are upgraded.
#
# The mysqld --no-dd-upgrade option can be used to prevent automatic upgrade of data dictionary
# tables at startup.
#
# When --no-dd-upgrade is specified, and the server finds that the data dictionary version of the
# server is different from the version stored in the data dictionary, startup fails with an error
# stating that the data dictionary upgrade is prohibited.
#
# VIEWING DATA DICTIONARY TABLES USING A DEBUG BUILD OF MYSQL
#
# Data dictionary tables are protected by default but can be accessed by compiling MySQL
# with debugging support (using the -DWITH_DEBUG=1 CMake option) and specifying the
# +d, skip_dd_table_access_check debug option and modifier.
#
# For information about compiling debug builds, see SECTION 29.5.1.1, "COMPILING MYSQL FOR DEBUGGING"
#
# WARNING:
#
# 		Modifying or writing to data dictionary tables directly is not recommended and may render
# 		your MySQL instance inoperable.
#
# After compiling MySQL with debugging support, use this SET statement to make data dictionary
# tables visible to the mysql client session:
#
# 		SET SESSION debug='+d,skip_dd_table_access_check';
#
# Use this query to retrieve a list of data dictionary tables:
#
# 		SELECT name, schema_id, hidden, type FROM mysql.tables WHERE schema_id=1 AND hidden='System';
#
# Use SHOW_CREATE_TABLE to view data dictionary table definitions. For example:
#
# 		SHOW CREATE TABLE mysql.catalogs\G
#
# 14.2 REMOVAL OF FILE-BASED METADATA STORAGE
#
# In previous MySQL releases, dictionary data was partially stored in metadata files.
#
# Issues with file-based metadata storage included expensive file scans, suspectibiblity
# to file system-related bugs, complex code for handling of replication and crash recovery
# failure states, and a lack of extensibility that made it difficult to add metadata for new
# features and relational objects.
#
# The metadata files listed below are removed from MySQL. Unless otherwise noted, data previously
# stored in metadata files is now stored in data dictionary tables.
#
# 		) .frm files: Table metadata files. With the removal of .frm files:
#
# 			) The 64KB table definition size limit imposed by the .frm file structure is removed.
#
# 			) The INFORMATION_SCHEMA.TABLES VERSION column reports a hardcoded value of 10, which is the
# 				last .frm file version used in MySQL 5.7
#
# 		) .par files: Partition definition files. InnoDB stopped using partition definition files in MySQL 5.7
#			with the introduction of native partitioning support for InnoDB tables.
#
# 		) .TRN files: Trigger namespace files
#
# 		) .TRG files: Trigger parameter files
#
# 		) .isl files: InnoDB Symbolic Link files containing the location of file-per-table tablespace files
# 			created outside of the data directory.
#
# 		) db.opt files: Database configuration files. These files, one per database directory, contained database
# 			default character set attributes.
#
# 14.3 TRANSACTIONAL STORAGE OF DICTIONARY DATA
#
# The data dictionary schema stores dictionary data in transactional (InnoDB) tables.
#
# Data dictionary tables are located in the mysql database together with non-data dictionary
# system tables.
#
# Data dictionary tables are created in a single InnoDB tablespace named mysql.ibd, which resides
# in the MySQL data directory.
#
# The mysql.ibd tablespace file must reside in the MySQL data directory and its name cannot be
# modified or used by another tablespace.
#
# Dictionary data is protected by the same commit, rollback and crash-recovery capabilities
# that protect user data that is stored in InnoDB tables.
#
# 14.4 DICTIONARY OBJECT CACHE
#
# The dictionary object cache is a shared global cache that stores previously accessed data dictionary
# objects in memory to enable object reuse and minimize disk I/O.
#
# Similar to other cache mechanisms used by MySQL, the dictionary object cache uses an LRU-based
# eviction strategy to evict least recently used objects from memory.
#
# The dictionary object cache comprises cache partitions that store different object types.
#
# Some cache partition size limits are configurable, whereas others are hardcoded.
#
# 		) tablespace definition cache partition: Stores tablespace definition objects.
#
# 			The tablespace_definition_cache option sets a limit for the number of tablespace
# 			definition objects that can be stored in the dictionary object cache.
#
# 			The default value is 256.
#
# 		) schema definition cache partition: Stores schema definition objects.
#
# 			The schema_definition_cache option sets a limit for the number of schema
# 			definition objects that can be stored in the dictionary object cache.
#
# 			THe default value is 256.
#
# 		) table definition cache partition: Stores table definition objects.
#
# 			The object limit is set to the value of max_connections, which has a default
# 			value of 151.
#
# 			The table definition cache partition exists in parallel with the table definition
# 			cache that is configured using the table_definition_cache configuration option.
#
# 			Both caches store table definitions but serve different parts of the MySQL server.
#
# 			Objects in one cache have no dependence on the existence of objects in the other.
#
# 		) stored program definition cache partition: Stores stored program definition objects.
#
# 			The stored_program_definition_cache option sets a limit for the number of stored
# 			program definition objects that can be stored in the dictionary object cache.
#
# 			The default value is 256.
#
# 			The stored program definition cache partition exists in parallel with the stored procedure
# 			and stored function caches that are configured using the stored_program_cache option.
#
# 			The stored_program_cache option sets a soft upper limit for the number of cached stored
# 			procedures or functions per connection, and the limit is checked each time a connection
# 			executes a stored procedure or function.
#
# 			The stored program definition cache partition, on the other hand, is a shared cache that
# 			stores stored program definition objects for other purposes.
#
# 			The existence of objects in the stored program definition cache partition has no dependence
# 			on the existence of objects in the stored procedure cache or stored function cache, and 
# 			vice versa.
#
# 		) character set definition cache partition: Stores character set definition objects and has a hardcoded object
# 			limit of 256.
#
# 		) collation definition cache partition: Stores collation definition objects and has a hardcoded object limit of 256
#
# For information about valid values for dictionary object cache configuration options, refer to SECTION 5.1.8, "SERVER SYSTEM VARIABLES"
#
# 14.5 INFORMATION_SCHEMA AND DATA DICTIONARY INTEGRATION
#
# With the introduction of the data dictionary, the following INFORMATION_SCHEMA tables are implemented as views on
# data dictionary tables:
#
# 		) CHARACTER_SETS
#
# 		) COLLATIONS
#
# 		) COLLATION_CHARACTER_SET_APPLICABILITY
#
# 		) COLUMNS
#
# 		) COLUMN_STATISTICS
#
# 		) EVENTS
#
# 		) FILES
#
# 		) INNODB_COLUMNS
#
# 		) INNODB_DATAFILES
#
# 		) INNODB_FIELDS
#
# 		) INNODB_FOREIGN
#
# 		) INNODB_FOREIGN_COLS
#
# 		) INNODB_INDEXES
#
# 		) INNODB_TABLES
#
# 		) INNODB_TABLESPACES
#
# 		) INNODB_TABLESPACES_BRIEF
#
# 		) INNODB_TABLESTATS
#
# 		) KEY_COLUMN_USAGE
#
# 		) KEYWORDS
#
# 		) PARAMETERS
#
# 		) PARTITIONS
#
# 		) REFERENTIAL_CONSTRAINTS
#
# 		) RESOURCE_GROUPS
#
# 		) ROUTINES
#
# 		) SCHEMATA
#
# 		) STATISTICS
#
# 		) ST_GEOMETRY_COLUMNS
#
# 		) ST_SPATIAL_REFERENCE_SYSTEMS
#
# 		) TABLES
#
# 		) TABLE_CONSTRAINTS
#
# 		) TRIGGERS
#
# 		) VIEWS
#
# 		) VIEW_ROUTINE_USAGE
#
# 		) VIEW_TABLE_USAGE
#
# Queries on those tables are now more efficient because they obtain information from data dictionary
# tables rather than by other, slower means.
#
# In particular, for each INFORMATION_SCHEMA table that is a view on data dictionary tables:
#
# 		) The server no longer must create a temporary table for each query of the INFORMATION_SCHEMA table
#
# 		) When the underlying data dictionary tables store values previously obtained by directory scans (for example,
# 			to enumerate database names or table names within databases) or file-opening operations (for example, to read
# 			information from .frm files), INFORMATION_SCHEMA queries for those values now use table lookups instead.
#
# 			(Additionally, even for a non-view INFORMATION_SCHEMA table, values such as database and table names are retrieved
# 			by lookups from the data dictionary and do not require directory or file scans)
#
# 		) Indexes on the underlying data dictionary tables permit the optimizer to construct efficient query execution
# 			plans, something not true for the previous implementation that processed the INFORMATION_SCHEMA table
# 			using a temporary table per query.
#
# The preceding improvements also apply to SHOW statements that display information corresponding to the INFORMATION_SCHEMA
# tables that are views on data dictionary tables.
#
# For example, SHOW_DATABASES displays the same information as the SCHEMATA table.
#
# In addition to the introduction of views on data dictionary tables, table statistics contained in the STATISTICS and
# TABLES is now cached to improve INFORMATION_SCHEMA query performance.
#
# The information_schema_stats_expiry system variable defines the period of time before cached table statistics expire.
#
# The default is 86400 seconds (24 hours). If there are no cached statistics or statistics have expired, statistics
# are retrieved from storage engine when querying table statistics columns.
#
# To update cached values at any time for a given table, use ANALYZE_TABLE
#
# information_schema_stats_expiry can be set to 0 to have INFORMATION_SCHEMA queries retrieve the latest statistics
# directly from the storage engine, which is not as fast as retrieving cached statistics.
#
# For more information, see SECTION 8.2.3, "OPTIMIZING INFORMATION_SCHEMA QUERIES"
#
# 14.6 SERIALIZED DICTIONARY INFORMATION (SDI)
#
# In addition to storing metadata about database objects in the data dictionary, MySQL stores it in
# serialized form.
#
# This data is referred to as Serialized Dictionary Information (SDI)
#
# InnoDB stores SDI data within its tablespace files. Other storage engines store SDI data in .sdi files
# that are created in the schema directory.
#
# SDI data is generated in a compact JSON format.
#
# Serialized Dictionary Information (SDI) is present in all InnoDB tablespace files except for temporary
# tablespace and undo tablespace files.
#
# SDI records in an InnoDB tablespace file only describe table and tablespace objects contained within
# the tablespace.
#
# SDI data in within an InnoDB tablespace file is only updated by DDL operations on tables within the tablespace.
#
# The presence of SDI data provides metadata redundancy. For example, if the data dictionary becomes unavailable,
# object metadata can be extracted directly from InnoDB tablespace files using the ibd2sdi tool.
#
# For InnoDB, an SDI record requires a single index page, which is 16kb in size by default.
#
# However, SDI data is compressed to reduce the storage footprint.
#
# For partitioned InnoDB tables comprised of multiple tablespaces, SDI data is stored in the tablespace
# file of the first partition.
#
# The MySQL server uses an internal API that is accessed during DDL operations to create and maintain SDI
# records.
#
# The IMPORT_TABLE statement imports MyISAM tables based on information contained in .sdi files.
#
# For more information, see SECTION 13.2.5, "IMPORT TABLE SYNTAX"
#
# 14.7 DATA DICTIONARY USAGE DIFFERENCES
#
# Use of a data dictionary enabled MySQL server entails some operational differences compared to a server
# that does not have a data dictionary:
#
# 		) Previously, enabling the innodb_read_only system variable prevented creating and dropping tables only
# 			for the InnoDB storage.
#
# 			As of MySQL 8.0, enabling innodb_read_only prevents these operations for all storage engines.
#
# 			Table creation and drop operations for any storage engine modify data dictionary tables in the mysql
# 			system database, but those tables use the InnoDB storage engine and cannot be modified when innodb_read_only
# 			is enabled.
#
# 			The same principle applies to other table operations that require modifying data dictionary tables.
#
# 			Examples:
#
# 				) ANALYZE TABLE fails because it updates table statistics, which are stored in the data dictionary
#
# 				) ALTER_TABLE_tbl_name_ENGINE=engine_name fails because it updates the storage engine designation,
# 					which is stored in the data dictionary.
#
# NOTE:
#
# 		Enabling innodb_read_only also has important implications for non-data dictionary tables in the mysql
# 		system database.
#
# 		For details, see the description of innodb_read_only in SECTION 15.13, "INNODB STARTUP OPTIONS AND SYSTEM VARIABLES"
#
# 		) Previously, tables in the mysql system database were visible to DML and DDL statements.
#
# 			As of MySQL 8.0, data dictionary tables are invisible and cannot be modified or queried directly.
#
# 			However, in most cases there are corresponding INFORMATION_SCHEMA tables that can be queried
# 			instead.
#
# 			This enables the underlying data dictionary tables to be changed as server development proceeds,
# 			while maintaining a stable INFORMATION_SCHEMA interface for application use.
#
# 		) INFORMATION_SCHEMA tables in MySQL 8.0 are closely tried to the data dictionary, resulting in several
# 			usage differences:
#
# 			) Previously, INFORMATION_SCHEMA queries for table statistics in the STATISTICS and TABLES tables
# 				retrieved statistics directly from storage engines.
#
# 				As of MySQL 8.0, cached table statistics are used by default.
#
# 				The information_schema_stats_expiry system variable defines the period of time before cached
# 				table statistics expire.
#
# 				The default is 86400 (24 hours). (To update the cached values at any time for a given table,
# 				use ANALYZE_TABLE)
#
# 				If there are no cached statistics or statistics have expired, statistics are retrieved from storage
# 				engines when querying table statistics columns.
#
# 				To always retrieve the latest statistics directly from storage engines, set information_schema_stats_expiry
# 				to 0.
#
# 				For more information, see SECTION 8.2.3, "OPTIMIZING INFORMATION_SCHEMA QUERIES"
#
# 			) Several INFORMATION_SCHEMA tables are views on data dictionary tables, which enables the optimizer
# 				to use indexes on those underlying tables.
#
# 				Consequently, depending on optimizer choices, the row order of results for INFORMATION_SCHEMA
# 				queries might differ from previous results.
#
# 				If a query result must have specific row ordering characteristics, include an ORDER BY clause.
#
# 			) mysqldump and mysqlpump no longer dump the INFORMATION_SCHEMA database, even if explicitly
# 				named on the command line.
#
# 			) CREATE_TABLE_dst_tbl_LIKE_src_tbl requires that src_tbl be a base table and fails if it is an
# 				INFORMATION_SCHEMA table that is a view on data dictionary tables.
#
# 			) Previously, result set headers of columns selected from INFORMATION_SCHEMA tables used the
# 				capitalization specified in the query.
#
# 				This query produces a result set with a header of table_name:
#
# 					SELECT table_name FROM INFORMATION_SCHEMA.TABLES;
#
# 				As of MySQL 8.0, these headers are capitalized; the preceding query produces a result set
# 				with a header of TABLE_NAME.
#
# 				If necessary, a column alias can be used to achieve a different lettercase.
#
# 				For example:
#
# 					SELECT table_name AS 'table_name' FROM INFORMATION_SCHEMA.TABLES;
#
# 		) The data directory affects how mysqldump and mysqlpump dump information from the mysql system database:
#
# 			) Previously, it was possible to dump all tables in the mysql system database.
#
# 				As of MySQL 8.0, mysqldump and mysqlpump dump only non-data dictionary tables in that database.
#
# 			) Previously, the --routines and --events options were not required to include stored routines and events
# 				when using the --all-databases option:
#
# 					The dump included the mysql system database, and therefore also the proc and event tables containing
# 					stored routine and event definitions.
#
# 					As of MySQL 8.0, the event and proc tables are not used.
#
# 					Definitions for the corresponding objects are stored in data dictionary tables, but those 
# 					tables are not dumped.
#
# 					To include stored routines and events in a dump made using --all-databases, use the --routines
# 					and --events options explicitly.
#
# 			) Previously, the --routines option required the SELECT privilege for the proc table.
#
# 				AS of MySQL 8.0, that table is not used;
#
# 				--routines requires the global SELECT privilege instead.
#
# 			) Previously, it was possible to dump stored routine and event definitions together with their
# 				creation and modification timestamps, by dumping the proc and event tables.
#
# 				As of MySQL 8.0, those tables are not used, so it is not possible to dump timestamps.
#
# 		) Previously, creating a stored routine that contains illegal characters produced a warning.
#
# 			As of MySQL 8.0, this is an error.
#
# 14.8 DATA DICTIONARY LIMITATIONS
#
# This section describes temporary limitations introduced with the MySQL data dictionary.
#
# 		) Manual creation of database directories under the data directory (for example, with mkdir)
# 			is unsupported.
#
# 			Manually created database directories are not recognized by the MySQL server.
#
# 		) DDL operations take longer due to writing to storage, undo logs, and redo logs instead of .frm files
#
# CHAPTER 15 THE INNODB STORAGE ENGINE
#
# TABLE OF CONTENTS
#
# 15.1 INTRODUCTION TO INNODB
# 15.2 INNODB AND THE ACID MODEL
#
# 15.3 INNODB MULTI-VERSIONING
# 15.4 INNODB ARCHITECHTURE
#
# 15.5 INNODB IN-MEMORY STRUCTURES
# 15.6 INNODB ON-DISK STRUCTURES
#
# 15.7 INNODB LOCKING AND TRANSACTION MODEL
# 15.8 INNODB CONFIGURATION
#
# 15.9 INNODB TABLE AND PAGE COMPRESSION
# 15.10 INNODB ROW FORMATS
#
# 15.11 INNODB DISK I/O AND FILE SPACE MANAGEMENT
# 15.12 INNODB AND ONLINE DDL
#
# 15.13 INNODB STARTUP OPTIONS AND SYSTEM VARIABLES
# 15.14 INNODB INFORMATION_SCHEMA TABLES
#
# 15.15 INNODB INTEGRATION WITH MYSQL PERFORMANCE SCHEMA
# 15.16 INNODB MONITORS
#
# 15.17 INNODB BACKUP AND RECOVERY
# 15.18 INNODB AND MYSQL REPLICATION
#
# 15.19 INNODB MEMCACHED PLUGIN
# 15.20 INNODB TROUBLESHOOTING
#
# 15.1 INTRODUCTION TO INNODB
#
# 15.1.1 BENEFITS OF USING INNODB TABLES
# 15.1.2 BEST PRACTICES FOR INNODB TABLES
#
# 15.1.3 VERIFYING THAT INNODB IS THE DEFAULT STORAGE ENGINE
# 15.1.4 TESTING AND BENCHMARKING WITH INNODB
#
# InnoDB is a general-purpose storage engine that balances high reliability and high performance.
#
# In MySQL 8.0, InnoDB is the default MySQL storage engine. Unless you have configured a different
# default storage engine, issuing a CREATE_TABLE statement without an ENGINE= clause creates
# an InnoDB table.
#
# KEY ADVANTAGES OF INNODB
#
# 		) Its DML operations follow the ACID model, with transactions featuring commit, rollback, and crash-recovery
# 			capabilities to protect user data.
#
# 			See SECTION 15.2, "INNODB AND THE ACID MODEL" for more information
#
# 		) Row-level locking and Oracle-style consistent reads increase multi-user concurrency
# 			and performance.
#
# 			See SECTION 15.7, "InnoDB LOCKING AND TRANSACTION MODEL" for more information.
#
# 		) InnoDB tables arrange your data on disk to optimize queries based on primary keys.
#
# 			Each InnoDB table has a primary key index called the clustered index that organizes
# 			the data to minimize I/O for primary key lookups.
#
# 			See SECTION 15.6.2.1, "CLUSTERED AND SECONDARY INDEXES" for more information
#
# 		) To maintain data integrity, InnoDB supports FOREIGN_KEY constraints.
#
# 			With foreign keys, inserts, updates, and deletes are checked to ensure they do
# 			not result in inconsistencies across different tables.
#
# 			See SECTION 15.6.1.5, "InnoDB AND FOREIGN KEY CONSTRAINTS" for more information
#
# TABLE 15.1 INNODB STORAGE ENGINE FEATURES
#
# 		FEATURE 																											Support
#
# B-tree indexes 											Yes
# 
# Backup/point-in-time recovery (Implemented in the server, rather than in the storage engine) 	Yes
#
# Cluster database support 							No
#
# Clustered indexes 										Yes
#
# Compressed data 										Yes
#
# Data caches 												Yes
#
# Encrypted data 											Yes (Implemented in the server via encryption functions;
# 																In MySQL 5.7 and later, data-at-rest tablespace encryption is supported)
#
# Foreign key support 									Yes
#
# Full-text search indexes 							Yes (InnoDB support for FULLTEXT indexes is available in MySQL 5.6 and later)
#
# Geospatial data type support 						Yes
#
# Geospatial indexing support 						Yes (InnoDB support for geospatial indexing is available in MySQL 5.7 and later)
#
# Hash indexes 											No (InnoDB utilizes hash indexes internally for its Adaptive Hash Index feature)
#
# Index caches 											Yes
#
# Locking granularity 									Row
#
# MVCC 														Yes
#
# Replication support (Implemented in the server, rather than in the storage engine) 		Yes
#
# Storage limits 											64TB
#
# T-tree indexes 											No
#
# Transactions 											Yes
#
# Update statistics for data dictionary 			Yes
#
# To compare the features of InnoDB with other storage engines provided with MySQL, see the
# Storage Engine Features table in CHAPTER 16, ALTERNATIVE STORAGE ENGINES
#
# INNODB ENHANCEMENTS AND NEW FEATURES
#
# For information about InnoDB enhancements and new features, refer to:
#
# 		) The InnoDB enhancements list in SECTION 1.4, "WHAT IS NEW IN MYSQL 8.0"
#
# 		) The RELEASE NOTES
#
# ADDITIONAL INNODB INFORMATION AND RESOURCES
#
# 		) For InnoDB related terms and definitions, see the MySQL Glossary
#
# 		) For a forum dedicated to the InnoDB storage engine, see MySQL Forums::InnoDB
#
# 		) InnoDB is published under the same GNU GPL License Version 2 (of June 1991) as MySQL.
#
# 			For more information on MySQL licensin, see <links>
#
# 15.1.1 BENEFITS OF USING INNODB TABLES
#
# You may find InnoDB tables beneficial for the following reasons:
#
# 		) If your server crashes because of a hardware or software issue, regardless of what was happening
# 			in the database at the time, you do not need to do anything special after restarting the database.
#
# 			InnoDB crash recovery automatically finalizes any changes that were committed before the time of the
# 			crash, and undoes any changes that were in process but not committed.
#
# 			Just restart and continue where you left off.
#
# 		) The InnoDB storage engine maintains its own buffer pool that caches table and index data in main memory
# 			as data is accessed.
#
# 			Frequently used data is processed directly from memory.
#
# 			This cache applies to many types of information and speeds up processing.
#
# 			On dedicated database servers, up to 80% of physical memory is often assigned
# 			to the buffer pool.
#
# 		) If you split up related data into different tables, you can set up foreign keys that enforce
# 			referential integrity.
#
# 			Update or delete data, and the related data in other tables is updated or deleted automatically.
#
# 			Try to insert data into a secondary table without corresponding data in the primary table, and
# 			the bad data gets kicked out automatically.
#
# 		) If data becomes corrupted on disk or in memory, a checksum mechanism alerts you to the bogus data
# 			before you use it.
#
# 		) When you design your database with appropriate primary key columns for each table, operations involving
# 			those columns are automatically optimized.
#
# 			It is very fast to reference the primary key columns in WHERE clauses, ORDER_BY clauses, GROUP_BY clauses,
# 			and join operations.
#
# 		) Inserts, updates, and deletes are optimized by an automatic mechanism called change buffering.
#
# 			InnoDB not only allows concurrent read and write access to the same table, it caches changed
# 			data to streamline disk I/O
#
# 		) Performance benefits are not limited to giant tables with long-running queries.
#
# 			When the same rows are accessed over and over from a table, a feature called the Adaptive Hash Index
# 			takes over to make these lookups even faster, as if they came out of a hash table.
#
# 		) You can compress tables and associated indexes.
#
# 		) You can create and drop indexes with much less impact on performance and availability
#
# 		) Truncating a file-per-table tablespace is very fast, and can free up disk space for the operating
# 			system to reuse, rather than freeing up space within the system tablespace that only InnoDB can reuse.
#
# 		) The storage layout for table data is more efficient for BLOB and long text fields, with the DYNAMIC row format.
#
# 		) You can monitor the internal workings of the storage engine by querying INFORMATION_SCHEMA tables
#
# 		) You can monitor the performance details of the storage engine by querying Performance Schema tables.
#
# 		) You can freely mix InnoDB tables with tables from other MySQL storage engines, even within the same statement.
#
# 			For example, you can use a join operation to combine data from InnoDB and MEMORY tables in a single query.
#
# 		) InnoDB has been designed for CPU efficiency and maximum performance when processing large data volumes
#
# 		) InnoDB tables can handle large quantities of data, even on operating systems where file size is limited to 2GB
#
# For InnoDB-specific tuning techniques you can apply in your application code, see SECTION 8.5, "OPTIMIZING FOR INNODB TABLES"
#
# 15.1.2 BEST PRACTICES FOR INNODB TABLES
#
# This section describes best practices when using InnoDB tables.
#
# 		) Specifying a primary key for every table using the most frequently queried column or columns,
# 			or an auto-increment value if there is no obvious primary key
#
# 		) Using joins wherever data is pulled from multiple tables based on identical ID values from those tables.
#
# 			For fast join performance, define foreign keys on the join columns, and declare those columns with the same
# 			data type in each table.
#
# 			Adding foreign keys ensures that referenced columns are indexed, which can improve performance.
#
# 			Foreign key also propagate deletes or updates to all affected tables, and prevent insertion of data
# 			in a child table if the corresponding IDs are not present in the parent table.
#
# 		) Turning off autocommit. Committing hundreds of times a second puts a cap on performance (limited by the write speed
# 			of your storage device)
#
# 		) Grouping sets of related DML operations into transactions, by bracketing them with START TRANSACTION and COMMIT
# 			statements.
#
# 			While you don't want to commit too often, you also don't want to issue huge batches of INSERT, UPDATE or DELETE
# 			statements that run for hours without committing.
#
# 		) Not using LOCK_TABLES statements.
#
# 			InnoDB can handle multiple sessions all reading and writing to the same table at once,
# 			without sacrificing reliability or high performance.
#
# 			To get exclusive write access to a set of rows, use the SELECT_---_FOR_UPDATE syntax to
# 			lock just the rows you intend to update.
#
# 		) Enabling the innodb_file_per_table option or using general tablespaces to put the data and indexes
# 			for tables into separate files, instead of the system tablespace.
#
# 			The innodb_file_per_table option is enabled by default.
#
# 		) Evaluating whether your data and access patterns benefit from the InnoDB table or page compression
# 			features.
#
# 			You can compress InnoDB tables without sacrificing read/write capability.
#
# 		) Running your server with the option --sql_mode=NO_ENGINE_SUBSTITUTION to prevent tables being created
# 			with a different storage engine if there is an issue with the engine specified in the ENGINE=
# 			clause of CREATE_TABLE
#
# 15.1.3 VERIFYING THAT INNODB IS THE DEFAULT STORAGE ENGINE
#
# Issue the SHOW_ENGINES statement to view the available MySQL storage engines.
#
# Look for DEFAULT in the InnoDB line.
#
# 		SHOW ENGINES;
#
# Alternatively, query the INFORMATION_SCHEMA.ENGINES table.
#
# 		SELECT * FROM INFORMATION_SCHEMA.ENGINES;
#
# 15.1.4 TESTING AND BENCHMARKING WITH INNODB
#
# If InnoDB is not your default storage engine, you can determine if your database server or applications
# work correctly with InnoDB by restarting the server with --default-storage-engine=InnoDB defined on the
# command line or with default-storage-engine=innodb defined in the [mysqld] section of your MySQL server
# option file.
#
# Since changing the default storage engine only affects new tables as they are created, run all your
# application installation and setup steps to confirm that everything installs properly.
#
# Then exercise all the application features to make sure all the data loading, editing, and querying
# features work.
#
# If a table relies on a feature that is specific to another storage engine, you will receive an error;
# add the ENGINE=other_engine_name clause to the CREATE_TABLE statement to avoid the error.
#
# If you did not make a deliberate decision about the storage engine, and you want to preview how
# certain tables work when created using InnoDB, issue the command ALTER_TABLE_table_name_ENGINE=InnoDB;
# for each table.
#
# Or, to run test queries and other statements without disturbing the original table, make a copy:
#
# 		CREATE TABLE InnoDB_Table (---) ENGINE=InnoDB AS SELECT * FROM other_engine_table;
#
# To assess performance with a full application under a realistic workload, install the latest
# MySQL server and run benchmarks.
#
# Test the full application lifecycle, from installation, through heavy usage, and server restart.
#
# Kill the server process while the database is busy to simulate a power failure, and verify that
# the data is recovered successfully when you restart the server.
#
# Test any replication configurations, especially if you use different MySQL versions and options
# on the master and slaves.
#
# 15.2 InnoDB AND THE ACID MODEL
#
# The ACID model is a set of database design principles that emphasize aspects of reliability that are
# important for business data and mission-critical applications.
#
# MySQL includes components such as the InnoDB storage engine that adhere closely to the ACID model,
# so that data is not corrupted and results are not distorted by exceptional conditions such as
# software crashes and hardware malfunctions.
#
# When you rely on ACID-compliant features, you do not need to reinvent the wheel of consistency
# checking and crash recovery mechanisms.
#
# In cases where you have additional software safeguards, ultra-reliable hardware, or an application
# that can tolerate a small amount of data loss or inconsistency, you can adjust MySQL settings to trade
# some of the ACID reliability for greater performance or throughput.
#
# The following sections discuss how MySQL features, in particular the InnoDB storage engine, interact
# with the categories of the ACID model:
#
# 		) A: atomicity
#
# 		) C: Consistency
#
# 		) I: Isolation
#
# 		) D: durability
#
# ATOMICITY
#
# The atomicity aspect of the ACID model mainly involves InnoDB transactions.
#
# Related MySQL features include:
#
# 		) Autocommit setting
#
# 		) COMMIT statement
#
# 		) ROLLBACK statement
#
# 		) Operational data from the INFORMATION_SCHEMA tables
#
# CONSISTENCY
#
# The consistency aspect of the ACID model mainly involves internal InnoDB processing to
# protect data from crashes.
#
# Related MySQL features include:
#
# 		) InnoDB doublewrite buffer
#
# 		) InnoDB crash recovery
#
# ISOLATION
#
# The isolation aspect of the ACID model mainly involves InnoDB transactions, in particular the isolation level 
# that applies to each transaction.
#
# Related MySQL features include:
#
# 		) Autocommit setting
#
# 		) SET ISOLATION LEVEL statement
#
# 		) The low-level details of InnoDB locking. During performance tuning, you see these details through
# 			INFORMATION_SCHEMA tables.
#
# DURABILITY
#
# The durability aspect of the ACID model involves MySQL software features interacting with your
# particular hardware configuration.
#
# Because of the many possibilities depending on the capabilities of your CPU, network, and storage
# devices, this aspect is the most complicated to provide concrete guidelines for.
#
# (And those guidelines might take the form of buy "new hardware")
#
# Related MySQL features include:
#
# 		) InnoDB doublewrite buffer, turned on and off by the innodb_doublewrite configuration option
#
# 		) Configuration option innodb_flush_log_at_trx_commit
#
# 		) Configuration option sync_binlog
#
# 		) Configuration option innodb_file_per_table
#
# 		) Write buffer in a storage device, such as a disk drive, SSD, or RAID array
#
# 		) Battery-backed cache in a storage device
#
# 		) The operating system used to MySQL, in particular its support for the fsync() system call
#
# 		) Uninterruptible power supply (UPS) protecting the electrical power to all computer servers and storage
# 			devices that run MySQL servers and store MySQL data.
#
# 		) Your backup strategy, such as frequency and types of backups, and backup retention periods.
#
# 		) For distributed or hosted data applications, the particular characteristics of the data centers
# 			where the hardware for the MySQL servers is located, and network connections between the data
# 			centers.
#
# 15.3 INNODB MULTI-VERSIONING
#
# InnoDB is a multi-versioned storage engine: it keeps information about old versions of changed rows,
# to support transactional features such as concurrency and rollback.
#
# This information is stored in the tablespace in a data structure called a rollback segment (after an
# analogous data structure in Oracle)
#
# InnoDB uses the information in the rollback segment to perform the undo operations needed in a transaction
# rollback.
#
# It also uses the information to build earlier versions of a row for a consistent read.
#
# Internally, InnoDB adds three fields to each row stored in the database. A 6-byte DB_TRX_ID
# field indicates the transaction identifier for the last transaction that inserted or updated
# the row.
#
# Also, a deletion is treated internally as an update where a special bit in the row is set
# to mark it as deleted.
#
# Each row also contains a 7-byte DB_ROLL_PTR field called the roll pointer.
#
# The roll pointer points to an undo log record written to the rollback segment.
#
# If the row was updated, the undo log record contains the information necessary
# to rebuild the content of the row before it was updated.
#
# A 6-byte DB_ROW_ID field contains a row ID that increases monotonically as new rows are
# inserted.
#
# If InnoDB generates a clustered index automatically, the index contains row ID values.
#
# Otherwise, the DB_ROW_ID column does not appear in any index.
#
# Undo logs in the rollback segment are divided into insert and update undo logs.
# Insert undo logs are needed only in transaction rollback and can be discarded as
# soon as the transaction commits.
#
# Update undo logs are used also in consistent reads, but they can be discarded
# only after there is no transaction present for which InnoDB has assigned a snapshot
# that in a consistent read could need the information in the update undo log to build
# an earlier version of a database row.
#
# Commit your transactions regularly, including those transactions that issue only consistent
# reads.
#
# Otherwise, InnoDB cannot discard data from the update undo logs, and the rollback segment
# may grow too big, filling up your tablespace.
#
# The physical size of an undo log record in the rollback segment is typically smaller than the
# corresponding inserted or updated row.
#
# You can use this information to calculate the space needed for your rollback segment.
#
# In the InnoDB multi-versioning scheme, a row is not physically removed from the database
# immediately when you delete it with an SQL statement.
#
# InnoDB only physically removes the corresponding row and its index records when it discards
# the update undo log record written for the deletion.
#
# This removal operation is called a purge, and it is quite fast, usually taking the same order
# of time as the SQL statement that did the deletion.
#
# If you insert and delete rows in smallish batches at about the same rate in the table, the purge
# thread can start to lag behind and the table can grow bigger and bigger because of all the
# "dead" rows, making everything disk-bound and very slow.
#
# In such a case, throttle new row operations, and allocate more resources to the purge thread
# by tuning the innodb_max_purge_lag system variable.
#
# See SECTION 15.13, "InnoDB STARTUP OPTIONS AND SYSTEM VARIABLES" for more information
#
# MULTI-VERSIONING AND SECONDARY INDEXES
#
# InnoDB multiversion concurrency control (MVCC) treats secondary indexes differently than
# clustered indexes.
#
# Records in a clustered index are updated-in-place, and their hidden system columns point
# undo log entries from which earlier versions of records can be reconstructed.
#
# Unlike clustered index records, secondary index records do not contain hidden system
# columns nor are they updated in-place.
#
# When a secondary index column is updated, old secondary index records are delete-marked,
# new records are inserted, and delete-marked records are eventually purged.
#
# When a secondary index record is delete-marked or the secondary index page is updated
# by a newer transaction, InnoDB looks up the database record in the clustered index.
#
# In the clustered index, the record's DB_TRX_ID is checked, and the correct version of
# the record is retrieved from the undo log if the record was modified after the reading 
# transaction was initiated.
#
# If a secondary index record is marked for deletion or the secondary index page is updated
# by a newer transaction, the covering index technique is not used.
#
# Instead of returning values from the index structure, InnoDB looks up the record in the
# clustered index.
#
# However, if the index condition pushdown (ICP) optimization is enabled, and parts of the
# WHERE condition can be evaluated using only fields from the index, the MySQL server still
# pushes this part of the WHERE condition down to the storage engine where it is evaluated
# using the index.
#
# If no matching records are found, the clustered index lookup is avoided.
#
# If matching records are found, even among delete-marked records, InnoDB looks
# up the record in the clustered index.
#
# 15.4 INNODB ARCITECHTURE
#
# The following refers to in-memory and on-disk structures that comprise the InnoDB
# storage engine architechture.
#
# For information about each structure, see SECTION 15.5, "InnoDB IN-MEMORY STRUCTURES"
# and SECTION 15.6, "INNODB ON-DISK STRUCTURES"
#
# FIGURE 15.1 INNODB ARCHITECHTURE
#
# 		In-Memory Structures 												On-Disk Structures 				File-per-table tablespaces
# 		
# 			Buffer Pool -> O_DIRECT | OS CACHE | -> O_DIRECT  |	System Tablespace	| 				innodb_file_per_table=ON
# 			 																		(ibdata1)  								t1.ibd t2.ibd
# 	Adaptive Hash INdex  													
# 																					Doublewrite Buffer 
#
# 																					Change Buffer
#											| OS CACHE |
# 																																General Tablespaces
#												VVVVVV 
# 																															ts1.ibd -> t3 t4 t5
# 																															ts2.ibd -> t6 t7 t8
#
# 			Change Buffer 														Undo Tablespaces
#											| OS CACHE | 
# 																					undo_001 undo_003.ibu
# 												VVVVV
# 																					undo_002 (system) 
# 											| OS CACHE | 						undo_004.ibu (user-defined) 				Temporary Tablespaces

# 		|	Log Buffer 		| 			| OS CACHE |							| Redo Log | 										ibtmp1 (global)
#
# 																				V	| ib_logfile0 | <									temp_1.ibt, temp_2.ibt, temp_3.ibt (Session)
# 																				>  | ib_logfile1 | ^
# 			
#
# 
# 15.5 INNODB IN-MEMORY STRUCTURES
#
# 15.5.1 BUFFER POOL
# 15.5.2 CHANGE BUFFER
# 15.5.3 ADAPTIVE HASH INDEX
# 15.5.4 LOG BUFFER
#
# This section describes InnoDB in-memory structures and related topics.
#
# 15.5.1 BUFFER POOL
#
# The buffer pool is an area in main memory where caches table and index data as it is
# accessed.
#
# The buffer pool permits frequently used data to be processed directly from memory,
# which speeds up processing.
#
# On dedicated servers, up to 80% of physical memory is often assigned to the buffer pool.
#
# For efficiency of high-volume read operations, the buffer pool is divided into pages that
# can potentially hold multiple rows.
#
# For efficiency of cache management, the buffer pool is implemented as a linked list of
# pages; data that is rarely used is aged out of the cache using a variation of the LRU
# algorithm.
#
# Knowing how to take advantage of the buffer pool to keep frequently accessed data in
# memory is an important aspect of MySQL tuning.
#
# BUFFER POOL LRU ALGORITHM
#
# The buffer pool is managed as a list using a variation of the least recently used (LRU)
# algorithm.
#
# When room is needed to add a new page to the buffer pool, the least recently used
# page is evicted and a new page is added to the middle of the list.
#
# This midpoint insertion strategy treats the list as two sublists:
#
# 		) At the head, a sublist of new ("young") pages that were accessed recently
#
# 		) At the tail, a sublist of old pages that were accessed less recently
#
# FIGURE 15.2 BUFFER POOL LIST
#							+------+ ^
# 							| HEAD | | Accessed pages are made young
# 							+------+ |
# 							|  5/8 | 	| Unused pages become old
# 			New Sublist	|buffer|    |
# 							| pool |		v
# 							|      |
# 							+------+
# Midpoint insertion | Tail |
# 			------------+------+
# of pages read into	| Head |
# the buffer pool 	+------+
# 							| 3/8  |
# 			Old Sublist	|buffer|
# 							| pool |
# 							+------+
# 							| Tail |
# 							+------+
# 							  V V V Pages that remain unused are eventually evicted
#
# The algorithm keeps pages that are heavily used by queries in the new sublist.
#
# The old sublist contains less-used pages; these pages are candidates for eviction.
#
# By default, the algorithm operates as follows:
#
# 		) 3/8 of the buffer pool is devoted to the old sublist
#
# 		) The midpoint of the list is the boundary where the tail of the new sublist meets the head of the old sublist
#
# 		) When InnoDB reads a page into the buffer pool, it initially inserts it at the midpoint
# 			(the head of the old sublist)
#
# 			A page can be read because it is required for a user-specified operation such as an SQL
# 			query, or as part of a read-ahead operation performed automatically by InnoDB
#
# 		) Accessing a page in the old sublist makes it "young", moving it to the head of the buffer pool
# 			(the head of the new sublist)
#
# 			If the page was read because it was required, the first access occurs immediately and the
# 			page is made young.
#
# 			If the page was read due to read-ahead, the first access does not occur immediately (and might
# 			not occur at all before the page is evicted)
#
# 		) As the database operates, pages in the buffer pool that are not accessed "age" by moving toward the
# 			tail of the list.
#
# 			Pages in both the new and old sublists age as other pages are made new.
#
# 			Pages in the old sublist also age as pages are inserted at the midpoint.
#
# 			Eventually, a page that remains unused reaches the tail of the old sublist
# 			and is evicted.
#
# By default, pages read by queries immediately move into the new sublist, meaning they stay
# in the buffer pool longer.
#
# A table scan (such as performed for a mysqldump operation, or a SELECT statement with no WHERE
# clause) can bring a large amount of data into the buffer pool and evict an equivalent amount of older
# data, even if the new data is never used again.
#
# Similarly, pages that are loaded by the read-ahead background thread and then accessed only once move
# to the head of the new list.
#
# These situations can push frequently used pages to the old sublist where they become subject to eviction.
#
# For information about optimizing this behavior, see SECTION 15.8.3.3, "MAKING THE BUFFER POOL SCAN RESISTANT"
# and SECTION 15.8.3.4, "CONFIGURING INNODB BUFFER POOL PREFETCHING (READ-AHEAD)"
#
# InnoDB Standard Monitor output contains several fields in the BUFFER POOL AND MEMORY section regarding
# operation of the buffer pool LRU algorithm.
#
# For details, see MONITORING THE BUFFER POOL USING THE INNODB STANDARD MONITOR
#
# BUFFER POOL CONFIGURATION
#
# You can configure the various aspects of the buffer pool to improve performance.
#
# 		) Ideally, you set the size of the buffer pool to as large a value as practical, leaving enough
# 			memory for other processes on the server to run without excessive paging.
#
# 			The larger the buffer pool, the more InnoDB acts like an in-memory database, reading data from
# 			disk once and then accessing the data from memory during subsequent reads.
#
# 			See SECTION 15.8.3.1, "CONFIGURING INNODB BUFFER POOL SIZE"
#
# 		) On 64-bit systems with sufficient memory, you can split the buffer pool into multiple parts
# 			to minimize contention for memory structures among concurrent operations.
#
# 			For details, see SECTION 15.8.3.2, "CONFIGURING MULTIPLE BUFFER POOL INSTANCES"
#
# 		) You can keep frequently accessed data in memory regardless of sudden spikes of activity
# 			from operations that would bring large amounts of infrequently accessed data into the
# 			buffer pool.
#
# 			For details, see SECTION 15.8.3.3, "MAKING THE BUFFER POOL SCAN RESISTANT"
#
# 		) You can control when and how to perform read-ahead requests to prefetch pages into the buffer
# 			pool asynchronously in anticipation that the pages will be needed soon.
#
# 			For details, see SECTION 15.8.3.4, "CONFIGURING INNODB BUFFER POOL PREFETCHING (READ-AHEAD)"
#
# 		) You can control when background flushing occurs and whether or not the rate of flushing is dynamically
# 			adjusted based on workload.
#
# 			For details, see SECTION 15.8.3.5, "CONFIGURING INNODB BUFFER POOL FLUSHING"
#
# 		) You can fine-tune aspects of buffer pool flushing behavior to improve performance.
#
# 			For details, see SECTION 15.8.3.6, "FINE-TUNING INNODB BUFFER POOL FLUSHING"
#
# 		) You can configure how InnoDB preserves the current buffer pool state to avoid a lengthy
# 			warmup period after a server restart.
#
# 			For details, see SECTION 15.8.3.7, "SAVING AND RESTORING THE BUFFER POOL STATE"
#
# MONITORING THE BUFFER POOL USING THE INNODB STANDARD MONITOR
#
# InnoDB Standard Monitor output, which can be accessed using SHOW_ENGINE_INNODB_STATUS, provides
# metrics regarding operation of the buffer pool.
#
# Buffer pool metrics are located in the BUFFER POOL AND MEMORY section of InnoDB Standard Monitor
# output and appear similar to the following:
#
# 		--------------------------
# 		BUFFER POOL AND MEMORY
# 		--------------------------
# 		Total large memory allocated 2198863872
# 		Dictionary memory allocated 776332
# 		Buffer pool size 				 131072
# 		Free buffers 	  				 124908
# 		Database pages   				 5720
# 		Old database pages 			 2071
# 		Modified db pages 			 910
# 		Pending reads 					 0
# 		Pending writes: LRU 0, flush list 0, single page 0
# 		Pages made young 4, not young 0
# 		0.10 youngs/s, 0.00 non-youngs/s
# 		Pages read 197, 190.89 creates/s, 244.94 writes/s
# 		0.00 reads/s, 190.89 creates/s, 244.94 writes/s
# 		Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not
# 		0 / 1000
# 		Pages read ahead 0.00/s, evicted without access 0.00/s Random read
# 		ahead 0.00/s
# 		LRU len: 5720, unzip_LRU len: 0
# 		I/O sum[0]:cur[0], unzip sum[0]:cur[0]
#
# The following table describes buffer pool metrics reported by the InnoDB Standard Monitor.
#
# NOTE:
#
# 		Per second averages provided in InnoDB Standard Monitor output are based on the elapsed
# 		time since InnoDB Standard Monitor output was last printed.
#
# TABLE 15.2 INNODB BUFFER POOL METRICS
#
# 		NAME 													DESCRIPTION
#
# Total memory allocated 			The total memory allocated for the buffer pool in bytes.
#
# Dictionary memory allocated 	The total memory allocated for the InnoDB data dictionary in bytes.
#
# Buffer pool size 					The total size in pages allocated to the buffer pool
#
# Free buffers 						The total size in pages of the buffer pool free list
#
# Database pages 						The total size in pages of the buffer pool LRU list
#
# Old database pages 				The total size in pages of the buffer pool old LRU sublist
#
# Modified db pages 					The current number of pages modified in the buffer pool
#
# Pending reads 						The number of buffer pool pages waiting to be read into the buffer pool
#
# Pending writes LRU 				The number of old dirty pages within the buffer pool to be written from the bottom of the LRU list
#
# Pending writes flush list 		The number of buffer pool pages to be flushed during checkpointing
#
# Pending writes single page 		The number of pending independent page writes within the buffer pool
#
# Pages made young 					The total number of pages made young in the buffer pool LRU list (moved to the head of sublist of "new" pages)
#
# Pages made not young 				The total number of pages not made young in the buffer pool LRU list (pages that have remained in the "old" sublist
# 											without being made young)
#
# youngs/s 								The per second average of accesses to old pages in the buffer pool LRU list that have resulted in making
# 											pages young. See the notes that follow this table for more information.
#
# non-youngs/s 						The per second average of accesses to old pages in the buffer pool LRU list that have resulted in not making
# 											pages young. See the notes that follow this table for more information.
#
# Pages read 							The total number of pages read from the buffer pool
#
# Pages created 						The total number of pages created within the buffer pool
#
# Pages written 						The total number of pages written from the buffer pool
#
# reads/s 								The per second average number of buffer pool page reads per second
#
# creates/s 							The per second average number of buffer pool pages created per second
#
# writes/s 								The per second average number of buffer pool page writes per second
#
# Buffer pool hit rate 				The buffer pool page hit rate for pages read from the buffer pool memory vs from disk storage
#
# young-making rate 					The average hit rate at which page accesses have resulted in making pages young.
#
# 											See the notes that follow this table for more information
#
# not (young-making rate) 			The average hit rate at which page accesses have not resulted in making pages young.
#
# 											See the notes that follow this table for more information
#
# Pages read ahead 					The per second average of read ahead operations
#
# Pages evicted without access 	The per second average of the pages evicted without being accessed from the buffer pool
#
# Random read ahead 					The per second average of random read ahead operations
#
# LRU len 								The total size in pages of the buffer pool LRU list
#
# unzip_LRU len 						The total size in pages of the buffer pool unzip_LRU list
#
# I/O sum 								The total number of buffer pool LRU list pages accessed, for the last 50 seconds
#
# I/O cur 								The total number of buffer pool LRU list pages accessed
#
# I/O unzip sum 						The total number of buffer pool unzip_LRU list pages accessed
#
# I/O unzip cur 						The total number of buffer pool unzip_LRU list pages accessed
#
# NOTES:
#
# 		) The young/s metric is applicable only to old pages.
#
# 			It is based on the number of accesses to pages and not the number of pages.
#
# 			There can be multiple accesses to a given page, all of which are counted.
#
# 			If you see very low youngs/s values when there are no large scans occurring,
# 			you might need to reduce the delay time or increase the percentage of the buffer
# 			pool used for the old sublist.
#
# 			Increasing the percentage makes the old sublist larger, so pages in that sublist
# 			take longer to move to the tail, which increases the likelihood that those pages
# 			will be accessed again and made young.
#
# 		) The non-youngs/s metric is applicable only to old pages.
#
# 			It is based on the number of accesses to pages and not the number of pages.
#
# 			There can be multiple accesses to a given page, all of which are counted.
# 			If you do not see a higher non-youngs/s value when performing large table scans
# 			(and a higher youngs/s value), increase the delay value.
#
# 		) The young-making rate accounts for accesses to all buffer pool pages, not just
# 			accesses to pages in the old sublist.
#
# 			The young-making rate and not rate do not normally add up to the overall buffer
# 			pool hit rate.
#
# 			Page hits in the old sublist cause pages to move to the new sublist, but page hits
# 			in the new sublist causes pages to move to the head of the list only if they are
# 			a certain distance from the head.
#
# 		) not (young-making rate) is the average hit rate at which page accesses have not resulted
# 			in making pages young due to the delay defined by innodb_old_blocks_time not being met,
# 			or due to page hits in the new sublist that did not result in pages being moved to the head.
#
# 			This rate accounts for accesses to all buffer pool pages, not just accesses to pages in the
# 			old sublist.
#
# Buffer pool server status variables and the INNODB_BUFFER_POOL_STATS table provide many of the
# same buffer pool metrics found in InnoDB Standard Monitor output.
#
# For more information, see EXAMPLE 15.10, "QUERYING THE INNODB_BUFFER_POOL_STATS TABLE"
#
# 15.5.2 CHANGE BUFFER
#
# The change buffer is a special data structure that caches changes to secondary index pages when
# those pages are not in the buffer pool.
#
# The buffered changes, which may result from INSERT, UPDATE or DELETE operations (DML), are merged
# later when the pages are loaded into the buffer pool by other read operations.
#
# 											FIGURE 15.3 CHANGE BUFFER
#
# 										The purge operation periodically and efficiently writes 
# 										updated index pages to disk
#
# 											| Buffer Pool | -> DISK
#
# Changes to secondary index  			^ ^ ^ 			V
# pages that are not in the  									--> Changes are periodically merged as secondary index pages 
# buffer pool are cached in >>	| Change Buffer |    ^ 	 are read into the buffer pool
# the change buffer
#
# Unlike clustered indexes, secondary indexes are usually nonunique, and inserts into secondary
# indexes happen in a relatively random order.
#
# Similarly, deletes and updates may affect secondary index pages that are not adjacently located
# in an index tree.
#
# Merging cached changes at a later time, when affected pages are read into the buffer
# pool by other operations, avoids substansial random access I/O that would be required to
# read secondary index pages into the buffer pool from disk.
#
# Periodically, the purge operation that runs when the system is mostly idle, or during a slow
# shutdown, writes the updated index pages to disk.
#
# The purge operation can write disk blocks for a series of index values more efficiently
# than if each value were written to disk immediately.
#
# Change buffer merging may take several hours when there are many affected rows and numerous
# secondary indexes to update.
#
# During this time, disk I/O is increased, which can cause a significant slowdown for disk-bound
# queries.
#
# Change buffer merging may also continue to occur after a transaction is committed, and even
# after a server shutdown and restart (see SECTION 15.20.2, "FORCING INNODB RECOVERY" for more information)
#
# In memory, the change buffer occupies part of the buffer pool. On disk, the change buffer is part of
# the system tablespace, where index changes are buffered when the database server is shut down.
#
# The type of data cached in the change buffer is governed by the innodb_change_buffering variable.
#
# For more information, see CONFIGURING CHANGE BUFFERING. You can also configure the maximum change
# buffer size.
#
# For more information, see CONFIGURING THE CHANGE BUFFER MAXIMUM SIZE
#
# Change buffering is not supported for a secondary index if the index contains a descending
# index column or if the primary key includes a descending index column.
#
# For answers to frequently asked questions about the change buffer, see SECTION A.15, "MYSQL 8.0 FAQ: INNODB CHANGE BUFFER"
#
# CONFIGURING CHANGE BUFFERING
#
# When INSERT, UPDATE and DELETE operations are performed on a table, the values of indexed columns
# (particularly the values of secondary keys) are often in an unsorted order, requiring substansial
# I/O to bring secondary indexes up to date.
#
# The change buffer caches changes to secondary index entries when the relevant page is not in the
# buffer pool, thus avoiding expensive I/O operations by not immediately reading in the page from disk.
#
# The buffered changes are merged when the page is loaded into the buffer pool, and the updated page
# is later flushed to disk.
#
# The InnoDB main thread merges buffered changes when the server is nearly idle, and during
# a slow shutdown.
#
# Because it can result in fewer disk reads and writes, the change buffer feature is most valuable
# for workloads that are I/O-bound, for example applications with a high volume of DML operations
# such as bulk inserts.
#
# However, the change buffer occupies a part of the buffer pool, reducing the memory available to cache
# data pages.
#
# If the working set almost fits in the buffer pool, or if your tables have relatively few secondary
# indexes, it may be useful to disable change buffering.
#
# If the working data set fits entirely within the buffer pool, change buffering does not impose extra
# overhead, because it only applies to pages that are not in the buffer pool.
#
# You can control the extent to which InnoDB performs change buffering using the innodb_change_buffering
# configuration parameter.
#
# You can enable or disable buffering for inserts, delete operations (when index records are initially
# marked for deletion) and purge operations (when index records are physically deleted)
#
# An update operation is a combination of an insert and a delete. The default innodb_change_buffering
# value is all.
#
# Permitted innodb_change_buffering values include:
#
# 		) all
#
# 			The default value: buffer inserts, delete-marking operations and purges
#
# 		) none
#
# 			Do not buffer any operations
#
# 		) inserts
#
# 			Buffer insert operations
#
# 		) deletes
#
# 			Buffer delete-marking operations
#
# 		) changes
#
# 			Buffer both inserts and delete-marking operations
#
# 		) purges
#
# 			Buffer the physical deletion operations that happen in the background
#
# You can set the innodb_change_buffering parameter in the MySQL option file (my.cnf or my.ini)
# or change it dynamically with the SET_GLOBAL statement, which requires privileges sufficient
# to set global system variables.
#
# See SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES"
#
# Changing the setting affects the buffering of new operations; the merging of existing
# buffered entries is not affected.
#
# CONFIGURING THE CHANGE BUFFER MAXIMUM SIZE
#
# The innodb_change_buffer_max_size variable permits configuring the maximum size of the
# change buffer as a percentage of the total size of the buffer pool.
#
# By default, innodb_change_buffer_max_size is set to 25
#
# The maximum setting is 50
#
# Consider increasing innodb_change_buffer_max_size on a MySQL server with heavy insert,
# update and delete activity, where change buffer merging does not keep pace with new
# change buffer entries, causing the change buffer to reach its maximum size limit.
#
# Consider decreasing innodb_change_buffer_max_size on a MySQL server with static data
# used for reporting, or if the change buffer consumes too much of the memory space shared
# with the buffer pool, causing pages to age out of the buffer pool sooner than desired.
#
# Test different settings with a representative workload to determine an optimal configuration.
#
# The innodb_change_buffer_max_size setting is dynamic, which permits modifying the setting
# without restarting the server.
#
# MONITORING THE CHANGE BUFFER
#
# The following options are available for change buffer monitoring:
#
# 		) InnoDB Standard Monitor output includes change buffer status information.
#
# 			To view monitor data, issue the SHOW ENGINE INNODB STATUS statement.
#
# 				SHOW ENGINE INNODB STATUS\G
#
# 			Change buffer status information is located under the INSERT BUFFER AND ADAPTIVE HASH INDEX
# 			heading and appears similar to the following:
#
# 				----------------------------------
# 				INSERT BUFFER AND ADAPTIVE HASH INDEX
# 				----------------------------------
# 				Ibuf: size 1, free list len 0, seg size 2, 0 merges
# 				merged operations:
# 					insert 0, delete mark 0, delete 0
# 				discarded operations:
# 					insert 0, delete mark 0, delete 0
# 				Hash table size 4425293, used cells 32, node heap has 1 buffer(s)
# 				13577.57 hash searches/s, 202.47 non-hash searches/s
#
# 			For more information, see SECTION 15.16.3, "INNODB STANDARD MONITOR AND LOCK MONITOR OUTPUT"
#
# 		) The INFORMATION_SCHEMA.INNODB_METRICS table provides most of the data points found in InnoDB
# 			Standard Monitor output, plus other data points.
#
# 			To view change buffer metrics and a description of each, issue the following query:
#
# 				SELECT NAME, COMMENT FROM INFORMATION_SCHEMA.INNODB_METRICS WHERE NAME LIKE '%ibuf%'\G
#
# 			For INNODB_METRICS table usage information, see SECTION 15.14.6, "INNODB INFORMATION_SCHEMA METRICS TABLE"
#
# 		) The INFORMATION_SCHEMA.INNODB_BUFFER_PAGE table provides metadata about each page in the buffer pool,
# 			including change buffer index and change buffer bitmap pages.
#
# 			Change buffer pages are identified by PAGE_TYPE.IBUF_INDEX is the page type for change buffer index pages,
# 			and IBUF_BITMAP is the page type for change buffer bitmap pages.
#
# 				WARNING:
#
# 					Querying the INNODB_BUFFER_PAGE table can introduce significant performance overhead.
#
# 					To avoid impacting performance, reproduce the issue you want to investigate on a test
# 					instance and run your queries on the test instance.
#
# 			For example, you can query the INNODB_BUFFER_PAGE table to determine the approximate number of
# 			IBUF_INDEX and IBUF_BITMAP pages as a percentage of total buffer pool pages.
#
# 				SELECT (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
# 				WHERE PAGE_TYPE LIKE 'IBUF%') AS change_buffer_pages,
# 				(SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE) AS total_pages,
# 				(SELECT ((change_buffer_pages/total_pages)*100))
# 				AS change_buffer_page_percentage;
# 				+-------------------------------+--------------------+------------------------------------------+
# 				| change_buffer_pages 			  | total_pages 		  | change_buffer_page_percentage 				|
# 				+-------------------------------+--------------------+------------------------------------------+
# 				| 25 									  | 8192 				  | 0.3052 												|
# 				+-------------------------------+--------------------+------------------------------------------+
#
# 			For information about other data provided by the INNODB_BUFFER_PAGE table, see SECTION 25.38.1, "THE INFORMATION_SCHEMA
# 			INNODB_BUFFER_PAGE TABLE"
#
# 			For related usage information, see SECTION 15.14.5, "INNODB INFORMATION_SCHEMA BUFFER POOL TABLES"
#
# 		) Performance Schema provides change buffer mutex wait instrumentation for advanced performance monitoring.
#
# 			To view change buffer instrumentation, issue the following query:
#
# 				SELECT * FROM performance_schema.setup_instruments
# 				WHERE NAME LIKE '%wait/synch/mutex/innodb/ibuf%';
# 				+--------------------------------------------------------+--------------+------------+
# 				| NAME 																 	| ENABLED 		 | TIMED 	 |
# 				+--------------------------------------------------------+--------------+------------+
# 				| wait/synch/mutex/innodb/ibuf_bitmap_mutex 				 	| YES 			 | YES 		 |
# 				| wait/synch/mutex/innodb/ibuf_mutex 						 	| YES 			 | YES 		 |
# 				| wait/synch/mutex/innodb/ibuf_pessimistic_insert_mutex  | YES 			 | YES 		 |
# 				+--------------------------------------------------------+---------------+-----------+
#
# 			For information about monitoring InnoDB mutex waits, see SECTION 15.15.2, "MONITORING INNODB MUTEX WAITS USING PERFORMANCE SCHEMA"
#
# 15.5.3 ADAPTIVE HASH INDEX
#
# The adaptive hash index feature enables InnoDB to perform more like an in-memory database
# on systems with appropriate combinations of workload and sufficient memory for the buffer
# pool without sacrificing transactional features or reliability.
#
# The adaptive hash index feature is enabled by the innodb_adaptive_hash_index variable,
# or turned off at server startup by --skip-innodb-adaptive-hash-index
#
# Based on the observed pattern of searches, a hash index is built using a prefix of the index key.
#
# The prefix can be any length, and it may be that only some values in the B-tree appear in the
# hash index.
#
# Hash indexes are built on demand for the pages of the index that are accessed often.
#
# If a table fits almost entirely in main memory, a hash index can speed up queries by enabling
# direct lookup of any element, turning the index value into a sort of pointer.
#
# InnoDB has a mechanism that monitors index searches.
#
# If InnoDB notices that queries could benefit from building a hash index, it does so automatically.
#
# With some workloads, the speedup from hash index lookups greatly outweighs the extra work to monitor
# index lookups and maintain the hash index structure.
#
# Access to the adaptive hash index can sometimes become a source of contention under heavy workloads,
# such as multiple concurrent joins.
#
# Queries with LIKE operators and % wildcards also tend not to benefit.
#
# For workloads that do not benefit from the adaptive hash index feature, turning it
# off reduces unnecessary performance overhead.
#
# Because it is difficult to predict in advance whether the adaptive hash index feature
# is appropriate for a particular system and workload, consider running benchmarks with
# it enabled and disabled.
#
# Architechtural changes in MySQL 5.6 make it more suitable to disable the adaptive hash
# index feature than in earlier releases.
#
# The adaptive hash index feature is partitioned. Each index is bound to a specific partition,
# and each partition is protected by a separate latch.
#
# Partitioning is controlled by the innodb_adaptive_hash_index_parts variable.
#
# The innodb_adaptive_hash_index_parts variable is set to 8 by default. The maximum setting is 512.
#
# You can monitor adaptive hash index use and contention in the SEMAPHORES section of SHOW_ENGINE_INNODB_STATUS
# output.
#
# If there are numerous threads waiting on RW-latches created in btr0sea.c, consider increasing the number of
# adaptive hash index partitions or disabling the adaptive hash index feature.
#
# For information about the performance characteristics of hash indexes, see SECTION 8.3.9, "COMPARISON OF B-TREE AND HASH INDEXES"
#
# 15.5.4 LOG BUFFER
#
# The log buffer is the memory area that holds data to be written to the log files on disk.
#
# Log buffer size is defined by the innodb_log_buffer_size variable. The default size is 16MB.
#
# The contents of the log buffer are periodically flushed to disk. A large log buffer enables
# large transactions to run without the need to write redo log data to disk before the transaction
# commit.
#
# Thus, if you have transactions that update, insert, or delete many rows, increasing the size
# of the log buffer saves disk I/O
#
# The innodb_flush_log_at_trx_commit variable controls how the contents of the log buffer are written and 
# flushed to disk.
#
# The innodb_flush_log_at_timeout variable controls log flushing frequency.
#
# For related information, see MEMORY CONFIGURATION, see SECTION 8.5.4, "OPTIMIZING INNODB REDO LOGGING"
#
# 15.6 INNODB ON-DISK STRUCTURES
#
# 15.6.1 TABLES
# 15.6.2 INDEXES
#
# 15.6.3 TABLESPACES
# 15.6.4 DOUBLEWRITE BUFFER
#
# 15.6.5 REDO LOG
# 15.6.6 UNDO LOGS
#
# This section describes InnoDB on-disk structures and related topics.
#
# 15.6.1 TABLES
#
# 15.6.1.1 CREATING INNODB TABLES
# 15.6.1.2 MOVING OR COPYING INNODB TABLES
#
# 15.6.1.3 CONVERTING TABLES FROM MYISAM TO INNODB
# 15.6.1.4 AUTO_INCREMENT HANDLING IN INNODB
#
# 15.6.1.5 INNODB AND FOREIGN KEY CONSTRAINTS
# 15.6.1.6 LIMITS ON INNODB TABLES
#
# This section covers topics related to InnoDB tables.
#
# 15.6.1.1 CREATING INNODB TABLES
#
# To create an InnoDB table, use the CREATE_TABLE statement
#
# 		CREATE TABLE t1 (a INT, b CHAR (20), PRIMARY KEY (a)) ENGINE=InnoDB;
#
# You do not need to specify the ENGINE=InnoDB clause if InnoDB is defined as the default
# storage engine, which it is by default.
#
# To check the default storage engine, issue the following statement:
#
# 		SELECT @@default_storage_engine;
# 		+------------------------------+
# 		| @@default_storage_engine 	 |
# 		+------------------------------+
# 		| InnoDB 							 |
# 		+------------------------------+
#
# You might still use ENGINE=InnoDB clause if you plan to use mysqldump or replication to replay the
# CREATE_TABLE statement on a server where the default storage engine is not InnoDB.
#
# An InnoDB table and its indexes can be created in the system tablespace, in a file-per-table
# tablespace, or in a general tablespace.
#
# When innodb_file_per_table is enabled, which is the default, an InnoDB table is implicitly created
# in an individual file-per-table tablespace.
#
# Conversely, when innodb_file_per_table is disabled, an InnoDB table is implicitly created in the
# InnoDB system tablespace.
#
# To create a table in a general tablespace, use CREATE_TABLE_---_TABLESPACE syntax.
#
# For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# When you create a table in a file-per-table tablespace, MySQL creates an .ibd tablespace file in
# a database directory under the MySQL data directory, by default.
#
# A table created in the InnoDB system tablespace is created in an existing ibdata file, which resides
# in the MySQL data directory.
#
# A table created in a general tablespace is created in an existing general tablespace .ibd file.
#
# General tablespace files can be created inside or outside of the MySQL data directory.
#
# For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# Internally, InnoDB adds an entry for each table to the data dictionary.
#
# The entry includes the database name. For example, if table t1 is created in the
# test database, the data dictionary entry for the database name is 'test/t1'
#
# This means you can create a table of the same name (t1) in a different database,
# and the table names do not collide inside InnoDB.
#
# INNODB TABLES AND ROW FORMATS
#
# The default row format for InnoDB tables is defined by the innodb_default_row_format configuration
# option, which has a default value of DYNAMIC.
#
# Dynamic and Compressed row format allow you to take advantage of InnoDB features such as table
# compression and efficient off-page storage of long column values.
#
# To use these row formats, innodb_file_per_table must be enabled (the default)
#
# 		SET GLOBAL innodb_file_per_table=1;
# 		CREATE TABLE t3 (a INT, b CHAR (20), PRIMARY KEY (a)) ROW_FORMAT=DYNAMIC;
# 		CREATE TABLE t4 (a INT, b CHAR (20), PRIMARY KEY (a)) ROW_FORMAT=COMPRESSED;
#
# Alternatively, you can use CREATE_TABLE_---_TABLESPACE syntax to create an InnoDB table in
# a general tablespace.
#
# General tablespaces support all row formats. For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# 		CREATE TABLE t1 (c1 INT PRIMARY KEY) TABLESPACE ts1 ROW_FORMAT=DYNAMIC;
#
# CREATE_TABLE_---_TABLESPACE syntax can also be used to create InnoDB tables with a Dynamic row format
# in the system tablespace, alongside tables with a Compact or Redundant row format.
#
# 		CREATE TABLE t1 (c1 INT PRIMARY KEY) TABLESPACE = innodb_system ROW_FORMAT=DYNAMIC;
#
# For more information about InnoDB row formats, see SECTION 15.10, "InnoDB ROW FORMATS"
#
# For how to determine the row format of an InnoDB table and the physical characteristics
# of InnoDB row formats, see SECTION 15.10, "InnoDB ROW FORMATS"
#
# InnoDB TABLES AND PRIMARY KEYS
#
# Always define a primary key for an InnoDB table, specifying the column or columns that:
#
# 		) Are referenced by the most important queries
#
# 		) Are never left blank
#
# 		) Never have duplicate values
#
# 		) Rarely if ever change value once inserted
#
# For example, in a table containing information about people, you would not create a primary key
# on (firstname, lastname) because more than one person can have the same name, some people have
# blank last names, and sometimes people change their names.
#
# With so many constraints, often there is not an obvious set of columns to use as a primary key,
# so you create a new column with a numeric ID to serve as all or part of the primary key.
#
# You can declare an auto-increment column so that ascending values are filled in automatically
# as rows are inserted:
#
# 		#The value of ID can act like a pointer between related items in different tables
# 		CREATE TABLE t5 (id INT AUTO_INCREMENT, b CHAR (20), PRIMARY KEY (id));
#
# 		#The primary key can consist of more than one column. Any autoinc column must come first.
# 		CREATE TABLE t6 (id INT AUTO_INCREMENT, a INT, b CHAR (20), PRIMARY KEY (id,a));
#
# Although the table works correctly without defining a primary key, the primary key is involved
# with many aspects of performance and is a crucial design aspect for any large or frequently
# used table.
#
# It is recommended that you always specify a primary key in the CREATE_TABLE statement.
#
# If you create the table, load data, and then run ALTER_TABLE to add a primary key later,
# that operation is much slower than defining the primary key when creating the table.
#
# VIEWING INNODB TABLE PROPERTIES
#
# To view the properties of an InnoDB table, issue a SHOW_TABLE_STATUS statement:
#
# 		SHOW TABLE STATUS FROM test LIKE 't%' \G;
# 		****************************** 1. row *******************************
# 						Name: t1
# 					 Engine: InnoDB
# 					Version: 10
# 				Row_format: Compact
# 						Rows: 0
# 		  Avg_row_length: 0
# 			 Data_length : 16384
# 		Max_data_length : 0
# 			Index_length : 0
# 				Data_free : 0
# 		Auto_increment  : NULL
# 			Create_time  : 2015-03-16 15:13:31
# 			Update_time  : NULL
# 			 Check_time  : NULL
# 				Collation : utf8mb4_0900_ai_ci
# 				 Checksum : NULL
# 		Create_options  : 
# 				Comment   : 
#
# For information about SHOW_TABLE_STATUS output, see SECTION 13.7.6.36, "SHOW TABLE STATUS SYNTAX"
#
# InnoDB table properties may also be queried using the InnoDB Information Schema system tables:
#
# 		SELECT * FROM INFORMATION_SCHEMA.INNODB_TABLES WHERE NAME='test/t1' \G
# 		******************************* 1. row ***************************************
# 					TABLE_ID: 45
# 						NAME : test/t1
# 						FLAG : 1
# 					 N_COLS : 5
# 						SPACE: 35
# 				ROW_FORMAT : Compact
# 			ZIP_PAGE_SIZE : 0
# 				SPACE_TYPE : Single
#
# For more information, see SECTION 15.14.3, "INNODB INFORMATION_SCHEMA SCHEMA OBJECT TABLES"
#
# 15.6.1.2 MOVING OR COPYING INNODB TABLES
#
# This section describes techniques for moving or copying some or all InnoDB tables to a different
# server or instance.
#
# For example, you might move an entire MySQL instance to a larger, faster server; you might clone an
# entire MySQL instance to a new replication slave server; you might copy individual tables to another
# instance to develop and test an app - Or to a data warehouse server to produce reports.
#
# On Windows, InnoDB always stores database and table names internally in lowercase.
#
# To move databases in a binary format from Unix to Windows or from Windows to Unix, create all
# databases and tables using lowercase names.
#
# A convenient way to accomplish this is to add the following line to the [mysqld] section of your
# my.cnf or my.ini file before creating any databases or tables:
#
# 		[mysqld]
# 		lower_case_table_names=1
#
# NOTE:
#
# 		It is prohibited to start the server with a lower_case_table_names setting that is different
# 		from the setting used when the server was initialized.
#
# Techniques for moving or copying InnoDB tables include:
#
# 		) Transportable Tablespaces
#
# 		) MySQL Enterprise Backup
#
# 		) Copying Data Files (Cold Backup Method)
#
# 		) Export and Import (mysqldump)
#
# TRANSPORTABLE TABLESPACES
#
# The transportable tablespaces features uses FLUSH_TABLES_---_FOR_EXPORT to ready
# InnoDB tables for copying from one server instnace to another.
#
# To use this feature, InnoDB tables must be created with innodb_file_per_table
# set to ON so that each InnoDB table has its own tablespace.
#
# For usage information, see SECTION 15.6.3.7, "COPYING TABLESPACES TO ANOTHER INSTANCE"
#
# MYSQL ENTERPRISE BACKUP
#
# The MySQL Enterprise Backup product lets you back up a running MySQL database with minimal
# disruption to operations while producing a consistent snapshot of the database.
#
# When MySQL Enterprise Backup is copying tables, reads and writes can continue.
#
# In addition, MySQL Enterprise Backup can create compressed backup files, and back
# up subsets of tables.
#
# In conjunction with the MySQL binary log, you can perform point-in-time recovery.
#
# MySQL Enterprise Backup is included as part of the MySQL Enterprise Subscription.
#
# For more details about MySQL Enterprise Backup, see SECTION 30.2, "MySQL ENTERPRISE BACKUP OVERVIEW"
#
# COPYING DATA FILES (COLD BACKUP METHOD)
#
# You can move an InnoDB database simply by copying all the relevant files listed under "Cold Backups"
# in SECTION 15.17.1, "INNODB BACKUP"
#
# InnoDB data and log files are binary-compatible on all platforms having the same floating-point number
# format.
#
# If the floating-point formats differ but you have not used FLOAT or DOUBLE data types in your tables,
# then the procedure is the same: simply copy the relevant files.
#
# When you move or copy file-per-table .ibd files, the database directory name must be the same on the
# source and destination systems.
#
# The table definition stored in the InnoDB shared tablespace includes the database name.
#
# The transaction IDs and log sequence numbers stored in the tablespace files also differ
# between databases.
#
# To move an .ibd file and the associated table from one database to another, use a RENAME_TABLE
# statement:
#
# 		RENAME TABLE db1.tbl_name TO db2.tbl_name;
#
# If you have a "clean" backup of an .ibd file, you can restore it to the MySQL installation from which
# it originated as follows:
#
# 		1. The table must not have been dropped or truncated since you copied the .ibd file, because doing so
# 			changes the table ID stored inside the tablespace.
#
# 		2. Issue this ALTER_TABLE statement to delete the current .ibd file:
#
# 			ALTER TABLE tbl_name DISCARD TABLESPACE;
#
# 		3. Copy the backup .ibd file to the proper database directory
#
# 		4. Issue this ALTER_TABLE statement to tell InnoDB to use the new .ibd file for the table:
#
# 			ALTER TABLE tbl_name IMPORT TABLESPACE;
#
# NOTE:
#
# 		The ALTER_TABLE_---_IMPORT_TABLESPACE feature does not enforce foreign key constraints on imported data.
#
# In this context, a "clean" .ibd file backup is one for which the following requirements are satisfied:
#
# 		) There are no uncommitted modifications by transactions in the .ibd file
#
# 		) There are no unmerged insert buffer entries in the .ibd file
#
# 		) Purge has removed all delete-marked index records from the .ibd file
#
# 		) mysqld has flushed all modified pages of the .ibd file from the buffer pool to the file
#
# You can make a clean backup .ibd file using the following method:
#
# 		1. Stop all activity from the mysqld server and commit all transactions.
#
# 		2. Wait until SHOW_ENGINE_INNODB_STATUS shows that there are no active transactions
# 			in the database, and the main thread status of InnoDB is Waiting for server activity.
#
# 			Then you can make a copy of the .ibd file
#
# Another method for making a clean copy of an .ibd file is to use the MySQL Enterprise Backup product:
#
# 		1. Use MySQL Enterprise Backup to back up the InnoDB installation
#
# 		2. Start a second mysqld server on the backup and let it clean up the .ibd files in the backup
#
# EXPORT AND IMPORT (MYSQLDUMP)
#
# You can use mysqldump to dump your tables on one machine and then import the dump files on
# the other machine.
#
# Using this method, it does not matter whether the formats differ or if your tables contain floating-point data.
#
# One way to increase the performance of this method is to switch off autocommit mode when importing data,
# assuming that the tablespace has enough space for the big rollback segment that the import transactions
# generate.
#
# Do the commit only after importing a whole table or a segment of a table.
#
# 15.6.1.3 CONVERTING TABLES FROM MYISAM TO INNODB
#
# If you have MyISAM tables that you want to convert to InnoDB for better reliability and scalability,
# review the following guidelines and tips before converting.
#
# NOTE:
#
# 		Partitioned MyISAM tables created in previous versions of MySQL are not compatible with MySQL 8.0
#
# 		Such tables must be prepared prior to upgrade, either by removing the partitioning, or by converting
# 		them to InnoDB.
#
# 		See SECTION 23.6.2, "PARTITIONING LIMITATIONS RELATING TO STORAGE ENGINES", for more information.
#
# 		) ADJUSTING MEMORY USAGE FOR MYISAM AND INNODB
#
# 		) HANDLING TOO-LONG OR TOO-SHORT TRANSACTIONS
#
# 		) HANDLING DEADLOCKS
#
# 		) PLANNING THE STORAGE LAYOUT
#
# 		) CONVERTING AN EXISTING TABLE
#
# 		) CLONING THE STRUCTURE OF A TABLE
#
# 		) TRANSFERRING EXISTING DATA
#
# 		) STORAGE REQUIREMENTS
#
# 		) DEFINING A PRIMARY KEY FOR EACH TABLE
#
# 		) APPLICATION PERFORMANCE CONSIDERATIONS
#
# 		) UNDERSTANDING FILES ASSOCIATED WITH INNODB TABLES
#
# ADJUSTING MEMORY USAGE FOR MYISAM AND INNODB
#
# As you transition away from MyISAM tables, lower the value of the key_buffer_size configuration
# option to free memory no longer needed for caching results.
#
# Increase the value of the innodb_buffer_pool_size configuration option, which performs a similar
# role of allocating cache memory for InnoDB tables.
#
# The InnoDB buffer pool caches both table data and index data, speeding up lookups for queries
# and keeping query results in memory for reuse.
#
# For guidance regarding buffer pool size configuration, see SECTION 8.12.3.1, "HOW MYSQL USES MEMORY"
#
# HANDLING TOO-LONG OR TOO-SHORT TRANSACTIONS
#
# Because MyISAM tables do not support transactions, you might not have paid much attention to the
# autocommit configuration option and the COMMIT and ROLLBACK statements.
#
# These keywords are important to allow multiple sessions to read and write InnoDB tables concurrently,
# providing substantial scalability benefits in write-heavy workloads.
#
# While a transaction is open, the system keeps a snapshot of the data as seen at the beginning of the
# transaction, which can cause substantial overhead if the system inserts, updates and deletes millions
# of rows while a stray transaction keeps running.
#
# Thus, take care to avoid transactions that run for too long:
#
# 		) If you are using a mysql session for interactive experiments, always COMMIT (to finalize the changes)
# 			or ROLLBACK (to undo the changes) when finished.
#
# 			Close down interactive sessions rather than leave them open for long periods, to avoid keeping
# 			transactions open for long periods by accident.
#
# 		) Make sure that any error handlers in your application also ROLLBACK incomplete changes or COMMIT completed changes
#
# 		) ROLLBACK is a relatively expensive operation, because INSERT, UPDATE and DELETE operations are written to InnoDB
# 			tables prior to the COMMIT, with the expectation that most changes are committed successfully and rollbacks are
# 			rare.
#
# 			When experimenting with large volumes of data, avoid making changes to large numbers of rows and then rolling
# 			back those changes.
#
# 		) When loading large volumes of data with a sequence of INSERT statements, periodically COMMIT the results to avoid
# 			having transactions that last for hours.
#
# 			In typical load operations for data warehousing, if something goes wrong, you truncate the table (using TRUNCATE_TABLE)
# 			and start over from the beginning rather than doing a ROLLBACK
#
# The preceding tips save memory and disk space that can be wasted during too-long transactions.
#
# When transactions are shorter than they should be, the problem is excessive I/O. With each COMMIT,
# MySQL makes sure each change is safely recorded to disk, which involves some I/O
#
# 		) For most operations on InnoDB tables, you should use the setting autocommit=0 
#
# 			From an efficiency perspective, this avoids unnecessary I/O when you issue large
# 			numbers of consecutive INSERT, UPDATE or DELETE statements.
#
# 			From a safety perspective, this allows you to issue a ROLLBACK statement to recover
# 			lost or garbled data if you make a mistake on the mysql command line, or in an exception
# 			handler in your application.
#
# 		) The time when autocommit=1 is suitable for InnoDB tables is when running a sequence of queries
# 			for generating reports or analyzing statistics.
#
# 			In this situation, there is no I/O penalty related to COMMIT or ROLLBACK, and InnoDB can 
# 			automatically optimize the read-only workload
#
# 		) If you make a series of related changes, finalize all the changes at once with a single COMMIT at the end.
#
# 			For example, if you insert related pieces of information into several tables, do a single COMMIT
# 			after making all the changes.
#
# 			Or if you run many consecutive INSERT statements, do a single COMMIT after all the data is loaded;
# 			if you are doing millions of INSERT statements, perhaps split up the huge transaction by issuing
# 			a COMMIT every ten thousand or hundred thousand records, so the transaction does not grow too large.
#
# 		) Remember that even a SELECT statement opens a transaction, so after running some report or debugging
# 			queries in an interactive mysql session, either issue a COMMIT or close the mysql session
#
# HANDLING DEADLOCKS
#
# You might see warning messages referring to "deadlocks" in the MySQl error log, or the output of
# SHOW_ENGINE_INNODB_STATUS
#
# Despite the scary-sounding name, a deadlock is not a serious issue for InnoDB tables, and often does not
# require any corrective action.
#
# When two transactions start modifying multiple tables, accessing the tables in a different order, they
# can reach a state where each transaction is waiting for the other and neither can proceed.
#
# When deadlock detection is enabled (the default), MySQL immediately detects this condition and cancels
# (rolls back) the "smaller" transaction, allowing the other to proceed.
#
# If deadlock detection is disabled using the innodb_deadlock_detect configuration option, InnoDB
# relies on the innodb_lock_wait_timeout setting to roll back transactions in case of a deadlock.
#
# Either way, your applications need error-handling logic to restart a transaction that is forcibly
# cancelled due to a deadlock.
#
# When you re-issue the same SQL statements as before, the original timing issue no longer applies.
#
# Either the other transaction has already finished and yours can proceed, or the other transaction
# is still in progress and your transaction waits until it finishes.
#
# If deadlock warnings occur constantly, you might review the application code to reorder the SQL
# operations in a consistent way, or to shorten the transactions.
#
# You can test with the innodb_print_all_deadlocks option enabled to see all deadlock warnings
# in the MySQL error log, rather than only the last warning in the SHOW_ENGINE_INNODB_STATUS
# output.
#
# For more information, see SECTION 15.7.5, "DEADLOCKS IN INNODB"
#
# PLANNING THE STORAGE LAYOUT
#
# To get the best performance from InnoDB tables, you can adjust a number of parameters related
# to storage layout.
#
# When you convert MyISAM tables that are large, frequently accessed and hold vital data - investigate
# and consider the innodb_file_per_table and innodb_page_size configuration options, and the ROW_FORMAT
# and KEY_BLOCK_SIZE clauses of the CREATE_TABLE statement.
#
# During your initial experiments, the most important setting is innodb_file_per_table
#
# When this setting is enabled, which is the default, new InnoDB tables are implicitly
# created in file-per-table tablespaces.
#
# In contrast with the InnoDB system tablespace, file-per-table tablespaces allow disk space
# to be reclaimed by the operating system when a table is truncated or dropped.
#
# File-per-table tablespaces also support DYNAMIC and COMPRESSED row formats and associated
# features such as table compression, efficient off-page storage for long variable-length
# columns, and large index prefixes.
#
# For more information, see SECTION 15.6.3.2, "FILE-PER-TABLE TABLESPACES"
#
# You can also store InnoDB tables in a shared general tablespace, which support multiple
# tables and all row formats.
#
# For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# CONVERTING AN EXISTING TABLE
#
# TO convert a non-InnoDB table to use InnoDB use ALTER_TABLE:
#
# 		ALTER TABLE table-name ENGINE=InnoDB;
#
# CLONING THE STRUCTURE OF A TABLE
#
# You might make an InnoDB table that is a clone of a MyISAM table, rather than using
# ALTER_TABLE to perform conversion, to test the old and new table side-by-side before
# switching.
#
# Create an empty InnoDB table with identical column and index definitions.
#
# Use SHOW CREATE TABLE table_name\G to see the full CREATE_TABLE statement to use.
#
# Change the ENGINE clause to ENGINE=INNODB
#
# TRANSFERRING EXISTING DATA
#
# To transfer a large volume of data into an empty InnoDB table created as shown in the previous section,
# insert the rows with INSERT INTO innodb_table SELECT * FROM myisam_table ORDER BY primary_key_columns
#
# You can also create the indexes for the InnoDB table after inserting the data.
#
# Historically, creating a new secondary indexes was a slow operation for InnoDB, but now you can
# create the indexes after the data is loaded with relatively little overhead from the index
# creation step.
#
# If you have UNIQUE constraints on secondary keys, you can speed up a table import by turning
# off the unqiueness checks temporarily during the import operation:
#
# 		SET unique_checks=0;
# 		------ IMPORT OPERATION -------
# 		SET unique_checks=1;
#
# For big tables, this saves disk I/O because InnoDB can use its change buffer to write secondary
# index records as a batch.
#
# Be certain that the data contains no duplicate keys.
#
# unique_checks permits but does not require storage engines to ignore duplicate keys.
#
# For better control over the insertion process, you can insert big tables in pieces:
#
# 		INSERT INTO newtable SELECT * FROM oldtable
# 			WHERE yourkey > something AND yourkey <= somethingelse;
#
# After all records are inserted, you can rename the tables.
#
# During the conversion of big tables, increase the size of the InnoDB buffer pool to reduce
# disk I/O, to a maximum of 80% of physical memory.
#
# You can also increase the size of InnoDB log files.
#
# STORAGE REQUIREMENTS
#
# If you intend to make several temporary copies of your data in InnoDB tables during the conversion
# process, it is recommended that you create the tables in file-per-table tablespaces so that you
# can reclaim the disk space when you drop the tables.
#
# When the innodb_file_per_table configuration option is enabled (the default), newly created InnoDB
# tables are implicitly created in file-per-table tablespaces.
#
# Whether you convert the MyISAM table directly or create a cloned InnoDB table, make sure that you
# have sufficient disk space to hold both the old and new tables during the process.
#
# InnoDB tables require more disk space than MyISAM tables.
#
# If an ALTER_TABLE operation runs out of space, it starts a rollback, and that can take hours
# if it is disk-bound.
#
# For inserts, InnoDB uses the insert buffer to merge secondary index records to indexes in batches.
#
# That saves a lot of disk I/O.
#
# For rollback, no such mechanism is used and the rollback can take 30 times longer than the insertion.
#
# In the case of a runaway rollback, if you do not have valuable data in your database, it may be advisable
# to kill the database process rather than wait for millions of disk I/O operations to complete.
#
# For the complete procedure, see SECTION 15.20.2, "FORCING INNODB RECOVERY"
#
# DEFINING A PRIMARY KEY FOR EACH TABLE
#
# The PRIMARY KEY clause is a critical factor affecting the performance of MySQL queries and the space
# usage for tables and indexes.
#
# THe primary key uniquely identifies a row in a table.
#
# Every row in the table must have a primary key value, and no two rows can have the same primary key value.
#
# THese are guidelines for the primary key, followed by more detailed explanations.
#
# 		) Declare a PRIMARY KEY for each table. Typically, it is the most important column that you refer to in
# 			WHERE clauses when looking up a single row.
#
# 		) Declare the PRIMARY KEY clause in the original CREATE_TABLE statement, rather than adding it later
# 			through an ALTER_TABLE statement
#
# 		) Choose the column and its data type carefully. Prefer numeric columns over character or string ones.
#
# 		) Consider using an auto-increment column if there is not another stable, unique, non-null, numeric column to use
#
# 		) An auto-increment column is also a good choice if there is any doubt whether the value of the primary key
# 			column could ever change.
#
# 			Changing the value of a primary key column is an expensive operation, possibly involving rearraging data within
# 			the table and within each secondary index.
#
# Consider adding a primary key to any table that does not already have one.
#
# Use the smallest practical numerical type based on the maximum projected size of the table.
#
# This can make each row slightly more compact, which can yield substantial space savings for
# large tables.
#
# The space savings are multiplied if the table has any secondary indexes, because the primary key
# value is repeated in each secondary index entry.
#
# In addition to reducing data size on disk, a small primary key also lets more data fit into
# the buffer pool, speeding up all kinds of operations and improving concurrency.
#
# If the table already has a primary key on some longer column, such as a VARCHAR, consider adding
# a new unsigned AUTO_INCREMENT column and switching the primary key to that, even if that column
# is not referenced in queries.
#
# This design change can produce substantial space savings in the secondary indexes.
#
# You can designate the former primary key columns as UNIQUE NOT NULL to enforce the same constraints
# as the PRIMARY KEY clause, that is, to prevent duplicate or null values across all those columns.
#
# If you spread related information across multiple tables, typically each table uses the same column
# for its primary key.
#
# For example, a personnel database might have several tables, each with a primary key of employee number.
#
# A sales database might have some tables with a primary key of customer number, and other tables
# with a primary key of order number.
#
# Because lookups using the primary key are very fast, you can construct efficient join queries
# for such tables.
#
# If you leave the PRIMARY KEY clause out entirely, MySQL creates an invisible one for you.
#
# It is a 6-byte value that might be longer than you need, thus wasting space.
#
# Because it is hidden, you cannot refer to it in queries.
#
# APPLICATION PERFORMANCE CONSIDERATIONS
#
# The reliability and scalability features of InnoDB require more disk storage than equivalent
# MyISAM tables.
#
# You might change the column and index definitions slightly, for better space utilization,
# reduced I/O and memory consumption when processing result sets, and better query optimization
# plans making efficient use of index lookups.
#
# If you do set up a numeric ID column for the primary key, use that value to cross-reference
# with related values in any other tables, particularly for join queries.
#
# For example, rather than accepting a country name as input and doing queries searching for the
# same name, do one lookup to determine the country ID, then do other queries (or a single join query)
# to look up relevant information across several tables.
#
# Rather than storing a customer or catalog item number as a string of digits, potentially using up
# several bytes, convert it to a numeric ID for storing and querying.
#
# A 4-byte unsigned INT column can index over 4 billion items (with the US meaning of BIllion: 1k Mils)
#
# For the ranges of the different integer types, see SECTION 11.2.1, "INTEGER TYPES (EXACT VALUE) - INTEGER, INT, SMALLINT,
# TINYINT, MEDIUMINT, BIGINT"
#
# UNDERSTANDING FILES ASSOCIATED WITH INNODB TABLES
#
# InnoDB files require more care and planning than MyISAM files do.
#
# 		) You must not delete the ibdata files that represent the InnoDB system tablespace
#
# 		) Methods of moving or copying InnoDB tables to a different server are described in SECTION 15.6.1.2, "MOVING OR COPYING INNODB TABLES"
#
# 15.6.1.4 AUTO_INCREMENT HANDLING IN INNODB
#
# InnoDB provides a configurable locking mechanism that can significantly improve scalability and performance
# of SQL statements that add rows to tables with AUTO_INCREMENT columns.
#
# To use the AUTO_INCREMENT mechanism with an InnoDB table, an AUTO_INCREMENT column must be defined as part
# of an index such that it is possible to perform the equivalent of an indexed SELECT MAX(ai_col) lookup on
# the table to obtain the maximum column value.
#
# Typically, this is achieved by making the column the first column of some table index.
#
# This section describes the behavior of AUTO_INCREMENT lock modes, usage implications for different
# AUTO_INCREMENT lock mode settings, and how InnoDB initializes the AUTO_INCREMENT counter.
#
# 		) InnoDB AUTO_INCREMENT LOCK MODES
#
# 		) InnoDB AUTO_INCREMENT LOCK MODE USAGE IMPLICATIONS
#
# 		) InnoDB AUTO_INCREMENT COUNTER INITIALIZATION
#
# InnoDB AUTO_INCREMENT LOCK MODES
#
# This section describes the behavior of AUTO_INCREMENT lock modes used to generate auto-increment values,
# and how each lock mode affects replication.
#
# Auto-increment lock modes are configured at startup using the innodb_autoinc_lock_mode configuration
# parameter.
#
# The following terms are used in describing innodb_autoinc_lock_mode settings:
#
# 		) "INSERT-like" statements
#
# 			All statements that generate new rows in a table, including INSERT, INSERT_---_SELECT, REPLACE,
# 			REPLACE_---_SELECT and LOAD_DATA
#
# 			Includes "simple-inserts", "bulk-inserts" and "mixed-mode" inserts
#
# 		) "Simple inserts"
#
# 			Statements for which the number of rows to be inserted can be determined in advance (when the statement
# 			is initially processed)
#
# 			This includes single-row and multiple-row INSERT and REPLACE statements that do not have a nested
# 			subquery, but not INSERT_---_ON_DUPLICATE_KEY_UPDATE
#
# 		) "Bulk Inserts"
#
# 			Statements for which the number of rows to be inserted (and the number of required auto-increment values)
# 			is not known in advance.
#
# 			This includes INSERT_---_SELECT, REPLACE_---_SELECT and LOAD_DATA statements, but not plain INSERT.

# 			InnoDB assigns new values for the AUTO_INCREMENT column one at a time as each row is processed.
#
# 		) "Mixed-mode inserts"
#
# 			These are "simple insert" statements that specify the auto-increment value for some (but not all)
# 			of the new rows.
#
# 			An example follows, where c1 is an AUTO_INCREMENT column of table t1:
#
# 				INSERT INTO t1 (c1,c2) VALUES (1,'a'), (NULL,'b'), (5,'c'), (NULL,'d');
#
# 			Another type of "mixed-mode insert" is INSERT_---_ON_DUPLICATE_KEY_UPDATE, which in the worst
# 			case is in effect an INSERT followed by a UPDATE, where the allocated value for the AUTO_INCREMENT
# 			column may or may not be used during the update phase.
#
# There are three possible settings for the innodb_autoinc_lock_mode configuration parameter.
#
# The settings are 0, 1, or 2 for "traditional", "consecutive" or "interleaved" lock mode, respectively.
#
# As of MySQL 8.0, interleaved lock mode (innodb_autoinc_lock_mode=2) is the default setting
#
# Prior to MySQL 8.0, consecutive lock mode is the default (innodb_autoinc_lock_mode=1)
#
# The default setting of interleaved lock mode in MySQL 8.0 reflects the change from statement-based
# replication to row based replication as the default replication type.
#
# Statement-based replication requires the consecutive auto-increment lock mode to ensure that auto-increment
# values are assigned in a predictable and repeatable order for a given sequence of SQL statements, whereas
# row-based replication is not sensitive to the execution order of SQL statements.
#
# 		) innodb_autoinc_lock_mode = 0 ("traditional" lock mode)
#
# 			The traditional lock mode provides the same behavior that existed before the innodb_autoinc_lock_mode
# 			configuration parameter was introduced in MySQL 5.1
#
# 			The traditional lock mode option is provided for backward compatibility, performance testing,
# 			and working around issues with "mixed-mode inserts", due to possible differences in semantics.
#
# 			In this lock mode, all "INSERT-like" statements obtain a special table-level AUTO-INC lock
# 			for inserts into tables with AUTO_INCREMENT columns.
#
# 			This lock is normally held to the end of the statement (not to the end of the transaction)
# 			to ensure that auto-increment values are assigned in a predictable and repeatable order for
# 			a given sequence of INSERT statements, and to ensure that auto-increment values assigned by
# 			any given statement are consecutive.
#
# 			In the case of statement-based replication, this means that when an SQL statement is replicated
# 			on a slave server, the same values are used for the auto-increment column as on the master server.
#
# 			The result of execution of multiple INSERT statements is deterministic, and the slave reproduces
# 			the same data as on the master.
#
# 			If auto-increment value generated by multiple INSERT statements were interleaved, the result of
# 			two concurrent INSERT statements would be nondeterministic, and could not reliably be propagated
# 			to a slave server using statement-based replication.
#
# 			To make this clear, consider an example that uses this table:
#
# 				CREATE TABLE t1 (
# 					c1 INT(11) NOT NULL AUTO_INCREMENT,
# 					c2 VARCHAR(10) DEFAULT NULL,
# 					PRIMARY KEY (c1)
# 				) ENGINE=InnoDB;
#
# 			Suppose that there are two transactions running, each inserting rows into a table with an
# 			AUTO_INCREMENT column.
#
# 			One transaction is using an INSERT_---_SELECT statement that inserts 1000 rows, and another
# 			is using a simple INSERT statement that inserts one row:
#
# 				Tx1: INSERT INTO t1 (c2) SELECT 1000 ROWS FROM another TABLE ---
# 				Tx2: INSERT INTO t1 (c2) VALUES ('xxx');
#
# 			InnoDB cannot tell in advance how many rows are retrieved from the SELECT in the INSERT
# 			statement in Tx1, and it assigns the auto-increment values one at a time as the statement
# 			proceeds.
#
# 			With a table-level lock, held to the end of the statement, only one INSERT statement referring
# 			to table t1 can execute at a time, and the generation of auto-increment numbers by different
# 			statements is not interleaved.
#
# 			The auto-increment value generated by the Tx1 INSERT_---_SELECT statement are consecutive,
# 			and the (single) auto-increment value used by the INSERT statement in Tx2 are either smaller
# 			or larger than all those used for Tx1, depending on which statement executes first.
#
# 			As long as the SQL statements execute in the same order when replayed from the binary log
# 			(when using statement-based replication, or in recovery scenarios), the results are the same
# 			as they were when Tx1 and Tx2 first ran.
#
# 			Thus, table-level locks held until the end of a statement make INSERT statements using auto-increment
# 			safe for use with statement-based replication.
#
# 			However, those table-level locks limit concurrency and scalability when multiple transactions
# 			are executing insert statements at the same time.
#
# 			In the preceding example, if there were no table-level lock, the value of the auto-increment
# 			column used for the INSERT in Tx2 depends on precisely when the statement executes.
#
# 			If the INSERT of Tx2 executes while the INSERT of Tx1 is running (rather than before it starts
# 			or after it completes), the specific auto-increment value assigned by the two INSERT statements
# 			are nondeterministic, and may vary from run to run.
#
# 			Under the consecutive lock mode, InnoDB can avoid using table-level AUTO-INC locks for 
# 			"simple insert" statements where the number of rows is known in advance, and still preserve
# 			deterministic execution and safety for statement-based replication.
#
# 			If you are not using the binary log to replay SQL statements as part of recovery or replication,
# 			the interleaved lock mode can be used to eliminate all use of table-level AUTO-INC locks for even
# 			greater concurrency and performance, at hte cost of permitting gaps in auto-increment numbers assigned
# 			by a statement and potentially having the numbers assigned by concurrently executing statements interleaved.
#
# 		) innodb_autoinc_lock_mode = 1 ("consecutive" lock mode)
#
# 			In this mode, "bulk inserts" use the special AUTO-INC table-level lock and hold it until the end
# 			of the statement.
#
# 			This applies to all INSERT_---_SELECT, REPLACE_---_SELECT and LOAD_DATA statements.
#
# 			Only one statement holding the AUTO-INC lock can execute at a time.
#
# 			If the source table of the bulk insert operation is different from the target table,
# 			the AUTO-INC lock on the target table is taken after a shared lock is taken on the first
# 			row selected from the source table.
#
# 			If the source and target of the bulk insert operation are the same table, the AUTO-INC
# 			lock is taken after shared locks are taken on all selected rows.
#
# 			"Simple inserts" (for which the number of rows to be inserted is known in advanced) avoid table
# 			level AUTO-INC locks by obtaining the required number of auto-increment values under the
# 			control of a mutex (a light-weight lock) that is only held for the duration of the allocation
# 			process, not until the statement completes.
#
# 			No table-level AUTO-INC lock is used unless an AUTO-INC lock is held by another transaction.
#
# 			If another transaction holds an AUTO-INC lock, a "simple insert" waits for the AUTO-INC lock,
# 			as if it were a "bulk insert"
#
# 			This lock mode ensures that, in the presence of INSERT statements where the number of rows
# 			is not known in advance (and where auto-increment numbers are assigned as the statement
# 			progresses), all auto-increment values assigned by any "INSERT-like" statement are consecutive,
# 			and operations are safe for statement-based replication.
#
# 			Simply put, this lock mode significantly improves scalability while being safe for use with
# 			statement-based replication.
#
# 			Further, as with "traditional" lock mode, auto-increment numbers assigned by any given statement
# 			are consecutive.
#
# 			There is no change in semantics compared to "traditional" mode for any statement that uses
# 			auto-increment, with one important exception.
#
# 			The exception is for "mixed-mode inserts", where the user provides explicit values for an
# 			AUTO_INCREMENT column for some, but not all, rows in a multiple-row "simple insert".
#
# 			For such inserts, InnoDB allocates more auto-increment values than the number of rows to be
# 			inserted.
#
# 			However, all values automatically assigned are consecutively generated (and thus higher than)
# 			the auto-increment value generated by the most recently executed previous statement.
#
# 			"Excess" numbers are lost.
#
# 		) innodb_autoinc_lock_mode = 2 ("interleaved" lock mode)
#
# 			In this lock mode, no "INSERT-like" statements use the table-level AUTO-INC lock, and
# 			multiple statements can execute at the same time.
#
# 			This is the fastest and most scalable lock mode, but it is not safe when using statement-based
# 			replication or recovery scenarios when SQL statements are replayed from the binary log.
#
# 			In this lock mode, auto-increment values are guaranteed to be unique and monotonically increasing
# 			across all concurrently executing "INSERT-like" statements.
#
# 			However, because multiple statements can be generating numbers at the same time (that is, allocation
# 			of numbers is interleaved across statements), the values generated for the rows inserted by any given
# 			statement may not be consecutive.
#
# 			If the only statements executing are "simple inserts" where the number of rows to be inserted
# 			is known ahead of time, there are no gaps in the numbers generated for a single statement,
# 			except for "mixed-mode inserts"
#
# 			However, when "bulk inserts" are executed, there may be gaps in the auto-increment values
# 			assigned by any given statement.
#
# INNODB AUTO_INCREMENT LOCK MODE USAGE IMPLICATIONS
#
# 		) Using auto-increment with replication
#
# 			If you are using statement-based replication, set innodb_autoinc_lock_mode to 0 or 1
# 			and use the same value on the master and its slaves.
#
# 			Auto-increment values are not ensured to be the same on the slaves as on the master
# 			if you use innodb_autoinc_lock_mode = 2 ("interleaved") or configurations where the
# 			master and slaves do not use the same lock mode.
#
# 			If you are using row-based or mixed-format replication, all of the auto-increment lock
# 			modes are safe, since row-based replication is not sensitive to the order of execution
# 			of the SQL statements (and the mixed format uses row-based replication for any statements
# 			that are unsafe for statement-based replication)
#
# 		) "Lost" auto-increment values and sequence gaps
#
# 			In all lock modes (0, 1, 2) if a transaction that generated auto-increment values rolls back,
# 			those auto-increment values are "lost".
#
# 			Once a value is generated for an auto-increment column, it cannot be rolled back, whether or
# 			not the "INSERT-like" statement is completed, and whether or not the containing transaction
# 			is rolled back.
#
# 			Such lost values are not reused. Thus, there may be gaps in the values stored in an AUTO_INCREMENT
# 			column of a table.
#
# 		) Specifying NULL or 0 for the AUTO_INCREMENT column
#
# 			In all lock modes (0, 1, and 2), if a user specifies NULL or 0 for the AUTO_INCREMENT
# 			column in an INSERT, InnoDB treats the row as if the value was not specified and generates
# 			a new value for it.
#
# 		) Assigning a negative value to the AUTO_INCREMENT column
#
# 			In all lock modes (0, 1, and 2), the behavior of the auto-increment mechanism
# 			is not defined if you assign a negative value to the AUTO_INCREMENT column.
#
# 		) If the AUTO_INCREMENT value becomes larger than the maximum integer for the specified integer type
#
# 			In all lock modes (0, 1, and 2), the behavior of the auto-increment mechanism is not defined
# 			if the value becomes larger than the maximum integer that can be stored in the specified
# 			integer type.
#
# 		) Gaps in auto-increment values for "bulk inserts"
#
# 			With innodb_autoinc_lock_mode set to 0 ("traditional") or 1 ("consecutive"), the auto-increment
# 			values generated by any given statement are consecutive, without gaps, because the table-level
# 			AUTO-INC lock is held until the end of the statement, and only one such statement can execute
# 			at a time.
#
# 			With innodb_autoinc_lock_mode set to 2 ("interleaved"), there may be gaps in the auto-increment
# 			values generated by "bulk inserts", but only if there are concurrently executing "INSERT-like"
# 			statements.
#
# 			For lock modes 1 or 2, gaps may occur between successive statements because for bulk inserts
# 			the exact number of auto-increment values required by each statement may not be known and
# 			overestimation is possible.
#
# 		) Auto-increment values assigned by "mixed-mode inserts"
#
# 			Consider a "mixed-mode insert", where a "simple insert" specifies the auto-increment value
# 			for some (but not all) resulting rows.
#
# 			Such a statement behaves differently in lock modes 0, 1 and 2.
#
# 			For example, assume c1 is an AUTO_INCREMENT column of table t1, and that the most
# 			recent automatically generated sequence number is 100.
#
# 				CREATE TABLE t1 (
# 					c1 INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
# 					c2 CHAR(1)
# 				) ENGINE = INNODB;
#
# 			Now, consider the following "mixed-mode insert" statement:
#
# 				INSERT INTO t1 (c1, c2) VALUES (1,'a'), (NULL, 'b'), (5,'c'), (NULL,'d');
#
# 			With innodb_autoinc_lock_mode set to 0 ("traditional"), the four new rows are:
#
# 				SELECT c1, c2 FROM t1 ORDER BY c2;
# 				+------+---------+
# 				| c1   | c2 	  |
# 				+------+---------+
# 				| 1 	 | a 		  |
# 				| 101  | b 		  |
# 				| 5 	 | c 		  |
# 				| 102  | d 		  |
# 				+------+---------+
#
# 			The next available auto-increment value is 103 because the auto-increment values
# 			are allocated one at a time, not all at once at the beginning of statement execution.
#
# 			This result is true whether or not there are concurrently executing "INSERT-like"
# 			statements (of any type)
#
# 			With innodb_autoinc_lock_mode set to 1 ("consecutive"), the four new rows are also:
#
# 				SELECT c1, c2 FROM t1 ORDER BY c2;
# 				+-----+------+
# 				| c1  | c2 	 |
# 				+-----+------+
# 				| 1   | a    |
# 				| 101 | b 	 |
# 				| 5 	| c 	 |
# 				| 102 | d 	 |
# 				+-----+------+
#
# 			However, in this case, the next available auto-increment value is 105, not 103, because
# 			four auto-increment values are allocated at the time the statement is processed, but
# 			only two are used.
#
# 			The result is true whether or not there are concurrently executing "INSERT-like" statements
# 			(of any type)
#
# 			With innodb_autoinc_lock_mode set to mode 2 ("interleaved"), the four new rows are:
#
# 				SELECT c1, c2 FROM t1 ORDER BY c2;
# 				+------+------+
# 				| c1   | c2   |
# 				+------+------+
# 				| 1 	 | a 	  |
# 				| x    | b 	  |
# 				| 5    | c 	  |
# 				| y 	 | d 	  |
# 				+------+------+
#
# 			The values of x and y are unique and larger than any previously generated rows.
#
# 			However, the specific values of x and y depend on the number of auto-increment
# 			values generated by concurrently executing statements.
#
# 			Finally, consider the following statement, issued when the most-recently generated
# 			sequence number is 100:
#
# 				INSERT INTO t1 (c1, c2) VALUES (1, 'a'), (NULL, 'b'), (101, 'c'), (NULL, 'd');
#
# 			With any innodb_autoinc_lock_mode setting, this statement generates a duplicate-key error
# 			23000 (Can't write; duplicate key in table) because 101 is allocated for the row (NULL, 'b'),
# 			and insertion of the row (101, 'c') fails.
#
# 		) Modifying AUTO_INCREMENT column values in the middle of a sequence of INSERT statements
#
# 			In MySQL 5.7 and earlier, modifying an AUTO_INCREMENT column value in the middle of a sequence
# 			of INSERT statements could lead to "Duplicate entry" errors.
#
# 			For example, if you performed an UPDATE operation that changed an AUTO_INCREMENT column value
# 			to a value larger than the current maximum auto-increment value, subsequent INSERT operations
# 			that did not specify an unused auto-increment value could encounter "Duplicate entry" errors.
#
# 			In MySQL 8.0 and later, if you modify an AUTO_INCREMENT column value to a value larger than the
# 			current maximum auto-increment value, the new value is persisted, and subsequent INSERT operations
# 			allocate auto-increment values starting from the new, larger value.
#
# 			This behavior is demonstrated in the following example.
#
# 				CREATE TABLE t1 (
# 					c1 INT NOT NULL AUTO_INCREMENT,
# 					PRIMARY KEY (c1)
# 					) ENGINE = InnoDB;
#
# 				INSERT INTO t1 VALUES(0), (0), (3);
#
# 				SELECT c1 FROM t1;
# 				+-----+
# 				| c1  |
# 				+-----+
# 				| 1   |
# 				| 2   |
# 				| 3   |
# 				+-----+
#
# 				UPDATE t1 SET c1 = 4 WHERE c1 = 1;
#
# 				SELECT c1 FROM t1;
# 				+----+
# 				| c1 |
# 				+----+
# 				| 2  |
# 				| 3  |
# 				| 4  |
# 				+----+
#
# 				INSERT INTO t1 VALUES(0);
#
# 				SELECT c1 FROM t1;
# 				+----+
# 				| c1 |
# 				+----+
# 				| 2  |
# 				| 3  |
# 				| 4  |
# 				| 5  |
# 				+----+
#
# InnoDB AUTO_INCREMENT COUNTER INITIALIZATION
#
# This section describes how InnoDB initializes AUTO_INCREMENT counters.
#
# If you specify an AUTO_INCREMENT column for an InnoDB table, the in-memory table object
# contains a special counter called the auto-increment counter that is used when assigning
# new values for the column.
#
# In MySQl 5.7 and earlier, the auto-increment counter is stored only in main memory, not on disk.
#
# To initialize an auto-increment counter after a server restart, InnoDB would execute the equivalent
# of the following statement on the first insert into a table containing an AUTO_INCREMENT column.
#
# 		SELECT MAX(ai_col) FROM table_name FOR UPDATE;
#
# In MySQL 8.0, this behavior is changed.
#
# The current maximum auto-increment counter value is written to the redo log each time it changes
# and is saved to an engine-private system table on each checkpoint.
#
# These changes make the current maximum auto-increment counter value persistent across server
# restarts.
#
# On a server restart following a normal shutdown, InnoDB initializes the in-memory auto-increment
# counter using the current maximum auto-increment value stored in the data dictionary system table.
#
# On a server restart during crash recovery, InnoDB initializes the in-memory auto-increment counter
# using the current maximum auto-increment value stored in the data dictionary system table and scans
# the redo log for auto-increment counter values written since the last checkpoint.
#
# If a redo-logged value is greater than the in-memory counter value, the redo-logged value is applied.
#
# However, in the case of a server crash, reuse of a previously allocated auto-increment value
# cannot be guaranteed.
#
# Each time the current maximum auto-increment value is changed due to an INSERT or UPDATE operation,
# the new value is written to the redo log, but if the crash occurs before the redo log is flushed
# to disk, the previously allocated value could be reused when the auto-increment counter is initialized
# after the server is restarted.
#
# The only circumstance in which InnoDB uses the equivalent of a SELECT MAX(ai_col) FROM table_name FOR UPDATE
# statement in MySQL 8.0 and later to initialize an auto-increment counter is when importing a tablespace
# without a .cfg metadata file.
#
# Otherwise, the current maximum auto-increment counter value is read from the .cfg metadata file.
#
# In MysQL 5.7 and earlier, a server restart cancels the effect of the AUTO_INCREMENT = N table option,
# which may be used in a CREATE TABLE or ALTER TABLE statement to set an intiial counter value or
# alter the existing counter value, respectively.
#
# In MySQL 8.0, a server restart does not cancel the effect of the AUTO_INCREMENT = N table
# option.
#
# If you initialize the auto-increment counter to a specific value, or if you alter the auto-increment
# counter value to a larger value, the new value is persisted across server restarts.
#
# NOTE:
#
#		ALTER_TABLE_..._AUTO_INCREMENT_=_N can only change the auto-increment counter value to a value
# 		larger than the current maximum.
#
# In MySQL 5.7 and earlier, a server restart immediately following a ROLLBACK operation could
# result in the reuse of auto-increment values that were previously allocated to the rolled-back
# transaction, effectively rolling back the current maximum auto-increment value.
#
# In MySQL 8.0, the current maximum auto-increment value is persisted, preventing the reuse of
# previously allocated values.
#
# If a SHOW_TABLE_STATUS statement examines a table before the auto-increment counter is initialized,
# InnoDB opens the table and initializes the counter value using the current maximum auto-increment
# value that is stored in the data dictionary system table.
#
# The value is stored in memory for use by later inserts or updates. Initialization of the counter-value
# uses a normal exclusive-locking read on the table which lasts to the end of the transaction.

# InnoDB follows the same procedure when intitializing the auto-increment counter for a newly created
# table that has a user-specified auto-increment value that is greater than 0.
#
# After the auto-increment counter is initialized, if you do not explicitly specify an auto-increment
# value when inserting a row, InnoDB implicitly increments the counter and assigns the new value to the
# column.

# If you insert a row that explicitly specifies an auto-increment column value, and the value is greater
# than the current maximum counter value, the counter is set to the specified value.
#
# InnoDB uses the in-memory auto-increment counter as long as the server runs. When the server is stopped
# and restarted, InnoDB reinitializes the auto-increment counter, as described earlier.
#
# The auto_increment_offset configuration option determines the starting point for the AUTO_INCREMENT
# column value.
#
# The default setting is 1.
#
# The auto_increment_increment configuration option controls the interval between successive column values.
# THe default setting is 1.
#
# 15.6.1.5 InnoDB AND FOREIGN KEY CONSTRAINTS
#
# How the InnoDB storage engine handles foreign key constraints is described under the following topics in this section:
#
# 		) Foreign Key Definitions

# 		) Referential Actions
#
# 		) Foreign Key Restrictions for Generated Columns and Virtual Indexes
#
# For foreign key usage information and examples, see Section 13.1.20.6, "Using FOREIGN KEY Constarints"
#
# Foreign Key Definitions
#
# Foreign key definitions for InnoDB tables are subject to the following conditions:
#
# 	) InnoDB permits a foreign key to reference any index column or group of columns.
#
# 		However, in the referenced table, there must be an index where the referenced
# 		columns are the first columns in the same order. Hidden columns that InnoDB adds
# 		to an index are also considered (see Section 15.6.2.1, "Clustered and Secondary Indexes")
#
# 	) InnoDB does not currently support foreign keys for tables with user-defined partitioning.
#
# 		This means that no user-partitioned InnoDB table may contain foreign key references
# 		or columns referenced by foreign keys.
#
# 	) InnoDB allows a foreign key constraint to reference a nonunique key. This is an InnoDB extension to standard SQL.
#
# Referential Actions
#
# 	Referential actions for foreign keys of InnoDB tables are subject to the following conditions:
#
# 		) While SET DEFAULT is allowed by the MySQL server, it is rejected as invalid by InnoDB. CREATE_TABLE and ALTER_TABLE
# 			statements using this clause are not allowed for InnoDB tables.
#
# 		) 	If there are several rows in the parent table that have the same referenced key value, InnoDB acts in foreign key checks
# 			as if the other parent rows with the same key value does not exist.
#
# 			For example, if you have defined a RESTRICT type constraint, and there is a child row with several parent rows, InnoDB
# 			does not permit the deletion of any of those parent rows.
#
# 		) InnoDB performs cascading operations through a depth-first algorithm, based on records in the indexes corresponding to the
# 			foreign key constraints.
#
# 		) If ON UPDATE CASCADE or ON UPDATE SET NULL recurses to update the same table it has previously updated during the cascade, it acts
# 			like RESTRICT.
#
# 			This means that you cannot use self-referential ON UPDATE CASCADE or ON UPDATE SET NULL operations.
#
# 			This is to prevent infinite loops resulting from cascaded updates. A self-referential ON DELETE SET NULL, on the other hand,
# 			is possible, as is a self-referential ON DELETE CASCADE.
#
# 			Cascading operations may not be nested more than 15 levels deep.
#
# 		) Like MySQL in general, in an SQL statement that inserts, deletes, or updates many rows, InnoDB checks UNIQUE and FOREIGN KEY
# 			constraints row-by-row.
#
# 			When performing foreign key checks, InnoDB sets shared row-level locks on child or parent records it has to look at .
#
# 			InnoDB checks foreign key constraints immediately; the check is not deferred to transaction commit. According to the SQL standard,
# 			the default behavior should be deferred checking.
#
# 			That is, constraints are only checked after the entire SQL statement has been processed.

# 			Until InnoDB implements deferred constraint checking, some things are impossible, such as deleting a record that
# 			refers to itself using a foreign key.
#
# Foreign Key Restrictions for Generated Columns and Virtual Indexes
#
# 	) A foreign key constraint on a stored generated column cannot use CASCADE, SET NULL, or SET DEFAULT as ON UPDATE referential
# 		actions, nor can it use SET NULL or SET DEFAULT as ON DELETE referential actions.
#
#  ) A foreign key constraint on the base column of a stored generated column cannot use CASCADE, SET NULL, or SET DEFAULT as ON UPDATE
# 		or ON DELETE referential actions.
#
# 	) A foreign key constraint cannot reference a virtual generated column.
#
# 	) Prior to MySQL 8.0, a foreign key constraint cannot reference a secondary index defined on a virtual generated column.
#
# 15.6.1.6 Limits on InnoDB Tables
#
# Limits on InnoDB tables are described under the following topics in this section:
#
# 		) Maximums and Minimums
#
# 		) Restrictions on InnoDB Tables
#
# 		) Locking and Transactions
#
# 		Warning:
#
# 			Before using NFS with InnoDB, review potential issues outlined in Using NFS with MySQL.
#
# Maximums and Minimums
#
# 	) A table can contain a maximum of 1017 columns. Virtual generated columns are included in this limit.
#
# 	) A table can contain a maximum of 64 secondary indexes.
#
# 	) The index key prefix length limit is 3072 bytes for InnoDB tables that use DYNAMIC or COMPRESSED row format.
#
# 		The index key prefix length limit is 767 bytes for InnoDB tables that use REDUNDANT or COMPACT row format.
#
# 		For example, you might hit this limit with a column prefix index of more than 191 characters on a TEXT
# 		or VARCHAR column, assuming a utf8mb4 character set and the maximum of 4 bytes for each character.
#
# 		Attempting to use an index key prefix length that exceeds the limit returns an error.
#
# 		The limits that apply to index key prefixes also apply to full-column index keys.
#
# 	) If you reduce the InnoDB page size to 8kb or 4kb by specifying the innodb_page_size option when creating
# 		the MySQL instance, the maximum length of the index key is lowered proportionally, based on the limit of
# 		3072 bytes for a 16kb page size.
#
# 		That is, the maximum index key length is 1536 bytes when the page size is 8kb, and 768 bytes when the
# 		page size is 4kb.
#
# 	) A maximum of 16 columns is permitted for multicolumn indexes. Exceeding the limit returns an error.
#
# 		ERROR 1070 (42000): Too many key parts specified; max 16 parts allowed
#
# 	) The maximum row length, except for variable-length columns (VARBINARY, VARCHAR, BLOB and TEXT), is slightly
# 		less than half of a page for 4kb, 8kb, 16kb and 32kb page sizes.
#
# 		For example, the maximum row length for the default innodb_page_size of 16kb is about 8000 bytes.
#
# 		However, for an InnoDB page size of 64kb, the maximum row length is approximately 16k bytes.
#
# 		LONGBLOB and LONGTEXT columns must be less than 4GB, and the total row length, including BLOB and TEXT
# 		columns, must be less than 4GB.
#
# 		If a row is less than half a page long, all of it is stored locally within the page.
#
# 		If it exceeds half a page, variable-length columns are chosen for external off-page storage until
# 		the row fits within half a page, as described in Section 15.11.2, "File Space Management"
#
# 	) Although InnoDB supports row sizes larger than 65,535 bytes internally, MySQL itself imposes a row-size limit
# 		of 65,535 for the combined size of all columns:
#
# 		CREATE TABLE t (a VARCHAR(8000), b VARCHAR(10000),
# 		c VARCHAR(10000), d VARCHAR(10000), e VARCHAR(10000),
# 		f VARCHAR(10000), g VARCHAR(10000)) ENGINE=InnoDB;
# 		ERROR 1118 (42000): Row size too large. The maximum row size for the
# 		used table type, not counting BLOBs, is 65535. You have to change some
# 		columns to TEXT or BLOBs.
#
# 		See SECTION C.10.4, "LIMITS ON TABLE COLUMN COUNT AND ROW SIZE"
#
# 	) On some older operating systems, files must be less than 2GB. This is not a limitation
# 		of InnoDB itself, but if you require a large tablespace, configure it using several
# 		smaller data files rather than one large data file.
#
# 	) The combined size of the InnoDB log files can be up to 512GB.
#
# 	) The minimum tablespace size is slightly larger than 10MB. The maximum tablespace size depends
# 		on the InnoDB page size.
#
# 		Table 15.3 InnoDB Maximum Tablespace Size
#
# 			InnoDB Page Size 		Maximum Tablespace Size
#
# 			4kb 						16TB
# 			8kb 						32TB
# 			16kb 						64TB
# 			32kb 						128TB
# 			64kb 						256TB
#
# 		The maximum tablespace size is also the maximum size for a table.
#
# 	) The path of a tablespace file, including the file name, cannot exceed the MAX_PATH limit on Windows.
#
# 		Prior to Windows 10, the MAX_PATH limit is 260 characters. As of Windows 10, version 1607, MAX_PATH
# 		limitations are removed from common Win32 file and directory functions, but you must enable the new
# 		behavior.
#
# 	) The default page size in InnoDB is 16kb. You can increase or decrease the page size by configuring the innodb_page_Size
# 		option when creating the MySQL instance.
#
# 		32kb and 64kb page sizes are supported, but ROW_FORMAT=COMPRESSED is unsupported for page sizes greater than
# 		16kb. For both 32KB and 64KB page sizes, the maximum record size is 16kb.
#
# 		For innodb_page_size = 32kb, extent size is 2MB.
#
# 		For innodb_page_size=64kb, extent size is 4MB.
#
# 		A MySQL instance using a particular InnoDB page size cannot use data files or log files from an instance
# 		that uses a different page size.
#
# Restrictions on InnoDB Tables
#
# 	) ANALYZE_TABLE determines index cardinality (as displayed in the Cardinality column of SHOW_INDEX output)
# 		by performing random dives on each of the index trees and updating index cardinality estimates accordingly.
#
# 		Because these are only estimates, repeated runs of ANALYZE_TABLE could produce different numbers.
#
# 		This makes ANALYZE_TABLE fast on InnoDB tables but not 100% accurate because it does not take all rows
# 		into account.
#
# 		You can make the statistics collected by ANALYZE_TABLE more precise and more stable by turning on the
# 		innodb_stats_persistent configuration option, as explained in Section 15.8.10.1, "Configuring Persistent
# 		Optimizer Statistics Parameters"
#
# 		When that setting is enabled, it is important to run ANALYZE_TABLE after major changes to indexed column
# 		data, because the statistics are not recalculated periodically (such as after a server restart)
#
# 		If the persistent statistics setting is enabled, you can change the number of random dives by modifying
# 		the innodb_stats_persistent_sample_pages system variable.
#
# 		If the persistent statistics setting is disabled; modify the innodb_stats_transient_sample_pages system
# 		variable instead.
#
# 		MySQL uses index cardinality estimates in join optimization. If a join is not optimized in the right way,
# 		try using ANALYZE_TABLE.
#
# 		In the few cases that ANALYZE_TABLE does not produce values good enough for your particular tables, you can
# 		use FORCE INDEX with your queries to force the use of a particular index, or set the max_seeks_for_key
# 		system variable to ensure that MySQL prefers index lookups over table scans.
#
# 		See SECTION B.4.5, "Optimizer-Related Issues"
#
# 	) If statements or transactions are running on a table, and ANALYZE_TABLE is run on the same table followed by a 
# 		second ANALYZE_TABLE operation, the second ANALYZE_TABLE operation is blocked until the statements or transactions
# 		are completed.
#
# 		This behavior occurs because ANALYZE_TABLE marks the currently loaded table definition as obsolete when ANALYZE_TABLE
# 		is finished running.
#
# 		New statements or transactions (including a second ANALYZE_TABLE statement) must load the new table definition into the
# 		table cache, which cannot occur until currently running statements or transactions are completed and the old table
# 		definition is purged.
#
# 		Loading multiple concurrent table definitions is not supported.
#
# ) SHOW_TABLE_STATUS does not give accurate statistics on InnoDB tables except for the physical size reserved by the table.

# 		The row count is only a rough estimate used in SQL optimization.
#
# ) InnoDB does not keep an internal count of rows in a table because concurrent transactions might "see" different number
# 		of rows at the same time.
#
# 		Consequently, SELECT COUNT(*) statements only count rows visible to the current transaction.
#
# 		For information about how InnoDB proceses SELECT COUNT(*) statements, refer to the COUNT() description in 
# 		Section 12.20.1, "Aggregate (GROUP BY) Function Descriptions"
#
# ) On Windows, InnoDB always stores database and table names internally in lowercase. To move database in a binary format
#   from Unix to Windows or from Windows to Unix, create all databases and tables using lowercase names.
#
# ) An AUTO_INCREMENT column ai_col must be defined as part of an index such that it is possible to perform the equivalent
# 	of an indexed SELECT MAX(ai_col) lookup on the table to obtain the maximum column value.
#
#  Typically, this is achieved by making the column the first column of some table index.
#
# ) InnoDB sets an exclusive lock on the end of the index associated with the AUTO_INCREMENT column while initializing a previously
# 	specified AUTO_INCREMENT column on a table.
#
# With innodb_autoinc_lock_mode=0, InnoDB uses a special AUTO-INC table lock mode where the lock is obtained and held to the end
# of the current SQL statement while accessing the auto-increment counter.
#
# Other clients cannot insert into the table while the AUTO-INC table lock is held. The same behavior occurs for "bulk inserts"
# with innodb_autoinc_lock_mode=1.
#
# Table-level AUTO-INC locks are not used with innodb_autoinc_lock_mode=2. For more information, see Section 15.6.1.4, "AUTO_INCREMENT
# Handling in InnoDB"
#
# ) When an AUTO_INCREMENT integer column runs out of values, a subsequent INSERT operation returns a duplicate-key error.
#
# This is general MySQL behavior.
#
# ) DELETE FROM tbl_name does not regenerate the table but instead deletes all rows, one by one.
#
# ) Cascaded foreign key actions do not activate triggers.
#
# ) You cannot create a table with a column name that matches the name of an internal INnoDB column (including DB_ROW_ID, DB_TRX_ID, DB_ROLL_PTR and DB_MIX_ID)
# 		This restriction applies to use of the names in any letter case.
#
# 		CREATE TABLE t1 (c1 INT, db_row_id INT) ENGINE=INNODB;
# 		ERROR 1166 (42000): Incorrect column name 'db_row_id'
#
# Locking and Transactions
#
# ) LOCK_TABLES acquires two locks on each table if innodb_table_locks=1 (the default). In addition to a table lock on the MySQL layer, it also
# 	acquires an InnoDB table lock.
#
#  Versions of MySQL before 4.1.2 did not acquire InnoDB table locks; the old behavior can be selected by setting innodb_table_locks=0.

#  If no InnoDB table lock is acquired, LOCK_TABLES completes even if some records of the tables are being locked by other transactions.
#
# In MySQL 8.0, innodb_table_locks=0 has no effect for tables locked explicitly with LOCK_TABLES_..._WRITE. It does have an effect for tables
# locked for read or write by LOCK_TABLES_..._WRITE implicitly (for example, through triggers) or by LOCK_TABLES_..._READ.
#
# ) All InnoDB locks held by a transaction are released when the transaction is committed or aborted. Thus, it does not make much sense to invoke
# 	LOCK_TABLES on InnoDB tables in autocommit=1 mode because the acquired InnoDB table locks would be released immediately.
#
# ) You cannot lock additional tables in the middle of a transaction because LOCK_TABLES performs an implicit COMMIT and UNLOCK_TABLES
#
# ) For limits associated with concurrent read-write transactions, see SECTION 15.6.6, "Undo logs"
#
# 15.6.2 INDEXES
#
# 15.6.2.1 Clustered and Secondary Indexes
# 15.6.2.2 The Physical Structure of an InnoDB Index
# 15.6.2.3 Sorted Index Builds
# 15.6.2.4 InnoDB FULLTEXT Indexes
#
# This section covers topics related to InnoDB indexes.
#
# 15.6.2.1 Clustered and Secondary Indexes
#
# Every InnoDB table has a special index called the clustered index where the data for the rows is stored.
#
# Typically, the clustered index is synonymous with the primary key. To get hte best performance from queries,
# inserts, and other database operations, you must understand how InnoDB uses the clustered index to optimize the
# most common lookup and DML operations for each table.
#
# 	) When you define a PRIMARY KEY on your table, InnoDB uses it as the clustered index. Define a primary key for each table
# 		that you create.
#
# 		If there is no logical unique and non-null column or set of columns, add a new auto-increment column, whose values
# 		are filled in automatically.
#
# 	) If you do not define a PRIMARY KEY for your table, MySQL locates the first UNIQUE index where all the key columns are NOT NULL
# 		and InnoDB uses it as the clustered index.
#
# 	) If the table has no PRIMARY KEY or suitable UNIQUE index, InnoDB internally generates a hidden clustered index named GEN_CLUST_INDEX
# 		on a synthetic column containing row ID values.
#
# 		The rows are ordered by the ID that InnoDB assigns to the rows in such a table.
#
# 		The row ID is a 6-byte field that increases monotonically as new rows are inserted.
# 		Thus, the rows ordered by the row ID are physically in insertion order.
#
# How the clustered Index speeds up Queries
#
# Accessing a row through the clustered index is fast because the index search leads directly to the page with all the row data.
# If a table is large, the clustered index architechture often saves a disk I/O operation when compared to storage organizations
# that store row data using a different page from the index record.
#
# How Secondary INdexes Relate to the Clustered Index
#
# All indexes other than the clustered index are known as secondary indexes. In InnoDB, each record in a secondary index contains the
# primary key columns for the row, as well as the columns specified for the secondary index.
#
# InnoDB uses this primary key value to search for the row in the clustered index.
#
# If the primary key is long, the secondary indexes use more space, so it is advantageous to have a short primary key.
#
# For guidelines to take advantage of InnoDB clustered and secondary indexes, see Section 8.3, "Optimization and Indexes"
#
# 15.6.2.2 THE PHYSICAL STRUCTURE OF AN INNODB INDEX
#
# With the exception of spatial indexes, InnoDB indexes are B-tree data structures. Spatial indexes use R-trees, which are specialized
# data structures for indexing multi-dimensional data.
#
# Index records are stored in the leaf pages of their B-tree or R-tree data structure. The default size of an index page is 16kb.
#
# When new records are inserted into an InnoDB clustered index, InnoDB tries to leave 1/16 of the page free for future insertions
# and updates of the index records.
#
# If index records are inserted in a sequential order (ascending or descending), the resulting index pages are about 15/16 full.
#
# If records are inserted in a random order, the pages are from 1/2 to 15/16 full.
#
# InnoDB performs a bulk load when creating or rebuilding B-tree indexes. This method of index creation is known as a sorted index build.
#
# The innodb_fill_factor configuration option defines the percentage of space on each B-tree page that is filled during a sorted index build,
# with the remaining space reserved for future index growth.
#
# Sorted index builds are not supported for spatial indexes. For more information, see Section 15.6.2.3, "Sorted Index Builds"
#
# An innodb_fill_factor setting of 100 leaves 1/16 of the space in clustered index pages free for future index growth.
#
# If the fill factor of an InnoDB index page drops below the MERGE_THRESHOLD, which is 50% by default if not specified, InnoDB
# tries to contract the index tree to free the page.
#
# The MERGE_THRESHOLD setting applies to both B-tree and R-tree indexes. For more information, see SECTION 15.8.11, "Configuring the Merge Threshold for Index Pages"
#
# You can define the page size for all InnoDB tablespaces in a MySQL instance by setting the innodb_page_size configuration option prior to initializing the MySQL
# instance.
#
# Once the page size for an instance is defined, you cannot change it without reinitializing the instance. Supported sizes are 64kb, 32kb, 16kb (default), 8kb and 4kb.
#
# A MySQL instance using a particular InnoDB page size cannot use data files or log files from an instance that uses a different page size.
#
# 15.6.2.3 SORTED INDEX BUILDS
#
# InnoDB performs a bulk load instead of inserting one index record at a time when creating or rebuilding indexes. 
#
# This method of index creation is also known as a sorted index build. Sorted index builds are not supported for spatial indexes.
#
# There are three phases to an index build. In the first phase, the clustered index is scanned, and index entries are generated and
# added to the sort buffer.
#
# When the sort buffer becomes full, entries are sorted and written out to a temporary intermediate file.
#
# THis process is also known as a "run". In the second phase, with one or more runs written ot the temporary
# intermediate files, a merge sort is performed on all entries in the file.
#
# In the third and final phase, the sorted entries are inserted into the B-Tree.
#
# Prior to the introduction of sorted index builds, index entries were inserted into the B-tree one record at a time using insert APIs.
#
# This method involved opening a B-tree cursor to find the insert pos and then inserting entries into a B-tree page using an optimistic
# insert.
#
# If an insert failed due to page being full, a pessimistic insert would be performed, which involves opening a B-tree cursor and splitting
# and merging B-tree nodes as necessary to find space for the entry.
#
# The drawbacks of this "top-down" method of building an index are the cost of searching for an insert position and the constant splitting
# and merging of B-tree nodes.
#
# Sorted index builds use a "bottom-up" approach to building an index. With this approach, a reference to the right-most leaf page is held
# at all levels of the B-tree.
#
# The right-most leaf page at the necessary B-tree depth is allocated and entries are inserted according to their sorted order.
#
# Once a leaf page is full, a node pointer is appended to the parent page and a sibling leaf page is allocated for the next insert.
#
# This process continues until all entries are inserted, which may result in inserts up to the root level.
#
# When a sibling page is allocated, the reference to the previously pinned leaf page is released, and the newly allocated leaf page becomes
# the right-most leaf page and new default insert location.
#
# RESERVING B-TREE PAGE SPACE FOR FUTURE INDEX GROWTH
#
# To set aside space for future index growth, you can use the innodb_fill_factor configuration option to reserve a percentage of
# B-tree page space. For example, setting innodb_fill_factor to 80 reserves 20 percent of the space in B-tree pages during a sorted
# index build.
#
# This setting applies to both B-tree leaf and non-leaf pages. It does not apply to external pages used for TEXT or BLOB entries.
#
# The amount of space that is reserved may not be exactly as configured, as the innodb_fill_factor value is interpreted as a hint
# rather than a hard limit.
#
# SORTED INDEX BUILDS AND FULL-TEXT INDEX SUPPORT
#
# Sorted index builds are supported for fulltext indexes. Previously, SQL was used to insert entries into a fulltext index.
#
# SORTED INDEX BUILDS AND COMPRESSED TABLES
#
# For compressed tables, the previous index creation method appended entries to both compressed and uncompressed pages.
# When the modification log (representing free space on the compressed page) became full, the compressed page would be
# recompressed.
#
# If compression failed due to a lack of space, the page would be split.
#
# With sorted index builds, entries are only appended to uncompressed pages. When an uncompressed page becomes full,
# it is compressed.
#
# Adaptive padding is used to ensure that compression succeeds in most cases, but if compression fails, the page is split
# and compression is attempted again.
#
# This process continues until compression is successful.
#
# For more information about compression of B-tree pages, see SECTION 15.9.1.5, "HOW COMPRESSION WORKS FOR INNODB TABLES"
#
# SORTED INDEX BUILDS AND REDO LOGGING
#
# Redo logging is disabled during a sorted index build. Instead, there is a checkpoint to ensure that the index build
# can withstand a crash or failure.
#
# The checkpoint forces a write of all dirty pages to disk. During a sorted index build, the page cleaner thread is signaled
# periodically to flush dirty pages to ensure that the checkpoint operation can be processed quickly.
#
# Normally, the page cleaner thread flushes dirty pages when the number of clean pages falls below a set threshold.
#
# For sorted index builds, dirty pages are flushed promptly to reduce checkpoint overhead and to parallelize I/O and CPU
# activity.
#
# SORTED INDEX BUILDS AND OPTIMIZER STATISTICS
#
# Sorted index builds may result in optimizer statistics that differ from those generated by the previous method of index creation.
# The difference in statistics, which is not expected to affect workload performance, is due to the different algorithm
# used to populate the index.
#
# 15.6.2.4 InnoDB FULLTEXT INDEXES
#
# FULLTEXT indexes are created on text-based columns (CHAR, VARCHAR or TEXT columns) to help speed up queries and DML operations
# on data contained within those columns, omitting any words that are defined as stopwords.
#
# A FULLTEXT index is defined as part of a CREATE_TABLE statement or added to an existing table using ALTER_TABLE or CREATE_INDEX.
#
# Full-text search is performed using MATCH()_..._AGAINST syntax. For usage information, see SECTION 12.9, "FULL-TEXT SEARCH FUNCTIONS".
#
# InnoDB FULLTEXT indexes are described under the following topics in this section:
#
# 		) InnoDB Full-Text Index Design
#
# 		) InnoDB Full-Text Index Tables
#
# 		) InnoDB Full-Text Index Cache
#
# 		) InnoDB Full-Text Index Document ID and FTS_DOC_ID Column
#
# 		) InnoDB Full-Text Index Deletion Handling
#
# 		) InnoDB Full-Text Index Transaction Handling
#
# 		) Monitoring InnoDB Full-Text Indexes
#
# InnoDB Full-Text Index Design
#
# InnoDB FULLTEXT indexes have an inverted index design. Inverted indexes store a list of words, and for each word, a list of
# documents that the word appears in.
#
# To support proximity search, position information for each word is also stored, as a byte offset.
#
# InnoDB Full-Text index Tables
#
# When creating an InnoDB FULLTEXT index, a set of index tables is created, as shown in the following example:
#
# 		CREATE TABLE opening_lines (
# 			id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY,
# 			opening_line TEXT(500),
# 			author VARCHAR(200),
# 			title VARCHAR(200),
# 			FULLTEXT idx (opening_line)
# 			) ENGINE=InnoDB;
#
# 		SELECT table_id, name, space from INFORMATION_SCHEMA.INNODB_TABLES WHERE name LIKE 'test/%';
# 		+-----------+---------------------------------------------------------+-------------+
# 		| table_id  | name 																    | space 	   |
# 		+-----------+---------------------------------------------------------+-------------+
# 		| 333 		| test/fts_000000000000000000000147_0000000001c9_index_1  | 289 			|
# 		etc.
#
# The first six tables represent the inverted index and are referred to as auxilliary index tables.
#
# When incoming documents are tokenized, the individual words (also referred to as "tokens") are inserted
# into the index tables along with position information and the associated Document ID (DOC_ID).
#
# The words are fully sorted and partitioned among the six index tables based on the character set sort
# weight of the word's first character.
#
# The inverted index is partitioned into six auxiliary index tables to support parallel index creation.
#
# By default, two threads tokenize, sort, and insert words and associated data into the index tables.
#
# The number of threads is configurable using the innodb_ft_sort_pll_degree option.
#
# Consider increasing the number of threads when creating FULLTEXT indexes on large tables.
#
# Auxiliary index table names are prefixed with fts_ and postfixed with index_*.

# Each index table is associated with the indexed table by a hex value in the index table name
# that matches the table_id of the indexed table.
#
# For example, the table_id of the test/opening_lines table is 327, for which the hex value is
# 0x147.
#
# As shown in the preceding example, the "147" hex value appears in the names of index tables
# that are associated with the test/opening_lines table.
#
# A hex value representing the index_id of the FULLTEXT index also appears in auxiliary index table names.
# For example, in the auxiliary table name test/fts_00000000000147_000000000001c9_index_1, the hex value
# 1c9 has a decimal value of 457.
#
# The index defined on the opening_lines table (idx) can be identified by querying the INFORMATION_SCHEMA.INNODB_INDEXES
# table for this value (457)
#
# SELECT index_id, name, table_id, space FROM INFORMATION_SCHEMA.INNODB_INDEXES
# WHERE index_id=457;
# 	+-----------+----------+--------------+-------------+
#  | index_id  | name 	  | table_id 	  | space 		 |
# 	+-----------+----------+--------------+-------------+
# 	| 457 	   | idx 	  | 327 			  | 283 			 |
# 	+-----------+----------+--------------+-------------+
#
# Index tables are stored in their own tablespace if the primary table is created in a file-per-table tablespace.
#
# The other index tables shown in the preceding example are referred to as common index tables and are used
# for deletion handling and storing the internal state of FULLTEXT indexes.
#
# Unlike the inverted index tables, which are created for each full-text index, this set of tables is common to
# all full-text indexes created on a particular table.
#
# Common auxiliary tables are retained even if full-text indexes are dropped. When a full-text index is dropped,
# the FTS_DOC_ID column that was created for the index is retained, as removing the FTS_DOC_ID column would
# require rebuilding the table.
#
# Common axiliary tables are required to manage the FTS_DOC_ID column.
#
# 	) fts_*_deleted and fts_*_deleted_cache
#
# 		Contain the document IDs (DOC_ID) for documents that are deleted but whose data is not yet removed from the full-text index.
#
# 		The fts_*_deleted_cache is in the in-memory version of the fts_*_deleted table.
#
# 	) fts_*_being_deleted and fts_*_being_deleted_cache
#
# 		Contain the document IDs (DOC_ID) for documents that are deleted and whose data is currently in the process of being
# 		removed from the full-text index.
#
# 		The fts_*_being_deleted_cache table is the in-memory version of the fts_*_being_deleted table.
#
# 	) fts_*_config
#
# 		Stores information about the internal state of the FULLTEXT index. Most importantly, it stores the FTS_SYNCED_DOC_ID,
# 		which identifies documents that have been parsed and flushed to disk.
#
# 		In case of crash recovery, FTS_SYNCED_DOC_ID values are used to identify documents that have not been flushed to disk
# 		so that the documents can be re-parsed and added back to the FULLTEXT index cache.
#
# 		To view the data in this table, query the INFORMATION_SCHEMA.INNODB_FT_CONFIG table.
#
# InnoDB Full-Text Index Cache
#
# When a document is inserted, it is tokenized, and the individual words and associated data are inserted into the
# FULLTEXT index.
#
# This process, even for small documents, could result in numerous small insertions into the auxiliary index tables,
# making concurrent access to these tables a point of contention.
#
# To avoid this problem, InnoDB uses a FULLTEXT index cache to temporarily cache index table insertions for recently
# inserted rows.
#
# This in-memory cache structure holds insertions until the cache is full and then batch flushes them to disk
# (to the auxiliary index tables)
#
# You can query the INFORMATION_SCHEMA.INNODB_FT_INDEX_CACHE table to view tokenized data for recently inserted rows.
#
# The caching and batch flushing behavior avoids frequent updates to auxiliary index tables, which could result in 
# concurrent access issues during busy insert and update times.
#
# The batching technique also avoids multiple insertions for the same word, and minimizes duplicate entries.
#
# Instead of flushing each word individually, insertions for the same word are merged and flushed to disk as a 
# single entry, improving insertion efficiency while keeping auxiliary index tables as small as possible.
#
# The innodb_ft_cache_size variable is used to configure the full-text index cache size (on a per-table basis),
# which affects how often the full-text index cache is flushed.
#
# You can also define a global full-text index cache size limit for all tables in a given instance using the
# innodb_ft_total_cache_size option.
#
# THe full-text index cache stores the same information as auxiliary index tables. However, the full-text index
# cache only caches tokenized data for recently inserted rows.
#
# The data that is already flushed to disk (to the full-text auxiliary tables) is not brought back into the full-text
# index cache when queries.
#
# The data in auxiliary index tables is queried directly, and results from the auxiliary index tables are merged with
# results from the full-text index cache before being returned.
#
# InnoDB Full-Text Index Document ID and FTS_DOC_ID Column
#
# InnoDB uses a unique document identifier referred to as a Document ID (DOC_ID) to map words in the full-text index to
# document records where the word appears.
#
# The mapping requires an FTS_DOC_ID column on the indexed table. If an FTS_DOC_ID column is not defined,, InnoDB
# automatically adds a hidden FTS_DOC_ID column when the full-text index is created.
#
# The following example demonstrates this behavior.
#
# The following table definition does not include an FTS_DOC_ID Column:
#
# 		CREATE TABLE opening_lines (
# 			id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY,
# 			opening_line TEXT(500),
# 			author VARCHAR(200),
# 			title VARCHAR(200),
# 			) ENGINE=InnoDB;
#
# When you create a full-text index on the table using CREATE FULLTEXT INDEX syntax, a warning is returned which reports
# that InnoDB is rebuilding the table to add the FTS_DOC_ID column.
#
#
# 		CREATE FULLTEXT INDEX idx ON opening_lines(opening_line);
# 		Query OK, 0 rows affected, 1 warning (0.19 sec)
# 		Records: 0 Duplicates: 0 Warnings: 1
#
# 		SHOW WARNINGS;
# 		+------------+-----------+----------------------------------------------------+
# 		| Level 	    | Code 		 | Message 														   |
# 		+------------+-----------+----------------------------------------------------+
# 		| Warning 	 | 124 		 | InnoDB rebuilding table to add column: FTS_DOC_ID  |
# 		+------------+-----------+----------------------------------------------------+
#
# The same warning is returned when using ALTER_TABLE to add a full-text index to a table that does not have an 
# FTS_DOC_ID column.
#
# If you create a full-text index at CREATE_TABLE time and do not specify an FTS_DOC_ID column, InnoDB adds a hidden
# FTS_DOC_ID column, without warning.
#
# Defining an FTS_DOC_ID column at CREATE_TABLE time is less expensive than creating a full-text index on a table
# that is already loaded with data.
#
# If an FTS_DOC_ID column is defined on a table prior to loading data, the table and its indexes do not have to be
# rebuilt to add the new column.
#
# If you are not concerned with CREATE FULLTEXT INDEX performance, leave out the FTS_DOC_ID column to have InnoDB
# create it for you.
#
# InnoDB creates a hidden FTS_DOC_ID column along with a unique index (FTS_DOC_ID_INDEX) on the FTS_DOC_ID column.
#
# If you want to create your own FTS_DOC_ID column, the column must be defined as BIGINT UNSIGNED NOT NULL and
# name FTS_DOC_ID (all upper case), as in the following example:
#
# 		Note:
#
# 			The FTS_DOC_ID column does not need to be defined as an AUTO_INCREMENT column, but AUTO_INCREMENT could make
# 			loading data easier.
#
# CREATE TABLE opening_lines (
# FTS_DOC_ID BIGINT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY,
# opening_line TEXT(500),
# author VARCHAR(200),
# title VARCHAR(200)
# ) ENGINE=InnoDB;
#
# If you choose to define the FTS_DOC_ID column yourself, you are responsible for managing the column to avoid empty
# or duplicate values.
#
# FTS_DOC_ID values cannot be reused, which means FTS_DOC_ID values must be ever increasing.
#
# Optionally, you can create the required unique FTS_DOC_ID_INDEX (all upper case) on the FTS_DOC_ID column.
#
# CREATE UNIQUE INDEX FTS_DOC_ID_INDEX on opening_lines(FTS_DOC_ID);
#
# If you do not create the FTS_DOC_ID_INDEX, InnoDB creates it automatically.
#
# Note:
#
# 		FTS_DOC_ID_INDEX cannot be defined as a descending index because the InnoDB SQL parser does not use descending indexes.
#
# The permitted gap between the largest used FTS_DOC_ID value and new FTS_DOC_ID value is 65535.
#
# To avoid rebuilding the table, the FTS_DOC_ID column is retained when dropping a full-text index.
#
# InnoDB Full-Text Index Deletion Handling
#
# Deleting a record that has a full-text index column could result in numerous small deletions in the auxiliary index tables,
# making concurrent access to these tables a point of contention.
#
# To avoid this problem, the Document ID (DOC_ID) of a deleted document is logged in a special FTS_*_DELETED table whenever
# a record is deleted from an indexed table, and the indexed record remains in the full-text index.
#
# Before returning query results, information in the FTS_*_DELETED table is used to filter out deleted Document IDs.
#
# The benefit of this design is that deletions are fast and inexpensive. The drawback is that the size of the index is not
# immediately reduced after deleting records.
#
# To remove full-text index entries for deleted records, run OPTIMIZE TABLE on the indexed table with innodb_optimize_fulltext_only=ON
# to rebuild the full-text index.
#
# For more information, see Optimizing InnoDB Full-Text indexes.
#
# INNODB FULL-TEXT INDEX TRANSACTION HANDLING
#
# InnoDB FULLTEXT indexes have special transaction handling characteristics due to its caching and batch processing behavior.
#
# Specifically, updates and insertions on a FULLTEXT index are processed at transaction commit time, which means that a FULLTEXT
# search can only see committed data.
#
# The following example demonstrates this behavior.
#
# The FULLTEXT search only returns a result after the inserted lines are committed.
#
# CREATE TABLE opening_lines (
# 		id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY,
# 		opening_line TEXT(500),
# 		author VARCHAR(200),
# 		title VARCHAR(200),
# 		FULLTEXT idx (opening_line)
# 		) ENGINE=InnoDB;
#
# BEGIN;
#
# INSERT INTO opening_lines(opening_line,author,title) VALUES
# 		(// String values to insert//);
#
# SELECT COUNT(*) FROM opening_lines WHERE MATCH(opening_line) AGAINST (//STring value//);
# +-----------+
# | COUNT(*)  |
# +-----------+
# | 0 		  |
# +-----------+
#
# COMMIT;
#
# SELECT COUNT(*) FROM opening_lines WHERE MATCH(opening_line) AGAINST('//string value//');
# +-----------+
# | COUNT(*)  |
# +-----------+
# | 1 		  |
# +-----------+
#
# MONITORING INNODB FULL-TEXT INDEXES
#
# You can monitor and examine the special text-processing aspects of InnoDB FULLTEXT indexes by querying the following
# INFORMATION_SCHEMA tables:
#
# 		) INNODB_FT_CONFIG
#
# 		) INNODB_FT_INDEX_TABLE
#
# 		) INNODB_FT_INDEX_CACHE
#
# 		) INNODB_FT_DEFAULT_STOPWORD
#
# 		) INNODB_FT_DELETED
#
# 		) INNODB_FT_BEING_DELETED
#
# You can also view basic information for FULLTEXT indexes and tables by querying INNODB_INDEXES and INNODB_TABLES
#
# For more information, see SECTION 15.14.4, "InnoDB INFORMATION_SCHEMA FULLTEXT INDEX TABLES"
#
# 15.6.3 TABLESPACES
#
# 15.6.3.1 The System Tablespace
# 15.6.3.2 File-Per-Table Tablespaces
# 15.6.3.3 General Tablespaces
# 15.6.3.4 Undo Tablespaces
# 15.6.3.5 Temporary Tablespaces
# 15.6.3.6 Creating a Tablespace Outside of the Data Directory
# 15.6.3.7 Copying Tablespaces to Another Instance
# 15.6.3.8 Moving Tablespace Files While the Server is Offline
# 15.6.3.9 InnoDB Data-at-Rest Encryption
#
# This section covers topics related to InnoDB tablespaces.
#
# 15.6.3.1 The System Tablespace
#
# The InnoDB system tablespace is the storage area for the doublewrite buffer and the change buffer.
#
# The system tablespace also contains table and index data for user-created tables created in the system
# tablespace.
#
# In previous releases, the system tablespace contained the InnoDB data dictionary.
#
# In MySQL 8.0, InnoDB stores metadata in the MySQL data dictionary. See Chapter 14, MySQL Data Dictionary
#
# The system tablespace can have one or more data files. By default, one system tablespace data file, named ibdata1,
# is created in the data directory.
#
# The size and number of system tablespace data files is controlled by the innodb_data_file_path startup option.
#
# For related information, see System Tablespace Data File Configuration.
#
# Resizing the System Tablespace
#
# This section describes how to increase or decrease the size of the InnoDB system tablespace.
#
# Increasing the Size of the InnoDB System Tablespace
#
# The easiest way to increase the size of the InnoDB system tablespace is to configure it from the beginning
# to be auto-extending.
#
# Specify the autoextend attribute for the last data file in the tablespace definition.
#
# Then InnoDB increases the size of that file automatically in 64MB increments when it runs out of space.
#
# The increment size can be changed by setting the value of the innodb_autoextend_increment system variable,
# which is measured in megabytes.
#
# You can expand the system tablespace by a defined amount by adding another data file:
#
# 	1. Shut down the MySQL Server.
#
# 	2. If the previous last data file is defined with the keyword autoextend, change its definition to use a fixed size,
# 		based on how large it has actually grown.
#
# 		Check the size of the data file, round it down to the closest multiple of 1024 x 1024 bytes (= 1MB), and specify this
# 		rounded size explicitly in innodb_data_file_path.
#
# 	3. Add a new data file to the end of innodb_data_file_path, optionally making that file auto-extending. Only the last data file
# 		in the innodb_data_file_path can be specified as auto-extending.
#
# 	4. Start the MySQL server again.
#
# For example, this tablespace has just one auto-extending data file ibdata1:
#
# 		innodb_data_home_dir =
# 		innodb_data_file_path = /ibdata/ibdata1:10M:autoextend
#
# Suppose that this data file, over time, has grown to 988MB. Here is the configuration line after modifying the original data file
# to use a fixed size and adding a new auto-extending data file:
#
# 		innodb_data_home_dir =
# 		innodb_data_file_path = /ibdata/ibdata1:988M;/disk2/ibdata2:50M:autoextend
#
# When you add a new data file to the system tablespace configuration, make sure that the filename does not refer to an existing
# file.
#
# InnoDB creates and initializes the file when you restart the server.
#
# Decreasing The Size of the InnoDB System Tablespace
#
# You cannot remove a data file from the system tablespace. To decrease the system tablespace size, use this procedure:
#
# 		1. Use mysqldump to dump all your InnoDB tables, including InnoDB tables located in the MySQL database.
#
# 			SELECT TABLE_NAME from INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA='mysql' and ENGINE='InnoDB';
# 			+----------------------------------+
# 			| TABLE_NAME 							  |
# 			+----------------------------------+
# 			| columns_priv 						  |
# 			| / etc /
# 	
# 		2. Stop the server
#
# 		3. Remove all the existing tablespace files (*.ibd), including the ibdata and ib_log files. Do not forget to remove *.ibd files
# 			for tables located in the MySQL database.
#
# 		4. Configure a new tablespace.
#
# 		5. Restart the server
#
# 		6. Import the dump files
#
# NOTE:
#
# 		If your databases only use the InnoDB engine, it may be simpler to dump all databases, stop the server, remove all databases and
# 		InnoDB log files, restart the server, and import the dump files.
#
# USING RAW DISK PARTITIONS FOR THE SYSTEM TABLESPACE
#
# You can use raw disk partitions as data files in the InnoDB system tablespace. This technique enables nonbuffered I/O on Windows and
# on some Linux and Unix systems without file system overhead.
#
# Perform tests with and without raw partitions to verify whether this change actually improves performance on your system.
#
# When you use a raw disk partition, ensure that the user ID that runs the MySQL server has read and write privileges for that
# partition.
#
# For example, if you run the server as the mysql user, the partition must be readable and writable by mysql.
#
# If you run the server with the --memlock option, the server must be run as root, so the partition must be readable
# and writable by root.
#
# The procedures described below involve option file modification. For additional information, see Section 4.2.2.2, "Using OPtion Files"
#
# Allocating A Raw Disk Partition on Linux and Unix Systems
#
# 1. When you create a new data file, specify the keyword newraw immediately after the data file size for the innodb_data_file_path option.
#
# 		The partition must be at least as large as the size that you specify.
#
# 		Note that 1MB in InnoDB is 1024 x 1024 bytes, whereas 1MB in Disk specs usually means 1 mil bytes
#
# 			[mysqld]
# 			innodb_data_home_dir=
# 			innodb_data_file_path=/dev/hdd1:3Gnewraw;/dev/hdd2:2Gnewraw
#
# 2. Restart the server.
#
# 		InnoDB notices the newraw keyword and initializes the new partition. However, do not create or change any InnoDB
# 		tables yet.
#
# 		Otherwise, when you next restart the server, InnoDB reinitializes the partition and your changes are lost.
#
# 		(As a safety measure InnoDB prevents users from modifying data when any partition with newraw is specified)
#
# 3. After InnoDB has initialized the new partition, stop the server, change newraw in the data file specification to raw:
#
# 		[mysqld]
# 		innodb_data_home_dir=
# 		innodb_data_file_path=/dev/hdd1:3Graw;/dev/hdd2:2Graw
#
# 4. Restart the server. InnoDB now permits changes to be made.
#
# Allocating a Raw Disk Partition on Windows
#
# On Windows systems, the same steps and accompanying guidelines described for Linux and Unix systems apply except that
# the innodb_data_file_path setting differs slightly on Windows.
#
# 1. When you create a new data file, specify the keyword newraw immediately after the data file size for the innodb_data_file_path option:
#
# 		[mysqld]
# 		innodb_data_home_dir=
# 		innodb_data_file_path=//./D::10Gnewraw
#
# 		The //./ corresponds to the Windows syntax of \\.\ for accessing physical drives. In the example above, D: is the drive letter of the partition.
#
# 2. Restart the server. InnoDB notices the newraw keyword and initializes the new partition.
#
# 3. After InnoDB has initialized the new partition, stop the server, change newraw in the data file specification to raw:
#
# 		[mysqld]
# 		innodb_data_home_dir=
# 		innodb_data_file_path=//./D::10Graw
#
# 4. Restart the server. InnoDB now permits changes to be made.
#
# 15.6.3.2 File-Per-Table Tablespaces
#
# Historically, InnoDB tables were stored in the system tablespace.
#
# This monolithic approach was targeted at machines dedicated to database processing, with carefully
# planned data growth, where any disk storage allocated to MySQL would never be needed for other purposes.
#
# The file-per-table tablespace feature provides a more flexible alternative, where each InnoDB table is stored
# in its own tablespace data file (.ibd file).
#
# This feature is controlled by the innodb_file_per_table configuration option, which is enabled by default.
#
# Advantages 
#
# 	) You can reclaim disk space when truncating or dropping a table stored in a file-per-table tablespace.
#
# 		Truncating or dropping tables stored in the shared system tablespace creates free space internally
# 		in the system tablespace data files (ibdata files) which can only be used for new InnoDB data.
#
# 		Similarly, a table-copying ALTER_TABLE operation on table that resides in a shared tablespace can
# 		increase the amount of space used by the tablespace.
#
# 		Such operations may require as much additional space as the data in the table plus indexes.
#
# 		The additional space required for the table-copying ALTER_TABLE operation is not released
# 		back to the OS as it is for file-per-table tablespaces.
#
# ) The TRUNCATE_TABLE operation is faster when run on tables stored in file-per-table tablespaces.
#
# ) You can store specific tables on separate storage devices, for I/O optimization, space management, or backup
# 		purposes by specifying the location of each table using the syntax:
#
# 			CREATE TABLE ... DATA DIRECTORY = absolute_path_to_directory
#
# 		as explained in SECTION 15.6.3.6, "Creating a Tablespace Outside of the Data Directory"
#
# ) You can run OPTIMIZE_TABLE to compact or recreate a file-per-table tablespace. When you run an OPTIMIZE_TABLE,
# 		InnoDB creates a new .ibd file with a temporary name, using only the space required to store actual data.
#
# 		When the optimization is complete, InnoDB removes the old .ibd file and replaces it with the new one.
#
# 		If the previous .ibd file grew significantly but the actual data only accounted for a portion of its size,
# 		running OPTIMIZE_TABLE can reclaim the unused space.
#
# ) You can move individual InnoDB tables rather than entire databases.
#
# ) You can copy individual InnoDB tables from one MySQL instance to another (known as the transportable tablespace feature)
#
# ) Tables created in file-per-table tablespaces support features associated with compressed and dynamic row formats.
#
# ) You can enable more efficient storage for tables with large BLOB or TEXT columns using the dynamic row format.
#
# ) File-per-table tablespaces may improve chances for a successful recovery and save time when a corruption occurs,
# 		when a server cannot be restarted, or when backup and binary logs are unavailable.
#
# ) You can back up or restore individual tables quickly using the MySQL Enterprise Backup product, without interrupting the
# 		use of other InnoDB tables.
#
# 		This is beneficial if you have tables that require backup less frequently or on a different backup schedule.
#
# 		See Making a Partial Backup for details.
#
# ) File-per-table tablespaces are convenient for per-table status reporting when copying or backing up tables.
#
# ) You can monitor table size at a file system level without accessing MySQL.
#
# ) Common Linux file systems do not permit concurrent writes to a single file when innodb_flush_method is set to O_DIRECT.
# 		As a result, there are possible performance improvements when using file-per-table tablespaces in conjunction
# 		with innodb_flush_method
#
# ) The system tablespace stores the data dictionary and undo logs, and is limited in size by InnoDB tablespace size limits.
#
# 		See Section 15.6.1.6, "Limits on InnoDB Tables"
#
# 		With file-per-table tablespaces, each table has its own tablespace, which provides room for growth.
#
# Potential Disadvantages
#
# 	) With file-per-table tablespaces, each table may have unused space, which can only be utilized by rows of the same table.
#
# 		This could lead to wasted space if not properly managed.
#
# 	) fsync operations must run on each open table rather than on a single file.
#
# 		Because there is a separate fsync operation for each file, write operations on multiple tables cannot be combined
# 		into a single I/O operation.
#
# 		THis may require InnoDB to perform a higher total number of fsync operations.
#
# ) mysqld must keep one open file handle per table, which may impact performance if you have numerous tables in file-per-table tablespaces.
#
# ) More file descriptors are used.
#
# ) innodb_file_per_table is enabled by default in MySQL 5.6 and higher. You may consider disabling it if backward compatibility
# 		with earlier versions of MySQL is a concern.
#
# ) If many tables are growing there is potential for more fragmentation which can impede DROP_TABLE and table scan performance.
#
# 		However, when fragmentation is managed, having files in their own tablespace can improve performance.
#
# ) The buffer pool is scanned when dropping a file-per-table tablespace, which can take several seconds for buffer pools that are
# 		tens of gigabytes in size.
#
# 		The scan is performed with a broad internal lock, which may delay other operations.
#
# 		Tables in the system tablespace are not affected.
#
# ) The innodb_autoextend_increment variable, which defines increment size (in MB) for extending the size of an auto-extending
# 		shared tablespace file when it becomes full, does not apply to file-per-table tablespace files, which are auto-extending
# 		regardless of the innodb_autoextend_increment setting.
#
# 		The initial extensions are by small amounts, after which extensions occur in increments of 4MB.
#
# ENABLING FILE-PER-TABLE TABLESPACES
#
# The innodb_file_per_table option is enabled by default.
#
# To set the innodb_file_per_table option at startup, start the server with the --innodb_file_per_table command-line option,
# or add this line to the [mysqld] section of my.cnf:
#
# 		[mysqld]
# 		innodb_file_per_table=1
#
# You can also set innodb_file_per_table dynamically, while the server is running:
#
# 		SET GLOBAL innodb_file_per_table=1;
#
# With innodb_file_per_table enabled, you can store InnoDB tables in a tbl_name.ibd file.
#
# Unlike the MyISAM storage engine, with its separate tbl_name.MYD and tbl_name.MYI files
# for indexes and data, InnoDB stores the data and the indexes together in a single .ibd file.
#
# If you disable innodb_file_per_table in your startup options and restart the server, or disable it
# with the SET GLOBAL command, InnoDB creates new tables inside the system tablespace unless you have
# explicitly placed the table in file-per-table tablespace or general tablespace using the CREATE_TABLE_..._TABLESPACE
# option.
#
# You can always read and write any InnoDB tables, regardless of the file-per-table setting.
#
# To move a table from the system tablespace to its own tablespace, change the innodb_file_per_table setting and rebuild the table:
#
# 		SET GLOBAL innodb_file_per_table=1;
# 		ALTER TABLE table_name ENGINE=InnoDB;
#
# Tables added to the system tablespace using CREATE_TABLE_..._TABLESPACE or ALTER_TABLE_..._TABLESPACE syntax are not affected
# by the innodb_file_per_table setting.
#
# To move these tables from the system tablespace to a file-per-table tablespace, they must be moved explicitly using ALTER_TABLE_..._TABLESPACE
# syntax.
#
# Note:
#
# 		InnoDB always needs the system tablespace because it puts its internal data dictionary and undo logs there.
# 		The .ibd files are not sufficient for InnoDB to operate.
#
# 		When a table is moved out of the system tablespace into its own .ibd file, the data files that make up the system
# 		tablespace remain the same size.
#
# 		The space formerly occupied by the table can be reused for new InnoDB data, but is not reclaimed for use by the OS.
# 
# 		When moving large InnoDB tables out of the system tablespace, where disk space is limited, you may prefer to enable
# 		innodb_file_per_table and recreate the entire instance using the mysqldump command.
#
# 		As mentioned above, tables added to the system tablespace using CREATE_TABLE_..._TABLESPACE or ALTER_TABLE_..._TABLESPACE
# 		syntax are not affected by the innodb_file_per_table setting.
#
# 		These tables must be moved individually.
#
# 	15.6.3.3 GENERAL TABLESPACES
#
# A general tablespace is a shared InnoDB tablespace that is created using CREATE_TABLESPACE syntax.
#
# General tablespace capabilities and features are described under the following topics in this section:
#
# 		) General Tablespace Capabilities
#
# 		) Creating a General Tablespace
#
# 		) Adding Tables to a General Tablespace
#
# 		) General Tablespace Row Format Support
#
# 		) Moving Tables Between Tablespaces Using ALTER TABLE
#
# 		) Renaming a General Tablespace
#
# 		) Dropping a General Tablespace
#
# 		) General Tablespace Limitations
#
# General Tablespace Capabilities
#
# The general tablespace feature provides the following capabilities:
#
# 		) Similar to the system tablespace, general tablespaces are shared tablespaces that can store data for multiple tables.
#
# 		) General tablespaces have a potential memory advantage over file-per-table tablespaces.
#
# 			The server keeps tablespace metadata in memory for the lifetime of a tablespace.
#
# 			Multiple tables in fewer general tablespaces consume less memory for tablespace metadata than the same number
# 			of tables in separate file-per-table tablespaces.
#
# 		) General tablespace data files may be placed in a directory relative to or independent of the MySQL data directory,
# 			which provides you with many of the data file and storage management capabilities of file-per-table tablespaces.
#
# 			As with file-per-table tablespaces, the ability to place data files outside of the MySQL data directory
# 			allows you to manage performance of critical tables separately, setup RAID or DRBD for specific tables,
# 			or bind tables to particular disks, for example.
#
# 		) General tablespaces support both Antelope and barracuda file formats, and therefore support all table row formats
# 			and associated features.
#
# 			With support for both file formats, general tablespaces have no dependence on innodb_file_format or innodb_file_per_table
# 			settings, nor do these variables have any effect on general tablespaces.
#
# 		) The TABLESPACE option can be used with CREATE_TABLE to create tables in a general tablespaces, file-per-table tablespace,
# 			or in the system tablespace.
#
# 		) The TABLESPACE option can be used with ALTER_TABLE to move tables between general tablespaces, file-per-table tablespaces,
# 			and the system tablespace.
#
# 			Previously, it was not possible to move a table from a file-per-table tablespace to the system tablespace.
#
# 			With the general tablespace feature, you can now do so.
#
# Creating a General Tablespace
#
# General tablespaces are created using CREATE_TABLESPACE syntax.
#
# 		CREATE TABLESPACE tablespace_name
# 			[ADD DATAFILE 'file_name']
# 			[FILE_BLOCK_SIZE = value]
# 				[ENGINE [=] engine_name]
#
# A general tablespace can be created in the data directory or outside of it.
#
# To avoid conflicts with implicitly created file-per-table tablespaces, creating a general tablespace
# in a subdirectory under the data directory is not supported.
#
# When creating a general tablespace outside of the data directory, the directory must exist and must be
# known to InnoDB prior to creating the tablespace.
#
# To make an unknown directory known to InnoDB, add the directory to the innodb_directories argument value.
#
# innodb_directories is a read-only startup option. Configuring it requires restarting the server.
#
# Examples:
#
# 	Creating a general tablespace in the data directory:
#
# 		CREATE TABLESPACE `ts1` ADD DATAFILE 'ts1.ibd' Engine=InnoDB;
#
#  or
#
# 		CREATE TABLESPACE `ts1` Engine=InnoDB;
#
#
# The ADD DATAFILE clause is optional as of MySQL 8.0.14 and required before that. if the ADD DATAFILE clause is not specified
# when creating a tablespace, a tablespace data file with a unique file name is created implicitly.
#
# The unique file name is a 128 bit UUID formatted into five groups of hexadecimal numbers separated by dashes
# (aaaaaaaa-bbbb-cccc-dddd-eeeeeeee).
#
# General tablespace data files include an .ibd file extension. In a replication environment, the data file name created on the
# master is not the same as the data file name created on the slave.
#
# Creating a general tablespace in a directory outside of the data directory:
#
# 		CREATE TABLESPACE `ts1` ADD DATAFILE '/my/tablespace/directory/ts1.ibd' Engine=InnoDB;
#
# You can specify a path that is relative to the data directory as long as the tablespace directory is not under the data
# directory.
#
# In this example, the my_tablespace directory is at the same level as the data directory:
#
# 		CREATE TABLESPACE `ts1` ADD DATAFILE '../my_tablespace/ts1.ibd' Engine=InnoDB;
#
# Note:
#
# 		The ENGINE = InnoDB clause must be defined as part of the CREATE_TABLESPACE statement, or InnoDB must be defined
# 		as the default storage engine (default_storage_engine=InnoDB).
#
# Adding Tables to a General Tablespace
#
# After creating an InnoDB general tablespace, you can use CREATE_TABLE_tbl_name_..._TABLESPACE_[=]_tablespace_name or
# ALTER_TABLE_tbl_name_TABLESPACE_[=]_tablespace_name to add tables to the tablespace, as shown in the following examples:
#
# 		CREATE_TABLE
#
# 			CREATE TABLE t1 (c1 INT PRIMARY KEY) TABLESPACE ts1;
#
# 		ALTER_TABLE
#
# 			ALTER TABLE t2 TABLESPACE ts1;
#
# NOTE:
#
# 		Support for adding table partitions to shared tablespaces was deprecated in MySQl 5.7.24 and removed in MySQL 8.0.13.
#
# 		Shared tablespaces include the InnoDB system tablespace and general tablespaces.
#
# For detailed syntax information, see CREATE_TABLE and ALTER_TABLE
#
# General Tablespace Row Format Support
#
# General tablespaces support all table row formats (REDUNDANT, COMPACT, DYNAMIC, COMPRESSED) with the caveat that compressed
# and uncompressed tables cannot coexist in the same general tablespace due to different physical page sizes.
#
# For a general tablespace to contain compressed tables (ROW_FORMAT=COMPRESSED), FILE_BLOCK_SIZE must be specified, and the
# FILE_BLOCK_SIZE value must be a valid compressed page size in relation to the innodb_page_size value.
#
# Also, the physical page size of the compressed table (KEY_BLOCK_SIZE) must be equal to FILE_BLOCK_SIZE/1024.
#
# For example, if innodb_page_size=16kb and FILE_BLOCK_SIZE=8k, the KEY_BLOCK_SIZE of the table must be 8.
#
# The following table shows permitted innodb_page_size, FILE_BLOCK_SIZE, and KEY_BLOCK_SIZE combinations. FILE_BLOCK_SIZE
# values may also be specified in bytes.
#
# To determine a valid KEY_BLOCK_SIZE value for a given FILE_BLOCK_SIZE, divide the FILE_BLOCK_SIZE value by 1024.
#
# Table compression is not supported for 32k and 64k InnoDB page sizes.
#
# For more information about KEY_BLOCK_SIZE, see CREATE_TABLE, and Section 15.9.1.2, "Creating Compressed Tables"
#
# Table 15.4 Permitted Page Size, FILE_BLOCK_SIZE, and KEY_BLOCK_SIZE Combinations for Compressed Tables
#
# InnoDB Page Size (innodb_page_size) 			Permitted FILE_BLOCK_SIZE Value 				Permitted KEY_BLOCK_SIZE Value
#
# 		64KB 														64k (65536) 											Compression is not supported
#
# 		32kb 														32k (32768) 											Compression is not supported
#
# 		16kb 														16k (16384) 											N/A: If innodb_page_size is equal to FILE_BLOCK_SIZE,
# 																																the tablespace cannot contain a compressed table.
#
# 		16kb 														8k (8192) 												8
#
# 		16kb 														4k (4096) 												4
#
# 		16kb 														2k (2048) 												2
#
# 		16kb 														1k (1024) 												1
#
# 		8kb 														8K (8192) 												N/A: If innodb_page_size is equal to FILE_BLOCK_SIZE,
# 																																the tablespace cannot contain a compressed table
#
# 		8kb 														4k (4096) 												4
#
# 		8kb 														2k (2048) 												2
#
# 		8kb 														1k (1024) 												1
#
# 		4kb 														4k (4096) 												N/A: If innodb_page_size is equal to FILE_BLOCK_SIZE,
# 																																the tablespace cannot contain a compressed table.
#
# 		4kb 														2k (2048) 												2
#
# 		4kb 														1k (1024) 												1
#
# This example demonstrates creating a general tablespace and adding a compressed table. The example assumes a default innodb_page_size of 16kb.
#
# The FILE_BLOCK_SIZE of 8192 requires that the compressed table have a KEY_BLOCK_SIZE of 8.
#
# 		CREATE TABLESPACE `ts2` ADD DATAFILE 'ts2.ibd' FILE_BLOCK_SIZE = 8192 Engine=InnoDB;
#
# 		CREATE TABLE t4 (c1 INT PRIMARY KEY) TABLESPACE ts2 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8;
#
# If you do not specify FILE_BLOCK_SIZE when creating a general tablespace, FILE_BLOCK_SIZE defaults to innodb_page_size.
#
# When FILE_BLOCK_SIZE is equal to innodb_page_size, the tablespace may only contain tables with an uncompressed row format
# (COMPACT, REDUNDANT, and DYNAMIC row formats)
#
# Moving Tables Between Tablespaces using ALTER TABLE
#
# You can use ALTER_TABLE with the TABLESPACE option to move a table to an existing general tablespace, to a new file-per-table tablespace,
# or to the system tablespace.
#
# Note:
#
# 		Support for placing table partitions in shared tablespaces was deprecated in MySQL 5.7.24 and removed MySQL 8.0.13.
#
# 		Shared tablespaces include the InnoDB system tablespace and general tablespaces.
#
# To move a table from a file-per-table tablespace or from the system tablespace to a general tablespace, specify the name
# of the genral tablespace.
#
# The general tablesapce must exist. See CREATE_TABLESPACE for more information.
#
# 		ALTER TABLE tbl_name TABLESPACE [=] tablespace_name;
#
# To move a table from a general tablespace or file-per-table tablespace to the system tablespace, specify innodb_system as the
# tablespace name.
#
# 		ALTER TABLE tbl_name TABLESPACE [=] innodb_system;
#
# To move a table from the system tablespace or a general tablespace to a file-per-table tablespace, specify innodb_file_per_table
# 	as the tablespace name.
#
# 		ALTER TABLE tbl_name TABLESPACE [=] innodb_file_per_table;
#
# ALTER TABLE ... TABLESPACE operations always cause a full table rebuild, even if the TABLESPACE attribute has not changed from
# its previous value.
#
# ALTER TABLE ... TABLESPACE syntax does not support moving a table from a temporary tablespace to a persistent tablespace.
#
# The DATA DIRECTORY clause is permitted with CREATE TABLE ... TABLESPACE=innodb_file_per_table but is otherwise not supported
# for use in combination with the TABLESPACE option.
#
# Restrictions apply when moving tables from encrypted tablespaces. See Encryption Limitations.
#
# Renaming a General Tablespace
#
# Renaming a general tablespace is supported using ALTER_TABLESPACE_..._RENAME_TO syntax
#
# 		ALTER TABLESPACE s1 RENAME TO s2;
#
# The CREATE_TABLESPACE privilege is required to rename a general tablespace.
#
# RENAME TO operations are implicitly performed in autocommit mode, regardless of the autocommit setting.
#
# A RENAME TO operation cannot be performed while LOCK_TABLES or FLUSH_TABLES_WITH_READ_LOCK is in effect for
# tables that reside in the tablespace.
#
# Exclusive metadata locks are taken on tables within a general tablespace while the tablespace is renamed,
# which prevents concurrent DDL.
#
# Concurrent DML is supported.
#
# Dropping a General Tablespace
#
# the DROP_TABLESPACE statement is used to drop an InnoDB general tablespace.
#
# All tables must be dropped from the tablespace prior to a DROP_TABLESPACE operaiton.
#
# If the tablespace is not empty, DROP_TABLESPACE returns an error.
#
# Use a query similar to the following to identify tables in a general tablespace.
#
# 		SELECT a.NAME AS space_name, b.NAME AS table_name FROM INFORMATION_SCHEMA.INNODB_TABLESPACES a,
# 		INFORMATION_SCHEMA.INNODB_TABLES b WHERE a.SPACE=b.SPACE AND a.NAME LIKE 'ts1';
#
# 		+-------------+-------------+
# 		| space_name  | table_name  |
# 		+-------------+-------------+
# 		| ts1 		  | test/t1 	 |
# 		| ts1 		  | test/t2 	 |
# 		| ts1 		  | test/t3 	 |
# 		+-------------+-------------+
#
# A general InnoDB tablespace is not deleted automatically when the last table in the tablespace is dropped.
#
# The tablespace must be dropped explicitly using DROP_TABLESPACE_tablespace_name
#
# A general tablespace does not belong to any particular database. A DROP_DATABASE operation can drop tables
# that belong to a general tablespace but it cannot drop the tablespace, even if the DROP_DATABASE operation
# drops all tables that belong to the tablespace.
#
# A general tablespace must be dropped explicitly using DROP_TABLESPACE tablespace_name.
#
# SImilar to the system tablespace, truncating or dropping tables stored in a general tablespace creates free
# space internally in the general tablespace .ibd data file which can onl be used for new InnoDB data.
#
# Space is not released back to the OS as it is when a file-per-table tablespace is deleted during a DROP_TABLE
# operation.
#
# This example demonstrates how to drop an InnoDB general tablespace. The general tablespace ts1 is created
# with a single table.
#
# The table must be dropped before dropping the tablespace.
#
# CREATE TABLESPACE `ts1` ADD DATAFILE 'ts1.ibd' Engine=InnoDB;
#
# CREATE TABLE t1 (c1 INT PRIMARY KEY) TABLESPACE ts10 Engine=InnoDB;
#
# DROP TABLE t1;
#
# DROP TABLESPACE ts1;
#
# NOTE:
#
# 		tablespace_name is a case-sensitive identifier in MySQL
#
# GENERAL TABLESPACE LIMITATIONS
#
# ) A generated or existing tablespace cannot be changed to a general tablespace.
#
# ) Creation of temporary general tablespaces is not supported
#
# ) General tablespaces do not support temporary tables
#
# ) Similar to the system tablespace, truncating or dropping tables stored in a general tablespace creates free space internally
# 		in the general tablespace .ibd data file which can only be used for new InnoDB data.
#
# 		Space is not released back to the OS as it is for file-per-table tablespaces.
#
# 		Additionally, a table-copying ALTER_TABLE operation on table that resides in a shared tablespace (a general tablespace or the system
# 		tablespace) can increase the amount of space used by the tablespace.
#
# 		Such operations require as much additional space as the data in the table plus indexes.
#
# 		The additional space required for the table-copying ALTER_TABLE operation is not released back to the
# 		OS as it is for file-per-table tablespaces.
#
# ) ALTER_TABLE_..._DISCARD_TABLESPACE and ALTER_TABLE_..._IMPORT_TABLESPACE are not supported for tables that belong to a general tablespace.
#
# ) Support for placing table partitions in general tablespaces was deprecated in MySQl 5.7.24 and removed in MySQL 8.0.13
#
# 15.6.3.4 UNDO TABLESPACES
#
# Undo tablespaces contain undo logs, which are collections of undo log records that contain information about how to undo
# the latest change by a transaction to a clustered index record.
#
# Undo logs exist within undo log segments, which are contained within rollback segments.
#
# The innodb_rollback_segments variable defines the number of rollback segments allocated to each undo tablespace.
#
# Two default undo tablespaces are created when the MySQL instance is initialized. Default undo tablespaces are created at initialization
# time to provide a location for rollback segments that must exist before SQL statements can be accepted.
#
# A minimum of two undo tablespaces is required to support automated truncation of undo tablespaces.
#
# See Truncating Undo Tablespaces.
#
# Default undo tablespaces are created in the location defined by the innodb_undo_directory variable.
#
# If the innodb_undo_directory variable is undefined, default undo tablespaces are created in the data directory.
# Default undo tablespace data files are named undo_001 and undo_002.
#
# The corresponding undo tablespace names defined in the data dictionary are innodb_undo_001 and innodb_undo_002
#
# As of MySQL 8.0.14, additional undo tablespaces can be created at runtime using SQL. See Adding Undo Tablespaces.
#
# The initial size of an undo tablespace data file depends on the innodb_page_size value. For the default 16kb page size,
# the initial undo tablespace file size is 10MiB.
#
# For 4kb, 8kb, 32kb, and 64kb page sizes, the initial undo tablespace files sizes are 7MiB, 8MiB, 20MiB, and 40MiB, respectively.
#
# Adding Undo Tablespaces
#
# Because undo logs can become large during long-running transactions, creating additional undo tablespaces can help prevent
# individual undo tablespaces from becoming too large.
#
# As of MySQL 8.0.14, additional undo tablespaces can be created at runtime using CREATE_UNDO_TABLESPACE syntax.
#
# 		CREATE UNDO TABLESPACE tablespace_name ADD DATAFILE 'file_name.ibu'
#
# The undo tablespace file name must have an .ibu extension. It is not permitted to specify a relative path when defining
# the undo tablespace file name.
#
# A fully qualified path is permitted, but the path must be known to InnoDB. Known paths are those defined by the innodb_directories
# variable.
#
# Unique undo tablespace file names are recommended to avoid potential file name conflicts when moving or cloning data.
#
# At startup, directories defined by the innodb_directories variable are scanned for undo tablespace files.
#
# (The scan also traverses subdirectories). Directories defined by the innodb_data_home_dir, innodb_undo_directory, and datadir
# variables are automatically appended to the innodb_directories value, regardless of whether the innodb_directories variable
# is defined explicitly.
#
# An undo tablespace can therefore reside in paths defined by any of those variables.
#
# If the undo tablespace file name does not include a path, the undo tablespace is created in the directory defined by the
# innodb_undo_directory variable.
#
# If that variable is undefined, the undo tablespace is created in the data directory.
#
# NOTE:
#
# 		The InnoDB recovery process requires that undo tablespace files reside in known directories.
# 		Undo tablespace files must be discovered and opened before redo recovery and before other data
# 		files are opened to permit uncommitted transactions and data dictionary changes to be rolled back.
#
# 		An undo tablespace not found before recovery cannot be used, which can cause database inconsistencies.
#
# 		An error message is reported at startup if an undo tablespace known to the data dictionary is not found.
#
# 		The known directory requirement also supports undo tablespace portability. See Moving Undo Tablespaces.
#
# To create undo tablespaces in a path relative to the data directory, set the innodb_undo_directory variable
# to the relative path, and specify the file name only when creating an undo tablespace.
#
# To view undo tablespace names and paths, query INFORMATION_SCHEMA.FILES:
#
# 		SELECT TABLESPACE_NAME, FILE_NAME FROM INFORMATION_SCHEMA.FILES
# 			WHERE FILE_TYPE LIKE 'UNDO LOG';
#
# A MySQL instance supports up to 127 undo tablespaces including the two default undo tablespaces created when the
# MySQL instance is initialized.
#
# 	NOTE:
#
# 		Prior to MySQL 8.0.14, additional undo tablespaces are created by configuring the innodb_undo_tablespaces startup
# 		variable.
#
# 		This variable is deprecated and no longer configurable as of MySQL 8.0.14
#
# 		Prior to MySQL 8.0.14, increasing the innodb_undo_tablespaces setting creates the specified number of undo tablespaces
# 		and adds them to the list of active undo tablespaces.
#
# 		Decreasing the innodb_undo_tablespaces setting removes undo tablespaces from the list of active undo tablespaces.
#
# 		Undo tablespaces that are removed from the active list remain active until they are no longer used by existing transactions.
#
# 		The innodb_undo_tablespaces variable can be configured at runtime using a SET statement or defined in a configuration file.
#
# 		Prior to MySQL 8.0.14, deactivated undo tablespaces cannot be removed. Manual removal of undo tablespaces file is possible
# 		after a slow shutdown but is not recommended, as deactivated undo tablespaces may contain active undo logs for some time
# 		after hte server is restarted if open transactions were present when shutting down the server.
#
# 		As of MySQL 8.0.14, undo tablespaces can be dropped using DROP_UNDO_TABLESPACE syntax.
#
# 		See Dropping Undo Tablespaces.
#
# DROPPING UNDO TABLESPACES
#
# 		As of MySQL 8.0.14, undo tablespaces created using CREATE_UNDO_TABLESPACE syntax can be dropped at runtime using
# 		DROP_UNDO_TABLESPACE syntax.
#
# 		An undo tablespace must be empty before it can be dropped. To empty an undo tablespace, the undo tablespace must first
# 		be marked as inactive using ALTER_UNDO_TABLESPACE syntax so that the tablespace is no longer used for assigning rollback
# 		segments to new transactions.
#
# 			ALTER UNDO TABLESPACE tablespace_name SET INACTIVE;
#
# 		After an undo tablespace is marked as inactive, transactions currently using rollback segments in the undo tablespace are
# 		permitted to finish, as are any transactions started before those transactions are completed.
#
# 		After transactions are completed, the purge system frees the rollback segments in the undo tablespace, and the undo
# 		tablespace is truncated to its initial size.
#
# 		(The same process is used when truncating undo tablespaces. See Truncating Undo Tablespaces)
#
# 		When the undo tablespace is empty, it can be dropped:
#
# 			DROP UNDO TABLESPACE tablespace_name;
#
# 		NOTE:
#
# 			Alternatively, the undo tablespace can be left in an empty state and reactivated later, when needed,
# 			by issuing an ALTER_UNDO_TABLESPACE tablespace_name_SET_ACTIVE statement.
#
# 		The state of an undo tablespace can be monitored by querying the INFORMATION_SCHEMA.INNODB_TABLESPACE table.
#
# 			SELECT NAME, STATE FROM INFORMATION_SCHEMA.INNODB_TABLESPACES
# 				WHERE NAME LIKE tablespace_name;
#
# 		An inactive state indicates that rollback segments in an undo tablespace are no longer used by new transactions.
#
# 		An empty state indicates that an undo tablespace is empty and ready to be dropped, or made active again using an
# 		ALTER_UNDO_TABLESPACE_tablespace_name_SET_ACTIVE statement.
#
# 		Attempting to drop an undo tablespace that is not empty returns an error.
#
# 		The default undo tablespaces (innodb_undo_001 and innodb_undo_002) created when the MySQL instance is initialized
# 		cannot be dropped.
#
# 		They can, however, be made inactive using an ALTER_UNDO_TABLESPACE_tablespace_name_SET_INACTIVE statement.
#
# 		Before a default undo tablespace can be made inactive, there must be an undo tablespace to take its place.
#
# 		A minimum of two active undo tablespaces are required at all times to support automated truncation of undo tablespaces.
#
# MOVING UNDO TABLESPACES
#
# Undo tablespaces created with CREATE_UNDO_TABLESPACE syntax can be moved while the server is offline to any known directory.
#
# Known directories are those defined by the innodb_directories variable. Directories defined by innodb_data_home_dir, innodb_undo_directory,
# and datadir are automatically appended to the innodb_directories values regardless of whether the innodb_directories variable is defined
# explicitly.
#
# Those directories and their sub dirs are scanned at startup for undo tablespace files.
#
# An undo tablespace file moved to any of those directories is discovered at startup and assumed to be the undo tablespace that was moved.
#
# The default undo tablespaces (innodb_undo_001 and innodb_undo_002) created when the MySQL instance is initialized must always reside in the
# directory defined by the innodb_undo_directory variable.
#
# If the innodb_undo_directory variable is undefined, default undo tablespaces reside in the data directory.
#
# If default undo tablespaces are moved while the server is offline, the server must be started with the innodb_undo_directory variable
# configured to the new directory.
#
# The I/O patterns for undo logs make undo tablespaces good candidates for SSD storage.
#
# Configuring the Number of Rollback Segments
#
# The innodb_rollback_segments variable defines the number of rollback segments allocated to each undo tablespace
# and to the global temporary tablespace.
#
# The innodb_rollback_segments variable can be configured at startup or while the server is running.
#
# The default setting for innodb_rollback_segments is 128, which is also the maximum value. For more information
# about the number of transactions that a rollback segment supports, see SECTION 15.6.6, "Undo Logs"
#
# Truncating Undo Tablespaces
#
# There are two methods of truncating undo tablespaces, which can be used individually or in combination to manage
# undo tablespace size. One method is automated, enabled using configuration variables.
#
# The other method is manual, performed using SQL statements.
#
# The automated method does not require monitoring undo tablespace size and, once enabled, it performs deactivation,
# truncation and reactivation of undo tablespaces without manual intervention.
#
# The manual truncation method may be preferable if you want to control when undo tablespaces are taken offline for
# truncation. For example, you may want to avoid truncating undo tablespaces during peak workload times.
#
# Automated Truncation
#
# Automated truncation of undo tablespaces requires a minimum of two active undo tablespaces, which ensures that one
# undo tablespace remains active while the other is taken offline to be truncated.
#
# By default, two undo tablespaces are created when the MySQL instance is initialized.
#
# To have undo tablespaces automatically truncated, enable the innodb_undo_log_truncate variable. For example:
#
# 		SET GLOBAL innodb_undo_log_truncate=ON;
#
# When the innodb_undo_log_truncate variable is enabled, undo tablespaces that exceed the size limit defined by the
# innodb_max_undo_log_size variable are subject to truncation.
#
# The innodb_max_undo_log_size variable is dynamic and has a default value of 1073741824 bytes (1024 MiB)
#
# SELECT @@innodb_max_undo_log_size;
# +--------------------------------+
# | @@innodb_max_undo_log_size 	  |
# +--------------------------------+
# | 				1073741824 			  |
# +--------------------------------+
#
# When the innodb_undo_log_truncate variable is enabled:
#
# 		1. Default and user-defined undo tablespaces that exceed the innodb_max_undo_log_size setting are marked for truncation.
#
# 			Selection of an undo tablespace for truncation is performed in a circular fashion to avoid truncating the same undo tablespace
# 			each time.
#
# 		2. Rollback segments residing in the selected undo tablespace are made inactive so that they are not assigned to new transactions.
#
# 			Existing transactions that are currently using rollback segments are permitted to finish.
#
# 		3. The purge system frees rollback segments that are no longer in use.
#
# 		4. After all rollback segments in the undo tablespace are freed, the truncate operation runs and truncates the undo tablespace to its
# 			initial size.
#
# 			The initial size of an undo tablespace depends on the innodb_page_size value. For the default 16kb page size, the initial undo tablespace
# 			file size is 10MiB.
#
# 			For 4kb, 8kb, 32kb and 64kb page sizes, the initial undo tablespace files sizes are 7MiB, 8MiB, 20MiB, and 40MiB, respectively.
#
# 			The size of an undo tablespace after a truncate operation may be larger than the initial size due to immediate use following the completion
# 			of the operation.
#
# 			The innodb_undo_directory variable defines the location of default undo tablespace files. If the innodb_undo_directory variable is undefined,
# 			default undo tablespaces reside in the data directory.
#
# 			The location of all undo tablespace files including user-defined undo tablespaces created using CREATE_UNDO_TABLESPACE syntax can be determined
# 			by querying the INFORMATION_SCHEMA.FILES table:
#
#
# 				SELECT TABLESPACE_NAME, FILE_NAME FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE LIKE 'UNDO LOG';
#
# 		5. Rollback segments are reactivated so that they can be assigned to new transactions.
#
# MANUAL TRUNCATION
#
# Manual truncation of undo tablespaces requires a minimum of three active undo tablespaces. Two active undo tablespaces are required at all times
# to support the possibility that automated truncation is enabled.
#
# A minimum of three undo tablespaces satisfies this requirement while permitting an undo tablespace to be taken offline manually.
#
# To manually initiate truncation of an undo tablespace, deactivate the undo tablespace by issuing the following statement:
#  			
# 		ALTER UNDO TABLESPACE tablespace_name SET INACTIVE;
#
# After the undo tablespace is marked as inactive, transactions currently using rollback segments in the undo tablespace are permitted
# to finish, as are any transactions started before those transactions are completed.
#
# After transactions are completed, the purge system frees the rollback segments in the undo tablespace, the undo tablespace
# is truncated to its initial size, and the undo tablespace state changes from inactive to empty.
#
# NOTE:
#
# 		When an ALTER UNDO TABLESPACE tablespace_name SET INACTIVE statement deactivates an undo tablespace, the purge thread looks for that
# 		undo tablespaces at the next oppurtunity.
#
# 		Once the undo tablespace is found and marked for truncation, the purge thread returns with increased frequency to quickly empty
# 		and truncate the undo tablespace.
#
# To check the state of an undo tablespace, query the INFORMATION_SCHEMA.INNODB_TABLESPACES table.
#
# 		SELECT NAME, STATE FROM INFORMATION_SCHEMA.INNODB_TABLESPACES
# 			WHERE NAME LIKE tablespace_name;
#
# Once the undo tablespace is in an empty state, it can be reactivated by issuing the following statement:
#
# 		ALTER UNDO TABLESPACE tablespace_name SET ACTIVE;
#
# An undo tablespace in an empty state can also be dropped. See Dropping Undo Tablespaces.
#
# Expediting Automated Truncation of Undo Tablespaces
#
# The purge thread is responsible for emptying and truncating undo tablespaces. By default, the purge thread looks for undo tablespaces
# to truncate once every 128 times that purge is invoked.
#
# The frequency with which the purge thread looks for undo tablespaces to truncate is controlled by the innodb_purge_rseg_truncate_frequency
# variable, which has a default setting of 128.
#
# 		SELECT @@innodb_purge_rseg_truncate_frequency;
# 		+------------------------------------------------+
# 		| @@innodb_purge_rseg_truncate_frequency 		    |
# 		+------------------------------------------------+
# 		| 		128 													 |
# 		+------------------------------------------------+
#
# To increase that frequency, decrease the innodb_purge_rseg_truncate_frequency setting. For example, to have the purge thread look for undo
# tablespaces once every 32 times that purge is invoked, set innodb_purge_rseg_truncate_frequency to 32.
#
# SET GLOBAL innodb_purge_rseg_truncate_frequency=32;
#
# When the purge thread finds an undo tablespace that requires truncation, the purge thread returns with increased frequency to quickly empty
# and truncate the undo tablespace.
#
# Performance Impact of Truncating Undo Tablespace Files
#
# When an undo tablespace is truncated, the rollback segments in the undo tablespace are deactivated.
#
# The active rollback segments in other undo tablespaces assume responsibility for the entire system load, which may result
# in a slight performance degradation.
#
# The amount of performance degradation depends on a number of factors:
#
# 		) Number of undo tablespaces
#
# 		) Number of undo logs
#
# 		) Undo tablespace size
#
# 		) Speed of the I/O subsystem
#
# 		) Existing long running transactions
#
# 		) System load
#
# The easiest way to avoid impacting performance when truncating undo tablespaces is to increase the number of undo tablespaces.
#
# Monitoring Undo Tablespace Truncation
#
# As of MySQL 8.0.16, undo and purge subsystem counters are provided for monitoring background activities associated with undo log
# truncation. For counter names and descriptions, query the INFORMATION_SCHEMA.INNODB_METRICS table.
#
# SELECT NAME, SUBSYSTEM, COMMENT FROM INFORMATION_SCHEMA.INNODB_METRICS WHERE NAME LIKE '%truncate%';
#
# For information about enabling counters and querying counter data, see Section 15.14.6, "InnoDB INFORMATION_SCHEMA Metrics Table"
#
# 15.6.3.5 Temporary Tablespaces
#
# InnoDB uses session temporary tablespaces and a global temporary tablespace.
#
# Session Temporary Tablespaces
#
# Session temporary tablespaces store user-created temporary tables and internal temporary tables created by the optimizer
# when InnoDB is configured as the storage engine for on-disk internal temporary tables.
#
# Beginning with MySQL 8.0.16, the storage engine used for on-disk internal temporary tables is always InnoDB.
#
# (Previously, the storage engine was determined by the value of internal_tmp_disk_storage_engine)
#
# Session temporary tablespaces are allocated to a session from a pool of temporary tablespaces on the first request to
# create an on-disk temporary table.
#
# A maximum of two tablespaces is allocated to a session, one for user-created temporary tables and the other for internal
# temporary tables created by the optimizer.
#
# The temporary tablespaces allocated to a session are used for all on-disk temporary tables created by the session.
#
# When a session disconnects, its temporary tablespaces are truncated and released back to the pool.
#
# A pool of 10 temporary tablespaces is created when the server is started. The size of the pool never shrinks and
# tablespaces are added to the pool automatically as necessary.
#
# The pool of temporary tablespaces is removed on normal shutdown or on an aborted initialization.
#
# Session temporary tablespace files are five pages in size when created and have an .ibt file name extension.
#
# A range of 400 thousand space IDs is reserved for session temporary tablespaces. Because the pool of session temporary
# tablespaces is recreated each time the server is started, space IDs for session temporary tablespaces are not persisted
# when the server is shut down and may be reused.
#
# The innodb_temp_tablespaces_dir variable defines the location where session temporary tablespaces are created.
#
# The default location is the #innodb_temp directory in the data directory. Startup is refused if the pool of temporary
# tablespaces cannot be created.
#
# cd BASEDIR/data/#innodb_temp
# ls
# temp_10.ibt temp_2.ibt temp_4.ibt temp_6.ibt temp_8.ibt
# temp_1.ibt  temp_3.ibt temp_5.ibt temp_7.ibt temp_9.ibt
#
# In statement based replication (SBR) mode, temporary tables created on a slave reside in a single session temporary
# tablespace that is truncated only when the MySQL server is shut down.
#
# The INNODB_SESSION_TEMP_TABLESPACES table provides metadata about session temporary tablespaces.
#
# The INFORMATION_SCHEMA.INNODB_TEMP_TABLE_INFO table provides metadata about user-created temporary tables that are active
# in an InnoDB instance.
#
# Global Temporary Tablespace
#
# The global temporary tablespace (ibtmp1) stores rollback segments for changes made to user-created temporary tables.
#
# The innodb_temp_data_file_path variable defines the relative path, name, size and attributes for global temporary tablespace data files.
# If no value is specified for innodb_temp_data_file_path, the default behavior is to create a single auto-extending data file
# named ibtmp1 in the innodb_data_home_dir dir.
#
# The initial file size is slightly larger than 12MB.
#
# The global temporary tablespace is removed on normal shutdown or on an aborted initialization, and recreated each time the server is started.
#
# The global temporary tablespace receives a dynamically generated space ID when it is created.
#
# Startup is refused if the global temporary tablespace cannot be created. The global temporary tablespace is not removed if the server halts
# unexpectedly.
#
# In this case, a database admin can remove the global temp tablespace manually or restart the MySQL server. Restarting the MySQL server removes
# and recreates the global temporary tablespace automatically.
#
# The global temporary tablespace cannot reside on a raw device.
#
# INFORMATION_SCHEMA.FILES provides metadata about the global temporary tablespace. Issue a query similar to this one to view global
# temporary tablespace metadata:
#
# 		SELECT * FROM INFORMATION_SCHEMA.FILES WHERE TABLESPACE_NAME='innodb_temporary'\G
#
# By default, the global temporary tablespace data file is autoextending and increases in size as necessary.
#
# To determine if a global temporary tablespace data file is autoextending, check the innodb_temp_data_file_path setting:
#
# 		SELECT @@innodb_temp_data_file_path;
# 		+--------------------------------------+
# 		| @@innodb_temp_data_file_path 		   |
# 		+--------------------------------------+
# 		| ibtmp1:12M:autoextend 					|
# 		+--------------------------------------+
#
# To check the size of global temporary tablespace data files, query the INFORMATION_SCHEMA.FILES table using a query similar ot this one:
#
# 		SELECT FILE_NAME, TABLESPACE_NAME, ENGINE, INITIAL_SIZE, TOTAL_EXTENTS*EXTENT_SIZE
# 		AS TotalSizeBytes, DATA_FREE, MAXIMUM_SIZE FROM INFORMATION_SCHEMA.FILES
# 		WHERE TABLESPACE_NAME = 'innodb_temporary'\G
# 		****************************** 1. row ***********************************
# 			FILE_NAME: ./ibtmp1
# 			TABLESPACE_NAME: innodb_temporary
# 			ENGINE: InnoDB
# 			INITIAL_SIZE: 12582912
# 			TotalSizeBytes: 12582912
# 			DATA_FREE: 6291456
# 			MAXIMUM_SIZE: NULL
#
# TotalSizeBytes shows the current size of the global temporary tablespace data file. For information about other field values, see 
# SECTION 25.11, "The INFORMATION_SCHEMA FILES Table"
#
# Alternatively, check the global temporary tablespace data file size on your OS.
#
# The global temporary tablespace data file is located in the directory defined by the innodb_temp_data_file_path variable.
#
# To reclaim disk space occupied by a global temporary tablespace data file, restart the MySQL server. Restarting the server removes
# and recreates the global temporary tablespace data file according to the attributes defined by innodb_temp_data_file_path.
#
# To limit the size of the global temporary tablespace data file, configure innodb_temp_data_file_path to specify a maximum file size.
#
# For example:
#
# 		[mysqld]
# 		innodb_temp_data_file_path=ibtmp1:12M:autoextend:max:500M
#
# Configuring innodb_temp_data_file_path requires restarting the server.
#
# 15.6.3.6 Creating a Tablespace Outside of the Data Directory
#
# The CREATE_TABLE_..._DATA_DIRECTORY clause permits creating a file-per-table tablespace outside of the data directory.
#
# For example, you can use the DATA DIRECTORY clause to create a tablespace on a separate storage device with particular
# performance or capacity characteristics, such as a fast SSD or a high-capacity HDD.
#
# Be sure of the location that you choose. The DATA DIRECTORY clause cannot be used with ALTER_TABLE to change the location later.
#
# The tablespace data file is created in the specified directory, within in a subdirectory named for the schema to which the table
# belongs.
#
# The following example demonstrates creating a file-per-table tablespace outside of the data directory. It is assumed that the
# innodb_file_per_table variable is enabled.
#
# USE test;
# Database changed
#
# CREATE TABLE t1 (c1 INT PRIMARY KEY) DATA DIRECTORY = '/remote/directory';
#
# #MySQL creates the tablespace file in a subdirectory that is named 
# # for the schema to which the table belongs
#
# shell> cd /remote/directory/test
# shell> ls
# t1.ibd
#
# When creating a tablespace outside of the data directory, ensure that the directory is known to InnoDB.
#
# Otherwise, if the server halts unexpectedly before tablespace data file pages are fully flushed, startup
# fails when the tablespace is not found during the pre-recovery discovery phase that searches known directories
# for tablespace data files (see Tablespace discovery during Crash Recovery)
#
# To make a directory known, add it to the innodb_directories argument value. innodb_directories is a read-only
# startup option that defines directories to scan at startup for tablespace data files.
#
# Configuring it requires restarting the server.
#
# CREATE_TABLE_..._TABLESPACE syntax can also be used in combination with the DATA DIRECTORY clause to create a
# file-per-table tablespace outside of the data directory.
#
# To do so, specify innodb_file_per_table as the tablespace name.
#
# 		CREATE TABLE t2 (c1 INT PRIMARY KEY) TABLESPACE = innodb_file_per_table
# 		DATA DIRECTORY = '/remote/directory';
#
# The innodb_file_per_table variable does not need to be enabled when using this method.
#
# USAGE NOTES:
#
# 		) MySQL initially holds the tablespace data file open, preventing you from dismounting the device, but might eventually close
# 			the table if hte server is busy.
#
# 			Be careful not to accidentally dismount an external device while MySQL is running, or start MySQL while the device is disconnected.
#
# 			Attempting to access a table when the associated tablespace data file is missing causes a serious error that requires a server restart.
#
# 			A server restart issues errors and warnings if the tablespace data file is not at the expected path. In this case, you can restore the
# 			tablespace data file from a backup or drop the table to remove the information about it from the data dictionary.
#
# 		) Before placing a tablespace on an NFS-mounted volume, review potential issues outlined in Using NFS with MySQL.
#
# 		) If using an LVM snapshot, file copy or other file based mechanisms to back up the tablespace data file, always use the FLUSH_TABLE_..._FOR_EXPORT
# 			statement first to ensure that all changes buffered in memory are flushed to disk before the backup occurs.
#
# 		) Using the DATA DIRECTORY clause is an alternative to using symbolic links, which is not supported.
#
# 15.6.3.7 COPYING TABLESPACES TO ANOTHER INSTANCE
#
# This section describes how to copy a file-per-table tablespaces from one MySQL instance to another, otherwise known as the
# Transportable Tablespaces feature.
#
# This feature also supports partitioned InnoDB tables and individual InnoDB table partitions and subpartitions.
#
# For information about other InnoDB table copying methods, see SECTION 15.6.1.2, "MOVING OR COPYING InnoDB TABLES"
#
# There are many reasons why you might copy an InnoDB file-per-table tablespace to a different instance:
#
# 		) To run reports without putting extra load on a production server.
#
# 		) To set up identical data for a table on a new slave server.
#
# 		) To restore a backed-up version of a table or partition after a problem or mistake.
#
# 		) As a faster way of moving data around than importing the results of a mysqldump command.
#
# 			The data is available immediately, rather than having to be re-inserted and the indexes rebuilt.
#
# 		) To move a file-per-table tablespace to a server with storage medium that better suits system requirements.
#
# 			For example, you may want to have busy tables on an SSD device, or large tables on a high-capacity HDD device.
#
# Limitations and Usage Notes
#
# ) The tablespace copy procedure is only possible when innodb_file_per_table is enabled, which is the default setting.
#
# 		Tables residing in the shared system tablespace cannot be quiesced.
#
# ) When a table is queisced, only read-only transactions are allowed on the affected table.
#
# ) When importing a tablespace, the page size must match the page size of the importing instance.
#
# ) ALTER_TABLE_..._DISCARD_TABLESPACE is supported for partitioned InnoDB tables, and ALTER_TABLE_..._DISCARD_PARTITION_..._TABLESPACE
# 		is supported for InnoDB table partitions.
#
# ) DISCARD TABLESPACE is not supported for tablespaces with a parent-child (primary-key-foreign key) relationship when foreign_key_checks
# 		is set to 1.
#
# 		Before discarding a tablespace for parent-child tables, set foreign_key_checks=0. Partitioned InnoDB tables do not support foreign keys.
#
# ) ALTER_TABLE_..._IMPORT_TABLESPACE does not enforce foreign key constraints on imported data.
#
# 		If there are foreign key constraints between tables, all tables should be exported at the same (logical) point in time.
#
# 		Partitioned InnoDB tables do not support foreign keys.
#
# ) ALTER_TABLE_..._IMPORT_TABLESPACE and ALTER_TABLE_..._IMPORT_PARTITION_..._TABLESPACE do not require a .cfg metadata file to import
# 		a tablespace.
#
# 		However, metadata checks are not performed when importing without a .cfg file, and a warning similar to the following is issued:
#
# 			Message: InnoDB: IO Read error: (2, No such file or Directory) Error opening '.\
# 			test\t.cfg', will attempt to import without schema verification 
# 			1 row in set (0.00 sec)
#
# 		The ability to import without a .cfg file may be more convenient when no schema mismatches are expected.
#
# 		Additionally, the ability to import without a .cfg file could be useful in crash recovery scenarios in which
# 		metadata cannot be collected from an .ibd file
#
# 		If no .cfg file is used, InnoDB uses the equivalent of a SELECT MAX(ai_col) FROM table_name FOR UPDATE statement
# 		to initialize the in-memory auto-increment counter that is used in assigning values for to an AUTO_INCREMENT column.
#
# 		Otherwise, the current maximum auto-increment counter value is read from the .cfg metadata file.
#
# 		For related information, see InnoDB AUTO_INCREMENT Counter Initialization
#
# ) Due to a .cfg metadata file limitation, schema mismatches are not reported for partition type or partition definition
# 		differences when importing tablespace files for partitioned tables.
#
# 		Column differences are reported.
#
# ) When running ALTER_TABLE_..._DISCARD_PARTITION_..._TABLESPACE and ALTER_TABLE_..._IMPORT_PARTITION_..._TABLESPACE on subpartitioned
# 		tables, both partition and subpartition table names are allowed.
#
# 		When a partition name is specified, subpartitions of that partition are included in the operation.
#
# ) Importing a tablespace file from another MySQL server instance works if both instances have GA (General Availability) status
# 		and the server instance into which the file is imported is at the same or higher release level within the same release series.
#
# 		Importing a tablespace file into a server instance running an earlier release of MySQL is not supported.
#
# ) In replication scenarios, innodb_file_per_table must be set to ON on both the master and slave.
#
# ) On Windows, InnoDB stores database, tablespace and table names internally in lowercase.
#
# 		To avoid import problems on case-sensitive OS systems such as Linux and UNIX, create all databases, tablespaces,
# 		and tables using lowercase names.
#
# 		A convenient way to accomplish this is to add the following line to the [mysqld] section of your my.cnf or my.ini
# 		file before creating databases, tablespaces, or tables:
#
# 			[mysqld]
# 			lower_case_table_names=1
#
# 		NOTE:
#
# 			It is prohibited to start the server with a lower_case_table_names setting that is different from the setting
# 			used when the server was initialized.
#
# 	) ALTER_TABLE_..._DISCARD_TABLESPACE and ALTER_TABLE_..._IMPORT_TABLESPACE are not supported with tables that belong to an InnoDB
# 		general tablespace.
#
# 		For more information, see CREATE_TABLESPACE.
#
# ) The default row format for InnoDB tables is configurable using the innodb_default_row_format configuration option.
#
# 		Attempting to import a table that does not explicitly define a row format (ROW_FORMAT), or that uses ROW_FORMAT=DEFAULT,
# 		could result in a schema mismatch error if the innodb_default_row_format setting on the source instance differs from
# 		the setting on the destination instance.
#
# 		For related information, see Defining the Row Format of a Table.
#
# ) When exporting an encrypted tablespace, InnoDB generates a .cfp file in addition to a .cfg metadata file.
#
# 		The .cfp file must be copied to the destination instance together with the .cfg file and tablesapce file before
# 		performing the ALTER_TABLE_..._IMPORT_TABLESPACE operation on the destination instance.
#
# 		The .cfp file contains a transfer key and an encrypted tablespace key. 
#
# 		On import, InnoDB uses the transfer key to decrypt the tablespace key. For related information, see
# 		Section 15.6.3.9, "InnoDB Data-at-Rest Encryption"
#
# ) FLUSH_TABLES_..._FOR_EXPORT is not supported on tables that have a FULLTEXT index.
#
# 		Full-text search auxiliary tables are not flushed. After importing a table with a FULLTEXT index,
# 		run OPTIMIZE_TABLE to rebuild the FULLTEXT indexes.
#
# 		Alternatively, drop FULLTEXT indexes before the export operation and recreate them after importing the
# 		table on the destination instance.
#
# 15.6.3.7.1 Transportable Tablespace Examples
#
# 		Note:
#
# 			If you are transporting tables that are encrypted using the InnoDB tablespace encryption, see Limitations and Usage notes
# 			before you begin for additional procedural information.
#
# Example 1: Copying an InnoDB Table to Another Instance
#
# 		This procedure demonstrates how to copy a regular InnoDB table from a running MySQL server instance to another running instance.
#
# 		The same procedure with minor adjustments can be used to perform a full table restore on the same instance.
#
# 		1. On the source instance, create a table if one does not exist:
#
# 			USE test;
# 			CREATE TABLE t(c1 INT) ENGINE=InnoDB;
#
# 		2. On the destination instance, create a table if one does not exist:
#
# 			USE test;
# 			CREATE TABLE t(c1 INT) ENGINE=InnoDB;
#
# 		3. On the destination instance, discard the existing tablespace. (Before a tablespace can be imported, InnoDB must discard
# 			the tablespace that is attached to the receiving table.)
#
# 				ALTER TABLE t DISCARD TABLESPACE;
#
# 		4. On the source instance, run FLUSH_TABLES_..._FOR_EXPORT to quiesce the table and create the .cfg metadata file:
#
# 			USE test;
#  		FLUSH TABLES t FOR EXPORT;
#
# 			The metadata (.cfg) is created in the InnoDB data directory.
#
# 			NOTE:
#
# 				The FLUSH_TABLES_..._FOR_EXPORT statement ensures that changes to the named table have been flushed to disk
# 				so that a binary table copy can be made while the instance is running.
#
# 				When FLUSH_TABLES_..._FOR_EXPORT is run, InnoDB produces a .cfg file in the same database directory
# 				as the table. The .cfg file contains metadata used for schema verification when importing the tablespace file.
#
# 		5. Copy the .ibd file and .cfg metadata file from the source instance to the destination instance. For example:
#
# 			scp /path/to/datadir/test/t.{ibd,cfg} destination-server:/path/to/datadir/test
#
# 			NOTE:
#
# 				The .ibd file and .cfg file must be copied before releasing the shared locks, as described in the next step.
#
# 		6. On the source instance, use UNLOCK_TABLES to release the locks acquired by FLUSH_TABLES_..._FOR_EXPORT:
#
# 			USE test;
# 			UNLOCK TABLES;
#
# 		7. On the destination instance, import the tablespace:
#
# 			USE test;
# 			ALTER TABLE t IMPORT TABLESPACE;
#
# 			NOTE:
#
# 				The ALTER_TABLE_..._IMPORT_TABLESPACE feature does not enforce foreign key constraints on imported data.
#
# 				If there are foreign key constraints between tables, all tables should be exported at the same (logical)
# 				point in time.
#
# 				In this case you would stop updating the tables, commit all transactions, acquire shared locks on the
# 				tables, and then perform the export operation.
#
# EXAMPLE 2: Copying an InnoDB Partitioned Table to Another Instance
#
# This procedure demonstrates how to copy a partitioned InnoDB table from a running MySQL server instance to another
# running instance.
#
# The same procedure with minor adjustments can be used to perform a full restore of a partitioned InnoDB table on
# the same instance.
#
# 		1. On the source instance, create a partitioned table if one does not exist. In the following example, a table with three
# 			partitions (p0, p1, p2) is created:
#
# 				USE test;
# 				CREATE TABLE t1 (i int) ENGINE = InnoDB PARTITION BY KEY (i) PARTITIONS 3;
#
# 			In the /datadir/test directory, there is a separate tablespace (.ibd) file for each of the three partitions.
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd t1#P#p1.ibd t1#P#p2.ibd
#
# 		2. On the destination instance, create the same partitioned table:
#
# 			mysql> USE test;
# 			mysql> CREATE TABLE t1 (i int) ENGINE = InnoDB PARTITION BY KEY (i) PARTITIONS 3;
#
# 			In the /datadir/test directory, there is a separate tablespace (.ibd) file for each of the three partitions.
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd t1#P#p1.ibd t1#P#p2.ibd
#
# 		3. On the destination instance, discard the tablespace for the partitioned table.
#
# 			(Before the tablespace can be imported on the destination instance, the tablespace that is attached
# 			to the receiving table must be discarded.)
#
# 				mysql> ALTER TABLE t1 DISCARD TABLESPACE;
#
# 			The three .ibd files that make up the tablespace for the partitioned table are discarded from the /datadir/test directory.
#
# 		4. On the source instance, run FLUSH_TABLES_..._FOR_EXPORT to quiesce the partitioned table and create the .cfg metadata files:
#
# 			mysql> USE test;
# 			mysql> FLUSH TABLES t1 FOR EXPORT;
#
# 			Metadata (.cfg) files, one for each tablespace (.ibd) file, are created in the /datadir/test directory on the source instance:
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd t1#P#p1.ibd t1#P#p2.ibd
# 				t1#P#p0.cfg t1#P#p1.cfg t1#P#p2.cfg
#
# 			NOTE:
#
# 				FLUSH_TABLES_..._FOR_EXPORT statement ensures that changes to the named table have been flushed to disk so that binary
# 				table copy can be made while the instance is running.
#
# 				When FLUSH_TABLES_..._FOR_EXPORT is run, InnoDB produces a .cfg metadata file for the table's tablespace files in the same
# 				database directory as the table.
#
# 				The .cfg files contain metadata used for schema verification when importing tablespace files.
#
# 				FLUSH_TABLES_..._FOR_EXPORT can only be run on the table, not on individual table partitions.
#
# 		5. Copy the .ibd and .cfg files from the source instance database directory to the destination instance database directory. For example:
#
# 			shell>scp /path/to/datadir/test/t1*.{ibd,cfg} destination-server:/path/to/datadir/test
#
# 			NOTE:
#
# 				The .ibd and .cfg files must be copied before releasing the shared locks, as described in the next step.
#
# 		6. On the source instance, use UNLOCK_TABLES to release the locks acquired by FLUSH_TABLES_..._FOR_EXPORT
#
# 			mysql> USE test;
# 			mysql> UNLOCK TABLES;
#
# 		7. On the destination instance, import the tablespace for the partitioned table:
#
# 			mysql> USE test;
# 			mysql> ALTER TABLE t1 IMPORT TABLESPACE;
#
# Example 3: Copying InnoDB Table Partitions to Another Instance
#
# This procedure demonstrates how to copy InnoDB table partitions from a running MySQL server instance to another
# running instance.
#
# The same procedure with minor adjustments can be used to perform a restore of InnoDB table partitions on the same instance.
#
# In the following example, a partitioned table with four partitions (p0,p1,p2,p3) is created on the source instance.
#
# Two of the partitions (p2 and p3) are copied to the destination instance.
#
# 		1. On the source instance, create a partitioned table if one does not exist. In the following example, a table with
# 			four partitions (p0,p1,p2,p3) is created:
#
# 				mysql> USE test;
# 				mysql> CREATE TABLE t1 (i int) ENGINE = InnoDB PARTITION BY KEY (i) PARTITIONS 4;
#
# 			In the /datadir/test directory, there is a separate tablespace (.ibd) file for each of the four partitions.
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd t1#P#p1.ibd t1#P#p2.ibd  t1#P#p3.ibd
#
# 		2. On the destination instance, create the same partitioned table:
#
# 			mysql> USE test;
# 			mysql> CREATE TABLE t1 (i int) ENGINE = InnoDB PARTITION BY KEY (i) PARTITIONS 4;
#
# 			In the /datadir/test directory, there is a separate tablespace (.ibd) file for each of the four partitions.
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd 	t1#P#p1.ibd 	t1#P#p2.ibd 	t1#P#p3.ibd
#
# 		3. On the destination instance, discard the tablespace partitions that you plan to import from the source instance.
#
# 			(Before tablespace partitions can be imported on the destination instance, the corresponding partitions that are attached
# 			to the receiving table must be discarded)
#
# 				mysql> ALTER TABLE t1 DISCARD PARTITION p2, p3 TABLESPACE;
#
# 			The .ibd files for the two discarded partitions are removed from the /datadir/test directory on the destination instance,
# 			leaving the following files:
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd 	t1#P#p1.ibd
#
# 			NOTE:
#
# 				When ALTER_TABLE_..._DISCARD_PARTITION_..._TABLESPACE is run on subpartitioned tables, both partition and subpartition
# 				table names are allowed.
#
# 				When a partition name is specified, subpartitions of that partition are included in the operation.
#
# 		4. On the source instance, run FLUSH_TABLES_..._FOR_EXPORT to quiesce the partitioned table and create the .cfg metadata files.
#
# 			mysql> USE test;
# 			mysql> FLUSH TABLES t1 FOR EXPORT;
#
# 			The metadata files (.cfg files) are created in the /datadir/test directory on the source instance.
#
# 			There is a .cfg file for each tablespace (.ibd) file.
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd 	t1#P#p1.ibd 	t1#P#p2.ibd 	t1#P#p3.ibd
# 				t1#P#p0.cfg 	t1#P#p1.cfg 	t1#P#p2.cfg 	t1#P#p3.cfg
#
# 			NOTE:
#
# 				FLUSH_TABLES_..._FOR_EXPORT statements ensure that changes to the named tables have been flushed to disk
# 				so that binary table copy can be made while the instance is running.
#
# 				When FLUSH_TABLES_..._FOR_EXPORT is run, InnoDB produces a .cfg metadata file for the table's tablespace
# 				files in the same database directory as the table.
#
# 				The .cfg files contain metadata used for schema verification when importing tablespace files.
#
# 				FLUSH_TABLES_..._FOR_EXPORT can only be run on the table, not on individual table partitions.
#
# 		5. Copy the .ibd and .cfg files from the source instance database directory to the destination instance
# 			database directory. For example:
#
# 				shell>scp /path/to/datadir/test/t1*.{ibd,cfg} destination-server:/path/to/datadir/test
#
# 			NOTE:
#
# 				The .ibd and .cfg files must be copied before releasing the shared locks, as described in
# 				the next step.
#
# 		6. On the source instance, use UNLOCK_TABLES to release the locks acquired by FLUSH_TABLES_..._FOR_EXPORT:
#
# 			mysql> USE test;
# 			mysql> UNLOCK TABLES;
#
# 		7. On the destination instance, import the tablespace for the partitioned table:
#
# 			mysql> USE test;
# 			mysql> ALTER TABLE t1 IMPORT TABLESPACE;
#
# Example 3: Copying InnoDB Table partitions to Another Instance
#
# This procedure demonstrates how to copy InnoDB table partitions from a running MySQL server instance to another running
# instance.
#
# The same procedure with minor adjustments can be used to perform a restore of InnoDB table partitions on the same instance.
#
# In the following example, a partitioned table with four partitions (p0,p1,p2,p3) is created on the source instance.
#
# Two of the partitions (p2 and p3) are copied to the destination instance.
#
# 		1. On the source instance, create a partitioned table if one does not exist. In the following example, a table with
# 			four partitions (p0,p1,p2,p3) is created:
#
# 				mysql> USE test;
# 				mysql> CREATE TABLE t1 (i int) ENGINE = InnoDB PARTITION BY KEY (i) PARTITIONS 4;
#
# 			In the /datadir/test directory, there is a separate tablespace (.ibd) file for each of the four partitions.
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd t1#P#p1.ibd t1#P#p2.ibd t1#P#p3.ibd
#
# 		2. On the destination instance, create the same partitioned table:
#
# 				mysql> USE test;
# 				mysql> CREATE TABLE t1 (i int) ENGINE = InnoDB PARTITION BY KEY (i) PARTITIONS 4;
#
# 			In the /datadir/test directory, there is a separate tablespace (.ibd) file for each of the four partitions.
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd  t1#P#p1.ibd 	t1#P#p2.ibd 	t1#P#p3.ibd
#
# 		3. On the destination instance, discard the tablespace partitions that you plan to import from the source instance.
#
# 			(Before tablespace partitions can be imported on the destination instance, the corresponding partitions that are
# 			attached to the receiving table must be discarded)
#
# 				mysql> ALTER TABLE t1 DISCARD PARTITION p2, p3 TABLESPACE;
#
# 			The .ibd files for the two discarded partitions are removed from the /datadir/test directory on the destination instance,
# 			leaving the following files:
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd 	t1#P#p1.ibd
#
# 			NOTE:
#
# 				When ALTER_TABLE_..._DISCARD_PARTITION_..._TABLESPACE is run on subpartitioned tables, both partition and subpartition
# 				table names are allowed.
#
# 				When a partition name is specified, subpartitions of that partition are included in the operation.
#
# 		4. On the source instance, run FLUSH_TABLES_..._FOR_EXPORT to quiesce the partitioned table and create the .cfg metadata files.
#
# 				mysql> USE test;
# 				mysql> FLUSH TABLES t1 FOR EXPORT;
#
# 			The metadata files (.cfg files) are created in the /datadir/test directory on the source instance.
# 			There is a .cfg file for each tablespace (.ibd) file
#
# 				mysql> \! ls /path/to/datadir/test/
# 				t1#P#p0.ibd 	t1#P#p1.ibd 	t1#P#p2.ibd 	t1#P#p3.ibd
# 				t1#P#p0.cfg 	t1#P#p1.cfg 	t1#P#p2.cfg 	t1#P#p3.cfg
#
# 			NOTE:
#
# 				FLUSH_TABLES_..._FOR_EXPORT statement ensures that changes to the named table have been flushed
# 				to disk so that binary table copy can be made while the instance is running.
#
# 				When FLUSH_TABLES_..._FOR_EXPORT is run, InnoDB produces a .cfg metadata file for the table's
# 				tablespace files in the same database directory as the table.
#
# 				The .cfg files contain metadata used for schema verification when importing tablespace files.
#
# 				FLUSH_TABLES_..._FOR_EXPORT can only be run on the table, not on individual table partitions.
#
# 		5. Copy the .ibd and .cfg files from the source instance database directory to the destination instance database directory.
#
# 			In this example, only the .ibd and .cfg files for partition 2 (p2) and partition 3 (p3) are copied to the data directory
# 			on the destination instance.
#
# 			Partition 0 (p0) and partition 1 (p1) remain on the source instance.
#
# 				shell> scp t1#P#p2.ibd 	t1#P#p2.cfg 	t1#P#p3.ibd 	t1#P#p3.cfg 	destination-server:/path/to/datadir/test
#
# 			NOTE:
#
# 				The .ibd files and .cfg files must be copied before releasing the shared locks, as described in the next step.
#
# 		6. On the source instance, use UNLOCK_TABLES to release the locks acquired by FLUSH_TABLES_..._FOR_EXPORT
#
# 				mysql> USE test;
# 				mysql> UNLOCK TABLES;
#
# 		7. ON the destination instance,, import the tablespace partitions (p2 and p3):
#
# 				mysql> USE test;
# 				mysql> ALTER TABLE t1 IMPORT PARTITION p2, p3 TABLESPACE;
#
# 			NOTE:
#
# 				When ALTER_TABLE_..._IMPORT_PARTITION_..._TABLESPACE is run on subpartitioned tables, both partition
# 				and subpartition table names are allowed.
#
# 				When a partition name is specified, subpartitions of that partition are included in the operation.
#
# 15.6.3.7.2 Transportable Tablespace INternals
#
# The following information describes internals and error log messaging for the transportable tablespaces copy procedure
# for a regular InnoDB table.
#
# When ALTER_TABLE_..._DISCARD_TABLESPACE is run on the destination instance:
#
# 		) The table is locked in X mode
#
# 		) The tablespace is detached from the table.
#
# When FLUSH_TABLES_..._FOR_EXPORT is run on the source instance:
#
# 		) The table being flushed for export is locked in shared mode
#
# 		) The purge coordinator thread is stopped
#
# 		) Dirty pages are synchronized to disk
#
# 		) Table metadata is written to the binary .cfg file
#
# Expected error log messages for this operation:
#
# 		2013-09-24T13:10:19.<etc> 2 [Note] InnoDB: Sync to disk of '"test"."t"' started:
# 		2013-09-24T13:10:19.<etc> 2 [Note] InnoDB: Stopping purge
# 		2013-09-24T13:10:19.<etc> 2 [Note] InnoDB: Writing table metadata to './test/t.cfg'
# 		2013-09-24T13.10.19.<etc> 2 [Note] InnoDB: Table '"test"."t"' flushed to disk
#
# When UNLOCK_TABLES is run on the source instance:
#
# 		) The binary .cfg file is deleted.
#
# 		) The shared lock on the table or tables being imported is released and the purge coordinator thread is restarted.
#
# Expected error log messages for this operation:
#
# 		2013-09-24T13:10:21.<etc> 2 [Note] InnoDB: Deleting the meta-data file './test/t.cfg'
# 		2013-09-24T13:10:21.<etc> 2 [Note] InnoDB: Resuming purge
#
# When ALTER_TABLE_..._IMPORT_TABLESPACE is run on the destination instance, the import algorithm performs the following
# operations for each tablespace being imported:
#
# 		) Each tablespace page is checked for corruption
#
# 		) The space ID and log sequence numbers (LSNs) on each page are updated
#
# 		) Flags are validated and LSN updated for the header page.
#
# 		) Btree pages are updated.
#
# 		) The page state is set to dirty so that it is written to disk.
#
# Expected error log messages for this operation:
#
# 		2013-07-18 15:15:01 34960 [Note] InnoDB: Importing tablespace for table 'test/t' that was exported from host 'ubuntu'
# 		2013-07-18 15:15:01 34960 [Note] InnoDB: Phase I - Update all pages
# 		2013-07-18 15:15:01 34960 [Note] InnoDB: Sync to disk
# 		2013-07-18 15:15:01 34960 [Note] InnoDB: Sync to disk - done!
# 		2013-07-18 15:15:01 34960 [Note] InnoDB: Phase III - Flush changes to disk
# 		2013-07-18 15:15:01 34960 [Note] InnoDB: Phase IV - Flush complete
#
# NOTE:
#
# 		You may also receive a warning that a tablespace is discarded (if you discarded the tablespace for the destination table)
# 		and a message stating that statistics could not be calculated due to a missing .ibd file:
#
# 			2013-07-18 15:14:38 34960 [Warning] InnoDB: Table "test"."t" tablespace is set as discarded.
# 			2013-07-18 15:14:38 7f34d9a37700 InnoDB: cannot calculate statistics for table "test"."t"
# 			because the .ibd file is missing. For help, please refer to 
# 			http://dev.mysql.com/doc/refman/8.0/en/innodb-troubleshooting.html
#
# 15.6.3.8 Moving Tablespace Files While the Server is Offline
#
# The innodb_directories option, which defines directories to scan at startup for tablespace files, supports moving or
# restoring tablespace files to a new location while the server is offline.
#
# During startup, discovered tablespace files are used instead those referenced in the data directory, and the data
# dictionary is updated to reference the relocated files.
#
# If duplicate tablespace files are discovered by the scan, startup fails with an error indicating that multiple files
# were found for the same tablespace ID.
#
# The directories defined by the innodb_data_home_dir, innodb_undo_directory, and datadir configuration options are
# automatically appended to the innodb_directories argument value.
#
# These directories are scanned at startup regardless of whether the innodb_directories option is specified explicitly.
#
# The implicit addition of these directories permits moving system tablespace files, the data directory, or undo tablespace
# files without configuring the innodb_directories setting.
#
# However, settings must be updated when directories change. For example, after relocating the data directory, you must
# update the --datadir setting before restarting the server.
#
# The innodb_directories option may be specified in a startup command or MySQL option file.
#
# Quotes are used around the argument value because otherwise a semicolon (;) is interpreted as a special
# character by some command interpreters. (Unix shells treat it as a command terminator, for example)
#
# Startup command:
#
# 		mysqld --innodb-directories="directory_path_1;directory_path_2"
#
# MySQL option file:
#
# 		[mysqld]
# 		innodb_directories="directory_path_1;directory_path_2"
#
# The following procedure is applicable to moving individual file-per-table and general tablespace files, system tablespace files,
# undo tablespace files, or the data directory.
#
# Before moving files or directories, review the usage notes that follow.
#
# 	1. Stop the server
#
# 	2. Move the tablespace files or directories
#
# 	3. Make the new directory known to InnoDB.
#
# 		) If moving individual file-per-table or general tablespace files, add unknown directories to the innodb_directories value.
#
# 			) The directories defined by the innodb_data_home_dir, innodb_undo_directory, and datadir configuration options are automatically
# 				appended to the innodb_directories argument value, so you need not specify these.
#
# 			) A file-per-table tablespace file can only be moved to a directory with same name as the schema. For example, if the actor
# 				table belongs to the sakila schema, then the actor.ibd data file can only be moved to a directory named sakila.
#
# 			) General tablespace files cannot be moved to the data directory or a subdirectory of the data directory.
#
# 		) If moving system tablespace files, undo tablespaces, or the data directory, update the innodb_data_home_dir, innodb_undo_directory,
# 			and datadir settings, as necessary.
#
# 	4. Restart the server.
#
# USAGE NOTES
#
# ) Wildcard expressions cannot be used in the innodb_directories argument value.
#
# ) The innodb_directories scan also traverses subdirectories of specified directories. Duplicate directories and subdirectories are discarded
# 		from the list of directories to be scanned.
#
# ) The innodb_directories option only supports moving InnoDB tablespace files. Moving files that belong to a storage engine other than InnoDB
# 		is not supported.
#
# 		This restriction also applies when moving the entire data directory.
#
# ) The innodb_directories option supports renaming of tablespace files when moving files to a scanned directory.
#
# 		It also supports moving tablespaces files to other supported OS's.
#
# ) When moving tablespace files to a different OS, ensure that tablespace file names do not include prohibit chars or chars with a special
# 		meaning on the destination OS.
#
# ) When moving a data directory from a Windows OS to a Linux OS, modify the binary log file paths in the binary log index file to use 
# 		backward slashes instead of forward slashes.
#
# 		By default, the binary log index file has the same base name as the binary log file, with the extension '.index'
#
# 		The location of the binary log index file is defined by --log-bin. The default location is the data directory.
#
# ) If moving tablespace files to a different OS introduces cross-platform replication, it is the responisbility of the DB admin to ensure
# 		proper replication of DDL statements that contain platform-specific DIrs.
#
# 		Statements that permit specifying dirs include CREATE_TABLE_..._DATA_DIRECTORY and CREATE_TABLESPACE.
#
# ) The directory of file-per-table and general tablespace files created with an absolute path or in a location outside of the data
# 		directory should be added to the innodb_directories argument value.
#
# 		Otherwise, InnoDB is not able to locate these files during recovery.
#
# 		CREATE_TABLE_..._DATA_DIRECTORY and CREATE_TABLESPACE permit creation of tablespace files with absolute paths.
#
# 		CREATE_TABLESPACE also permits tablespace file directories that are relative to the data directory.
#
# 		To view tablespace file locations, query the INFORMATION_SCHEMA.FILES table:
#
# 			mysql> SELECT TABLESPACE_NAME, FILE_NAME FROM INFORMATION_SCHEMA.FILES \G
#
# ) CREATE_TABLESPACE requires that the target directory exists and is known to InnoDB. Known directories include those implicitly and explicitly
# 		defined by the innodb_directories option.
#
# 15.6.3.9 InnoDB Data-at-Rest Encryption
#
# InnoDB supports data-at-rest encryption for file-per-table tablespaces, general tablespaces, the mysql system tablespace, redo logs, and undo logs.
#
# As of MySQL 8.0.16, setting an encryption default for schemas and general tablespaces is also supported, which permits DBAs to control whether
# tables created in those schemas and tablespaces are encrypted.
#
# InnoDB data-at-rest encryption features and capabilities are described under the following topics in this section.
#
# 		) About Data-at-Rest Encryption
#
# 		) Encryption Prerequisites
#
# 		) Defining an Encryption Default for Schemas and General Tablespaces
#
# 		) File-Per-Table tablespace encryption
#
# 		) General Tablespace Encryption
#
# 		) mysql System Tablespace Encryption
#
# 		) Redo Log Encryption
#
# 		) Undo Log Encryption
#
# 		) Master Key Rotation
#
# 		) Encryption and Recovery
#
# 		) Exporting Encrypted Tablespaces
#
# 		) Encryption and Replication
#
# 		) Identifying Encrypted Tablespaces and Schemas
#
# 		) Monitoring Encryption Progress
#
# 		) Encryption Usage Notes
#
# 		) Encryption Limitations
#
# ABOUT DATA-AT-REST ENCRYPTION
#
# InnoDB uses a two tier encryption key architechture, consisting of a master encryption key and tablespace keys.
#
# When a tablespace is encrypted, a tablespace key is encrypted and stored in the tablespace header.
#
# When an application or authenticated user wants to access encrypted tablespace data, InnoDB uses a master encryption
# key to decrypt the tablespace key. The decrypted version of a tablespace key never changes, but the master encryption
# key can be changed as required.
#
# This action is referred to as master key rotation.
#
# The data-at-rest encryption feature relies on a keyring plugin for master encryption key management.
#
# All MySQL editions provide a keyring_file plugin, which stores keyring data in a file local to the server host.
#
# MySQL Enterprise Edition offers additional keyring plugins:
#
# 		) The keyring_encrypted_file plugin, which stores keyring data in an encrypted file local to the server host.
#
# 		) The keyring_okv plugin, which includes a KMIP client (KMIP 1.1) that uses a KMIP-compatible product as a back end
# 			for keyring storage.
#
# 			Supported KMIP-compatibble products include centralized key management solutions such as Oracle Key Vault,
# 			Gemalto KeySecure, etc.
#
# 		) The keyring_aws plugin, which communicates with the Amazon Web Services Key Management Service (AWS KMS) as
# 			a back end for key generation and uses a local file for key storage.
#
# 			WARNING:
#
# 				The keyring_file and keyring_encrypted file plugins are not intended as regulatory compliance solutions.
#
# 				Security standards such as PCI, FIPS, and others require use of key management systems to secure, manage,
# 				and protect encryption keys in key vaults or hardware security modules (HSMs)
#
# A secure and robust encryption key management solution is critical for security and for compliance with various
# security standards.
#
# When the data-at-rest encryption feature uses a centralized key management solution, the feature is referred to
# as "MySQL Enterprise Transparent Data Encryption (TDE)"
#
# The data-at-rest encryption feature supports the Advanced Encryption Standard (AES) block-based encryption algorithm.
#
# It uses Electronic Codebook (ECB) block encryption mode for tablespace key encryption and Cipher Block Chaining
# (CBC) block encryption mode for data encryption.
#
# For frequently asked questions about the data-at-rest Encryption feature, see SECTION A.16, "MySQL 8.0 FAQ: InnoDB Data-At-Rest Encryption"
#
# Encryption Prerequisites
#
# 		) A keyring plugin must be installed and configured. Keyring plugin installation is performed at startup using the early-plugin-load option.
#
# 			Early loading ensures that the plugin is available prior to initialization of the InnoDB storage engine.
#
# 			For keyring plugin installation and configuration instructions, see SECTION 6.4.4, "The MySQL Keyring"
#
# 			Only one keyring plugin can be enabled at a time. Enabling multiple keyring plugins is not supported.
#
# 			IMPORTANT:
#
# 				Once encrypted tablespaces are created in a MySQL instance, the keyring plugin that was loaded when creating
# 				the encrypted tablespace must continue to be loaded at startup using the early-plugin-load option.
#
# 				Failing to do so results in errors when starting the server and during InnoDB recovery.
#
# To verify that a keyring plugin is active, use the SHOW_PLUGINS statement or query the INFORMATION_SCHEMA.PLUGINS table.
#
# For example:
#
# 		SELECT PLUGIN_NAME, PLUGIN_STATUS
# 		FROM INFORMATION_SCHEMA.PLUGINS
# 		WHERE PLUGIN_NAME LIKE 'keyring%';
# 		+--------------+----------------+
# 		| PLUGIN_NAME  | PLUGIN_STATUS  |
# 		+--------------+----------------+
# 		| keyring_file | ACTIVE 		  |
# 		+--------------+----------------+
#
# 	) When encrypting production data, ensure that you take steps to prevent loss of the master encryption key.
#
# 		If the master encryption key is lost, data stored in encrypted tablespace files is unrecoverable. If you 
# 		use the keyring_file or keyring_encrypte_file plugin, create a backup of the keyring data file immedaitely
# 		after creating the first encrypted tablespace, before master key rotation, and after master key rotation.
#
# 		The keyring_file_data configuration option defines the keyring data file location for the keyring_file plugin.
#
# 		The keyring_encrypted_file_data configuration option defines the keyring data file location for the 
# 		keyring_encrypted_file plugin. If you use the keyring_okv or keyring_aws plugin, ensure that you have
# 		performed the necessary configuration.
#
# 		For instructions, see SECTION 6.4.4, "The MySQL Keyring"
#
# DEFINING AN ENCRYPTION DEFAULT FOR SCHEMAS AND GENERAL TABLESPACES
#
# As of MySQL 8.0.16, the default_table_encryption variable defines the default encryption setting for schemas and general
# tablespaces.
#
# CREATE_TABLESPACE and CREATE_SCHEMA operations apply the default_table_encryption setting when an ENCRYPTION clause is not
# specified explicitly.
#
# ALTER_SCHEMA and ALTER_TABLESPACE operations do not apply the default_table_encryption setting. An ENCRYPTION clause must be
# specified explicitly to alter the encryption of an existing schema or general tablespace.
#
# The default_table_encryption variable can be set for an individual client connection or globally using SET syntax.
#
# For example, the following statement enables default schema and tablespace encryption globally:
#
# 		mysql> SET GLOBAL default_table_encryption=ON;
#
# The default encryption setting for a schema can also be defined using the DEFAULT ENCRYPTION clause when creating
# or altering a schema, as in this example:
#
# 		mysql> CREATE SCHEMA test DEFAULT ENCRYPTION = 'Y';
#
# If the DEFAULT ENCRYPTION clause is not specified when creating a schema, the default_table_encryption setting is applied.
#
# The DEFAULT ENCRYPTION clause must be specified to alter the default encryption of an existing schema.
#
# Otherwise, the schema retains its current encryption setting.
#
# By default, a table inherits the encryption setting of the schema or general tablespace it is created in. For example,
# a table created in an encryption-enabled schema is encrypted by default.
#
# This behavior enables a DBA to control table encryption usage by defining and enforcing schema and general tablesapce
# encryption defaults.
#
# Encryption defaults are enforced by enabling the table_encryption_privilege_check variable. When table_encryption_privilege_check
# is enabled, a privilege check occurs when creating or altering a schema or general tablespace with an encryption setting that
# differs from the default_table_encryption setting, or when creating or altering a table with an encryption setting that differs
# from the default schema encryption.
#
# When table_encryption_privilege_check is disabled (the default), the privilege check does not occur and the previously
# mentioned operations are permitted to proceed with a warning.
#
# The TABLE_ENCRYPTION_ADMIN privilege is required to override default encryption settings when table_encryption_privilege_check
# is enabled.
#
# A DBA can grant this privilege to enable a user to deviate from the default_table_encryption setting when creating or altering
# a schema or general tablespace, or to deviate from the default schema encryption when creating or altering a table.
#
# This privilege does not permit deviating from the encryption of a general tablespace when creating or altering a table.
#
# A table must have the same encryption setting as the general tablespace it resides in.
#
# FILE-PER-TABLE TABLESPACE ENCRYPTION
#
# As of MySQL 8.0.16, a file-per-table tablespace inherits the default encryption of the schema in which the table is created
# unless an ENCRYPTION clause is specified explicitly in the CREATE_TABLE statement.
#
# Prior to MySQL 8.0.16, the ENCRYPTION clause must be specified to enable encryption.
#
# 	mysql> CREATE TABLE t1 (c1 INT) ENCRYPTION = 'Y';
#
# To alter the encryption of an existing file-per-table tablespace, an ENCRYPTION clause must be specified.
#
# 	mysql> ALTER TABLE t1 ENCRYPTION = 'Y';
#
# As of MySQL 8.0.16, if the table_encryption_privilege_check variable is enabled, specifying an ENCRYPTION clause
# with a setting that differs from the default schema encryption requires the TABLE_ENCRYPTION_ADMIN privilege.
#
# See Defining an Encryption Default for Schemas and General Tablespaces
#
# General Tablespace Encryption
#
# 		As of MySQL 8.0.16, the default_table_encryption variable determines the encryption of a newly created tablespace
# 		unless an ENCRYPTION clause is specified explicitly in the CREATE_TABLESPACE statement.
#
# 		Prior to MySQL 8.0.16, an ENCRYPTION clause must be specified to enable encryption.
#
# 			mysql> CREATE TABLESPACE `ts1` ADD DATAFILE 'ts1.ibd' ENCRYPTION = 'Y' Engine=InnoDB;
#
# 		To alter the encryption of an existing general tablespace, an ENCRYPTION clause must be specified.
#
# 			mysql> ALTER TABLESPACE ts1 ENCRYPTION = 'Y';
#
# As of MySQL 8.0.16, if the table_encryption_privilege_check variable is enabled, specifying an ENCRYPTION
# clause with a setting that differs from the default_table_encryption setting requires the TABLE_ENCRYPTION_ADMIN
# privilege.
#
# See Defining an Encryption Default for Schemas and General Tablespaces.
#
# Mysql System Tablespace Encryption
#
# Encryption support for the mysql system tablespace is available as of MySQL 8.0.16
#
# The mysql system tablespace contains the mysql system database and MySQL data dictionary tables. It is unencrypted
# by default.
#
# To enable encryption for the mysql system tablespace, specify the tablespace name and the ENCRYPTION option in an
# ALTER_TABLESPACE statement.
#
# 		mysql> ALTER TABLESPACE mysql ENCRYPTION = 'Y';
#
# To disable encryption for the mysql system tablespace, set ENCRYPTION = 'N' using an ALTER_TABLESPACE statement.
#
# 		mysql> ALTER TABLESPACE mysql ENCRYPTION = 'N';
#
# Enabling or disabling encryption for the mysql system tablespace requires the CREATE_TABLESPACE privilege on all tables
# in the instance (CREATE TABLESPACE on *.*)
#
# Redo Log Encryption
#
# Redo log data encryption is enabled using the innodb_redo_log_encrypt configuration option. Redo log encryption is disabled
# by default.
#
# As with tablespace data, redo log data encryption occurs when redo log data is written to disk, and decryption occurs when redo
# log data is read from disk.
#
# Once redo log data is read into memory, it is in unencrypted form. Redo log data is encrypted and decrypted using the tablespace
# encryption key.
#
# When innodb_redo_log_encrypt is enabled, unencrypted redo log pages that are present on disk remain unencrypted, and new redo log
# pages are written to disk in encrypted form.
#
# Likewise, when innodb_redo_log_encrypt is disabled, encrypted redo log pages that are present on disk remain encrypted, and new redo
# log pages are written to disk in unencrypted form.
#
# Redo log encryption metadata, including the tablespace encryption key, is stored in the header of the first redo log file (ib_logfile0).
#
# if this file is removed, redo log encryption is disabled.
#
# Once redo log encryption is enabled, a normal restart without the keyring plugin or without the encryption key is not possible,
# as InnoDB must be able to scan redo pages during startup, which is not possible if redo log pages are encrypted.
#
# Without the keyring plugin or the encryption key, only a forced startup without the redo logs (SRV_FORCE_NO_LOG_REDO) is possible.
#
# See SECTION 15.20.2, "Forcing InnoDB Recovery"
#
# UNDO LOG ENCRYPTION
#
# Undo log data encryption is enabled using the innodb_undo_log_encrypt configuration option. Undo log encryption applies to undo logs
# that reside in undo tablespaces.
#
# See SECTION 15.6.3.4, "UNDO TABLESPACES". Undo log data encryption is disabled by default.
#
# As with tablespace data, undo log data encryption occurs when undo log data is written to disk, and decryption occurs when undo log
# data is read from disk.
#
# Once undo log data is read into memory, it is in unencrypted form. Undo log data is encrypted and decrypted using the tablespace
# encryption key.
#
# When innodb_undo_log_encrypt is enabled, unencrypted undo log pages that are present on disk remain unencrypted, and new undo log pages
# are writen to disk in encrypted form.
#
# Likewise, when innodb_undo_log_encrypt is disabled, encrypted undo log pages that are present on disk remain encrypted, and new undo
# log pages are written to disk in unencrypted form.
#
# Undo log encryption metadata, including the tablespace encryption key, is stored in the header of the undo log file.
#
# MASTER KEY ROTATION
#
# The master encryption key should be rotated periodically and whenever you suspect that the key has been compromised.
#
# Master key rotation is an atomic, instance-level operation. Each time the master encryption key is rotated, all tablespace keys
# in the MySQL instance are re-encrypted and saved back to their respective tablespace headers.
#
# As an atomic operation, re-encryption must succeed for all tablespace keys once a rotation operation is initiated.
#
# If master key rotation is interrupted by a server failure, InnoDB rolls the operation forward on server restart.
#
# For more information, see Encryption and Recovery.
#
# Rotating the master encryption key only changes the master encryption key and re-encrypts tablespace keys. It does not
# decrypt or re-encrypt associated tablespace data.
#
# Rotating the master encryption key requires the ENCRYPTION_KEY_ADMIN or SUPER privilege.
#
# To rotate the master encryption key, run:
#
# 		mysql> ALTER INSTANCE ROTATE INNODB MASTER KEY;
#
# ALTER_INSTANCE_ROTATE_INNODB_MASTER_KEY supports concurrent DML. however, it cannot be run concurrently with tablespace
# encryption operations, and locks are taken to prevent conflicts that could arise from concurrent execution.
#
# If an ALTER_INSTANCE_ROTATE_INNODB_MASTER_KEY operation is running, it must finish before a tablespace encryption
# operation can proceed, and vice versa.
#
# ENCRYPTION AND RECOVERY
#
# If a server failure occurs during an encryption operation, the operation is rolled forward when the server is restarted.
#
# For general tablespaces, the encryption operation is resumed in a background thread from the last processed page.
#
# If a server failure occurs during master key rotation, InnoDB continues the operation on server restart.
#
# The keyring plugin must be loaded prior to storage engine initialization so that the information necessary to decrypt
# tablespace data pages can be retrieved from tablespace headers before InnoDB initialization and recovery activities
# access tablespace data.
#
# (See Encryption Prerequisites)
#
# When InnoDB initialization and recovery begin, the master key rotation operation resumes. Due to the server failure,
# some tablespace keys may already be encrypted using the new master encryption key.
#
# InnoDB reads the encryption data from each tablesapce header, and if the data indicates that the tablespace key is
# encrypted using the old master encryption key, InnoDB retrieves the old key from the keyring and uses it to decrypt
# the tablespace key.
#
# InnoDB then re-encrypts the tablespace key using the new master encryption key and saves the re-encrypted tablespace
# key back to the tablespace header.
#
# EXPORTING ENCRYPTED TABLESPACES
#
# Tablespace export is only supported for file-per-table tablespaces.
#
# When an encrypted tablespace is exported, InnoDB generates a transfer key that is used to encrypt the tablespace key.
#
# The encrypted tablespace key and transfer key are stored in a tablespace_name.cfp file.
#
# This file together with the encrypted tablespace file is required to perform an import operation.
#
# On import, InnoDB uses the transfer key to decrypt the tablespace key in the tablespace_name.cfp file.
#
# For related information, see SECTION 15.6.3.7, "COPYING TABLESPACES TO ANOTHER INSTANCE"
#
# ENCRYPTION AND REPLICATION
#
# 		) The ALTER_INSTANCE_ROTATE_INNODB_MASTER_KEY statement is only supported in replication environments where the master
# 			and slaves run a version of MySQL that supports tablespace encryption.
#
# 		) Successful ALTER_INSTANCE_ROTATE_INNODB_MASTER_KEY statements are written to the binary log for replication on slaves.
#
# 		) If an ALTER_INSTANCE_ROTATE_INNODB_MASTER_KEY statement fails, it is not logged to the binary log and is not replicated on slaves.
#
# 		) Replication of an ALTER_INSTANCE_ROTATE_INNODB_MASTER_KEY operation fails if the keyring plugin is installed on the master but not on the slave.
#
# 		) If the keyring_file or keyring_encrypted_file plugin is installed on both the master and slave but the slave does not have a keyring data file,
# 			the replicated ALTER_INSTANCE_ROTATE_INNODB_MASTER_KEY statement creates the keyring data file on the slave, assuming the keyring file data is
# 			not cached in memory.
#
# 			ALTER_INSTANCE_ROTATE_INNODB_MASTER_KEY uses keyring file data that is cached in memory, if available.
#
# IDENTIFYING ENCRYPTED TABLESPACES AND SCHEMAS
#
# The INFORMATION_SCHEMA.INNODB_TABLESPACES table, introduced in MySQL 8.0.13, includes an ENCRYPTION column that can be used to identify encrypted tablespaces.
#
# 		SELECT SPACE, NAME, SPACE_TYPE, ENCRYPTION FROM INFORMATION_SCHEMA.INNODB_TABLESPACES
# 		WHERE ENCRYPTION='Y'\G
# 		************************* 1. row **********************************
# 			SPACE: 4294967294
# 			NAME: mysql
# 			SPACE_TYPE: General
# 			ENCRYPTION: Y
#
# 		************************* 2. row ***********************************
# 			SPACE: 2
# 			NAME:  test/t1
# 			SPACE_TYPE: Single
# 			ENCRYPTION: Y
# 		************************* 3. row ***********************************
# 			SPACE: 3
# 			NAME: ts1
# 			SPACE_TYPE: General
# 			ENCRYPTION: Y
#
# When the ENCRYPTION option is specified in a CREATE_TABLE or ALTER_TABLE statement, it is recorded in the CREATE_OPTIONS
# column of INFORMATION_SCHEMA.TABLES
#
# This column can be queried to identify tables that reside in encrypted file-per-table tablespaces.
#
# 		SELECT TABLE_SCHEMA, TABLE_NAME, CREATE_OPTIONS FROM INFORMATION_SCHEMA.TABLES
# 		WHERE CREATE_OPTIONS LIKE '%ENCRYPTION%';
# 		+--------------------+------------------+-------------------------------+
# 		| TABLE_SCHEMA 	   | TABLE_NAME 	    | CREATE_OPTIONS 					|
# 		+--------------------+------------------+-------------------------------+
# 		| test 					| t1 					 | ENCRYPTION="Y" 					|
# 		+--------------------+------------------+-------------------------------+
#
# Query INFORMATION_SCHEMA.INNODB_TABLESPACES to retrieve information about the tablespace associated with a particular
# schema and table.
#
# 		SELECT SPACE, NAME, SPACE_TYPE FROM INFORMATION_SCHEMA.INNODB_TABLESPACES WHERE NAME='test/t1';
# 		+-----------+------------+----------------+
# 		| SPACE 	   | NAME 		 | SPACE_TYPE 	   |
# 		+-----------+------------+----------------+
# 		| 3 			| test/t1 	 | Single 			|
# 		+-----------+------------+----------------+
#
# You can identify encryption-enabled Schemas by querying the INFORMATION_SCHEMA.SCHEMATA table.
#
# 		SELECT SCHEMA_NAME, DEFAULT_ENCRYPTION FROM INFORMATION_SCHEMA.SCHEMATA
# 		WHERE DEFAULT_ENCRYPTION='YES';
# 		+---------------+---------------------+
# 		| SCHEMA_NAME   | DEFAULT_ENCRYPTION  |
# 		+---------------+---------------------+
# 		| test 			 | YES 					  |
# 		+---------------+---------------------+
#
# SHOW_CREATE_SCHEMA also shows the DEFAULT ENCRYPTION clause.
#
# MONITORING ENCRYPTION PROGRESS
#
# You can monitor general tablespace and mysql system tablespace encryption progress using Performance Schema.
#
# The stage/innodb/alter tablespace (encryption) stage event instrument reports WORK_ESTIMATED and WORK_COMPLETED information
# for general tablespace encryption operations.
#
# The following example demonstrates how to enable the stage/innodb/alter tablespace (encryption) stage event instrument
# and related consumer tables to monitor general tablespace or mysql system tablespace encryption progress.
#
# For information about Performance Schema stage event instruments and related consumers, see SECTION 26.12.5, "PERFORMANCE
# 	SCHEMA STAGE EVENT TABLES"
#
# 		1. Enable the stage/innodb/alter tablespace (encryption) instrument:
#
# 				USE performance_schema;
# 				UPDATE setup_instruments SET ENABLED = 'YES'
# 				WHERE NAME LIKE 'stage/innodb/alter tablespace (encryption)';
#
# 		2. Enable the stage event consumer tables, which include events_stages_current, events_stages_history and events_stages_history_long
#
# 				UPDATE setup_consumers SET ENABLED = 'YES' WHERE NAME LIKE '%stages%';
#
# 		3. Run a tablespace encryption operation. In this example, a general tablespace named ts1 is encrypted.
#
# 				ALTER TABLESPACE ts1 ENCRYPTION = 'Y';
#
# 		4. Check the progress of the encryption operation by querying the Performance Schema events_stages_current table.
#
# 			WORK_ESTIMATED reports the total number of pages in the tablespace. WORK_COMPLETED reports the number of pages processed.
#
# 				SELECT EVENT_NAME, WORK_ESTIMATED, WORK_COMPLETED FROM events_stages_current;
#
# 				+--------------------------------------------+-----------------+------------------------+
# 				| EVENT_NAME 									      | WORK_COMPLETED  | WORK_ESTIMATED 			 |
# 				+--------------------------------------------+-----------------+------------------------+
# 				| stage/innodb/alter tablespace (encryption) | 1056 				| 1407 						 |
# 				+--------------------------------------------+-----------------+------------------------+
#
# 			The events_stages_current table returns an empty set if the encryption operation has completed.
#
# 			In this case, you can check the events_stages_history table to view event data for the completed
# 			operation. For example:
#
# 				SELECT EVENT_NAME, WORK_COMPLETED, WORK_ESTIMATED FROM events_stages_history;
# 				+---------------------------------------------+------------------+--------------------+
# 				| EVENT_NAME 											 | WORK_COMPLETED   | WORK_ESTIMATED 	  |
# 				+---------------------------------------------+------------------+--------------------+
# 				| stage/innodb/alter tablespace (encryption)  | 1407 				  | 1407 				  |
# 				+---------------------------------------------+------------------+--------------------+
#
# ENCRYPTION USAGE NOTES
#
# ) Plan appropriately when altering an existing file-per-table tablespace with the ENCRYPTION option.
#
# 		Tables residing in file-per-table tablespace are rebuilt using the COPY algorithm.
# 		The INPLACE algorithm is used when altering the ENCRYPTION attribute of a general tablespace or the
# 		mysql system tablespace.
#
# 		The INPLACE algorithm permits concurrent DML on tables that reside in the general tablespace.
#
# 		Concurrent DDL is blocked.
#
# ) When a general tablespace or the mysql system tablespace is encrypted, all tables residing in the tablespace
# 		are encrypted.
#
# 		Likewise, a table created in an encrypted tablesapce is encrypted.
#
# ) If the server exits or is stopped during normal operation, it is recommended to restart the server using the same
# 		encryption settings that were configured previously.
#
# ) The first master encryption key is generated when the first new or existing tablespace is encrypted.
#
# ) Master key rotation re-encrypts tablespaces keys but does not change the tablespace key itself. To change a tablespace
# 		key, you must disable and re-enable encryption.
#
# 		For file-per-table tablespaces, re-encrypting the tablespace is an ALGORITHM=COPY operation that rebuilds the table.
#
# 		For general tablespaces and the mysql system tablespace, it is an ALGORITHM=INPLACE operation, which does not require
# 		rebuilding tables that reside in the tablespace.
#
# ) If a table is created with both the COMPRESSION and ENCRYPTION options, compression is performed before tablespace data is encrypted.
#
# ) If a keyring data file (the file named by keyring_file_data or keyring_encrypted_file_data) is empty or missing, the first execution
# 		of ALTER_INSTANCE_ROTATE_INNODB_MASTER_KEY creates a master encryption key.
#
# ) Uninstalling the keyring_file or keyring_encrypted_file plugin does not remove an existing keyring data file.
#
# ) It is recommended that you do not place a keyring data file under the same directory as tablespace data files.
#
# ) Modifying the keyring_file_data or keyring_encrypted_file_data setting at runtime or when restarting the server can cause
# 		previously encrypted tablespaces to become inaccessible, resulting in lost data.
#
# ENCRYPTION LIMITATIONS
#
# ) Advanced Encryption Standard (AES) is the only supported encryption algorithm. InnoDB tablespace encryption uses Electronic Codebook (ECB) block
# 		encryption mode for tablespace key encryption and Cipher Block Chaining (CBC) block encryption mode for data encryption.
#
# ) Encryption is only supported for file-per-table tablespaces, general tablespaces, and the mysql system tablespace.
#
# 		Encryption support for general tablespaces was introduced in MySQL 8.0.13. Encryption support for the mysql system tablespace
# 		is available as of MySQL 8.0.16. Encryption is not supported for other tablespace types including the InnoDB system tablespace.
#
# ) You cannot move or copy a table from an encrypted file-per-table tablespace, general tablespace or the mysql system tablespace to a tablespace
# 		type that does not support encryption.
#
# ) You cannot move or copy a table from an encrypted tablespace to an unencrypted tablespace. However, moving a table from an unencrypted tablespace
# 		to an encrypted one is permitted.
#
# 		For example, you can move or copy a table from a unencrypted file-per-table or general tablespace to an encrypted general tablesapce.
#
# ) By default, tablespace encryption only applies to data in the tablespace. Redo log and undo log data can be encrypted by enablibg innodb_redo_log_encrypt
# 		and innodb_undo_log_encrypt. See Redo Log Encryption, and Undo Log Encryption. Binary log data is not encrypted.
#
# ) It is not permitted to change the storage engine of a table that resides in, or previously resided in, an encrypted tablesapce.
#
# 15.6.4 DOUBLEWRITE BUFFER
#
# The doublewrite buffer is a storage area located in the system tablespace where InnoDB writes pages that are flushed from teh InnoDB
# buffer pool, before the pages are written to their proper positions in the data file.
#
# Only after flushing and writing pages to the doublewrite buffer, does InnoDB write pages to their proper positions.
#
# If there is an OS, storage subsystem or mysqld process crash in the middle of a page write, InnoDB can later find a good copy
# of the page from the doublewrite buffer during crash recovery.
#
# Although data is always written twice, the doublewrite buffer does not require twice as much I/O overhead or twice as many I/O
# operations.
#
# Data is written to the doublewrite buffer itself as a large sequential chunk, with a single fsync() call to the OS.
#
# The doublewrite buffer is enabled by default in most cases. To disable the doublewrite buffer, set innodb_doublewrite to 0.
#
# If system tablespace files ("ibdata files") are located on Fusion-io devices that support atomic writes, doublewrite buffering
# is automatically disabled and Fusion-io atomic writers are used for all data files.
#
# Because the doublewrite buffer setting is global, doublewrite buffering is also disabled for data files residing on non-Fusion-io
# hardware.
#
# This feature is only supported on Fusion-io hardware and is only enabled for Fusion-io NVMFS on Linux.
#
# To take full advantage of this feature, an innodb_flush_method setting of O_DIRECT is recommended.
#
# 15.6.5 REDO LOG
#
# The redo log is a disk-based data structure used during crash recovery to correct data written by incomplete transactions.
#
# During normal operations, the redo log encodes requests to change table data that result from SQL statements or low-level API
# calls.
#
# Modifications that did not finish updating the data files before an unexpected shutdown are replayed automatically during initialization,
# and before the connections are accepted.
#
# For information about the role of the redo log in crash recovery, see SECTION 15.17.2, "InnoDB Recovery"
#
# By default, the redo log is physically represented on disk by two files named ib_logfile0 and ib_logfile1.
#
# MySQL writes to the redo log files in a circular fashion. Data in the redo log is encoded in terms of records affected;
# this data is collectively referred to as redo. The passage of data through the redo log is represented by an ever-increasing
# LSN value.
#
# For related information, see Redo Log File Configuration, and Section 8.5.4, "Optimizing InnoDB Redo Logging"
#
# For information about data-at-rest encryption for redo logs, see Redo Log Encryption.
#
# CHANGING THE NUMBER or SIZE OF REDO LOG FILES
#
# To change the number or the size of redo log files, perform the following steps:
#
# 		1. Stop the MySQL server and make sure that it shuts down without errors.
#
# 		2. Edit my.cnf to change the log file configuration. To change the log file size, configure innodb_log_file_size.
#
# 			To increase the number of log files, configure innodb_log_files_in_group.
#
# 		3. Start the MySQL server again.
#
# If InnoDB detects that the innodb_log_file_size differs from the redo log file size, it writes a log checkpoint, closes
# and removes the old log files, creates new log files at the requested size, and opens the new log files.
#
# GROUP COMMIT FOR REDO LOG FLUSHING
#
# InnoDB, like any other ACID-compliant database engine, flushes the redo log of a transaction before it is committed.
#
# InnoDB uses group commit functionality to group multiple such flush requests together to avoid one flush for each commit.
#
# With group commit, InnoDB issues a single write to the log file to perform the commit action for multiple user transactions
# that commit at about the same time, significantly improving throughput.
#
# For more information about performance of COMMIT and other transactional operations, see SECTION 8.5.2, "Optimizing InnoDB Transaction Management"
#
# 15.6.6 Undo Logs
#
# An undo log is a collection of undo log records associated with a single read-write transaction. An undo log record contains information about how
# to undo the latest change by a transaction to a clustered index record.
#
# If another transaction needs to see the original data as part of a consistent read operation, the unmodified data is retrieved from undo log records.
#
# Undo logs exist within undo log segments, which are contained within rollback segments. Rollback segments reside in undo tablespaces
# and in the global temporary tablespace.
#
# Undo logs that reside in the global temporary tablespace are used for transactions that modify data in user-defined temporary tables.
#
# These undo logs are not redo-logged, as they are not required for crash recovery. They are used only for rollback while the server is
# running.
#
# This type of undo log benefits performance by avoiding redo logging I/O.
#
# For information about data-at-rest encryption for undo logs, see Undo Log Encryption.
#
# Each undo tablespace and the global temporary tablespace individually support a maximum of 128 rollback segments.
#
# The innodb_rollback_segments variable defines the number of rollback segments.
#
# The number of transactions that a rollback segment supports depends on the number of undo slots in the rollback segment
# and the number of undo logs required by each transaction.
#
# The number of undo slots in a rollback segment differs according to InnoDB page size.
#
# 		InnoDB Page Size 					Number of Undo Slots in a Rollback Segment (InnoDB Page Size / 16)
#
# 		4096 (4kb) 							256
#
# 		8192 (8kb) 							512
#
# 		16384 (16kb) 						1024
#
# 		32768 (32kb) 						2048
#
# 		65536 (64kb) 						4096
#
# A transaction is assigned up to four undo logs, one for each of the following operation types:
#
# 		1. INSERT operations on user-defined tables
#
# 		2. UPDATE and DELETE operations on user-defined tables
#
# 		3. INSERT operations on user-defined temporary tables
#
# 		4. UPDATE and DELETE operations on user-defined temporary tables
#
# Undo logs are assigned as needed. For example, a transaction that performs INSERT, UPDATE and DELETE operations on regular
# and temporary tables requires a full assignment of four undo logs.
#
# A transaction that performs only INSERT operations on regular tables requires a single undo log.
#
# A transaction that performs operations on regular tables is assigned undo logs from an assigned undo tablespace rollback segment.
#
# A transaction that performs operations on temporary tables is assigned undo logs from an assigned global temporary tablespace rollback segment.
#
# An undo log assigned to a transaction remains tied to the transaction for its duration. For example, an undo log assigned to a transaction for
# an INSERT operation on a regular table is used for all INSERT operations on regular tables performed by that transaction.
#
# Given the factors described above, the following formulas can be used to estimate the number of concurrent read-write transactions that InnoDB
# is capable of supporting.
#
# 		NOTE:
#
# 			A transaction can encounter a concurrent transaction limit error before reaching the number of concurrent read-write transactions
# 			that InnoDB is capable of supporting.
#
# 			This occurs when a rollback segment assigned to a transaction runs out of undo slots.
#
# 			In such cases, try rerunning the transaction.
#
# 			When transactions perform operations on temporary tables, the number of concurrent read-write transactions that InnoDB is capable
# 			of supporting is constrained by the number of rollback segments allocated to the global temporary tablespace, which is 128 by default.
#
# 		) If each transaction performs either an INSERT or an UPDATE or DELETE operation, the number of concurrent read-write transactions that InnoDB
# 			is capable of supporting is:
#
# 				(innodb_page_size / 16) * innodb_rollback_segments * number of undo tablespaces
#
# 		) If each transaction performs an INSERT and an UPDATE or DELETE operation, the number of concurrent read-write transactions that InnoDB
# 			is capable of supporting is:
#
# 				(innodb_page_size / 16 / 2) * innodb_rollback_segments * number of undo tablespaces
#
# 		) If each transaction performs an INSERT operation on a temporary table, the number of concurrent read-write transactions that InnoDB is capable
# 			of supporting is:
#
# 				(innodb_page_size / 16) * innodb_rollback_segments
#
# 		) If each transaction performs an INSERT and an UPDATE or DELETE operation on a temporary table, the number of concurrent read-write transactions
# 			that InnoDB is capable of supporting is:
#
# 				(innodb_page_size / 16 / 2) * innodb_rollback_segments
#
# 15.7 InnoDB Locking and Transaction Model
#
# 15.7.1 InnoDB Locking
# 15.7.2 InnoDB Transaction Model
# 15.7.3 Locks Set by Different SQL Statements in InnoDB
# 15.7.4 Phantom Rows
# 15.7.5 Deadlocks in InnoDB
#
# To implement a large-scale, busy, or highly reliable database application, to port substantial code from a different database system, or to tune
# MySQL performance, it is important to understand InnoDB locking and the InnoDB transaction model.
#
# This section discusses several topics related to InnoDB locking and the InnoDB transaction model with which you should be familiar.
#
# 		) Section 15.7.1, "InnoDB Locking" describes lock types used by InnoDB
#
# 		) Section 15.7.2, "InnoDB Transaction Model" describes transaction isolation levels and the locking strategies used by each.
#
# 								It also discusses the use of autocommit, consistent non-locking reads, and locking reads.
#
# 		) Section 15.7.3, "Locks Set by Different SQL Statements in InnoDB" discusses specific types of locks set in InnoDB for various statements.
#
# 		) Section 15.7.4, "Phantom Rows" describes how InnoDB uses next-key locking to avoid phantom rows.
#
# 		) Section 15.7.5, "Deadlocks in InnoDB" provides a deadlock example, discusses deadlock detection and rollback, and provides tips for minimizing
# 								and handling deadlocks in InnoDB.
#
# 15.7.1 InnoDB Locking
#
# This section describes lock types used by InnoDB.
#
# 		) Shared and Exclusive Locks
#
# 		) Intention Locks
#
# 		) Record Locks
#
# 		) Gap Locks
#
# 		) Next-Key Locks
#
# 		) Insert Intention Locks
#
# 		) AUTO-INC Locks
#
# 		) Predicate Locks for Spatial Indexes
#
# Shared and Exclusive Locks
#
# InnoDB implements standard row-level locking where there are two types of locks, shared (S) locks and exclusive (X) locks.
#
# 	) A shared (S) lock permits the transaction that holds the lock to read a row.
#
# 	) An exclusive (X) lock permits the transaction that holds the lock to update or delete a row.
#
# If transaction T1 holds a shared (S) lock on row r, then requests from some distinct transaction T2 for a lock on row r are handled
# as follows:
#
# 	) A request by T2 for an S lock can be granted immediately. As a result, both T1 and T2 hold an S lock on r.
#
# 	) A request by T2 for an X lock cannot be granted immediately.
#
# If a transaction T1 holds an exclusive (X) lock on row r, a request from some distinct transaction T2 for a lock of either type
# on r cannot be granted immediately.
#
# Instead, transaction T2 has to wait for transaction T1 to release its lock on row r.
#
# Intention Locks
#
# InnoDB supports multiple granularity locking which permits coexistence of row locks and table locks.
#
# For example, a statement such as LOCK_TABLES_..._WRITE takes an exclusive lock (an X lock) on the specified table.
#
# To make locking at multiple granularity levels practical, InnoDB uses intention locks.
#
# Intention locks are table-level locks that indicate which type of lock (shared or exclusive) a transaction requires
# later for a row in a table.
#
# There are two types of intention locks:
#
# 		) An intention shared lock (IS) indicates that a transaction intends to set a shared lock on individual rows in a table
#
# 		) An intention exclusive lock (IX) indicates that a transaction intends to set an exclusive lock on individual rows in a table.
#
# For example, SELECT_..._FOR_SHARE sets an IS lock, and SELECT_..._FOR_UPDATE sets an IX lock.
#
# The intention locking protocol is as follows:
#
# 		) Before a transaction can acquire a shared lock on a row in a table, it must first acquire an IS lock or stronger on the table.
#
# 		) Before a transaction can acquire an exclusive lock on a row in a table, it must first acquire an IX lock on the table.
#
# Table-level lock type compatibility is summarized in the following matrix.
#
# 		-/- 		X 				IX 			S 				IS
# 	
# 		X 			Conflict 	Conflict 	Conflict 	Conflict
#
# 		IX 		Conflict 	Compatible 	Conflict 	Compatible
# 
# 		S 			Conflict 	Conflict 	Compatible 	Compatible
#
# 		IS  		Conflict 	Compatible 	Compatible 	Compatible
#
# A lock is granted to a requesting transaction if it is compatible with existing locks, but not if it conflicts
# with existing locks.
#
# A transaction waits until the conflicting existing lock is released.
#
# If a lock request conflicts with an existing lock and cannot be granted because it would cause deadlock, an error occurs.
#
# Intention locks do not block anything except full table requests (for example, LOCK_TABLES_..._WRITE). The main purpose of
# intention locks is to show that someone is locking a row, or going to lock a row in the table.
#
# Transaction data for an intention lock appears similar to the following in SHOW_ENGINE_INNODB_STATUS and InnoDB monitor output:
#
# 		TABLE LOCK table `test`.`t` trx id 10080 lock mode IX
#
# Record Locks
#
# A record lock is a lock on an index record. For example, SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE; prevents any other transaction
# from inserting, updating, or deleting rows where the value of t.c1 is 10.
#
# Record locks always lock index records, even if a table is defined with no indexes. For such cases, InnoDB creates a hidden
# clustered index and uses this index for record locking.
#
# See SECTION 15.6.2.1, "Clustered and Secondary Indexes"
#
# Transaction data for a record lock appears similar to the following in SHOW_ENGINE_INNODB_STATUS and InnoDB monitor output:
#
# 		RECORD LOCKS space id 58 page no 3 n bits 72 index `PRIMARY` of table `test`.`t`
# 		trx id 10078 lock_mode X locks rec but not gap
# 		Record lock, heap no 2 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
# 			0: len 4; hex 800000000a; asc 		;;
# 			1: len 6; hex 0000000000274f; asc 			'0;;
# 			2: len 7; hex b600000019d0110;  asc 				;;
#
# GAP LOCKS
#
# A gap lock is a lock on a gap between index records, or a lock on the gap before the first or after the last index record.
#
# For example, SELECT c1 FROM t WHERE c1 BETWEEN 10 and 20 FOR UPDATE; prevents other transactions from inserting a value of
# 15 into column t.c1, whether or not there was already any such value in the column, because the gaps between all existing
# values in the range are locked.
#
# A gap might span a single index value, multiple index values, or even be empty.
#
# Gap locks are part of the tradeoff between performance and concurrency, and are used in some transaction isolation levels
# and not others.
#
# Gap locking is not needed for statements that lock rows using a unique index to search for a unique row.
#
# (This does not include the case that the search condition includes only some columns of a multiple-column unique index;
#	in that case, gap locking does occur).
#
# For example, if the id column has a unique index, the following statement uses only an index-record lock for the row having
# id value 100 and it does not matter whether other sessions insert rows in the preceding gap:
#
# 		SELECT * FROM child WHERE id = 100;
#
# If id is not indexed or has a nonunique index, the statement does lock the preceding gap.
#
# It is also worth noting here that conflicting locks can be held on a gap by different transactions. For example, transaction A
# can hold a shared gap lock (gap S-lock) on a gap while transaction B holds an exclusive gap lock (gap X-lock) on the same gap.
#
# The reason conflicting gap locks are allowed is that if a record is purged from an index, the gap locks held on the record
# by different transactions must be merged.
#
# Gap locks in InnoDB are "purely inhibitive", which means that their only purpose is to prevent other transactions from inserting
# to the gap.
#
# Gap locks can co-exist. 
#
# A gap lock taken by one transaction does not prevent another transaction from taking a gap lock on the same gap.
#
# There is no difference between shared and exclusive gap locks. They do not conflict with each other, and they perform
# the same function.
#
# Gap locking can be disabled explicitly. This occurs if you change the transaction isolation level to READ_COMMITTED. Under these
# circumstances, gap locking is disabled for searches and index scans and is used only for foreign-key constraint checking and
# duplicate-key checking.
#
# There are also other effects of using the READ_COMMITTED isolation level. 
#
# Record locks for nonmatching rows are released after MySQL has evaluated the WHERE condition. For UPDATE statements, InnoDB
# does a "semi-consistent" read, such that it returns the latest committed version to MySQL so that MySQL can determine
# whether the row matches the WHERE condition of the UPDATE.
#
# NEXT-KEY lOCKS
#
# A next-key lock is a combination of a record lock on the index record and a gap lock on the gap before the index record.
#
# InnoDB performs row-level locking in such a way that when it searches or scans a table index, it sets shared or exclusive
# locks on the index records it encounters.
#
# Thus, the row-level locks are actually index-record locks.
#
# A next-key lock on an index record also affects the "gap" before that index record. That is, a next-key lock is an index-record
# lock plus a gap lock on the gap preceding the index record.
#
# If one session has a shared or exclusive lock on record R in an index, another session cannot insert a new index record in the
# gap immediately before R in the index order.
#
# Suppose that an index contains the value 10, 11, 13, and 20. The possible next-key locks for this index cover the following intervals,
# where a round bracket denotes exclusion of the interval endpoint and a square bracket denotes inclusion of the endpoint:
#
#  ( or ) = Exclusion of endpoint
# 	[ or ] = INclusion of endpoint

# 		(negative_infinity, 10]
# 		(10, 11]
# 		(11, 13]
# 		(13, 20]
# 		(20, positive infinity)
#
# For the last interval, the next-key lock locks the gap above the largest value in the index and the "supremum" pseudo-record having
# a value higher than any value actually in the index.
#
# THe supremum is not a real index record, so, in effect, this next-key lock locks only the gap following the largest index value.
#
# By default, InnoDB operates in REPEATABLE_READ transaction isolation level. In this case, InnoDB uses next-key locks for searches
# and index scans, which prevents phantom rows (see SECTION 15.7.4, "Phantom Rows")
#
# Transaction data for a next-key lock appears similar to the following in SHOW_ENGINE_INNODB_STATUS and InnoDB monitor output:
#
# 		RECORD LOCKS space id 58 page no 3 n bits 72 index `PRIMARY` of table `test`.`t`
# 		trx id 10080 lock_mode X
# 		Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0
# 		0: len 8; hex 73757072656d756d; asc supremum;;
#
# 		Record lock, heap no 2 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
# 		0: len 4; hex 80000000a; asc 		;;
# 		1: len 6; hex 000000000274f; asc 		'0;;
# 		2: len 7; hex b600000019d0110; asc 			  ;;
#
# INSERT INTENTION LOCKS
#
# An insert intention lock is a type of gap lock set by INSERT operations prior to row insertion.
#
# This lock signals the intent in such a way that multiple transactions inserting into the same index gap need
# not wait for each other if they are not inserting at the same position within the gap.
#
# Suppose that there are index records with values of 4 and 7. Separate transactions that attempt to insert values
# of 5 and 6, respectively, each lock the gap between 4 and 7 with insert intention locks prior to obtaining
# the exclusive lock on the inserted row, but do not block each other because the rows are nonconflicting.
#
# The following example demonstrates a transaction taking an insert intention lock prior to obtaining an exclusive lock
# on the inserted record.
#
# The example involves two clients, A and B.
#
# Client A creates a table containing two index records (90 and 102) and then starts a transaction that places an
# exclusive lock on index records with an ID greater than 100.
#
# The exclusive lock includes a gap lock before record 102:
#
# 		mysql> CREATE TABLE child (id int(11) NOT NULL, PRIMARY KEY(id)) ENGINE=InnoDB;
# 		mysql> INSERT INTO child (id) values (90), (102);
#
# 		mysql> START TRANSACTION;
# 		mysql> SELECT * FROM child WHERE id > 100 FOR UPDATE;
# 		+--------+
# 		| id 		|
# 		+--------+
# 		| 102 	|
# 		+--------+
#
# Client B begins a transaction to insert a record into the gap. The transaction takes an insert intention lock while it 
# waits to obtain an exclusive lock.
#
# 		mysql> START TRANSACTION;
# 		mysql> INSERT INTO child (id) VALUES (101);
#
# Transaction data for an insert intention lock appears similar to the following in SHOW_ENGINE_INNODB_STATUS and InnoDB monitor
# output:
#
# 		RECORD LOCKS space id 31 page no 3 n bits 72 index `PRIMARY` of table `test`.`child`
# 		trx id 8731 lock_mode X locks gap before rec insert intention waiting
# 		Record lock, heap no 3 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
# 			0: len 4; hex 800000066; asc 		f;;
# 			1: len 6; hex 00000000002215; asc 		" ;;
# 			2: len 7; hex 900000000172011c; asc 	  r	;;...
#
# AUTO-INC LOCKS
#
# An AUTO-INC lock is a special table-level lock taken by transactions inserting into tables with AUTO_INCREMENT columns.
#
# In the simplest case, if one transaction is inserting values into the table, any other transactions must wait to do their
# own inserts into that table, so that rows inserted by the first transaction receive consecutive primary key values.
#
# The innodb_autoinc_lock_mode configuration option controls the algorithm used for auto-increment locking.
#
# It allows you to choose how to trade off between predictable sequences of auto-increment values and maximum concurrency
# for insert operations.
#
# For more information, see SECTION 15.6.1.4, "AUTO_INCREMENT Handling in InnoDB"
#
# Predicate Locks for Spatial Indexes
#
# InnoDB supports SPATIAL indexing of columns containing spatial columns (see SECTION 11.5.9, "Optimizing Spatial Analysis")
#
# To handle locking for operations involving SPATIAL indexes, next-key locking does not work well to support REPEATABLE_READ or
# SERIALIZABLE transaction isolation levels.
#
# There is no absolute ordering concept in multidimensional data, so it is not clear which is the "next" key.
#
# To enable support of isolation levels for tables with SPATIAL indexes, InnoDB uses predicate locks. A SPATIAL index contains
# minimum bounding rectangle (MBR) values, so InnoDB enforces consistent read on the index by setting a predicate lock on the
# MBR value used for a query.
#
# Other transactions cannot insert or modify a row that would match the query condition.
#
# 15.7.2 InnoDB Transaction Model
#
# 15.7.2.1 Transaction Isolation Levels
# 15.7.2.2 Autocommit, Commit, and Rollback
# 15.7.2.3 Consistent Nonlocking Reads
# 15.7.2.4 Locking Reads
#
# In the InnoDB transaction model, the goal is to combine the best properties of a multi-versioning database with traditional
# two-phase locking.
#
# InnoDB performs locking at the row level and runs queries as nonlocking consistent reads by default, in the style of Oracle.
#
# The lock information in InnoDB is stored space-efficiently so that lock escalation is not needed.
#
# Typically, several users are permitted to lock every row in InnoDB tables, or any random subset of rows,
# without causing InnoDB memory exhaustion.
#
# 15.7.2.1 Transaction Isolation Levels
#
# Transaction isolation is one of the foundations of database processing. Isolation is in the I in the acronym ACID; The isolation level
# is the setting that fine-tunes the balance between performance and reliability, consistency, and reproducibility of results when
# multiple transactions are making changes and performing queries at the same time.
#
# InnoDB offers all four transaction isolation levels described by the SQL:1992 standard: READ_UNCOMMITTED, READ_COMMITTED, REPEATABLE_READ,
# and SERIALIZABLE.
#
# The deault isolation level for InnoDB is REPEATABLE_READ.
#
# A user can change the isolation level for a single session or for all subsequent connections with the SET_TRANSACTION statement.
#
# To set the server's default isolation level for all connections, use the --transaction-isolation option on the command line or in
# an option file.
#
# For detailed information about isolation levels and level-setting syntax, see SECTION 13.3.7, "SET TRANSACTION SYNTAX"
#
# InnoDB supports each of the transaction isolation levels described here using different locking strategies. You can enforce a high degree
# of consistency with the default REPEATABLE_READ level, for operations on crucial data where ACID compliance is important.
#
# Or you can relax the consistency rules with READ_COMMITTED or even READ_UNCOMMITTED, in situations such as bulk reporting where precise
# consistency and repeatable results are less important than minimizing the amount of overhead for locking.
#
# SERIALIZABLE enforces even stricter rules than REPEATABLE_READ, and is used mainly in specialized situations, such as with XA transactions
# and for troubleshooting issues with concurrency and deadlocks.
#
# The following list describes how MySQL supports the different transaction levels. The list goes from the most commonly used level to the least used.
#
# 		) REPEATABLE READ
#
# 			This is the default isolation level for InnoDB. Consistent reads within the same transaction read the snapshot established by the first read.
#
# 			This means that if you issue several plain (nonlocking) SELECT statements within the same transaction, these SELECT statements are consistent
# 			also with respect to each other. See SECTION 15.7.2.3, "CONSISTENT NONLOCKING READS"
#
# 			For locking reads (SELECT with FOR UPDATE or FOR SHARE), UPDATE, and DELETE statements, locking depends on whether the statement uses a unique
# 			index with a unique search condition, or a range-type search condition.
#
# 				) For a unique index with a unique search condition, InnoDB locks only the index record found, not the gap before it.
#
# 				) For other search conditions, InnoDB locks the index range scanned, using gap locks or next-key locks to block insertions by
# 					other sessions into the gaps covered by the range.
#
# 					For information about gap locks and next-key locks, see SECTION 15.7.1, "InnoDB Locking"
#
# 		) READ COMMITTED
#
# 			Each consistent read, even within the same transaction, sets and reads its own fresh snapshot. For information about consistent
# 			reads, see SECTION 15.7.2.3, "Consistent Nonlocking Reads"
#
# 			For locking reads (SELECT with FOR UPDATE or FOR SHARE), UPDATE statements, and DELETE statements,
# 			InnoDB locks only index records, not the gaps before them, and thus permits the free insertion of new records
# 			next to locked records.
#
# 			Gap locking is only used for foreign-key constraint checking and duplicate-key checking.
#
# 			Because gap locking is disabled, phantom problems may occur, as other sessions can insert new rows into the gaps.
# 			For information about phantoms, see Section 15.7.4, "Phantom Rows"
#
# 			Only row-based binary logging is supported with the READ COMMITTED isolation level. If you use READ COMMITTED with
# 			binlog_format=MIXED, the server automatically uses row-based logging.
#
# 			Using READ COMMITTED has additional effects:
#
# 				) For UPDATE or DELETE statements, InnoDB holds lock only for rows that it updates or deletes.
#
# 					Record locks for nonmatching rows are released after MySQL has evaluated the WHERE Condition.
#
# 					This greatly reduces the probability of deadlocks, but they can still happen.
#
# 				) For UPDATE statements, if a row is already locked, InnoDB performs a "semi-consistent" read, returning
# 					the latest committed version to MySQL so that MySQL can determine whether the row matches the WHERE
# 					condition of the UPDATE.
#
# 					If the row matches (must be updated), MySQL reads the row again and this time InnoDB either locks it or
# 					waits for a lock on it.
#
# 			Consider the following example, beginning with this table:
#
# 				CREATE TABLE t (a INT NOT NULL, b INT) ENGINE = InnoDB;
# 				INSERT INTO t VALUES (1,2), (2,3), (3,2), (4,3), (5,2);
# 				COMMIT;
#
# 			In this case, the table has no indexes, so searches and index scans use the hidden clustered index for record locking
# 			(see SECTION 15.6.2.1, "Clustered and Secondary Indexes") rather than indexed columns.
#
# 			Suppose that one session performs an UPDATE using these statements:
#
# 				#Session A
# 				START TRANSACTION;
# 				UPDATE t SET b = 5 WHERE b = 3;
#
# 			Suppose also that a second session performs an UPDATE by executing these statements following those of the first session:
#
# 				#Session B
# 				UPDATE t SET b = 4 WHERE b = 2;
#
# 			As InnoDB executes each UPDATE, it first acquires an exclusive lock for each row, and then determines whether to modify it.
#
# 			If InnoDB does not modify the row, it releases the lock. Otherwise, InnoDB retains the lock until the end of the transaction.
# 			This affects transaction processing as follows.
#
# 			When using the default REPEATABLE READ isolation level, the first UPDATE acquires an x-lock on each row that it reads and does
# 			not release any of them:
#
# 				x-lock(1,2); retain x-lock
# 				x-lock(2,3); update(2,3) to (2,5); retain x-lock
# 				x-lock(3,2); retain x-lock
# 				x-lock(4,3); update(4,3) to (4,5); retain x-lock
# 				x-lock(5,2); retain x-lock
#
# 			The second UPDATE blocks as soon as it tries to acquire any locks (because first update has retained locks on all rows),
# 			and does not proceed until the first UPDATE commits or rolls back:
#
# 				x-lock(1,2); block and wait for first UPDATE to commit or roll back
#
# 			If READ COMMITTED is used instead, the first UPDATE acquires an x-lock on each row that it reads and releases those for rows
# 			that it does not modify:
#
# 				x-lock(1,2); unlock(1,2)
# 				x-lock(2,3); update(2,3) to (2,5); retain x-lock
# 				x-lock(3,2); unlock(3,2)
# 				x-lock(4,3); update(4,3) to (4,5); retain x-lock
# 				x-lock(5,2); unlock(5,2)
#
# 			For the second UPDATE, InnoDB does a "semi-consistent" read, returning the latest committed version of each row that
# 			it reads to MySQL so that MySQL can determine whether the row matches the WHERE condition of the UPDATE:
#
#  			x-lock(1,2); update(1,2) to (1,4); retain x-lock
# 				x-lock(2,3); unlock(2,3)
# 				x-lock(3,2); update(3,2) to (3,4); retain x-lock
# 				x-lock(4,3); unlock(4,3)
# 				x-lock(5,2); update(5,2) to (5,4); retain x-lock
#
# 			However, if the WHERE condition includes an indexed column, and InnoDB uses the index, only the indexed column is
# 			considered when taking and retaining record locks.
#
# 			In the following example, the first UPDATE takes and retains an x-lock on each row where b = 2. The second UPDATE
# 			blocks when it tries to acquire x-locks on the same records, as it also uses the index defined on column b.
#
# 				CREATE TABLE t (a INT NOT NULL, b INT, c INT, INDEX (b)) ENGINE = InnoDB;
# 				INSERT INTO t VALUES (1,2,3),(2,2,4);
# 				COMMIT;
#
# 				# Session A
# 				START TRANSACTION;
# 				UPDATE t SET b = 3 WHERE b = 2 AND c = 3;
#
# 				# Session B
# 				UPDATE t SET b = 4 WHERE b = 2 AND c = 4;
#
# 			The effect of using the READ COMMITTED isolation level are the same as enabling the deprecated innodb_locks_unsafe_for_binlog
# 			configuration option, with these exceptions:
#
# 				) Enabling innodb_locks_unsafe_for_binlog is a global setting and affects all sessions, whereas the isolation level can be set
# 					globally ffor all sessions, or individually per session.
#
# 				) innodb_locks_unsafe_for_binlog can be set only at server startup, whereas the istolation level can be set at startup or changed at runtime.
#
# 			READ COMMITTED therefore offers finer and more flexible control than innodb_locks_unsafe_for_binlog
#
# 				) READ UNCOMMITTED
#
# 					SELECT statements are performed in a nonlocking fashion, but a possible earlier version of a row might be used.
#
# 					Thus, using this isolation level, such reads are not consistent. This is also called a dirty read. Otherwise, the isolation level
# 					works like READ_COMMITTED.
#
# 				) SERIALIZABLE
#
# 					This level is like REPEATABLE_READ, but InnoDB implicitly converts all plain SELECT statements to SELECT_..._FOR_SHARE if autocommit
# 					is disabled.
#
# 					If autocommit is enabled, the SELECT is its own transaction. It therefore is known to be read only and can be serialized if performed
# 					as a consistent (nonblocking) read and need not block for other transactions.
#
# 					(To force a plain SELECT to block if other transactions have modified the selected rows, disable autocommit)
#
# 15.7.2.2 autocommit, Commit and Rollback
#
# 	In InnoDB, all user activity occurs inside a transaction. If autocommit mode is enabled, each SQL statement forms a single transaction
# 	on its own.
#
# 	By default, MySQL starts the session for each new connection with autocommit enabled, so MySQL does a commit after each SQL statement if that
#	statement did not return an error.
#
# 	If a statement returns an error, the commit or rollback behavior depends on the error. See SECTION 15.20.4, "InnoDB Error Handling"
#
# 	A session that has autocommit enabled can perform a multiple-statement transaction by starting it with an explicit START_TRANSACTION or BEGIN
# 	statement and ending it with a COMMIT or ROLLBACK statement.
#
# 	See Section 13.3.1, "START TRANSACTION, COMMIT, and ROLLBACK SYNTAX"
#
# 	If autocommit mode is disabled within a session with SET autocommit = 0, the session always has a transaction open.
#
# 	A COMMIT or ROLLBACK statement ends the current transaction and a new one starts.
#
# 	If a session that has autocommit disabled ends without explicitly commiting the final transaction, MySQL rolls back
# 	that transaction.
#
# 	Some statements implicitly end a transaction, as if you had done a COMMIT before executing the statement. For details,
# 	see SECTION 13.3.3, "Statements That Cause an Implicit Commit"
#
# 	A COMMIT means that the changes made in the current transaction are made permanent and become visible to other sessions.
#
# 	A ROLLBACK statement, on the other hand, cancels all modifications made by the current transaction.
#
# 	Both COMMIT and ROLLBACK release all InnoDB locks that were set during the current transaction.
#
# Grouping DML OPERATIONS WITH TRANSACTIONS
#
# By default, connection to the MySQL server begins with autocommit mode enabled, which automatically commits every SQL statement
# as you execute it.
#
# This mode of operation might be unfamiliar if you have experience with other DB systems, where it is standard practice to issue
# a sequence of DML statements and commit them or roll them back all together.
#
# To use multiple-statement transactions, switch autocommit off with the SQL statement SET autocommit = 0 and end each transaction with
# COMMIT or ROLLBACK as appropriate.
#
# To leave autocommit on, begin each transaction with START_TRANSACTION and end it with COMMIT or ROLLBACK. The following example shows
# two transactions.
#
# The first is committed, the second is rolled back.
#
# 		shell> mysql test
#
# 		mysql> CREATE TABLE customer (a INT, b CHAR (20), INDEX (a));
# 		Query OK, 0 rows affected (0.00 sec)
# 		mysql> -- Do a transaction with autocommit turned on.
# 		mysql> START TRANSACTION;
# 		Query OK, 0 rows affected (0.00 sec)
# 		mysql> INSERT INTO customer VALUES (10, 'Heikki');
# 		Query OK, 1 row affected (0.00 sec)
# 		mysql> COMMIT;
# 		Query OK, 0 rows affected (0.00 sec)
# 		mysql> -- Do another transaction with autocommit turned off
# 		mysql> SET autocommit=0;
# 		Query OK, 0 rows affected (0.00 sec)
# 		mysql> INSERT INTO customer VALUES (15, 'John');
# 		Query OK, 1 row affected (0.00 sec)
# 		mysql> INSERT INTO customer VALUES (20, 'Paul');
# 		Query OK, 1 row affected (0.00 sec)
# 		mysql> DELETE FROM customer WHERE b = 'Heikki';
# 		Query OK, 1 row affected (0.00 sec)
# 		mysql> -- Now we undo these last 2 inserts and the delte
# 		mysql> ROLLBACK;
# 		Query OK, 0 rows affected (0.00 sec)
# 		mysql> SELECT * FROM customer;
# 		+---------+--------------+
# 		| a 	 	 | b 				 |
# 		+---------+--------------+
# 		| 10 		 | Heikki 		 |
# 		+---------+--------------+
# 		1 row in set (0.00 sec)
# 		mysql>
#
# Transactions in Client-Side Languages
#
# In APIs such as PHP, Perl DBI, JDBC, ODBC or the standard C call interface of MySQL, you can send transaction control statements such as
# COMMIT to the MySQL server as strings just like any other SQL statements such as SELECT or INSERT.
#
# Some APIs also offer separate special transaction commit and rollback functions or methods.
#
# 15.7.2.3 Consistent Nonlocking Reads
#
# A consistent read means that InnoDB uses multi-versioning to present to a query a snapshot of the database at a point in time.
#
# The query sees the changes made by transactions that committed before that point of time, and no changes made by later or uncommitted
# transactions.
#
# The exception to this rule is that the query sees the changes made by earlier statements within the same transaction.
#
# This exception causes the following anomaly: if you update some rows in a table, a SELECT sees the latest version of the
# updated rows, but it might also see older versions of any rows.
#
# If other sessions simultaneously update the same table, the anomaly means that you might see the table in a state that never
# existed in the database.
#
# If the transaction isolation level is REPEATABLE_READ (the default level), all consistent reads within the same transaction read
# the snapshot established by the first such read in that transaction.
#
# You can get a fresher snapshot for your queries by committing the current transaction and after that issuing new queries.
#
# With READ_COMMITTED isolation level, each consistent read within a transaction sets and reads its own fresh snapshot.
#
# Consistent read is the default mode in which InnoDB processes SELECT statements in READ_COMMITTED and REPEATABLE_READ isolation
# levels.
#
# A consistent read does not set any locks on the tables it accesses, and therefore other sessions are free to modify those tables
# at the same time a consistent read is being performed on the table.
#
# Suppose that you are running in the default REPEATABLE_READ isolation level. When you issue a consistent read (that is, an ordinary
# SELECT statement), InnoDB gives your transaction a timepoint according to which your query sees the database.
#
# If another transaction deletes a row and commits after your timepoint was assigned, you do not see the row as having been deleted.
#
# Inserts and updates are treated similarly.
#
# 		NOTE:
#
# 			The snapshot of the database state applies to SELECT statements within a transaction, not necessarily to DML statements.
#
# 			If you insert or modify some rows and then commit that transaction, a DELETE or UPDATE statement issued from another concurrent
# 			REPEATABLE READ transaction could affect those just-committed rows, even though the session could not query them.
#
# 			If a transaction does update or delete rows committed by a different transaction, those changes do become visible to the current
# 			transaction.
#
# 			For example, you might encounter a situation like the following:
#
# 				SELECT COUNT(c1) FROM t1 WHERE c1 = 'xyz';
# 				-- Returns 0: No rows match.
# 				DELETE FROM t1 WHERE c1 = 'xyz';
# 				-- Deletes several rows recently committed by other transaction.
#
# 				SELECT COUNT(c2) FROM t1 WHERE c2 = 'abc';
# 				-- Returns 0: no rows match.
# 				UPDATE t1 SET c2 = 'cba' WHERE c2 = 'abc';
# 				-- Affects 10 rows: another txn just committed 10 rows with 'abc' values.
# 				SELECT COUNT(c2) FROM t1 WHERE c2 = 'cba';
# 				-- Returns 10: THis txn can now see the rows it just updated.
#
# 	You can advance your timepoint by committing your transaction and then doing another SELECT or START_TRANSACTION_WITH_CONSISTENT_SNAPSHOT.
#
# 	This is called multi-versioned concurrency control.
#
# 	In the following example, session A sees the row inserted by B only when B has committed the insert and A has committed as well, so that
# 	the timepoint is advanced past the commit of B.
#
# 					Session A 				Session B
#
# 				SET autocommit=0; 	SET autocommit=0;
# 		time
# 		| 		SELECT * FROM t;
# 		| 		empty set
# 		| 									INSERT INTO t VALUES (1,2);
# 		|
# 		v 		SELECT * FROM t;
# 				empty set
# 											COMMIT;
#
# 				SELECT * FROM t;
# 				empty set
#
# 				COMMIT;
#
# 				SELECT * FROM t;
# 				+-------------+-----------------+
# 				| 1 			  | 		2 			  |
# 				+-------------+-----------------+
#
# If you want to see the "freshest" state of the database, use either the READ_COMMITTED isolation level or a locking read:
#
# 		SELECT * FROM t FOR SHARE;
#
# With READ_COMMITTED isolation level, each consistent read within a transaction sets and reads its own fresh snapshot.
#
# With FOR SHARE, a locking read occurs instead: A SELECT blocks until the transaction containing the freshest rows ends
# (see Section 15.7.2.4, "Locking Reads")
#
# Consistent read does not work over certain DDL statements:
#
# 		) Consistent read does not work over DROP_TABLE, because MySQL cannot use a table that has been dropped and InnoDB destroys the table.
#
# 		) Consistent read does not work over ALTER_TABLE, because that statement makes a temporary copy of the original table and deletes the original
# 			table when the temporary copy is built.
#
# 			WHen you reissue a consistent read within a transaction, rows in the new table are not visible because those rows did not exist when the
# 			transaction's snapshot was taken. In this case, the transaction returns an error: ER_TABLE_DEF_CHANGED, "Table definition has changed,
# 			please retry transaction".
#
# The type of read varies for selects in clauses like INSERT_INTO_..._SELECT, UPDATE_..._(SELECT), and CREATE_TABLE_..._SELECT that do not specify
# FOR UPDATE or FOR SHARE:
#
# 		) By default, InnoDB uses stronger locks and the SELECT part acts like READ_COMMITTED, where each consistent read, even within the same
# 			transaction, sets and reads its own fresh snapshot.
#
# 		) To use a consistent read in such cases, set the isolation level of the transaction to READ_UNCOMMITTED, READ_COMMITTED or REPEATABLE_READ
# 			(that is, anything other than SERIALIZABLE).
#
# 			In this case, no locks are set on rows read from the selected table.
#
# 15.7.2.4 LOCKING READS
#
# If you query data and then insert or update related data within the same transaction, the regular SELECT statement does not give enough
# protection.
#
# Other transactions can update or delete the same rows you just queried. InnoDB supports two types of locking reads that offer extra safety:
#
# 		) SELECT_..._FOR_SHARE
#
# 			Sets a shared mode lock on any rows that are read. Other sessions can read the rows, but cannot modify them until your transaction commits.
#
# 			If any of these rows were changed by another transaction that has not yet committed, your query waits until that tranasction ends and
# 			then uses the latest values.
#
# 				NOTE:
#
# 					SELECT_..._FOR_SHARE is a replacement for SELECT_..._LOCK_IN_SHARE_MODE, but LOCK_IN_SHARE_MODE remains available for backward
# 					compatibility.
#
# 					The statements are equivalent. However, FOR SHARE supports OF TABLE_NAME, NOWAIT, and SKIP LOCKED options. See Locking Read
# 					Concurrency with NOWAIT and SKIP LOCKED.
#
# 		) SELECT_..._FOR_UPDATE
#
# 			For index records that search encounters, locks the rows and any associated index entries, the same as if you issued an UPDATE statement
# 			for those rows.
#
# 			Other transactions are blocked from updating those rows, from doing SELECT_..._FOR_SHARE, or from reading the data in certain transaction
# 			isolation levels.
#
# 			Consistent reads ignore any locks set on the records that exist in the read view. (Old versions of a record cannot be locked: they are reconstructed
# 			by applying undo logs on an in-memory copy of the record)
#
# These clauses are primarily useful when dealing with tree-structured or graph-structured data, either in a single table or split across multiple tables.
#
# You traverse edges or tree branches from one place to another, while reserving the right to come back and change any of these "pointer" values.
#
# All locks set by FOR SHARE and  FOR UPDATE queries are released when the transaction is committed or rolled back.
#
# NOTE:
#
# 		Locking reads are only possible when autocommit is disabled (either by beginning transaction with START_TRANSACTION or by setting autocommit to 0)
#
# A locking read clause in an outer statement does not lock the rows of a table in a nested subquery unless a locking read clause is also specified
# in the subquery.
#
# For example, the following statement does not lock rows in table t2.
#
# 		SELECT * FROM t1 WHERE c1 = (SELECT c1 FROM t2) FOR UPDATE;
#
# To lock rows in table t2, add a locking read clause to the subquery:
#
# 		SELECT * FROM t1 WHERE c1 = (SELECT c1 FROM t2 FOR UPDATE) FOR UPDATE;
#
# LOCKING READ EXAMPLES
#
# Suppose that you want to insert a new row into a table child, and make sure that the child row has a parent row in table parent.
#
# Your application code can ensure referential integrity throughout this sequence of operations.
#
# First, use a consistent read to query the table PARENT and verify that the parent row exists. Can you safely insert child row to table
# CHILD?
#
# No, because some other session could delete the parent row in the moment between your SELECT and your INSERT, without you being aware of it.
#
# To avoid this potential issue, perform the SELECT using FOR SHARE:
#
# 		SELECT * FROM parent WHERE NAME = 'Jones' FOR SHARE;
#
# After the FOR SHARE query returns the parent 'Jones', you can safely add the child record to the CHILD table and commit the transaction.
#
# Any transaction that tries to acquire an exclusive lock in the applicable row in the PARENT table waits until you are finished, that is,
# until the data in all tables is in a consistent state.
#
# For another example, consider an integer counter field in a table CHILD_CODES, used to assign a unique identifier to each child added
# to table CHILD.
#
# Do not use either consistent read or a shared mode read to read the present value of the counter, because two users of the database
# could see the same value for the counter, and a duplicate-key error occurs if two transactions attempt to add rows with the same
# identifier to the CHILD table.
#
# Here, FOR SHARE is not a good solution because if two users read the counter at the same time, at least one of them ends up in deadlock
# when it attempts to update the counter.
#
# To implement reading and incrementing the counter, first perform a locking read of the counter using FOR UPDATE, and then increment
# the counter.
#
# For example:
#
# 		SELECT counter_field FROM child_codes FOR UPDATE;
# 		UPDATE child_codes SET counter_field = counter_field + 1;
#
# A SELECT_..._FOR_UPDATE reads the latest available data, setting exclusive locks on each row it reads.
#
# Thus, it sets the same locks a searched SQL UPDATE would set on the rows.
#
# The preceding description is merely an example of how SELECT_..._FOR_UPDATE works.
#
# In MySQL, the specific task of generating a unique identifier actually can be accomplished
# using only a single access to the table:
#
# 		UPDATE child_codes SET counter_field = LAST_INSERT_ID(counter_field + 1);
# 		SELECT LAST_INSERT_ID();
#
# The SELECT statement merely retrieves the identifier information (specific to the current connection).
# It does not access any table.
#
# LOCKING READ CONCURRENCY WITH NOWAIT AND SKIP LOCKED
#
# If a row is locked by a transaction, a SELECT_..._FOR_UPDATE or SELECT_..._FOR_SHARE transaction that requests the same locked row
# must wait until the blocking transaction releases the row lock.
#
# This behavior prevents transactions from updating or deleting rows that are queried for updates by other transactions.
#
# However, waiting for a row lock to be released is not necessary if you want the query to return immediately when a requested row is locked,
# or if excluding locked rows from the result set is acceptable.
#
# To avoid waiting for other transactions to release row locks, NOWAIT and SKIP LOCKED options may be used with SELECT_..._FOR_UPDATE or SELECT_..._FOR_SHARE
# locking read statements.
#
# 		) NOWAIT
#
# 			A locking read that uses NOWAIT never waits to acquire a row lock. The query executes immediately, failing with an error if a requested row is locked.
#
# 		) SKIP LOCKED
#
# 			A locking read that uses SKIP LOCKED never waits to acquire a row lock. The query executes immediately, removing locked rows from the result set.
#
# 				NOTE:
#
# 					Queries that skip locked rows return an inconsistent view of the data. SKIP LOCKED is therefore not suitable for general transactional work.
# 					However, it may be used to avoid lock contention when multiple sessions access the same queue-like table.
#
# NOWAIT and SKIP LOCKED only apply to row-level locks.
#
# Statements that use NOWAIT or SKIP LOCKED are unsafe for statement based replication.
#
# The following example demonstrates NOWAIT and SKIP LOCKED. Session 1 starts a transaction that takes a row lock on a single record.
#
# Session 2 attempts a locking read on the same record using the NOWAIT option. 
#
# Because the requested row is locked by Session 1, the locking read returns immediately with an error.
# In Session 3, the locking read with SKIP LOCKED returns the requested rows except for the row that is locked by Session 1.
#
# 		# Session 1:
#
# 		mysql> CREATE TABLE t (i INT, PRIMARY KEY (i)) ENGINE = InnoDB;
#
# 		mysql> INSERT INTO t (i) VALUES(1), (2), (3);
#
# 		mysql> START TRANSACTION;
#
# 		mysql> SELECT * FROM t WHERE i = 2 FOR UPDATE;
# 		+------+
# 		| i 	 |
# 		+------+
# 		| 2 	 |
# 		+------+
#
# 		# Session 2:
#
# 		mysql> START TRANSACTION;
#
# 		mysql> SELECT * FROM t WHERE i = 2 FOR UPDATE NOWAIT;
# 		ERROR 3572 (HY000): Do not wait for lock.
#
# 		# Session 3
#
# 		mysql> START TRANSACTION;
#
# 		mysql> SELECT * FROM t FOR UPDATE SKIP LOCKED;
# 		+------+
# 		| i 	 |
# 		+------+
# 		| 1 	 |
# 		| 3 	 |
# 		+------+
#
# 15.7.3 LOCKS SET BY DIFFERENT SQL STATEMENTS IN INNODB
#
# A locking read, an UPDATE, or a DELETE generally set record locks on every index record that is scanned in the processing of the SQL
# statement.
#
# It does not matter whether there are WHERE conditions in the statement that would exclude the row. InnoDB does not remember the exact
# WHERE condition, but only knows which index ranges were scanned.
#
# The locks are normally next-key locks that also block inserts into the "gap" immediately before the record. However, gap locking can be
# disabled explicitly, which causes next-key locking not to be used.
#
# For more information, see SECTION 15.7.1, "InnoDB Locking".
#
# The transaction isolation level also can affect which locks are set; see SECTION 15.7.2.1, "Transaction Isolation Levels".
#
# If a secondary index is used in a search and index record locks to be set are exclusive, InnoDB also retrieves the corresponding
# clustered index records and sets locks on them.
#
# If you have no indexes suitable for your statement and MySQL must scan the entire table to process the statement, every row of the
# table becomes locked, which in turn blocks all inserts by other users to the table.
#
# It is important to create good indexes so that your queries do not unnecessarily scan many rows.
#
# InnoDB sets specific types of locks as follows:
#
# 		) SELECT_..._FROM is a consistent read, reading a snapshot of the database and setting no locks unless the transaction
# 			isolation level is set to SERIALIZABLE.
#
# 			For SERIALIZABLE level, the search sets shared next-key locks on the index records it encounters.
#
# 			However, only an index record lock is required for statements that lock rows using a unique index to search
# 			for a unique row.
#
# 		) SELECT_..._FOR_UPDATE and SELECT_..._FOR_SHARE statements that use a unique index acquire locks for scanned rows,
# 			and release the locks for rows that do not qualify for inclusion in the result set (for example, if they do not meet
# 			the criteria given in the WHERE clause).
#
# 			However, in some cases, rows might not be unlocked immediately because the relationship between a result row and its
# 			original source is lost during query execution.
#
# 			For example, in a UNION, scanned (and locked) rows from a table might be inserted into a temporary table before
# 			evaluation whether they qualify for the result set.
#
# 			In this circumstance, the relationship of the rows in the temporary table to the rows in the original table is lost
# 			and the latter rows are not unlocked until the end of query execution.
#
# 		) For locking reads (SELECT with FOR UPDATE or FOR SHARE), UPDATE, and DELETE statements, the locks that are taken depend on whether
# 			the statement uses a unique index with a unique search condition, or a range-type search condition.
#
# 			) For a unique index with a unique search condition, InnoDB locks only the index record found, not the gap before it.
#
# 			) For other search conditions, and for non-unique indexes, InnoDB locks the index range scanned, using gap locks or next-key locks
# 				to block insertions by other sessions into the gaps covered by the range.
#
# 				For information about gap locks and next-key locks, see SECTION 15.7.1, "InnoDB Locking"
#
# 		) For index records that search encounters, SELECT_..._FOR_UPDATE blocks other sessions from doing SELECT_..._FOR_SHARE or from
# 			reading in certain transaction isolation levels.
#
# 			Consistent reads ignore any locks set on the records that exist in the read view.
#
# 		) UPDATE_..._WHERE_... sets an exclusive next-key lock on every record the search encounters.
#
# 			However, only an index record lock is required for statements that lock rows using a unique
# 			index to search for a unique row.
#
# 		) When UPDATE modifies a clustered index record, implicit locks are taken on affected secondary index records.
#
# 			The UPDATE operation also takes shared locks on affected secondary index records when performing duplicate check
# 			scans prior to inserting new secondary index records, and when inserting new secondary index records.
#
# 		) DELETE_FROM_..._WHERE_... sets an exclusive next-key lock on every record the search encounters.
#
# 			However, only an index record lock is required for statements that lock rows using a unique index
# 			to search for a unique row.
#
# 		) INSERT sets an exclusive lock on the inserted row. This lock is an index-record lock, not a next-key lock (that is,
# 			there is no gap lock) and does not prevent other sessions from inserting into the gap before the inserted row.
#
# 			Prior to inserting the row, a type of gap lock called an insert intention gap lock is set.
#
# 			This lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap
# 			need not wait for each other if they are not inserting at the same position within the gap.
#
# 			Suppose that there are index records with values of 4 and 7.
#
# 			Separate transactions that attempt to insert values of 5 and 6 each lock the gap between 4 and 7 with insert
# 			intention locks prior to obtaining the exclusive lock on the inserted row, but do not block each other because
# 			the rows are nonconflicting.
#
# 			If a duplicate-key error occurs, a shared lock on the duplicate index record is set. This use of a shared lock can
# 			result in deadlock should there be multiple sessions trying to insert the same row if another session already has an
# 			exclusive lock.
#
# 			This can occur if another session deletes the row. Suppose that an InnoDB table t1 has the following structure:
#
# 				CREATE TABLE t1 (i INT, PRIMARY KEY (i)) ENGINE = InnoDB;
#
# 			Now suppose that three sessions perform the following operations in order:
#
# 				Session 1:
#
# 					START TRANSACTION;
# 					INSERT INTO t1 VALUES(1);
#
# 				Session 2:
#
# 					START TRANSACTION;
# 					INSERT INTO t1 VALUES(1);
#
# 				Session 3:
# 					START TRANSACTION;
# 					INSERT INTO t1 VALUES(1);
#
# 				Session 1:
#
# 					ROLLBACK;
#
# 			The first operation by session 1 acquires an exclusive lock for the row.
#
# 			The operations by sessions 2 and 3 both result in a duplicate-key error and they both request a 
# 			shared lock for the row.
#
# 			WHen session 1 rolls back, it releases its exclusive lock on the row and the queued shared lock requests
# 			for sessions 2 and 3 are granted.
#
# 			At this point, sessions 2 and 3 deadlock: Neither can acquire an exclusive lock for the row because
# 			of the shared lock held by the other.
#
# 			A similar situation occurs if the table already contains a row with key value 1 and three sessions perform
# 			the following operations in order:
#
# 				Session 1:
#
# 					START TRANSACTION;
# 					DELETE FROM t1 WHERE i = 1;
#
# 				Session 2:
#
# 					START TRANSACTION;
# 					INSERT INTO t1 VALUES(1);
#
# 				Session 3:
#
# 					START TRANSACTION;
# 					INSERT INTO t1 VALUES(1);
#
# 				Session 1:
#
# 					COMMIT;
#
# 			The first operation by session 1 acquires an exclusive lock for the row. The operations by sessions 2 and 3 both result
# 			in a duplicate-key error and they both request a shared lock for the row.
#
# 			When session 1 commits, it releases its exclusive lock on the row and the queued shared lock requests for sessions 2 and 3
# 			are granted.
#
# 			At this point, sessions 2 and 3 deadlock: Neither can acquire an exclusive lock for the row because of the shared lock
# 			held by the other.
#
# 		) INSERT_..._ON_DUPLICATE_KEY_UPDATE differs from a simple INSERT in that an exclusive lock rather than a shared lock is placed on the row
# 			to be updated when a duplicate-key error occurs.
#
# 			An exclusive index-record lock is taken for a duplicate primary key value.
#
# 			An exclusive next-key lock is taken for a duplicate unique key value.
#
# 		) REPLACE is done like an INSERT if there is no collision on a unique key. Otherwise, an exclusive next-key lock is placed on the row to be replaced.
#
# 		) INSERT INTO T SELECT ... FROM S WHERE ... sets an exclusive index record lock (without a gap lock) on each row inserted into T.
#
# 			If the transaction isolation level is READ_COMMITTED, InnoDB does the search on S as a consistent read (no locks). Otherwise,
# 			InnoDB sets shared next-key locks on rows from S.
#
# 			InnoDB has to set locks in the latter case: During roll-forward recovery using a statement-based binary log, every SQL statement
# 			must be executed in exactly the same way it was done originally.
#
# 			CREATE_TABLE_..._SELECT_... performs the SELECT with shared next-key locks or as a consistent read, as for INSERT_..._SELECT
#
# 			When a SELECT is used in the constructs REPLACE INTO t SELECT ... FROM s WHERE ... or UPDATE t ... WHERE col IN (SELECT ... FROM s ...),
# 			InnoDB sets shared next-key locks on rows from table s.
#
# 		) While initializing a previously specified AUTO_INCREMENT column on a table, InnoDB sets an exclusive lock on the end of the index associated
# 			with the AUTO_INCREMENT column.
#
# 			In accessing the auto-increment counter, InnoDB uses a specific AUTO-INC table lock mode where the lock lasts only to the end of the current
# 			SQL statement, not to the end of the entire transaction.
#
# 			Other sessions cannot insert into the table while the AUTO-INC table lock is held; see SECTION 15.7.2, "InnoDB Transaction Model"
#
# 			InnoDB fetches the value of a previously initialized AUTO_INCREMENT column without setting any locks.
#
# 		) If a FOREIGN KEY constraint is defined on a table, any insert, update or delete that requires the constraint condition to be checked sets
# 			shared record level locks on the records that it looks at to check the constraint.
#
# 			InnoDB also sets these locks in the case where the constraint fails.
#
# 		) LOCK_TABLES sets table locks, but it is the higher MySQL layer above the InnoDB layer that sets these locks. 
#
# 			InnoDB is aware of table locks if innodb_table_locks = 1 (the default) and autocommit_=_0, and the MySQL layer
# 			above InnoDB knows about row-level locks.
#
# 			Otherwise, InnoDB's automatic deadlock detection cannot detect deadlocks where such table locks are involved.
#
# 			Also, because in this case the higher MySQL layer does not know about row-level locks, it is possible to get a table
# 			lock on a table where another session currently has row-level locks.
#
# 			However, this does not endanger transaction integrity, as discussed in SECTION 15.7.5.2, "Deadlock Detection and RollBack"
#
# 			See also SECTION 15.6.1.6, "Limits on InnoDB Tables"
#
# 15.7.4 PHANTOM ROWS
#
# The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times.
#
# For example, if a SELECT is executed twice, but returns a row the second time that was not returned the first time, the row is a "phantom" row.
#
# Suppose that there is an index on the id column of the child table and that you want to read and lock all rows from the table having an identifier
# value larger than 100, with the intention of updating some column in the selected rows later:
#
# 		SELECT * FROM child WHERE id > 100 FOR UPDATE;
#
# The query scans the index starting from the first record where id is bigger than 100. Let the table contain rows having id values of 90 and 102.
#
# If the locks set on the index records in the scanned range do not lock out inserts made in the gaps (in this case, the gap between 90 and 102), another
# session can insert a new row into the table with an id of 101.
#
# If you were to execute the same SELECT within the same transaction, you would see a new row with an id of 101 (a "phantom") in the result set returned by
# the query.
#
# If we regard a set of rows as a data item, the new phantom child would violate the isolation principle of transactions that a transaction should be able
# to run so that the data it has read does not change during the transaction.
#
# To prevent phantoms, InnoDB uses an algorithm called next-key locking that combines index-row locking with gap locking. InnoDB performs row-level locking
# in such a way that when it searches or scans a table index, it sets shared or exclusive locks on the index records it encounters.
#
# Thus, the row-level locks are actually index-record locks. In addition, a next-key lock on an index record also affects the "gap" before that index record.
#
# That is, a next-key lock is an index-record lock plus a gap lock on the gap preceding the index record. If one session has a shared or exclusive lock on record
# R in an index, another session cannot insert a new index record in the gap immediately before R in the index order.
#
# When InnoDB scans an index, it can also lock the gap after the last record in the index. Just that happens in the preceding example:
#
# 		To prevent any insert into the table where id would be bigger than 100, the locks set by InnoDB include a lock on the gap following
# 		id value 102.
#
# You can use next-key locking to implement a uniqueness check in your application: If you read your data in share mode and do not see a duplicate
# for a row you are going to insert, then you can safely insert your row and know that the next-key lock set on the successor of your row during
# the read prevents anyone meanwhile inserting a duplicate for your row.
#
# Thus, the next-key locking enables you to "lock" the nonexistence of something in your table.
#
# Gap locking can be disabled as discussed in Section 15.7.1, "InnoDB Locking". This may cause phantom problems because other sessions can insert
# new rows into the gaps when gap locking is disabled.
#
# 15.7.5 DEADLOCKS IN INNODB
#
# 15.7.5.1 An InnoDB Deadlock Example
# 15.7.5.2 Deadlock Detection and Rollback
# 15.7.5.3 How to Minimize and Handle Deadlocks
#
# A deadlock is a situation where different transactions are unable to proceed because each holds a lock that the other needs.
# Because both transactions are waiting for a resource to become available, neither ever release the locks it holds.
#
# A deadlock can occur when transaction lock rows in multiple tables (through statements such as UPDATE or SELECT_..._FOR_UPDATE),
# but in the opposite order. A deadlock can also occur when such statements lock ranges of index records and gaps, with each transaction
# acquiring some locks but not others due to a timing issue.
#
# For a deadlock example, see SECTION 15.7.5.1, "An InnoDB Deadlock Example"
#
# To reduce the possibility of deadlocks, use transactions rather than LOCK_TABLES statements; keep transactions that insert or update data
# small enough that they do not stay open for long periods of time; when different transactions update multiple tables or large ranges of rows,
# use the same order of operations (such as SELECT_..._FOR_UPDATE) in each transaction; create indexes on the columns used in SELECT_..._FOR_UPDATE
# and UPDATE_..._WHERE statements.
#
# The possibility of deadlocks is not affected by the isolation level, because the isolation level changes the behavior of read operations, while
# deadlocks occur because of write operations.
#
# For more information about avoiding and recovering from deadlock conditions, see SECTION 15.7.5.3, "How to Minimize and Handle Deadlocks"
#
# When deadlock detection is enabled (the default) and a deadlock does occur, InnoDB detects the condition and rolls back one of the transactions
# (the victim). If deadlock detection is disabled using the innodb_deadlock_detect configuration option, InnoDB relies on the innodb_lock_wait_timeout
# setting to roll back transactions in case of a deadlock.
#
# Thus, even if your application logic is correct, you must still handle the case where a transaction must be retried.
#
# To see the last deadlock in an InnoDB user transaction, use the SHOW_ENGINE_INNODB_STATUS command.
#
# If frequent deadlocks highlight a problem with transaction structure or application error handling, run with the innodb_print_all_deadlocks
# setting enabled to print information about all deadlocks to the mysqld error log.
#
# For more information about how deadlocks are automatically detected and handled, see SECTION 15.7.5.2, "Deadlock Detection and Rollback"
#
# 15.7.5.1 An InnoDB Deadlock Example
#
# The following example illustrates how an error can occur when a lock request would cause a deadlock. The example involves two clients.
# A and B.
#
# First, client A creates a table containing one row, and then begins a transaction. Within the transaction, A obtains an S lock on the row
# by selecting it in share mode:
#
# 		mysql> CREATE TABLE t (i INT) ENGINE = InnoDB;
# 		Query OK, 0 rows affected (1.07 sec)
#
# 		mysql> INSERT INTO t (i) VALUES(1);
# 		Query OK, 1 row affected (0.00 sec)
#
# 		mysql> START TRANSACTION;
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		mysql> SELECT * FROM t WHERE i = 1 FOR SHARE;
# 		+-----------+
# 		| i 		 	|
# 		+-----------+
# 		| 1 			|
# 		+-----------+
#
# Next, Client B begins a transaction and attempts to delete the row from the table:
#
# 		mysql> START TRANSACTION;
# 		Query OK, 0 rows affected (0.00 sec)
#
# 		mysql> DELETE FROM t WHERE i = 1;
#
# The delete operation requires an X lock. The lock cannot be granted because it is incompatible with the S lock that client
# A holds, so the request goes on the queue of lock requests for the row and client B blocks.
#
# Finally, client A also attempts to delete the row from the table:
#
# 		mysql> DELETE FROM t WHERE i = 1;
# 		ERROR 1213 (40001): Deadlock found when trying to get lock;
# 		try restarting transaction
#
# Deadlock occurs here because client A needs an X lock to delete the row. However, that lock request cannot be granted
# because client B already has a request for an X lock and is waiting for client A to release its S lock.
#
# Nor can the S lock held by A be upgraded to an X lock because of the prior request by B for an X lock.
#
# As a result, InnoDB generates an error for one of the clients and releases its locks. The client returns the error:
#
# 		ERROR 1213 (40001): Deadlock found when trying to get lock;
# 		try restarting transaction
#
# At that point, the lock request for the other client can be granted and it deletes the row from the table.
#
# 15.7.5.2 Deadlock Detection and Rollback
#
# When deadlock detection is enabled (the default), InnoDB automatically detects transaction deadlocks and rolls back
# a transaction or transactions to break the deadlock.
#
# InnoDB tries to pick small transactions to roll back, where the size of a transaction is determined by the number of
# rows inserted, updated, or deleted.
#
# InnoDB is aware of table locks if innodb_table_locks = 1 (the default) and autocommit_=_0, and the MySQL layer above
# it knows about row-level locks.
#
# Otherwise, InnoDB cannot detect deadlocks where a table lock set by a MySQL LOCK_TABLES statement or a lock set by a storage
# engine other than InnoDB is involved.
#
# Resolve these situations by setting the value of the innodb_lock_wait_timeout system variable.
#
# When InnoDB performs a complete rollback of a transaction, all locks set by the transaction are released. However, if just
# a single SQL statement is rolled back as a result of an error, some of the locks set by the statement may be preserved.
#
# This happens because InnoDB stores row locks in a format such that it cannot know afterward which lock was set by which
# statement.
#
# If a SELECT calls a stored function in a transaction, and a statement within the function fails, that statement rolls back.
# Furthermore, if ROLLBACK is executed after that, the entire transaction rolls back.
#
# If the LATEST DETECTED DEADLOCK section of InnoDB Monitor output includes a message stating, 
# "TOO DEEP OR LONG SEARCH IN THE LOCK TABLE WAITS-FOR GRAPH, WE WILL ROLL BACK FOLLOWING TRANSACTION",
# This indicates that hte number of transactions on the wait-for list has reached a limit of 200.
#
# A wait-for list that exceeds 200 transactions is treated as a deadlock and the transaction attempting to check the
# wait-for list is rolled back.
#
# The same error may also occur if the locking thread must look at more than 1,000,000 locks owned by transactions
# on the wait-for list.
#
# For techniques to organize database operations to avoid deadlocks, see SECTION 15.7.5, "Deadlocks in InnoDB"
#
# DISABLING DEADLOCK DETECTION
#
# On high concurrency systems, deadlock detection can cause a slowdown when numerous threads wait for the same lock.
# At times, it may be more efficient to disable deadlock detection and rely on the innodb_lock_wait_timeout setting
# for transaction rollback when a deadlock occurs.
#
# Deadlock detection can be disabled using the innodb_deadlock_detect configuration option.
#
# 15.7.5.3 HOW TO MINIMIZE AND HANDLE DEADLOCKS
#
# This section builds on the conceptual information about deadlocks in Section 15.7.5.2, "Deadlock Detection and Rollback".
#
# It explains how to organize database operations to minimize deadlocks and the subsequent error handling required in applications.
#
# Deadlocks are a classic problem in transactional databases, but they are not dangerous unless they are so frequent that you cannot
# run certain transactions at all.
#
# Normally, you must write your applications so that they are always prepared to re-issue a transaction if it gets rolled back because
# of a deadlock.
#
# InnoDB uses automatic row-level locking. You can get deadlocks even in the case of transactions that just insert or delete a single
# row.
#
# That is because these operations are not reall "atomic", they automatically set locks on the (possibly several) index records of the
# row inserted or deleted.
#
# You can cope with deadlocks and reduce thhe likelihood of their occurence with the following techniques:
#
# 		) At any time, issue the SHOW_ENGINE_INNODB_STATUS command to determine the cause of the most recent deadlock.
#
# 			That can help you to tune your application to avoid deadlocks.
#
# 		) If frequent deadlock warnings cause concern, collect more extensive debugging information by enabling the innodb_print_all_deadlocks
# 			configuration option.
#
# 			Information about each deadlock, not just the last one, is recorded in the MySQL error log. Disable this option when you are finished
# 			debugging.
#
# 		) Always be prepared to re-issue a transaction if it fails due to deadlock. Deadlocks are not dangerous. Just try again.
#
# 		) Keep transactions small and short in duration to make them less prone to collision.
#
# 		) Commit transactions immediately after making a set of related changes to make them less prone to collision.
#
# 			In particular, do not leave an interactive mysql session open for a long time with an uncommitted transaction.
#
# 		) If you use locking reads (SELECT_..._FOR_UPDATE or SELECT_..._FOR_SHARE), try using a lower isolation level such as READ_COMMITTED.
#
# 		) When modifying multiple tables within a transaction, or different sets of rows in the same table, do those operations in a consistent
# 			order each time.
#
# 			Then transactions form well-defined queues and do not deadlock. For example, organize database operations into functions within your
# 			application, or call stored routines, rather than coding multiple similar sequences of INSERT, UPDATE and DELETE statements in different
# 			places.
#
# 		) Add well-chosen indexes to your tables. Then your queries need to scan fewer index records and consequently set fewer locks.
#
# 			Use EXPLAIN_SELECT to determine which indexes the MySQL server regards as the most appropriate for your queries.
#
# 		) Use less locking. If you can afford to permit a SELECT to return data from an old snapshot, do not add the clause FOR UPDATE or FOR SHARE to it.
#
# 			Using the READ_COMMITTED isolation level is good here, because each consistent read within the same transaction reads from its own fresh
# 			snapshot.
#
# 		) If nothing else helps, serialize your transactions with table-level locks. The correct way to use LOCK_TABLES with transactional tables, such as
# 			InnoDB tables, is to begin a transaction with SET autocommit = 0 (not START_TRANSACTION) followed by LOCK_TABLES, and to not call UNLOCK_TABLES
# 			until you commit the transaction explicitly.
#
# 			For example, if you need to write to table t1 and read from table t2, you can do this:
#
# 				SET autocommit=0;
# 				LOCK TABLES t1 WRITE, t2 READ ...;
#		 			// do something with tables t1 and t2 here //
# 				COMMIT;
# 				UNLOCK TABLES;
#
# 			Table-level locks prevent concurrent updates to the table, avoiding deadlocks at the expense of less responsiveness for a busy system.
#
# 		) Another way to serialize transactions is to create an auxiliary "semaphore" table that contains just a single row.
#
# 			Have each transaction update that row before accessing other tables. In that way, all transactions happen in a serial fashion.
# 			Note that the InnoDB instant deadlock detection algorithm also works in this case, because the serializing lock is a row-level lock.
#
# 			With MySQL table-level locks, the timeout method must be used to resolve deadlocks.
#
# 15.8 InnoDB Configuration
#
# 15.8.1 InnoDB Startup Configuration
# 15.8.2 Configuring InnoDB for Read-Only Operation
# 15.8.3 InnoDB Buffer Pool Configuration
# 15.8.4 Configuring Thread Concurrency for InnoDB
# 15.8.5 Configuring the Number of Background InnoDB I/O Threads
# 15.8.6 Using Asynchronous I/O on Linux
# 15.8.7 Configuring the InnoDB Master Thread I/O Rate
# 15.8.8 Configuring Spin Lock Polling
# 15.8.9 Configuring InnoDB Purge Scheduling
# 15.8.10 Configuring Optimizer Statistics for InnoDB
# 15.8.11 Configuring the Merge Threshold for Index Pages
# 15.8.12 Enabling Automatic Configuration for a Dedicated MySQL Server
#
# This section provides configuration information and procedures for InnoDB initialization, startup, and various components
# and features of the InnoDB storage engine.
#
# For information about optimizing database operations for InnoDB tables, see Section 8.5, "Optimizing for InnoDB Tables"
#
# 15.8.1 InnoDB Startup Configuration
#
# The first decisions to make about InnoDB configuration involve the configuration of data files, log files, page size, and
# memory buffers.
#
# It is recommended that you define data file, log file, and page size configuration before creating the InnoDB instance.
#
# Modifying data file or log file configuration after the InnoDB instance is created may involve a non-trivial procedure,
# and page size can only be defined when the InnoDB instance is first initialized.
#
# In addition to these topics, this section provides information about specifying InnoDB options in a configuration file,
# viewing InnoDB initialization information, and important storage considerations.
#
# 		) Specifying Options in a MySQL Configuration File
#
# 		) Viewing InnoDB initialization Information
#
# 		) Important Storage Considerations
#
# 		) System Tablespace Data File Configuration
#
# 		) Redo Log File Configuration
#
# 		) Undo Tablespace Configuration
#
# 		) Global Temporary Tablespace Configuration
#
# 		) Session Temporary Tablespace Configuration
#
# 		) Page Size Configuration
#
# 		) Memory Configuration
#
# Specifying Options in a MySQL Configuration File
#
# Because MySQL uses data file, log file, and page size configuration settings to initialize the InnoDB instance, it is recommended
# that you define these settings in a configuration file that MySQL reads at startup, prior to initializing InnoDB for the first time.
#
# InnoDB is initialized when the MySQL server is started, and the first initialization of InnoDB normally occurs the first time
# you start the MySQL Server.
#
# You can place InnoDB options in the [mysqld] group of any option file that your server reads when it starts. The locations of MySQL
# option files are described in Section 4.2.2.2, "Using Option Files"
#
# To make sure that mysqld reads options only from a specific file (and mysqld-auto.cnf), use the --defaults-file option as the first
# option on the command line when starting the server:
#
# 		mysqld --defaults-file=path_to_configuration_file
#
# Viewing InnoDB initialization Information
#
# To view InnoDB initialization information during the startup, start mysqld from a command prompt. When mysqld is started from a command
# prompt, initialization information is printed to the console.
#
# For example, on Windows, if mysqld is located in C:\Program Files\MySQL\MySQL Server 8.0\bin, start the MySQL server like this:
#
# 		C:\> "C:\Program Files\MySQL\MySQL Server 8.0\bin\mysqld" --console
#
# On Unix-like systems, mysqld is located in the bin directory of your MySQL installation:
#
# 		shell> bin/mysqld --user=mysql &
#
# If you do not sned server output to the console, check the error log after startup to see the initialization information iNnoDB printed
# during the startup process.
#
# For information about starting MySQl using other methods, see Section 2.10.5, "Starting and Stopping MySQL Automatically"
#
# NOTE:
#
# 		InnoDB does not open all user tables and associated data files at startup. However, InnoDB does check for the existence
# 		of tablespace files (*.ibd files) that are referenced in the data dictionary.
#
# 		If a tablespace file is not found, InnoDB logs an error and continues the startup sequence. Tablespace files
# 		that are referenced in the redo log may be opened during crash recovery for redo application.
#
# IMPORTANT STORAGE CONSIDERATIONS
#
# Review the following storage-related considerations before proceeding with your startup configuration.
#
# 		) In some cases, database performance improves if the data is not all placed on the same physical disk.
#
# 			Putting log files on a different disk from data is very often beneficial for performance. For example,
# 			you can place system tablespace data files and log files on different disks.
#
# 			You can also use raw disk partitions (raw devices) for InnoDB data files, which may speed up I/O.
#
# 			See USING RAW DISK PARTITIONS FOR THE SYSTEM TABLESPACE.
#
# 		) InnoDB is a transaction-safe (ACID compliant) storage engine for MySQL that has commit, rollback, and crash-recovery
# 			capabilities to protect user data.
#
# 			However, it cannot do so if the underlying operating system or hardware does not work as advertised.
#
# 			Many OS's or disk subsystems may delay or reorder write operations to improve performance. On some OS's, the very
# 			fsync() system call that hsould wait until all unwritten data for a file has been flushed might actually be
# 			returning before the data has been flushed to stable storage.
#
# 			Because of htis, an OS crash or a power outage may destroy recently committed data, or in the worst case,
# 			even corrupt the DB because of write operations having been reordered.
#
# 			If data integrity is important to you, perform some "pull-the-plug" tests before using anything in production.
#
# 			On OS X 10.3 and higher, InnoDB uses a special fcntl() file flush method.
#
# 			Under Linux, it is adivsable to disable the write-back cache.
#
# 			On ATA/SATA disk drives, a command such as hdparm -W0 /dev/hda may work to disable the write-back cache.
#
# 			Beware that some drives or disk controllers may be unable to disable the write-back cache.
#
# 		) With regards to InnoDB recovery capabilities that protect user data, InnoDB uses a file flush technique
# 			involving a structure called the doublewrite buffer, which is enabled by default (innodb_doublewrite=ON)
#
# 			The doublewrite buffer adds safety to recovery following a crash or power outage, and improves performance
# 			on most varities of Unix by reducing the need for fsync() operations.
#
# 			It is recommended that the innodb_doublewrite option remains enabled if you are concerned with data integrity
# 			or possible failures.
#
# 			For additional information about the doublewrite buffer, see SECTION 15.11.1, "InnoDB Disk I/O"
#
# 		) Before using NFS with InnoDB, review potential issues outlined in Using NFS with MySQL.
#
# SYSTEM TABLESPACE DATA FILE CONFIGURATION
#
# The innodb_data_file_path configuration option defines the name, size, and attributes of InnoDB system tablespace data
# files.
#
# If you do not specify a value for innodb_data_file_path, the default behavior is to create a single auto-extending data
# file, slightly larger than 12MB, named ibdata1.
#
# To specify more than one data file, separate htem by semicolon (;) characters.
#
# 		innodb_data_file_path=datafile_spec1[;datafile_spec2]...
#
# The following setting configures a single 12MB data file named ibdata1 that is auto-extending. No location for the file is given,
# so by default, InnoDB creates it in the MySQL data directory:
#
# 		[mysqld]
# 		innodb_data_file_path=ibdata1:12M:autoextend
#
# File size is specified using K, M, or G suffix letters to indicate units of KB, MB, or GB. If specifying the data file size in kilobytes (KB),
# do so in multiples of 1024.
#
# Otherwise, KB values are rounded to nearest megabyte (MB) boundary.
#
# The sum of the sizes of the files must be at least slightly larger than 12MB.
#
# A minimum file size is enforced for the first system tablespace data file to ensure that there is enough space for doublewrite
# buffer pages:
#
# 		) For an innodb_page_size value of 16kb or less, the minimum file size is 3MB.
#
# 		) For an innodb_page_size value of 32kb, the minimum file size is 6MB.
#
# 		) For an innodb_page_size value of 64kb, the minimum file size is 12MB.
#
# A system tablespace with a fixed-size 50MB data file named ibdata1 and a 50MB auto-extending file named ibdata2 can be configured
# like this:
#
# 		[mysqld]
# 		innodb_data_file_path=ibdata1:50M;ibdata2:50M:autoextend
#
# The full syntax for a data file specification includes the file name, file size and optional autoextend and max attributes:
#
# 		file_name:file_size[:autoextend[:max:max_file_size]]
#
# The autoextend and max attributes can be used only for the data file that is specified last in the innodb_data_file_path setting.
#
# If you specify the autoextend option for the last data file, InnoDB extends the data file if it runs out of free space in the tablespace.
#
# the autoextend increment is 64MB at a time by default.
#
# To modify the increment, change the innodb_autoextend_increment system variable.
#
# If the disk becomes full, you might want to add another data file on another disk. For instructions, see Resizing The System Tablespace.
#
# The size limit of individual files is determined by your OS. You can set the file sizes to more than 4GB on OS systems that support
# large files.
#
# You can also use raw disk partitions as data files.
#
# InnoDB is not aware of hte file system maximum file size, so be cautious on Files sytems where the max file size is a small value
# such as 2GB.

# To specify a maximum size for an auto-extending data file, use the max attribute following the autoextend attribute.
#
# Use the max attribute only in cases where constraining disk usage is of critical importance, because exceeding the maximum size
# causes a fatal error, possibly causing the server to exit.
#
# The following configuration permits ibdata1 to grow to a limit of 500MB:
#
# 		[mysqld]
# 		innodb_data_file_path=ibdata1:12M:autoextend:max:500M
#
# InnoDB creates system tablespace files in the MySQL data directory by default (datadir).
#
# To specify a location explicitly, use the innodb_data_home_dir option. For example, to create two files
# named ibdata1 and ibdata2 in a direcotry named myibdata, configure InnoDB like this:
#
# 		[mysqld]
# 		innodb_data_home_dir = /path/to/myibdata/
# 		innodb_data_file_path=ibdata1:50M;ibdata2:50:autoextend
#
# NOTE:
#
# 		A trailing slash is required when specifying a value for innodb_data_home_dir
#
# 		InnoDB does not create directories, so make sure that the myibdata directory exists before
# 		you start the server. Use the Unix or DOS mkdir command to create dirs.
#
# 		Make sure that the MySQL server has the proper access rights to create files in the data direcotry.
#
# 		More generally, the server must have access rights in any directory where it needs to create data files.
#
# InnoDB forms the directory path for each data file by textually concatenating the value of innodb_data_home_dir to the
# data file name.
#
# If the innodb_data_home_dir option is not specified, the defautl value is the "dot" directory ./, which means the MySQL
# data direcotry.
#
# (The MySQL server changes its current working direcotry to its data direcotry when it begins executing)
#
# If you specify innodb_data_home_dir as an empty string, ou can specify absolute paths for data files listed in the
# innodb_data_file_path value.
#
# The following example is equivalent to the preceding one:
#
# 		[mysqld]
# 		innodb_data_home_dir = 
# 		innodb_data_file_path=/path/to/myibdata/ibdata1:50M;/path/to/myibdata/ibdata2:50M:autoextend
#
#
# REDO LOG FILE CONFIGURATION
#
# By default, InnoDB creates two 5MB redo log files in teh data direcotry named ib_logfile0 and ib_logfile1
#
# The following options can be used to modify the default configuration:
#
# 		) innodb_log_group_home_dir defines directory path to the INnoDB log files (the redo logs)
#
# 			If this option is not configured, InnoDB log files are created in the MySQL data directory (datadir)
#
# 			oyu might use this option to place InnoDB log files in a different physical storage location than InnoDB
# 			data files to avoid potential I/O resource conflicts.
#
# 			For example:
#
# 				[mysqld]
# 				innodb_log_group_home_dir = /dr3/iblogs
#
# 			NOTE:
#
# 				InnoDB does not create directories, so make sure that the log directory exists before you
# 				start the server.
#
# 				Use the Unix or DOS mkdir command to create any necessary directories.
#
# 				Make sure that the MySQL server has the proper access rights to create files in the log direcotry.
# 				More generally, the server must have access rights in any directory where it needs to create log files.
#
# 		) innodb_log_files_in_group defines the number of log files in the log group. The default and recommended value is 2.
#
# 		) innodb_log_file_size defines the size in bytes of each log file in the log group. The combined size of log files (innodb_log_file_size *
# 			innodb_log_files_in_group) cannot exceed a maximum value that is slightly less than 512Gb.
#
# 			A pair of 255GB log files, for example, approaches the limit but does not exceed it.
#
# 			The default log file size is 48MB.
#
# 			Generally, the combined size of the log files hsould be large enough that hte server can smooth out peaks and throughs in
# 			workload activity, which often means that there is enough redo log spaces to handle more than an hour of write activity.
#
# 			The larger the value, the less checkpoint flush activity is needed in the buffer pool, saving disk I/O.
#
# 			For additional info, see Section 8.5.4, "OPTIMIZING InnoDB REDO lOGGING"
#
# UNDO TABLESPACE CONFIGURATION
#
# By default, undo logs reside in two undo tablespaces that are created when the MySQL instance is initialized. The I/O patterns
# for undo logs make undo tablespaces good candidates for SSD storage.
#
# The innodb_undo_directory variable defines the path where InnoDB creates default undo tablespaces. If that variable is undefined,
# default undo tablespaces are created in the data directory.
#
# The innodb_undo_directory variable is not dynamic. Configuring it requires restarting the server.
#
# For information about configuring additional undo tablespaces, see SECTION 15.6.3.4, "UNDO TABLESPACES"
#
# GLOBAL TEMPORARY TABLESPACE CONFIGURATION
#
# The global temporary tablespace stores rollback segments for changes made to user-created temporary tables.
#
# By default, InnoDB creates a single auto-extending global temporary tablespace data file named ibtmp1 in the innodb_data_home_dir
# directory.
#
# The initial file size is slightly larger than 12MB.
#
# The innodb_temp_data_file_path variable specifies the path, file name, and file size for global temporary tablespace data files.
# File size is specified in KB, MB or GB by appending K, M, or G to the size value.
#
# The sum of the sizes of the files must be slightly larger than 12MB.
#
# To specify an alternate location for global temporary tablespace data files, configure the innodb_temp_data_file_path variable
# at startup.
#
# SESSION TEMPORARY TABLESPACE CONFIGURATION
#
# In MySQL 8.0.15 and earlier, session temporary tablespaces store user-created temporary tables and internal temporary tables created
# by the optimizer when InnoDB is configured as the on-disk storage engine for internal temporary tables (internal_tmp_disk_storage_engine=InnoDB)
#
# In MySQL 8.0.16 and later, the InnoDB storage engine is always used for internal temporary tables on disk.
#
# The innodb_temp_tablespaces_dir variable defines the location where InnoDB creates session temporary tablespaces.
#
# The default location is the #innodb_temp directory in the data directory.
#
# To specify an alternate location for session temporary tablespaces, configure the innodb_temp_tablespaces_dir variable at startup.
# A fully qualified path or path relative to the data directory is permitted.
#
# PAGE SIZE CONFIGURATION
#
# The innodb_page_size option specifies the page size for all InnoDB tablespaces in a MySQL instance.
#
# This value is set when the instance is created and remains constant afterward. Valid values are 64kb, 32kb, 16kb (the default),
# 8kb and 4kb.
#
# Alternatively, you can specify page size in bytes (65536, 32768, 16384, 8192, 4096)
#
# The default page size of 16kb is appropriate for a wide range of workloads, particularly for queries involving table scans and
# DML operations involving bulk updates.
#
# Smaller page sizes might be more efficient for OLTP workloads involving many small writes, where contention can be an issue
# when a single page contains many rows.
#
# Smaller pages might also be efficient with SSD storage devices, which typically use small block sizes. Keeping the InnoDB
# page size close to the storage device block size minimizes the amount of unchanged data that is rewritten to disk.
#
# MEMORY CONFIGURATION
#
# MySQL allocates memory to various caches and buffers to improve performance of database operations. When allocating memory for
# InnoDB, always consider memory required by the operating system, memory allocated to other applications, and memory allocated
# for other MySQL buffers and caches.
#
# For example, if you use MyISAM tables, consider the amount of memory allocated for the key buffer (key_buffer_size).
#
# For an overview of MySQL buffers and caches, see Section 8.12.3.1, "How MySQL Uses Memory"
#
# Buffers specific to InnoDB are configured using the following parameters:
#
# 		) innodb_buffer_pool_size defines size of the buffer pool, which is the memory area that holds cached data for InnoDB tables,
# 			indexes, and other auxiliary buffers.
#
# 			The size of the buffer pool is important for system performance, and it is typically recommended that innodb_buffer_pool_size
# 			is configured to 50 to 75 percent of system memory.
#
# 			The default buffer size is 128MB. For additional guidance, see SECTION 8.12.3.1, "How MySQL Uses Memory".
#
# 			For information about how to configure InnoDB buffer pool size, see SECTION 15.8.3.1, "Configuring InnoDB Buffer Pool Size"
#
# 			Buffer pool size can be configured at startup or dynamically.
#
#			On systems with a large amount of memory, you can improve concurrency by dividing the buffer pool into multiple buffer pool
# 			instances.
#
# 			The number of buffer pool instances is controlled by the by innodb_buffer_pool_instances option. By default, InnoDB creates
# 			one buffer pool instance. The number of buffer pool instances can be configured at startup.
#
# 			For more information, see SECTION 15.8.3.2, "Configuring Multiple Buffer Pool Instances"
#
# 		) innodb_log_buffer_size defines the size in bytes of the buffer that INnoDB uses to write to the log files on disk.
#
# 			The default size is 16MB. A large log buffer enables large transactions to run without a need to write the log to disk
# 			before the transactions commit.
#
# 			If you have transactions that update, insert, or delete many rows, you might consider increasing the size of the log
# 			buffer to save disk I/O. Innodb_log_buffer_size can be configured at startup. For related information, see Section 8.5.4,
# 			"Optimizing InnoDB Redo Logging"
#
# 				WARNING:
#
# 					On 32-bit GNU/Linux x86, be careful not to set memory usage too high. glibc may permit the process heap to grow over thread
# 					stacks, which crashes your server.
#
# 					It is a risk if the memory allocated to the mysqld process for global and per-thread buffers and caches is close to or
# 					exceeds 2GB.
#
# 					A formula similar to the following that calculates global and per-thread memory allocation for MySQL can be used to estimate
# 					MySQL memory usage.
#
# 					You may need to modify the formula to account for buffers and caches in your MySQL version and configuration.
#
# 					For an overview of MySQL buffers and caches. see SECTION 8.12.3.1, "How MySQL Uses Memory"
#
# 						innodb_buffer_pool_size
# 						+ key_buffer_size
# 						+ max_connections*(sort_buffer_size+read_buffer_size+binlog_cache_size)
# 						+ max_connections*2MB
#
# 					Each thread uses a stack (often 2MB, but only 256kb in MySQl binaries provided by oracle corp) and in the worst
# 					case also uses sort_buffer_size + read_buffer_size additional memory.
#
# 	On Linux, if the kernel is enabled for large page support, InnoDB can use large pages to allocate memory for its buffer pool.
#
# See SECTION 8.12.3.2, "ENABLING LARGE PAGE SUPPORT"
#
# 15.8.2 CONFIGURING INNODB FOR READ-ONLY OPERATION
#
# You can now query InnoDB tables where the MySQL data directory is on read-only media, by enabling the --innodb-read-only configuration
# option at server startup.
#
# HOW TO ENABLE
#
# To prepare an instance for read-only operation, make sure all the necessary information is flushed to the data files before storing
# it on the read-only medium.
#
# Run the server with change buffering disabled (innodb_change_buffering=0) and do a slow shutdown.
#
# To enable read-only mode for an entire MySQL instance, specify the following configuration options at server startup:
#
# 		) --innodb-read-only=1
#
# 		) If the instance is on read-only media such as DVD or CD, or the /var dir is not writable by all: --pid-file=path_on_writeable_media and
# 			--event-scheduler=disabled
#
# 		) --innodb-temp-data-file-path. This option specifies the path, file name, and file size for InnoDB temporary tablespace data files.
#
# 			The default setting is ibtmp1:12M:autoextend, which creates the ibtmp1 temporary tablespace data file in the data directory.
# 			To prepare an instance for read-only operation, set innodb_temp_data_file_path to a location outside of the data directory.
#
# 			The path must be relative to the data directory. For example:
#
# 				--innodb-temp-data-file-path=../../../tmp/ibtmp1:12M:autoextend
#
# As of MySQL 8.0, enabling innodb_read_only prevents the table creation and drop operations for all storage engines.
#
# These operations modify data dictionary tables in the mysql system database, but those tables use the InnoDB storage engine
# and cannot be modified when innodb_read_only is enabled.
#
# The same restriction applies to any operation that modifies data dictionary tables, such as ANALYZE_TABLE and ALTER_TABLE_tbl_name_ENGINE=engine_name
#
# In addition, other tables in the mysql system database use the InnoDB storage engine in MySQL 8.0. Making those tables read only 
# results in restrictions on operations that modify them.
#
# For example, CREATE_USER, GRANT, REVOKE and INSTALL_PLUGIN operations are not permitted in read-only mode.
#
# USAGE SCENARIOS
#
# This mode of operation is appropriate in situations such as:
#
# 		) Distibuting a MySQL application, or a set of MySQL data, on a read-only storage medium such as a DVD or CD.
#
# 		) Multiple MySQL instances querying the same data directory simultaneously, typically in a data warehousing configuraiton.
#
# 			You might use this technique to avoid bottlenecks that can occur with a heavily loaded MySQL instance, or you might use
# 			different configuration options for the various instances to tune each one for particular kinds of queries.
#
# 		) Querying data that has been put into a read-only state for security or data integrity reasons, such as archived backup data.
#
# 			NOTE:
#
# 				This feature is mainly intended for flexibility in distirbution and deployment, rather than raw performance based on the
# 				read-only aspect.
#
# 				See SECTION 8.5.3, "Optimizing InnoDB Read-Only Transactions" for ways to tune the performance of read-only queries, which
# 				do not require making the entire server read-only.
#
# HOW IT WORKS
#
# When the server is run in read-only mode through the --innodb-read-only option, certain InnoDB features and components are reduced
# or turned off entirely:
#
# 		) No change buffering is done, in particular no merges from the change buffer. To make sure the change buffer is empty when you
# 			prepare the instance for read-only operation, disable change buffering (innodb_change_buffering=0) and do a slow shutdown first.
#
# 		) There is no crash recovery phase at startup. The instance must have performed a slow shutdown before being put into the read-only state.
#
# 		) Because the redo log is not used in read-only operation, you can set innodb_log_file_size to the smallest size possible (1 MB) before making
# 			the instance read-only.
#
# 		) All background threads other than I/O read threads are turned off. As a consequence, a read-only instance cannot encounter any deadlock.
#
# 		) Information about deadlocks, monitor output and so on is not written to temporary files. As a consequence, SHOW_ENGINE_INNODB_STATUS does
# 			not produce any output.
#
# 		) Changes to configuration option settings that would normally change the behavior of write operations, have no effect when the server is in
# 			read-only mode.
#
# 		) The MVCC processing to enforce isolation levels is turned off. All queries read the latest version of a record, because update and deletes
# 			are not possible.
#
# 		) The undo log is not used. Disable any settings for the innodb_undo_tablespaces and innodb_undo_directory configuration options.
#
# 15.8.3 InnoDB BUFFER POOL CONFIGURATION
#
# 15.8.3.1 Configuring InnoDB Buffer Pool Size
# 15.8.3.2 Configuring Multiple Buffer Pool Instances
# 15.8.3.3 Making the Buffer Pool Scan resistant
# 15.8.3.4 Configuring InnoDB Buffer Pool Prefetching (Read-Ahead)
# 15.8.3.5 Configuring InnoDB Buffer Pool Flushing
# 15.8.3.6 Fine-tuning INnoDB BUffer Pool Flushing
# 15.8.3.7 Saving and Restoring the Buffer Pool State
# 15.8.3.8 Excluding Buffer Pool Pages from Core Files
#
# This section provides configuration and tuning information for the INnoDB buffer pool.
#
# 15.8.3.1 Configuring InnoDB Buffer Pool Size
#
# You can configure InnoDB buffer pool size offline (at startup) or online, while the server is running. Behavior described
# in this section applies to both methods.
#
# For additional information about configuring buffer pool size online, see Configuring InnoDB Buffer Pool Size Online.
#
# When increasing or decreasing innodb_buffer_pool_size, the operation is performed in chunks. Chunk size is defined by the
# innodb_buffer_pool_chunk_size configuration option, which has a default of 128M.
#
# For more information, see Configuring InnoDB Buffer Pool Chunk Size.
#
# Buffer pool size must always be equal to or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances.
#
# If you configure innodb_buffer_pool_size to a value that is not equal to or a multiple of innodb_buffer_pool_chunk_size *
# innodb_buffer_pool_instances, buffer pool size is automatically adjusted to a value that is equal to or a multiple of
# innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances.
#
# In the following example, innodb_buffer_pool_size is set to 8G, and innodb_buffer_pool_instances is set to 16.
#
# Innodb_buffer_pool_chunk_size is 128M, which is the default value.
#
# 8G is a valid innodb_buffer_pool_size value because 8G is a multiple of innodb_buffer_pool_instances=16 * innodb_buffer_pool_chunk_size=128M,
# which is 2G.
#
# 		shell> mysqld --innodb-buffer-pool-size=8G --innodb-buffer-pool-instances=16
#
# 		mysql> SELECT @@innodb_buffer_pool_size/1024/1024/1024;
# 		+-------------------------------------------+
# 		| @@innodb_buffer_pool_size/1024/1024/1024  |
# 		+-------------------------------------------+
# 		| 						8.0000000000 				  |
# 		+-------------------------------------------+
#
# IN this example, innodb_buffer_pool_size is set to 9G, and innodb_buffer_pool_instances is set to 16.
#
# innodb_buffer_pool_chunk_size is 128M, which is the default value.
#
# In this case, 9G is not a multiple of innodb_buffer_pool_instances=16 * innodb_buffer_pool_chunk_size=128M,
# so innodb_buffer_pool_size is adjusted to 10G, which is a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances.
#
# 		shell> mysqld --innodb-buffer-pool-size=9G --innodb-buffer-pool-instances=16
#
# 		mysql> SELECT @@innodb_buffer_pool_size/1024/1024/1024;
# 		+------------------------------------------+
# 		| @@innodb_buffer_pool_size/1024/1024/1024 |
# 		+------------------------------------------+
# 		| 			10.00000000000000000000 			 |
# 		+------------------------------------------+
#
# CONFIGURING InnoDB BUFFER POOL CHUNK SIZE
#
# innodb_buffer_pool_chunk_size can be increased or decreased in 1MB (1048576 bytes) units but can only be modified at startup,
# in a command line string or in a MySQL config file.
#
# Command line:
#
# 		shell> mysqld --innodb-buffer-pool-chunk-size=134217728
#
# Configuration file:
#
# 		[mysqld]
# 		innodb_buffer_pool_chunk_size=134217728
#
# The following conditions apply when altering innodb_buffer_pool_chunk_size:
#
# 		) If the new innodb_buffer_pool_chunk_size value * innodb_buffer_pool_instances is larger than the current buffer pool size
# 			when the buffer pool is initialized, innodb_buffer_pool_chunk_size is truncated to innodb_buffer_pool_size / innodb_buffer_pool_instances.
#
# 			For example, if the buffer pool is initialized with a size of 2GB (2147483648 bytes), 4 buffer pool instances, and a chunk size of 1GB
# 			(1073741824 bytes), chunk size is truncated to a value equal to innodb_buffer_pool_size / innodb_buffer_pool_instances, as shown below:
#
# 				shell> mysqld --innodb-buffer-pool-size=2147483648 --innodb-buffer-pool-instances=4
# 				--innodb-buffer-pool-chunk-size=1073741824;
#
# 				mysql> SELECT @@innodb_buffer_pool_size;
# 				+---------------------------------+
# 				| @@innodb_buffer_pool_size 		 |
# 				+---------------------------------+
# 				| 2147483648 							 |
# 				+---------------------------------+
#
# 				mysql> SELECT @@innodb_buffer_pool_instances;
# 				+---------------------------------+
# 				| @@innodb_buffer_pool_instances  |
# 				+---------------------------------+
# 				| 	4 										 |
# 				+---------------------------------+
#
# 				# Chunk size was set to 1GB (1073741824 bytes) on startup but was
# 				# truncated to innodb_buffer_pool_size / innodb_buffer_pool_instances
#
# 				mysql> SELECT @@innodb_buffer_pool_chunk_size;
# 				+---------------------------------+
# 				| @@innodb_buffer_pool_chunk_size |
# 				+---------------------------------+
# 				|  		536870912 					 |
# 				+---------------------------------+
#
# 		) Buffer pool size must always be equal to or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances.
#
# 			If you alter innodb_buffer_pool_chunk_size, innodb_buffer_pool_size is automatically adjusted to a value that is equal
# 			to or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances.
#
# 			The adjustment occurs when the buffer pool is initialized. This behavior is demonstrated in the following example:
#
# 				# The buffer pool has a default size of 128MB (134217728 bytes)
#
# 				mysql> SELECT @@innodb_buffer_pool_size;
# 				+-----------------------------------+
# 				| @@innodb_buffer_pool_size 			|
# 				+-----------------------------------+
# 				| 134217728 								|
# 				+-----------------------------------+
#
# 				# The chunk size is also 128MB (134217728 bytes)
#
# 				mysql> SELECT @@innodb_buffer_pool_chunk_size;
# 				+-----------------------------------+
# 				| @@innodb_buffer_pool_chunk_size   |
# 				+-----------------------------------+
# 				|  	134217728 							|
# 				+-----------------------------------+
#
# 				# There is a single buffer pool instance
#
# 				mysql> SELECT @@innodb_buffer_pool_instances;
# 				+-----------------------------------+
# 				| @@innodb_buffer_pool_instances 	|
# 				+-----------------------------------+
# 				| 				1 								|
# 				+-----------------------------------+
#
# 				# Chunk size is decreased by 1MB (1048576 bytes) at startup
# 				# (134217728 - 1048576 = 133169152):
#
# 				shell> mysqld --innodb-buffer-pool-chunk-size=133169152
#
# 				mysql> SELECT @@innodb_buffer_pool_chunk_size;
# 				+--------------------------------------+
# 				| @@innodb_buffer_pool_chunk_size 		|
# 				+--------------------------------------+
# 				| 				133169152 						|
# 				+--------------------------------------+
#
# 				# Buffer pool size increases from 134217728 to 266338304
# 				# Buffer pool size is automatically adjusted to a value that is equal to
# 				# or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances
#
# 				mysql> SELECT @@innodb_buffer_pool_size;
# 				+-----------------------------+
# 				| @@innodb_buffer_pool_size   |
# 				+-----------------------------+
# 				| 266338304 						|
# 				+-----------------------------+
#
# 	This example demonstrates the same behavior but with multiple buffer pool instances:
#
# 		# The buffer pool has a default size of 2GB (2147483648 bytes)
#
# 		mysql> SELECT @@innodb_buffer_pool_size;
# 		+----------------------------+
# 		| @@innodb_buffer_pool_size  |
# 		+----------------------------+
# 		|  	2147483648 				  |
# 		+----------------------------+
#
# 		# The chunk size is .5 GB (536870912 bytes)
#
# 		mysql> SELECT @@innodb_buffer_pool_chunk_size;
# 		+---------------------------------+
# 		| @@innodb_buffer_pool_chunk_size |
# 		+---------------------------------+
# 		|  	536870912 						 |
# 		+---------------------------------+
#
# 		# There are 4 buffer pool instances
#
# 		mysql> SELECT @@innodb_buffer_pool_instances;
# 		+---------------------------------+
# 		| @@innodb_buffer_pool_instances  |
# 		+---------------------------------+
# 		| 				4 							 |
# 		+---------------------------------+
#
# 		# Chunk size is decreased by 1MB (1048576 bytes) at startup
# 		# (536870912 - 1048576 = 535822336):
#
# 		shell> mysqld --innodb-buffer-pool-chunk-size=535822336
#
# 		mysql> SELECT @@innodb_buffer_pool_chunk_size;
# 		+----------------------------------+
# 		| @@innodb_buffer_pool_chunk_size  |
# 		+----------------------------------+
# 		| 	535822336 							  |
# 		+----------------------------------+
#
# 		# Buffer pool size increases from 2147483648 to 4286578688
# 		# Buffer pool size is automatically adjusted to a value that is equal to
# 		# or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances
#
# 		mysql> SELECT @@innodb_buffer_pool_size;
# 		+----------------------------------+
# 		| @@innodb_buffer_pool_size 		  |
# 		+----------------------------------+
# 		| 	4286578688 							  |
# 		+----------------------------------+
#
# Care should be taken when changing innodb_buffer_pool_chunk_size, as changing this value can increase the size
# of the buffer pool, as shown in the examples above.
#
# Before you can change innodb_buffer_pool_chunk_size, calculate the effect on innodb_buffer_pool_size to ensure that
# the resulting buffer pool size is acceptable.
#
# NOTE:
#
# 		To avoid potential performance issues, the number o chunks (innodb_buffer_pool_size / innodb_buffer_pool_chunk_size)
# 		should not exceed 1000.
#
# Configuring InnoDB Buffer Pool Size Online
#
# The innodb_buffer_pool_size configuration option can be set dynamically using a SET statement, allowing you to resize
# the buffer pool without restarting the server.
#
# For example:
#
# 		mysql> SET GLOBAL innodb_buffer_pool_size=402653184;
#
# Active transactions and operations performed through InnoDB APIs should be completed before resizing the buffer pool.
#
# When initiating a resizing operation, the operation does not start until all active transactions are completed.
#
# Once the resizing operation is in progress, new transactions and operations that require access to the buffer pool
# must wait until the resizing operation finishes.
#
# The exception to the rule is that concurrent access to the buffer pool is permitted while the buffer pool is defragmented
# and pages are withdrawn when buffer pool size is decreased.
#
# A drawback of allowing concurrent access is that it could result in a temporary shortage of available pages while
# pages are being withdrawn.
#
# 		NOTE:
#
# 			Nested transactions could fail if initiated after the buffer pool resizing operation begins.
#
# MONITORING ONLINE BUFFER POOL RESIZING PROGRESS
#
# The Innodb_buffer_pool_resize_status reports buffer pool resizing progress. For example:
#
# 		mysql> SHOW STATUS WHERE Variable_name='InnoDB_buffer_pool_resize_status';
# 		+----------------------------------+---------------------------------------+
# 		| Variable_name 					     | Value 											 |
# 		+----------------------------------+---------------------------------------+
# 		| Innodb_buffer_pool_resize_status | Resizing also other hash tables 		|
# 		+----------------------------------+---------------------------------------+
#
# Buffer pool resizing progress is also logged in the server error log. This example shows notes that
# are logged when increasing the size of the buffer pool:
#
# 		[Note] InnoDB: Resizing buffer pool from 134217728 to 4294967296. (unit=134217728)
# 		[Note] InnoDB: disabled adaptive hash index.
# 		[Note] InnoDB: buffer pool 0: 31 chunks (253952 blocks) was added
# 		[Note] InnoDB: buffer pool 0: hash tables were resized
# 		[Note] InnoDB: Resized hash tables at lock_sys, adaptive hash index, dictionary.
# 		[Note] InnoDB: completed to resize buffer pool from 134217728 to 4294967296.
# 		[Note] InnoDB: re-enabled adaptive hash index
#
# This example shows notes that are logged when decreasing the size of the buffer pool:
#
# 		[Note] InnoDB: Resizing buffer pool from 4294967296 to 134217728. (unit=134217728)
# 		[Note] InnoDB: disabled adaptive hash index.
# 		[Note] InnoDB: buffer pool 0 : start to withdraw the last 253952 blocks.
# 		[Note] InnoDB: buffer pool 0 : withdrew 253952 blocks from free list. tried to relocate 0 pages.
# 		(253952/253952)
# 		[Note] InnoDB: buffer pool 0 : withdrawn target 253952 blocks.
# 		[Note] InnoDB: buffer pool 0 : 31 chunks (253952 blocks) was freed.
# 		[Note] InnoDB: buffer pool 0 : hash tables were resized.
# 		[Note] InnoDB: Resized hash tables at lock_sys, adaptive hash index, dictionary.
# 		[Note] InnoDB: completed to resize buffer pool from 4294967296 to 134217728.
# 		[Note] InnoDB: re-enabled adaptive hash index
#
# ONLINE BUFFER POOL RESIZING INTERNALS
#
# The resizing operation is performed by a background thread. When increasing the size of the buffer pool, the resizing operation:
#
# 		) Adds pages in chunks (chunk size is defined by innodb_buffer_pool_chunk_size)
# 		
# 		) Coverts hash tables, lists, and pointers to use new addresses in memory.
#
# 		) Adds new pages to the free list.
#
# Whilst these operations are in progress, other threads are blocked from accessing the buffer pool.
#
# When decreasing the size of the buffer pool, the resizing operation:
#
# 		) Defragments the buffer pool and withdraws (frees) pages
#
# 		) Removes pages in chunks (chunk size is defined by innodb_buffer_pool_chunk_size)
#
# 		) Converts hash tables, lists, and pointers to use new addresses in memory
#
# Of these operations, only defragmenting the buffer pool and withdrawing pages allow other threads to access to the buffer
# pool concurrently.
#
# 15.8.3.2 CONFIGURING MULTIPLE BUFFER POOL INSTANCES
#
# For systems with buffer pools in the multi-gigabyte range, dividing the buffer pool into separate instances can improve concurrency,
# by reducing contention as different threads read and write to cached pages.
#
# This feature is typically intended for systems with a buffer pool size in the multi-gigabyte range. Multiple buffer pool instances are
# configured using the innodb_buffer_pool_instances configuration option, and you might also adjust the innodb_buffer_pool_size value.
#
# When the InnoDB buffer pool is large, many data requests can be satisfied by retrieving from memory. You might encounter bottlenecks
# from multiple threads trying to access the buffer pool at once.
#
# You can enable multiple buffer pools to minimize this contention. Each page that is stored in or read from the buffer pool is assigned to
# one pool.
#
# Prior to MySQL 8.0, each buffer pool was protected by its own buffer pool mutex. In MySQL 8.0 and later, the buffer pool mutex was replaced
# by several list and hash protecting mutexes, to reduce contention.
#
# To enable multiple buffer pool instances, set the innodb_buffer_pool_instances configuration option to a value greater than 1 (the default)
# up to 64 (the maximum).
#
# This option takes effect only when you set innodb_buffer_pool_size to a size of 1GB or more. The total size you specify is divided among all
# the buffer pools.
#
# For best efficiency, specify a combination of innodb_buffer_pool_instances and innodb_buffer_pool_size so that each buffer pool instance is
# at least 1GB.
#
# For information about modifying InnoDB buffer pool size, see SECTION 15.8.3.1, "Configuring InnoDB Buffer Pool Size"
#
# 15.8.3.3 MAKING THE BUFFER POOL SCAN RESISTANT
#
# Rather than using a strict LRU algorithm, InnoDB uses a technique to minimize the amount of data that is brought into the buffer pool
# and never accessed again.
#
# The goal is to make sure that frequently accessed ("hot") pages remain in the buffer pool, even as read-ahead and full table scans bring
# in new blocks that might or might not be accessed afterward.
#
# Newly read blocks are inserted into the middle of the LRU list. All newly read pages are inserted at a location that by default is 3/8
# from the tail of the LRU list.
#
# The pages are moved to the front of the list (the most-recently used end) when they are accessed in the buffer pool for the first time.
#
# Thus, pages that are never accessed never make it to the front portion of the LRU list, and "age out" sooner than with a strict LRU approach.
#
# This arrangement divides the LRU list into two segments, where the pages downstream of the insertion point are considered "old" and are
# desirable victims for LRU eviction.
#
# For an explanation of the inner workings of the InnoDB buffer pool and specifics about the LRU algorithm, see SECTION 15.5.1, "Buffer Pool"
#
# You can control the insertion point in the LRU list and choose whether InnoDB applies the same optimization to blocks brought into the buffer
# pool by table or index scans.
#
# The configuration parameter innodb_old_blocks_pct controls the percentage of "old" blocks in the LRU list. The default value of innodb_old_blocks_pct
# is 37, corresponding to the original fixed ratio of 3/8.
#
# The value range is 5 (new pages in the buffer pool age out very quickly) to 95 (only 5% of the buffer pool is reserved for hot pages, making
# the algorithm close to the familiar LRU strategy)
#
# The optimization that keeps the buffer pool from being chruned by read-head can avoid similar problems due to table or index scans.
#
# In these scans, a data page is typically accessed a few times in quick succession and is never touched again. The configuration parameter
# innodb_old_blocks_time specifies the time window (in miliseconds) after the first access to a page during which it can be accessed without
# being moved to the front (most-recently used end) of the LRU list.
#
# The default value of innodb_old_blocks_time is 1000. Increasing this value makes more and more blocks likely to age out faster from the buffer pool.
#
# Both innodb_old_blocks_pct and innodb_old_blocks_time can be specified in the MySQL option file (my.cnf or my.ini) or changed at runtime with the
# SET_GLOBAL statement.
#
# Changing the value at runtime requires privileges sufficient to set global system variables. See SECTION 5.1.9.1, "System Variable Privileges"
#
# To help you gauge the effect of setting these parameters, the SHOW ENGINE INNODB STATUS command reports buffer pool statistics. For details,
# see Monitoring the Buffer Pool Using the InnoDB Standard Monitor.
#
# Because the effects of these parameters can vary widely based on your hardware configuration, your data and the details of your workload,
# always benchmark to verify the effectiveness before changing these settings in any performance-critical or production environment.
#
# In mixed workloads where most of the activity is OLTP type with periodic batch reporting queries which result in large scans, setting the value
# of innodb_old_blocks_time during the batch runs can help keep the working set of the normal workload in the buffer pool.
#
# When scanning large tables that cannot fit entirely in the buffer pool, setting innodb_old_blocks_pct to a small value keeps the data that is
# only read once from consuming a significant portion of the buffer pool.
#
# For example, setting innodb_old_blocks_pct=5 restricts this data that is only read once to 5% of the buffer pool.
#
# When scanning small tables that do fit into memory, there is less overhead for moving pages around within the buffer pool, so you can leave
# innodb_old_blocks_pct at its default value, or even higher, such as innodb_old_blocks_pct=50.
#
# The effect of the innodb_old_blocks_time parameter is harder to predict than the innodb_old_blocks_pct parameter, is relatively small and
# varies more with the workload.
#
# To arrive at an optimal value, conduct your own benchmarks if the performance improvement from adjusting innodb_old_blocks_pct is not sufficient.
#
# 15.8.3.4 CONFIGURING InnoDB BUFFER POOL PREFETCHING (Read-Ahead)
#
# A read-ahead request is an I/O request to prefetch multiple pages in the buffer pool asynch, in anticipation that these pages will be needed
# soon.
#
# The requests bring in all the pages in one extent. InnoDB uses two read-ahead algorithms to improve I/O performance:
#
# 		Linear read-ahead is a technique that predicts what pages might be needed soon based on pages in the buffer pool being accessed sequentially.
#
# 		You control when InnoDB performs a read-ahead operation by adjusting the number of sequential page accesses required to trigger an asynch read 
# 		request, using the configuration parameter innodb_read_ahead_threshold.
#
# 		Before this parameter was added, InnoDB would only calculate whether to issue an asynch prefetch request for the entire next
# 		extent when it read the last page of the current extent.
#
# 		The configuration parameter innodb_read_ahead_threshold controls how sensitive InnoDB is in detecting patterns of sequential page access.
#
# 		If the number of pages read sequentially from an extent is greater than or equal to innodb_read_ahead_threshold, InnoDB initiates an asynch
# 		read-ahead operation of hte entire following extent.
#
# 		innodb_read_ahead_threshold can be set to any value from 0-64. The default value is 56. The higher the value, the more strict the access pattern
# 		check.
#
# 		For example, if you set the value to 48, InnoDB triggers a linear read-ahead request only when 48 pages in the current extent have been accessed
# 		sequentially.
#
# 		If the value is 8, InnoDB triggers an asynch read-ahead even if as few as 8 pages in the extent are accessed sequentially.
#
# 		You can set the value of this parameter in the MySQL configuration file, or change it dynamically with the SET_GLOBAL statement,
# 		which requires privileges sufficient to set global system variables.
#
# 		See SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES"
#
# 		Random read-ahead is a technique that predicts when pages might be needed soon based on pages already in the buffer pool, regardless of
# 		the order in which those pages were read.
#
# 		If 13 consecutive pages from the same extent are found in the buffer pool, InnoDB asynch issues a request to prefetch the remaining pages
# 		of the extent.
#
# 		To enable this feature, set the configuration variable innodb_random_read_ahead to ON.
#
# 		The SHOW ENGINE INNODB STATUS command displays statistics to help you evaluate the effectiveness of the read-ahead algorithm.
#
# 		Statistics include counter information for the following global status variables:
#
# 			) Innodb_buffer_pool_read_ahead
#
# 			) Innodb_buffer_pool_read_ahead_evicted
#
# 			) Innodb_buffer_pool_read_ahead_rnd
#
# 		This information can be useful when fine-tuning the innodb_random_read_ahead setting.
#
# 		For more information about I/O performance, see SECTION 8.5.8, "Optimizing InnoDB Disk I/O" and Section 8.12.1, "Optimizing Disk I/O"
#
# 15.8.3.5 Configuring InnoDB Buffer Pool Flushing
#
# InnoDB performs certain tasks in the background, including flushing of dirty pages (those pages that have been changed but are not yet written
# to the database files) from the buffer pool.
#
# InnoDB starts flushing buffer pool pages when the percentage of dirty pages in the buffer pool reaches the low water mark setting defined by
# innodb_max_dirty_pages_pct_lwm.
#
# This option is intended to control the ratio of dirty pages in the buffer pool and ideally prevent the percentage of dirty pages from
# reaching innodb_max_dirty_pages_pct. 

# If the percentage of dirty pages in the buffer pool exceeds innodb_max_dirty_pages_pct, InnoDB begins to aggressively flush buffer pool pages.
#
# InnoDB uses an algorithm to estimate the required rate of flushing, based on the speed of redo log generation and the current rate of flushing.
#
# The intent is to smooth overall performance by ensuring that buffer flush activity keeps up with the need to keep the buffer pool "clean".
#
# Automatically adjusting the rate of flushing can help to avoid sudden dips in throughput, when excessive buffer pool flushing limits the I/O
# capacity available for ordinary read and write activity.
#
# InnoDB uses its log files in a circular fashion. Before reusing a portion of a log file, InnoDB flushes to disk all dirty buffer pool pages whose
# redo entries are contained in that portion of the log file, a process known as a sharp checkpoint.
#
# If a workload is write-intensive, it generates a lot of redo information, all written to the log file.
#
# If all available space in the log files is used up, a sharp checkpoint occurs, causing a temporary reduction in throughput.
#
# This situation can happen even if innodb_max_dirty_pages_pct is not reached.
#
# InnoDB uses a heuristic-based algorithm to avoid such a scenario, by measuring the number of dirty pages in teh buffer pool and the rate at which
# redo is being generated.
#
# Based on these numbers, InnoDB decides how many dirty pages to flush from the buffer pool each second.
#
# This self-adapting algorithm is able to deal with sudden changes in workload.
#
# Internal benchmarking has shown that this algorithm not only maintains throughput over time, but can also improve overall throughput significantly.
#
# Because adaptive flushing can significantly affect the I/O pattern of a workload, the innodb_adaptive_flushing configuration parameter lets you
# turn off this feature.
#
# The default value for innodb_adaptive_flushing is ON, enabling the adaptive flushing algorithm. You can set the value of this parameter in the MySQL
# option file (my.cnf or my.ini) or change it dynamically with the SET_GLOBAL statement, which requires privileges sufficient to set global system
# variables.
#
# See SECTION 5.1.9.1, "System Variable Privileges"
#
# For information about fine-tuning InnoDB buffer pool flushing behavior, see SECTION 15.8.3.6, "Fine-tuning InnoDB Buffer Pool Flushing"
#
# For more information about InnoDB I/O performance, see Section 8.5.8, "Optimizing InnoDB DISK I/O"
#
# 15.8.3.6 Fine-Tuning InnoDB Buffer Pool Flushing
#
# The configuration options innodb_flush_neighbors and innodb_lru_scan_depth let you fine-tune aspects of the
# flushing process for the InnoDB buffer pool.
#
# 		) innodb_flush_neighbors
#
# 			Specifies whether flushing a page from the buffer pool also flushes other dirty pages in the same extent.
# 			When the table data is stored on a traditional HDD storage device, flushing neighbor pages in one operation
# 			reduces I/O overhead (primarily for disk seek operations) compared to flushing individual pages at different
# 			times.
#
# 			For table data stored on SSD, seek time is not a significant factor and you can disable this setting to spread
# 			out write operations.
#
# 		) innodb_lru_scan_depth
#
# 			Specifies, per buffer pool instance, how far down the buffer pool LRU list the page cleaner thread scans looking
# 			for dirty pages to flush.
#
# 			This is a background operation performed once per second.
#
# These options primarily help write-intensive workloads. With heavy DML activity, flushing can fall behind if it is not aggressive
# enough, resulting in excssive memory use in the buffer pool; or, disk writes due to flushing can saturate your I/O capacity if that
# mechanism is too aggressive.
#
# The ideal settings depend on your workload, data access patterns and storage configuration (for example, whether data is stored on HDD,
# or SSD devices)
#
# For systems with constant heavy workloads, or workloads that fluctuate widely, several configuration options let you fine-tune the flushing
# behavior for InnoDB tables:
#
# 		) innodb_adaptive_flushing_lwm
#
# 		) innodb_max_dirty_pages_pct_lwm
#
# 		) innodb_io_capacity_max
#
# 		) innodb_flushing_avg_loops
#
# These options feed into the formula used by the innodb_adaptive_flushing option.
#
# The innodb_adaptive_flushing, innodb_io_capacity and innodb_max_dirty_pages_pct options are limited or extended by the following
# options:
#
# 		) innodb_adaptive_flushing_lwm
#
# 		) innodb_io_capacity_max
#
# 		) innodb_max_dirty_pages_pct_lwm
#
# The InnoDB adaptive flushing mechanism is not appropriate in all cases. It gives the most benefit when the redo log is in danger
# of filling up.
#
# The innodb_adaptive_flushing_lwm option specifies a "low water mark" percentage of redo log capacity; when that threshold is crossed,
# InnoDB turns on adaptive flushing even if not specified by the innodb_adaptive_flushing option.
#
# If flushing activity falls far behind, InnoDB can flush more aggressively than specified by innodb_io_capacity, innodb_io_capacity_max
# represents an upper limit on the I/O capacity used in such emergency situations, so that the spike in I/O does not consume all the
# capacity of the server.
#
# InnoDB tries to flush data from the buffer pool so that the percentage of dirty pages does not exceed the value of innodb_max_dirty_pages_pct.
#
# The default value for innodb_max_dirty_pages_pct is 75.
#
# 	NOTE:
#
# 		The innodb_max_dirty_pages_pct setting establishes a target for flushing activity. It does not affect the rate of flushing.
#
# 		For information about managing the rate of flushing, see SECTION 15.8.3.5, "CONFIGURING InnoDB BUFFER POOL FLUSHING"
#
# The innodb_max_dirty_pages_pct_lwm option specifies a "low water mark" value that represents the percentage of dirty pages where
# pre-flushing is enabled to control the dirty page ratio and ideally prevent the percentage of dirty pages from reaching innodb_max_dirty_pages_pct.
#
# A value of innodb_max_dirty_pages_pct_lwm=0 disables the "pre-flushing" behavior.
#
# Most of the options referenced above are most applicable to servers that run write-heavy workloads for long periods of time and have little reduced
# load time to catch up with changes waiting to be written to disk.
#
# innodb_flushing_avg_loops defines the number of iterations for which InnoDB keeps the previously calculated snapshot of the flushing state, which
# controls how quickly adaptive flushing responds to foreground load changes. Setting a high value for innodb_flushing_avg_loops means that InnoDB
# keeps the previously calculated snapshot longer, so adaptive flushing responds more slowly.
#
# A high value also reduces positive feedback between foreground and background work, but when setting a high value it is important to ensure that
# InnoDB redo log utilization does not reach 75% (the hardcoded limit at which async flushing starts) and that the innodb_max_dirty_pages_pct setting
# keeps the number of dirty pages to a level that is appropriate for the workload.
#
# Systems with consistent workloads, a large innodb_log_file_size, and small spikes that do not reach 75% redo log space utilization should use a high
# innodb_flushing_avg_loops value to keep flushing as smooth as possible.
#
# For systems with extreme load spikes or log files that do not provide a lot of space, consider a smaller innodb_flushing_avg_loops value.
#
# A smaller value allows flushing to closely track the load and helps avoid reaching 75% redo log space utilization.
#
# 15.8.3.7 SAVING AND RESTORING THE BUFFER POOL STATE
#
# To reduce the warmup period after restarting the server, InnoDB saves a percentage of the most recently used pages for each buffer pool
# at server shutdown and restores these pages at server startup.
#
# The percentage of recently used pages that is stored is defined by the innodb_buffer_pool_dump_pct configuration option.
#
# After restarting a busy server, there is typically a warmup period with steadily increasing throughput, as disk pages that were in the buffer
# pool are brought back into memory (as the same data is queried, updated and so on). The ability to restore the buffer pool at startup shortens
# the warmup period by reloading disk pages that were in the buffer pool before the restart rather than waiting for DML operations to access
# corresponding rows.
#
# Also, I/O requests can be performed in large batches, making the overall I/O faster. Page loading happens in the background, and does not delay
# database startup.
#
# In addition to saving the buffer pool state at shutdown and restoring it at startup, you can save and restore the buffer pool state at any time,
# while the server is running.
#
# For example, you can save the state of the buffer pool after reaching a stable throughput under a steady workload. You could also restore the previous
# buffer pool state after running reports or maintenance jobs that bring data pages into the buffer pool that are only required for those operations,
# or after running some other non-typical workload.
#
# Even though a buffer pool can be many gigabytes in size, the buffer pool data that InnoDB saves to disk is tiny by comparison.
# Only tablespace IDs and page IDs necessary to locate the appropriate pages are saved to disk.
#
# This information is derived from the INNODB_BUFFER_PAGE_LRU INFORMATION_SCHEMA table. By default, tablespace ID and page ID data is
# saved in a file named ib_buffer_pool, which is saved to the InnoDB data directory. The file name and location can be modified using
# the innodb_buffer_pool_filename configuration parameter.
#
# Because data is cached in and aged out of the buffer pool as it is with regular database operations, there is no problem if the disk
# pages are recently updated, or if a DML operation involves data that has not yet been loaded.
#
# The loading mechanism skips requested pages that no longer exist.
#
# The underlying mechanism involves a background thread that is dispatched to perform the dump and load operations.
#
# Disk pages from compressed tables are loaded into the buffer pool in their compressed form. Pages are uncompressed
# as usual when page contents are accessed during DML operations.
#
# Because uncompressing pages is a CPU-intensive process, it is more efficient for concurrency to perform the operation
# in a connection thread rather than in the single thread that performs the buffer pool restore operation.
#
# Operations related to saving and restoring the buffer pool state are described in the following topics:
#
# 		) Configuring the Dump Percentage for Buffer Pool Pages
#
# 		) Saving the Buffer Pool State at Shutdown and Restoring it at Startup
#
# 		) Saving and Restoring the Buffer Pool State Online
#
# 		) Displaying Buffer Pool Dump Progress
#
# 		) Displaying Buffer Pool Load Progress
#
# 		) Aborting a Buffer Pool Load Operation
#
# 		) Monitoring Buffer Pool Load Progress Using Performance Schema
#
# CONFIGURING THE DUMP PERCENTAGE FOR BUFFER POOL PAGES
#
# Before dumping pages from the buffer pool, you can configure the percentage of most-recently-used buffer pool pages
# that you want to dump by setting the innodb_buffer_pool_dump_pct option.
#
# If you plan to dump buffer pool pages while the server is running, you can configure the option dynamically:
#
# 		SET GLOBAL innodb_buffer_pool_dump_pct=40;
#
# If you plan to dump buffer pool pages at server shutdown, set innodb_buffer_pool_dump_pct in your configuration file.
#
# 		[mysqld]
# 		innodb_buffer_pool_dump_pct=40
#
# The innodb_buffer_pool_dump_pct default value is 25 (dump 25% of most-recently-used pages)
#
# SAVING THE BUFFER POOL STATE AT SHUTDOWN AND RESTORING IT AT STARTUP
#
# To save the state of the buffer pool at server shutdown, issue the following statement prior to shutting down the server:
#
# 		SET GLOBAL innodb_buffer_pool_dump_at_shutdown=ON;
#
# innodb_buffer_pool_dump_at_shutdown is enabled by default.
#
# To restore the buffer pool state at server startup, specify the --innodb-buffer-pool-load-at-startup option when starting
# the server:
#
# 		mysqld --innodb-buffer-pool-load-at-startup=ON;
#
# innodb_buffer_pool_load_at_startup is enabled by default.
#
# SAVING AND RESTORING THE BUFFER POOL STATE ONLINE
#
# To save the state of the buffer pool while MySQL server is running, issue the following statement:
#
# 		SET GLOBAL innodb_buffer_pool_dump_now=ON;
#
# To restore the buffer pool state while MySQL is running, issue the following statement:
#
# 		SET GLOBAL innodb_buffer_pool_load_now=ON;
#
# DISPLAYING BUFFER POOL DUMP PROGRESS
#
# To display the progress when saving the buffer pool state to disk, issue the following statement:
#
# 		SHOW STATUS LIKE 'Innodb_buffer_pool_dump_status';
#
# If the operation has not yet started, "not started" is returned. If the operation is complete, the completion time is
# printed (e.g Finished at 110505 12:18:02). If the operation is in progress, status information is provided (e.g Dumping buffer pool 5/7, page 237/2873)
#
# DISPLAYING BUFFER POOL LOAD PROGRESS
#
# To display progress when loading the buffer pool, issue the following statement:
#
# 		SHOW STATUS LIKE 'Innodb_buffer_pool_load_status';
#
# If the operation has not yet started, "not started" is returned. If the operation is complete, the completion time is printed
# (e.g Finished at 110505 12:23:24).
#
# If the operation is in progress, status information is provided (e.g Loaded 123/22301 pages)
#
# ABORTING A BUFFER POOL LOAD OPERATION
#
# To abort a buffer pool load operation, issue the following statement:
#
# 		SET GLOBAL innodb_buffer_pool_load_abort=ON;
#
# MONITORING BUFFER POOL LOAD PROGRESS USING PERFORMANCE SCHEMA
#
# You can monitor buffer pool load progress using Performance Schema.
#
# The following example demonstrates how to enable the stage/innodb/buffer pool load stage event instrument and related consumer
# tables to monitor buffer pool load progress.
#
# For information about buffer pool dump and load procedures used in this example, see SECTION 15.8.3.7, "Saving and Restoring the Buffer Pool State"
#
# For information about Performance Schema stage event instruments and related consumers, see SECTION 26.12.5, "Performance Schema Stage Event Tables"
#
# 		1. Enable the stage/innodb/buffer pool load instrument:
#
# 				mysql> UPDATE performance_schema.setup_instruments SET ENABLED = 'YES'
# 						 WHERE NAME LIKE 'stage/innodb/buffer%';
#
# 		2. Enable the stage event consumer tables, which include events_stages_current, events_stages_history, and events_stages_history_long
#
# 				mysql> UPDATE performance_schema.setup_consumers SET ENABLED = 'YES'
# 						 WHERE NAME LIKE '%stages%';
#
# 		3. Dump the current buffer pool state by enabling innodb_buffer_pool_dump_now
#
# 				mysql> SET GLOBAL innodb_buffer_pool_dump_now=ON;
#
# 		4. Check the buffer pool dump status to ensure that the operation has completed.
#
# 				mysql> SHOW STATUS LIKE 'Innodb_buffer_pool_dump_status'\G
# 				************************** 1. row ******************************
# 				Variable_name: Innodb_buffer_pool_dump_status
# 						  Value: Buffer pool(s) dump completed at 150202 16:38:58
#
# 		5. Load the buffer pool by enabling innodb_buffer_pool_load_now
#
# 				mysql> SET GLOBAL innodb_buffer_pool_load_now=ON;
#
# 		6. Check the current status of the buffer pool load operation by querying the Performance Schema events_stages_current table.
#
# 			The WORK_COMPLETED column shows the number of buffer pool pages loaded. The WORK_ESTIMATED column provides an estimate of the
# 			remaining work, in pages.
#
# 				mysql> SELECT EVENT_NAME, WORK_COMPLETED, WORK_ESTIMATED
# 				FROM performance_schema.events_stages_current;
# 				+-------------------------------+-------------------+------------------------+
# 				| EVENT_NAME 						  | WORK_COMPLETED 	 | WORK_ESTIMATED 		  |
# 				+-------------------------------+-------------------+------------------------+
# 				| stage/innodb/buffer pool load | 		5353 			 | 		7167 				  |
# 				+-------------------------------+-------------------+------------------------+
#
# 			The events_stages_current table returns an empty set if the buffer pool load operation has completed.
#
# 			In this case, you can check the events_stages_history table to view data for the completed event.
#
# 			For example:
#
# 				mysql> SELECT EVENT_NAME, WORK_COMPLETED, WORK_ESTIMATED
# 				FROM performance_schema.events_stages_history;
# 				+-----------------------------------+----------------------+---------------------------+
# 				| EVENT_NAME 								| WORK_COMPLETED 		  | WORK_ESTIMATED 				|
# 				+-----------------------------------+----------------------+---------------------------+
# 				| stage/innodb/buffer pool load 		| 		7167 				  | 		7167 					   |
# 				+-----------------------------------+----------------------+---------------------------+
#
# 			NOTE:
#
# 				You can also monitor buffer pool load progress using Performance Schema when loading the buffer pool
# 				at startup using innodb_buffer_pool_load_at_startup.
#
# 				In this case, the stage/innodb/buffer pool load instrument and related consumers must be enabled at
# 				startup.
#
# 				For more information, see SECTION 26.3, "PERFORMANCE SCHEMA STARTUP CONFIGURATION"
#
# 15.8.3.8 EXCLUDING BUFFER POOL PAGES FROM CORE FILES
#
# A core file records the status and memory image of a running process. Because the buffer pool resides in main memory,
# and the memory image of a running process is dumped to the core file, systems with large buffer pools can produce
# large core files when the mysqld process dies.
#
# Large core files can be problematic for a number of reasons including the time it takes to write them, the amount of disk
# space they consume, and the challenges associated with transferring large files.
#
# To reduce core file size, you can disable the innodb_buffer_pool_in_core_file variable to omit buffer pool pages from
# core dumps.
#
# The innodb_buffer_pool_in_core_file variable was introduced in MySQL 8.0.14 and is enabled by default.
#
# Excluding buffer pool pages may also be desirable from a security perspective if you have concerns about dumping database
# pages to core files that may be shared inside or outside of your organization for debugging purposes.
#
# NOTE:
#
# 		Access to the data present in buffer pool pages at the time the mysqld process died may be beneficial in some debugging
# 		scenarios. If in doubt whether to include or exclude buffer pool pages, consult MySQL support.
#
# Disabling innodb_buffer_pool_in_core_file takes effect only if the core_file variable is enabled and the operating system
# supports the MADV_DONTDUMP non-POSIX extension to the madvise() system call, which is supported in Linux 3.4 and later.
#
# The MADV_DONTDUMP extension causes pages in a specified range to be excluded from core dumps.
#
# Assuming the operating system supports the MADV_DONTDUMP extension, start the server with the --core-file and --innodb-buffer-pool-in-core-file=OFF
# options to generate core files without buffer pool pages.
#
# 		shell> mysqld --core-file --innodb-buffer-pool-in-core-file=OFF
#
# The core_file variable is read only and disabled by default. It is enabled by specifying the --core-file option at startup.
#
# The innodb_buffer_pool_in_core_file variable is dynamic. It can be specified at startup or configured at runtime using a SET
# statement.
#
# 		mysql> SET GLOBAL innodb_buffer_pool_in_core_file=OFF;
#
# If the innodb_buffer_pool_in_core_file variable is disabled but MADV_DONTDUMP is not supported by the operating system, or an
# madvise() failure occurs, a warning is written to the MySQL server error log and the core_file variable is disabled to prevent
# writing core files that unintentionally include buffer pool pages.
#
# If the read-only core_file variable becomes disabled, the server must be restarted to enable it again.
#
# The following table shows configuration and MADV_DONTDUMP support scenarios that determine whether core files are generated
# and whether they include buffer pool pages.
#
# TABLE 15.5 CORE FILE CONFIGURATION SCENARIOS
#
# 	core_file VARIABLE 			innodb_buffer_pool_in_core_file VARIABLE 			madvise() MADV_DONTDUMP SUPPORT 							OUTCOME
#
# 	OFF (default) 					Not relevant to outcome 								Not relevant to outcome 									Core file is not generated
#
# 	ON 								ON (default) 												Not relevant to outcome 									Core file is generated with buffer pool pages
#
# 	ON 								OFF 															Yes 																Core file is generated without buffer pool pages
#
# 	ON 								OFF 															No 																Core file is not generated, core_file is disabled,
# 																																											and a warning is written to the server error log
#
# The reduction in core file size achieved by disabling the innodb_buffer_pool_in_core_file variable depends on the size of the buffer pool, but it is also affected
# by the InnoDB page size.
#
# A smaller page size means more pages are required for the same amount of data, and more pages means more page metadata. The following table
# provides size reduction examples that you might see for a 1GB buffer pool with different pages sizes.
#
# TABLE 15.6 Core File Size with Buffer Pool Pages Included and Excluded
#
# 		innodb_page_size SETTING 						BUFFER POOL PAGES INCLUDED (innodb_buffer_pool_in_core_file=ON) 		BUFFER POOL PAGES EXCLUDED (innodb_buffer_pool_in_core_file=OFF)
#
# 			4kb 													2.1GB 																					0.9GB
#
# 			64kb 													1.7GB 																					0.7GB
#
# 15.8.4 Configuring Thread Concurrency for InnoDB
#
# InnoDB uses operating system threads to process requests from user transactions. (Transactions may issue many requests to InnoDB before they
# commit or roll back).
#
# On modern operating systems and servers with multi-core processors, where context switching is efficient, most workloads run well without any
# limit on the number of concurrent threads.
#
# In situations where it is helpful to minimize context switching between threads, InnoDB can use a number of techniques to limit the number
# of concurrently executing operating system threads (and thus the number of requests that are processed at any one time).
#
# When InnoDB receives a new request from a user session, if the number of threads concurrently executing is at a pre-defined limit, the new
# request sleeps for a short time before it tries again.
#
# A request that cannot be rescheduled after the sleep is put in a first-in/first-out queue and eventually is processed.
#
# Threads waiting for locks are not counted in the number of concurrently executing threads.
#
# You can limit the number of concurrent threads by setting the configuration parameter innodb_thread_concurrency. Once the number of
# executing threads reaches this limit, additional threads sleep for a number of microseconds, set by the configuration parameter
# innodb_thread_sleep_delay, before being placed into the queue.
#
# You can set the configuration option innodb_adaptive_max_sleep_delay to the highest value you would allow for innodb_thread_sleep_delay,
# and InnoDB automatically adjusts innodb_thread_sleep_delay up or down depending on the current thread-scheduling activity.
#
# This dynamic adjustment helps the thread scheduling mechanism to work smoothly during times when the system is lightly loaded and when
# it is operating near full capacity.
#
# The default value for innodb_thread_concurrency and the implied default limit on the number of concurrent threads has been changed in 
# various releases of MySQL and InnoDB.
#
# The default value of innodb_thread_concurrency is 0, so that by default there is no limit on the number of concurrently executing threads.
#
# InnoDB causes threads to sleep only when the number of concurrent threads is limited. When there is no limit on the number of threads,
# all contend equally to be scheduled.
#
# That is, if innodb_thread_concurrency is 0, the value of innodb_thread_sleep_delay is ignored.
#
# When there is a limit on the number of threads (when innodb_thread_concurrency is > 0), InnoDB reduces context switching overhead
# by permitting multiple requests made during the execution of a single SQL statement to enter InnoDB without observing the limit set
# by innodb_thread_concurrency.
#
# Since an SQL statement (such as a join) may comprise multiple row operations within InnoDB, InnoDB assigns a specified number of "tickets"
# that allow a thread to be scheduled repeatedly with minimal overhead.
#
# When a new SQL statement starts, a thread has no tickets, and it must observe innodb_thread_concurrency. Once the thread is entitled to enter
# InnoDB, it is assigned a number of tickets that it can use for subsequently entering InnoDB to perform row operations.
#
# If the tickets run out, the thread is evicted, and innodb_thread_concurrency is observed again which may place the thread back into the first-in/first-out
# queue of waiting threads.
#
# When the thread is once again entitled to enter InnoDB, tickets are assigned again. The number of tickets assigned is specified by the global option
# innodb_concurrency_tickets, which is 5000 by default.
#
# A thread that is waiting for a lock is given one ticket once the lock becomes available.
#
# The correct values of these variables depend on your environment and workload. Try a range of different values to determine what value works for your
# applications.
#
# Before limiting the number of concurrently executing threads, review configuration options that may improve the performance of InnoDB on multi-core and
# multi-processor computers, such as innodb_adaptive_hash_index.
#
# For general performance information about MySQL thread handling, see SECTION 8.12.4.1, "HOW MYSQL HANDLES CLIENT CONNECTIONS"
#
# 15.8.5 CONFIGURING THE NUMBER OF BACKGROUND INNODB I/O THREADS
#
# InnoDB uses background threads to service various types of I/O requests. You can configure the number of background threads that service
# read and write I/O on data pages using the innodb_read_io_threads and innodb_write_io_threads configuration parameters.
#
# These parameters signify the number of background threads used for read and write requests, respectively.
#
# They are effective on all supported platforms. You can set values for these parameters in the MySQL option file (my.cnf or
# my.ini); you cannot change values dynamically.
#
# The default value for these parameters is 4 and permissible values range from 1-64.
#
# The purpose of these configuration options to make InnoDB more scalable on high end systems. Each background thread can handle up to 256
# pending I/O requests.
#
# A major source of background I/O is read-ahead requests. InnoDB tries to balance the load of incoming requests in such a way that most background
# threads share work equally.
#
# InnoDB also attempts to allocate read requests from the same extent to the same thread, to increase the chances of coalescing the requests.
#
# If you have a high end I/O subsystem and you see more than 64 x innodb_read_io_threads pending read requests in SHOW ENGINE INNODB STATUS output,
# you might improve performance by increasing the value of innodb_read_io_threads.
#
# On Linux systems, InnoDB uses the asynch I/O subsystem by default to perform read-ahead and write requests for data file pages, which changes
# the way that InnoDB background threads service these types of I/O requests.
#
# For more information, see SECTION 15.8.6, "USING ASYNCH I/O ON LINUX"
#
# For more information about InnoDB I/O performance, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 15.8.6 USING ASYNCH I/O ON LINUX
#
# InnoDB uses the asynch I/O subsystem (native AIO) on Linux to perform read-ahead and write requests for data file pages.
#
# This behavior is controlled by the innodb_use_native_aio configuration option, which applies to Linux systems only and is
# enabled by default. On other Unix-like systems, InnoDB uses synchronous I/O only.
#
# Historically, InnoDB only used asynch I/O on Windows systems. Using the asynch I/O subsystem on Linux requires the libaio library.
#
# With synchronous I/O, query threads queue I/O requests, and InnoDB background threads retrieve the queued requests one at a time,
# issuing a synchronous I/O call for each.
#
# When an I/O request is completed and the I/O call returns, the InnoDB background thread that is handling the request calls an I/O
# completion routine and returns to process the next request.
#
# The number of requests that can be processed in parallel is n, where n is the number of InnoDB background threads.
#
# The number of InnoDB background threads is controlled by innodb_read_io_threads and innodb_write_io_threads.
#
# See SECTION 15.8.5, "Configuring the Number of Background InnoDB I/O Threads"
#
# With native AIO, query threads dispatch I/O requests directly to the operating system, thereby removing the limit imposed
# by the number of background threads.
#
# InnoDB background threads wait for I/O events to signal completed requests. When a request is completed, a background thread calls
# an I/O completion routine and resumes waiting for I/O events.
#
# The advantage of native AIO is scalability for heavily I/O-bound systems that typically show many pending reads/writes in SHOW ENGINE 
# INNODB STATUS\G output.
#
# The increase in parallel processing when using native AIO means that the type of I/O scheduler or properties of the disk array controller
# have a greater influence on I/O performance.
#
# A potential disadvantage of native AIO for heavily I/O-bound systems is lack of control over the number of I/O write requests dispatched to
# the operating system at once.
#
# Too many I/O write requests dispatched to the operating system for parallel processing could, in some cases, result in I/O read starvation,
# depending on the amount of I/O activity and system capabilities.
#
# If a problem with the asynch I/O subsystem in the OS prevents InnoDB from starting, you can start the server with innodb_use_native_aio=0.
#
# This option may also be disabled automatically during startup if InnoDB detects a potential problem such as a combination of tmpdir location,
# tmpfs file system, and Linux kernel that does not support asynch I/O on tmpfs.
#
# 15.8.7 CONFIGURING THE INNODB MASTER THREAD I/O RATE
#
# The master thread in InnoDB is a thread that performs various tasks in the background. Most of these tasks are I/O related, such as flushing
# dirty pages from the buffer pool or writing changes from the insert buffer to the appropriate secondary indexes.
#
# The master thread attempts to perform these tasks in a way that does not adversely affect the normal working of the server.
#
# It tries to estimate the free I/O bandwidth available and tune its activities to take advantage of this free capacity.
#
# Historically, InnoDB has used a hard coded value of 100 IOPs (input/output operations per second) as the total I/O capacity
# of the server.
#
# The parameter innodb_io_capacity indicates the overall I/O capacity available to InnoDB. This parameter should be set to approximately
# the number of I/O operations that the system can perform per second.
#
# The value depends on your system configuration. When innodb_io_capacity is set, the master threads estimates the I/O bandwidth
# available for background tasks based on the set value. Setting the value to 100 reverts to the old behavior.
#
# You can set the value of innodb_io_capacity to any number 100 or greater. The default value is 200, reflecting that the performance
# of typical modern I/O devices is higher than in the early days of MySQL.
#
# Typically, values around the previous default of 100 are appropriate for consumer-level storage devices, such as hard drives up to
# 7200 RPMs.
#
# Faster hard drives, RAID configurations, and SSDs benefit from higher values.
#
# The innodb_io_capacity setting is a total limit for all buffer pool instances. When dirty pages are flushed, the innodb_io_capacity
# limit is divided equally among buffer pool instances.
#
# For more information, see the innodb_io_capacity system variable description.
#
# You can set the value of this parameter in the MySQL option file (my.cnf or my.ini) or change it dynamically with the SET_GLOBAL
# statement, which requires privileges sufficient to set global system variables. See SECTION 5.1.9.1, "System Variable Privileges"
#
# The innodb_flush_sync configuration option causes the innodb_io_capacity setting to be ignored during bursts of I/O activity
# that occur at checkpoints.
#
# innodb_flush_sync is enabled by default.
#
# In earlier MySQL releases, the InnoDB master thread also performed any needed purge operations. Those I/O operations are now
# performed by other background threads, whose number is controlled by the innodb_purge_threads configuration option.
#
# For more information about InnoDB I/O performance, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 15.8.8 CONFIGURING SPIN LOCK POLLING
#
# InnoDB mutexes and rw-locks are typically reserved for short intervals. On a multi-core system, it can be more efficient
# for a thread to continuously check if it can acquire a mutex or rw-lock for a period of time before it sleeps.
#
# If the mutex or rw-lock becomes available during this period, the thread can continue immediately, in the same time
# slice.
#
# However, too-frequent polling of a shared object such as a mutex or rw-lock by multiple threads can cause "cache ping pong",
# which results in processors invalidating portions of each other's cache.
#
# InnoDB minimizes this issue by forcing a random delay between polls to desynchronize polling activity.
#
# The random delay is implemented as a spin-wait loop.
#
# The duration of a spin-wait loop is determined by the number of PAUSE instructions that occur in the loop. That number is generated
# by randomly selecting an integer ranging from 0 up to but not including the innodb_spin_wait_delay value, and multiplying that value
# by 50.
#
# (The multiplier value, 50 is hardcoded before MySQL 8.0.16, and configurable thereafter).
#
# For example, an integer is randomly selected from the following range for an innodb_spin_wait_delay setting of 6:
#
# 		{0,1,2,3,4,5}
#
# The selected integer is multiplied by 50, resulting in one of six possible PAUSE instruction values:
#
# 		{0,50,100,150,200,250}
#
# For that set of values, 250 is the maximum number of PAUSE instructions that can occur in a spin-wait loop. An innodb_spin_wait_delay
# setting of 5 results in a set of five possible values {0,50,100,150,200}, where 200 is the maximum number of PAUSE instructions, and so
# on.
#
# In this way, the innodb_spin_wait_delay setting controls the maximum delay between spin lock polls.
#
# On a system where all processor cores share a fast cache memory, you might reduce the maximum delay or disable the busy loop altogether
# by setting innodb_spin_wait_delay=0.
#
# On a system with multiple processor chips, the effect of cache invalidation can be more significant and you might increase the maximum
# delay.
#
# In the 100MHz Pentium era, an innodb_spin_wait_delay unit was calibrated to be equivalent to one microsecond. That time equivalence did 
# not hold, but PAUSE instruction duration remained fairly constant in terms of processor cycles relative to other CPU instructions until
# the introduction of the Skylake generation of processors, which have a comparatively longer PAUSE instruction.
#
# The innodb_spin_wait_pause_multiplier variable was introduced in MySQL 8.0.16 to provide a way to account for differences in PAUSE
# instruction duration.
#
# The innodb_spin_wait_pause_multiplier variable controls the size of PAUSE instruction values.
#
# For example, assuming an innodb_spin_wait_delay setting of 6, decreasing the innodb_spin_wait_pause_multiplier value from
# 50 (the default and previously hardcoded value) to 5 generates a set of smaller PAUSE instruction values:
#
# 		{0,5,10,15,20,25}
#
# The ability to increase or decrease PAUSE instruction values permits fine tuning InnoDB for different processor architechtures.
#
# Smaller PAUSE instruction values would be appropriate for processor architechtures with a comparatively longer PAUSE instruction,
# for example.
#
# The innodb_spin_wait_delay and innodb_spin_wait_pause_multiplier variables are dynamic. They can be specified in a MySQL option
# file or modified at runtime using a SET_GLOBAL statement.
#
# Modifying the variables at runtime requires privileges sufficient to set global system variables. See SECTION 5.1.9.1, "System Variable Privileges"
#
# 15.8.9 CONFIGURING INNODB PURGE SCHEDULING
#
# The purge operations (a type of garbage collection) that InnoDB performs automatically may be performed by one or more 
# separate threads rather than as part of the master thread.
#
# The use of separate threads improves scalability by allowing the main database operations to run independently from maintenance
# work happening in the background.
#
# To control this feature, increase the value of the configuration option innodb_purge_threads.
#
# If DML action is concentrated on a single table or a few tables, keep the setting low so that the threads do not contend with 
# each other for access to the busy tables.
#
# If DML operations are spread across many tables, increase the setting. Its maximum is 32. innodb_purge_threads is a non-dynamic
# configuration option, which means it cannot be configured at runtime.
#
# There is another related configuration option, innodb_purge_batch_size with a default value of 300 and maximum value of 5000.
#
# This option is mainly intended for experimentation and tuning of purge operations, and should not be interesting to typical users.
#
# For more information about InnoDB I/O performance, see SECTION 8.5.8, "Optimizing InnoDB Disk I/O"
#
# 15.8.10 CONFIGURING OPTIMIZER STATISTICS FOR INNODB
#
# 15.8.10.1 Configuring Persistent Optimizer Statistics Parameters
# 15.8.10.2 Configuring Non-Persistent Optimizer Statistics Parameters
# 15.8.10.3 Estimating ANALYZE TABLE Complexity for InnoDB Tables
#
# This section describes how to configure persistent and non-persistent optimizer statistics for InnoDB tables.
#
# Persistent optimizer statistics are persisted across server restarts, allowing for greater plan stability and more consistent
# query performance.
#
# Persistent optimizer statistics also provide control and flexibility with these additional benefits:
#
# 		) You can use the innodb_stats_auto_recalc configuration option to control whether statistics are updated automatically after
# 			substantial changes to a table.
#
# 		) You can use the STATS_PERSISTENT, STATS_AUTO_RECALC, and STATS_SAMPLE_PAGE clauses with CREATE_TABLE and ALTER_TABLE statements
# 			to configure optimizer statistics for individual tables.
#
# 		) You can query optimizer statistics data in the mysql.innodb_table_stats and mysql.innodb_index_stats tables
#
# 		) You can view the last_update column of the mysql.innodb_table_stats and mysql.innodb_index_stats tables to see when statistics were last updated.
#
# 		) You can manually modify the mysql.innodb_table_stats and mysql.innodb_index_stats tables to force a specific query optimization plan or to test
# 			alternative plans without modifying the database.
#
# The persistent optimizer statistics feature is enabled by default (innodb_stats_persistent=ON)
#
# Non-persistent optimizer statistics are cleared on each server restart and after some other operations, and recomputed on the next table access.
# As a result, different estimates could be produced when recomputing statistics, leading to different choices in execution plans and variations
# in query performance.
#
# This section also provides information about estimating ANALYZE_TABLE complexity, which may be useful when attempting to achieve a balance between
# accurate statistics and ANALYZE_TABLE execution time.
#
# 15.8.10.1 CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS
#
# The persistent optimizer statistics feature improves plan stability by storing statistics to disk and making them persistent
# across server restarts so that the optimizer is more likely to make consistent choices each time for a given query.
#
# Optimizer statistics are persisted to disk when innodb_stats_persistent=ON or when individual tables are created or altered with
# STATS_PERSISTENT=1.
#
# innodb_stats_persistent is enabled by default.
#
# Formerly, optimizer statistics were cleared on each server restart and after some other operations, and recomputed on the next table
# access. Consequently, different estimates could be produced when recalculating statistics, leading to different choices in query
# execution plans and thus variations in query performance.
#
# Persistent statistics are stored in the mysql.innodb_table_stats and mysql.innodb_index_stats tables, as described in 
# SECTION 15.8.10.1.5, "InnoDB Persistent Statistics Tables"
#
# To revert to using non-persistent optimizer statistics, you can modify tables using an ALTER TABLE tbl_name STATS_PERSISTENT=0 statement.
#
# For related information, see SECTION 15.8.10.2, "Configuring Non-Persistent Optimizer Statistics Parameters"
#
# 15.8.10.1.1 CONFIGURING AUTOMATIC STATISTICS CALCULATION FOR PERSISTENT OPTIMIZER STATISTICS
#
# The innodb_stats_auto_recalc configuration option, which is enabled by default, determines whether statistics are calculated automatically
# whenever a table undergoes substantial changes (to more than 10% of the rows).
#
# You can also configure automatic statistics recalculation for individual tables using a STATS_AUTO_RECALC clause in a 
# CREATE_TABLE or ALTER_TABLE statement. innodb_stats_auto_recalc is enabled by default.
#
# Because of the asynch nature of automatic statistics recalculation (which occurs in the background), statistics may not be
# recalculated instantly after running a DML operation that affects more than 10% of a table, even when innodb_stats_auto_recalc
# is enabled.
#
# In some cases, statistics recalculation may be delayed by a few seconds.
#
# If up-to-date statistics are required immediately after changing significant portions of a table, run ANALYZE_TABLE to initiate
# a synch (foreground) recalculations of Stats.
#
# If innodb_stats_auto_recalc is disabled, ensure the accuracy of optimizer stats by issuing the ANALYZE_TABLE statement for each applicable
# table after making substantial changes to indexed columns.
#
# You might run this statement in your setup scripts after representative data has been loaded into the table, and run it periodically 
# after DML operations significantly change the contents of indexed columns, or on a schedule at times of low activity.
#
# When a new index is added to an existing table, or a column is added or dropped, index statistics are calculated and added to the
# innodb_index_stats table regardless of the value of innodb_stats_auto_recalc.
#
# 		CAUTION:
#
# 			To ensure statistics are gathered when a new index is created, either enable the innodb_stats_auto_recalc option, or run
# 			ANALYZE_TABLE after creating each new index when the persistent statistics mode is enabled.
#
# 15.8.10.1.2 CONFIGURING OPTIMIZER STATISTICS PARAMETERS FOR INDIVIDUAL TABLES
#
# innodb_stats_persistent, innodb_stats_auto_recalc and innodb_stats_persistent_sample_pages are global configuration options.
#
# To override these system-wide settings and configure optimizer statistics parameters for individual tables, you can define STATS_PERSISTENT,
# STATS_AUTO_RECALC and STATS_SAMPLE_PAGES clauses in CREATE_TABLE or ALTER_TABLE statements.
#
# 		) STATS_PERSISTENT specifies whether to enable persistent statistics for an InnoDB table. The value DEFAULT causes the persistent
# 			statistics setting for the table to be determined by the innodb_stats_persistent configuration option.
#
# 			The value 1 enables persistent statistics for the table, while the value 0 turns off this feature.
#
# 			After enabling persistent statistics through a CREATE TABLE or ALTER TABLE statement, issue an ANALYZE_TABLE statement to calculate
# 			the statistics, after loading representative data into the table.
#
# 		) STATS_AUTO_RECALC specifies whether to automatically recalculate persistent statistics for an InnoDB table.
#
# 			The value DEFAULT causes the persistent statistics setting for the table to be determined by the innodb_stats_auto_recalc configuration
# 			option.
#
# 			The value 1 causes statistics to be recalculated when 10% of the data in the table has changed.
#
# 			The value 0 prevents automatic recalculation for this table; with this setting, issue an ANALYZE_TABLE statement to recalculate the
# 			statistics after making substantial changes to the table.
#
# 		) STATS_SAMPLE_PAGES specifies the number of index pages to sample when estimating cardinality and other statistics for an indexed column,
# 			such as those calculated by ANALYZE_TABLE.
#
# ALl three clauses are specified in the following CREATE_TABLE example:
#
# 		CREATE TABLE `t1` (
# 			`id` int(8) NOT NULL auto_increment,
# 			`data` varchar(255),
# 			`date` datetime,
# 		PRIMARY KEY (`id`),
# 		INDEX `DATE_IX` (`date`)
# 		) ENGINE=InnoDB,
# 			STATS_PERSISTENT=1,
# 			STATS_AUTO_RECALC=1,
# 			STATS_SAMPLE_PAGES=25;
#
# 15.8.10.1.3 CONFIGURING THE NUMBER OF SAMPLED PAGES FOR INNODB OPTIMIZER STATISTICS
#
# The MySQL query optimizer uses estimated statistics about key distributions to choose the indexes for an execution plan,
# based on the relative selectivity of the index.
#
# Operations such as ANALYZE_TABLE causes InnoDB to sample random pages from each index on a table to estimate the cardinality
# of the index. (This technique is known as random dives)
#
# To give you control over the quality of the statistics estimate (and thus better information for the query optimizer), you can change
# the number of sampled pages using the parameter innodb_stats_persistent_sample_pages, which can be set at runtime.
#
# innodb_stats_persistent_sample_pages has a default value of 20. As a general guideline, consider modifying this parameter when encountering
# the following issues:
#
# 		1. Statistics are not accurate enough and the optimizer chooses suboptimal plans, as shown by EXPLAIN output.
#
# 			The accuracy of statistics can be checked by comparing the actual cardinality of an index (as returned by running SELECT_DISTINCT
# 			on the index columns) with the estimates provided in the mysql.innodb_index_stats persistent statistics table.
#
# 			If it is determined that statistics are not accurate enough, the value of innodb_stats_persistent_sample_pages should be increased
# 			until the statistics estimates are sufficiently accurate.
#
# 			Increasing innodb_stats_persistent_sample_pages too much, however, could cause ANALYZE_TABLE to run slowly.
#
# 		2. ANALYZE_TABLE is too slow. In this case, innodb_stats_persistent_sample_pages should be decreased until ANALYZE_TABLE
# 			execution time is acceptable.
#
# 			Decreasing the value too much, however, could lead to the first problem of inaccurate statistics and suboptimal query
# 			execution plans.
#
# 			If a balance cannot be achieved between accurate statistics and ANALYZE_TABLE execution time, consider decreasing the
# 			number of indexed columns in the table or limiting the number of partitions to reduce ANALYZE_TABLE complexity.
#
# 			The number of columns in the table's primary key is also important to consider, as primary key columns are appended
# 			to each nonunique index.
#
# 			For related information, see SECTION 15.8.10.3, "ESTIMATING ANALYZE TABLE COMPLEXITY FOR INNODB TABLES"
#
# 15.8.10.1.4 INCLUDING DELETE-MARKED RECORDS IN PERSISTENT STATISTICS CALCULATIONS
#
# By default, InnoDB reads uncommitted data when calculating statistics. In the case of an uncommitted transaction that deletes
# rows from a table, InnoDB excludes records that are delete-marked when calculating row estimates and index statistics, which
# can lead to non-optimal execution plans for other transactions that are operating on the table concurrently using a transaction
# isolation level other than READ_UNCOMMITTED.
#
# To avoid this scenario, innodb_stats_include_delete_marked can be enabled to ensure that InnoDB includes delete-marked
# records when calculating persistent optimizer statistics.
#
# When innodb_stats_include_delete_marked is enabled, ANALYZE_TABLE considers delete-marked records when recalculating statistics.
#
# innodb_stats_include_delete_marked is a global setting that affects all InnoDB tables, and it is only applicable to persistent
# optimizer statistics.
#
# 15.8.10.1.5 InnoDB PERSISTENT STATISTICS TABLES
#
# The persistent statistics feature relies on the internally managed tables in the mysql database, named innodb_table_stats
# and innodb_index_stats.
#
# These tables are set up automatically in all install, upgrade, and build-from-source procedures.
#
# TABLE 15.7 Columns of innodb_table_stats
#
# 		Column Name 					Description
#
# 		database_name 					Database name
#
# 		table_name 						Table name, partition name, or subpartition name
#
# 		last_update 					A timestamp indicating the last time that InnoDB updated this row
#
# 		n_rows 							The number of rows in the table
#
# 		clustered_index_size 		The size of the primary index, in pages
#
# 		sum_of_other_index_sizes 	The total size of other (non-primary) indexes, in pages
#
# TABLE 15.8 Columns of innodb_index_stats
#
# 		COlumn name 					Description
#
# 		database_name 					Database name
#
# 		table_name 						Table name, partition name, or subpartition name
#
# 		index_name 						Index name
#
# 		last_update 					A timestamp indicating the last time that InnoDB updated this row
#
# 		stat_name 						The name of the statistics whose value is reported in the stat_value column
#
# 		stat_value 						The value of the statistics that is named in stat_name column
#
# 		sample_size 					The number of pages sampled for the estimate provided in the stat_value column
#
# 		stat_description 				Description of the statistic that is named in the stat_name column
#
# Both the innodb_table_stats and innodb_index_stats tables include a last_update column showing when InnoDB last updated
# index statistics, as shown in the following example:
#
# 		mysql> SELECT * FROM innodb_table_stats \G
# 		************************ 1. row *******************************
# 						database_name: sakila
# 						  table_name : actor
# 						 last_update : 2014-05-28 16:16:44
# 								n_rows : 200
# 			  clustered_index_size: 1
# 		 sum_of_other_index_sizes: 1
# 		/ etc /
#
# 		mysql> SELECT * FROM innodb_index_stats \G
# 		*********************** 1. row ********************************
# 						database_name: sakila
# 						 table_name  : actor
# 						 index_name  : PRIMARY
# 						last_update  : 2014-05-28 16:16:44
# 						 stat_name   : n_diff_pfx01
# 						stat_value   : 200
# 						sample_size  : 1
# 		/ etc /
#
# The innodb_table_stats and innodb_index_stats tables are ordinary tables and can be updated manually. The ability to update
# statistics manually makes it possible to force a specific query optimization plan or test alternative plans without modifying
# the database.
#
# If you manually update statistics, issue the FLUSH TABLE tbl_name command to make MySQL reload the updated statistics.
#
# Persistent statistics are considered local information, because they relate to the server instance. The innodb_table_stats and 
# innodb_index_stats tables are therefore not replicated when automatic statistics recalculation takes place.
#
# If you run ANALYZE_TABLE to initiate a synchronous recalculation of statistics, this statement is replicated (unless you suppressed
# logging for it), and recalculation takes place on the replication slaves.
#
# 15.8.10.1.6 InnoDB PERSISTENT STATISTICS TABLES EXAMPLE
#
# The innodb_table_stats table contains one row per table. The data collected is demonstrated in the following example.
#
# Table t1 contains a primary index (columns a,b) secondary index (columns c,d) and unique index (columns e, f):
#
# 		CREATE TABLE t1 (
# 			a INT, b INT, c INT, d INT, e INT, f INT,
# 			PRIMARY KEY (a,b), KEY i1 (c,d), UNIQUE KEY i2uniq (e,f)
# 		) ENGINE=INNODB;
#
# After inserting five rows of sample data, the table appears as follows:
#
# 		mysql> SELECT * FROM t1;
# 		+------+--------+---------------+-----------+----------+------------+
# 		| a 	 | b 		 | c 				  | d 		  | e 		 | f 			  |
# 		+------+--------+---------------+-----------+----------+------------+
# 		| 1 	 | 1 		 | 10 			  | 11 		  | 100 		 | 101 		  |
# 		| 1 	 | 2 	 	 | 10 			  | 11 		  | 200 		 | 102 		  |
# 		| 1 	 | 3 		 | 10 			  | 11  		  | 100 		 | 103 		  |
# 		| 1 	 | 4 		 | 10 			  | 12 		  | 200 		 | 104 		  |
# 		| 1 	 | 5 		 | 10 			  | 12 		  | 100 		 | 105 		  |
# 		+------+--------+---------------+-----------+----------+------------+
#
# To immediately update statistics, run ANALYZE_TABLE (if innodb_stats_auto_recalc is enabled, statistics are updated
# automatically within a few seconds assuming that the 10% threshold for changed table rows is reached):
#
# 		mysql> ANALYZE TABLE t1;
# 		+----------+---------+--------------+----------------+
# 		| Table 	  | Op 	   | Msg_type 	   |   Msg_text 	  |
# 		+----------+---------+--------------+----------------+
# 		| test.t1  | analyze | status 		| OK 				  |
# 		+----------+---------+--------------+----------------+
#
# Table statistics for table t1 show the last time InnoDB updated the table statistics (2014-03-14 14:36:34), the number of
# rows in the table (5), the clustered index size (1 page), and the combined size of the other indexes (2 pages)
#
# 		mysql> SELECT * FROM mysql.innodb_table_stats WHERE table_name like 't1'\G
# 		********************************** 1. row *********************************
# 						database_name: test
# 						 table_name  : t1
# 						last_update  : 2014-03-14 14:36:34
# 						 		n_rows : 5
# 			clustered_index_size  : 1
# 		 sum_of_other_index_sizes: 2
#
# The innodb_index_stats table contains multiple rows for each index. Each row in the innodb_index_stats table provides
# data related to a particular index statistic which is named in the stat_name column and described in the stat_description
# column.
#
# For example:
#
# 		mysql> SELECT index_name, stat_name, stat_value, stat_description
# 				 FROM mysql.innodb_index_stats WHERE table_name LIKE 't1';
# 		+--------------+--------------+------------------+------------------------------------+
# 		| index_name   | stat_name 	| stat_value 	    | stat_description 						  |
# 		+--------------+--------------+------------------+------------------------------------+
# 		| PRIMARY 		| n_diff_pfx01 | 1 					 | a 											  |
# 		| PRIMARY 	   | n_diff_pfx02 | 5 					 | a,b 										  |
# 		| PRIMARY 	   | n_leaf_pages | 1 					 | Number of leaf pages in the index  |
# 		| PRIMARY 		| size 			| 1 					 | Number of pages in the index 		  |
# 		| i1 				| n_diff_pfx01 | 1 					 | c 											  |
# 		| i1 				| n_diff_pfx02 | 2 					 | c,d 										  |
# 		| i1 				| n_diff_pfx03 | 2 					 | c,d,a 									  |
# 		| i1 				| n_diff_pfx04 | 5 					 | c,d,a,b 									  |
# 		| i1 				| n_leaf_pages | 1 					 | Number of leaf pages in the index  |
# 		| i1 				| size 			| 1 					 | Number of pages in the index 	     |
# 		| i2uniq 		| n_diff_pfx01 | 2 					 | e 											  |
# 		| i2uniq 		| n_diff_pfx02 | 5 					 | e,f 										  |
# 		| i2uniq 		| n_leaf_pages | 1 					 | Number of leaf pages in the index  |
# 		| i2uniq 		| size 		   | 1 					 | Number of pages in the index 		  |
# 		+--------------+--------------+------------------+------------------------------------+
#
# The stat_name column shows the following types of statistics:
#
# 		) size: Where stat_name=size, the stat_value column displays the total number of pages in the index
#
# 		) n_leaf_pages: Where stat_name=n_leaf_pages, the stat_value column displays the number of leaf pages in the index.
#
# 		) n_diff_pfxNN: Where stat_name=n_diff_pfx01, the stat_value column displays the number of distinct values in the first
# 			column of the index.
#
# 			Where stat_name=n_diff_pfx02, the stat_value column displays the number of distinct values in the first two columns
# 			of the index, and so on.
#
# 			Additionally, where stat_name=n_diff_pfxNN, the stat_description column shows a comma separated list of the index
# 			columns that are counted.
#
# To further illustrate the n_diff_pfxNN statistic, which provides cardinality data, consider once again the t1 table
# example that was introduced previously.
#
# As shown below, the t1 table is created with a primary index (columns a,b), a secondary index (columns c,d) and a unique
# index (columns e,f):
#
# 		CREATE TABLE t1 (
# 			a INT, b INT, c INT, d INT, e INT, f INT,
# 			PRIMARY KEY (a, b), KEY i1 (c, d), UNIQUE KEY i2uniq (e, f)
# 		) ENGINE=INNODB;
#
# After inserting five rows of sample data, the table appears as follows:
#
# 		mysql> SELECT * FROM t1;
# 		+-------+------+-----------+-------+-------+--------+
# 		| a 	  | b 	| c 			| d 	  | e 	 | f 		 |
# 		+-------+------+-----------+-------+-------+--------+
# 		| 1 	  | 1 	| 10 			| 11 	  | 100 	 | 101 	 |
# 		| 1 	  | 2 	| 10 			| 11 	  | 200 	 | 102 	 |
# 		| 1	  | 3 	| 10 			| 11 	  | 100 	 | 103 	 |
# 		| 1 	  | 4 	| 10 			| 12 	  | 200 	 | 104 	 |
# 		| 1 	  | 5 	| 10 			| 12 	  | 100 	 | 105 	 |
# 		+-------+------+-----------+-------+-------+--------+
#
# When you query the index_name, stat_name, stat_value, and stat_description where stat_name LIKE 'n_diff%', the following result
# set is returned:
#
# 		mysql> SELECT index_name, stat_name, stat_value, stat_description
# 				 FROM mysql.innodb_index_stats
# 				 WHERE table_name like 't1' AND stat_name LIKE 'n_diff%';
#
# 		+------------+-------------+-----------------+---------------------+
# 		| index_name | stat_name   | stat_value 	   |   stat_description  |
# 		+------------+-------------+-----------------+---------------------+
# 		| PRIMARY 	 | n_diff_pfx01| 		1 				| 		a 					 |
# 		| PRIMARY 	 | n_diff_pfx02| 		5 				| 		a,b 				 |
# 		| i1 			 | n_diff_pfx01| 		1 				| 		c 					 |
# 		| i1 			 | n_diff_pfx02| 		2 				|  	c,d 				 |
# 		| i1 			 | n_diff_pfx03|		2 				| 		c,d,a 			 |
# 		| i1 			 | n_diff_pfx04| 		5 				| 		c,d,a,b 			 |
# 		| i2uniq 	 | n_diff_pfx01| 		2 			   | 		e 					 |
# 		| i2uniq 	 | n_diff_pfx02| 		5 				| 		e,f 				 |
# 		+------------+-------------+-----------------+---------------------+
#
# For the PRIMARY index, there are two n_diff% rows. The number of rows is equal to the number of columns in the index.
#
# NOTE:
#
# 		For nonunique indexes, InnoDB appends the columns of the primary key.
#
# 		) Where index_name=PRIMARY and stat_name=n_diff_pfx01, the stat_value is 1, which indicates that there is a single
# 			distinct value in the first column of the index (column a).
#
# 			The number of distinct values in column a is confirmed by viewing the data in column a in table t1, in which
# 			there is a single distinct value (1).
#
# 			The counted column (a) is shown in the stat_description column of the result set.
#
# 		) Where index_name=PRIMARY and stat_name=n_diff_pfx02, the stat_value is 5, which indicates that there are five distinct
# 			values in the two columns of the index (a,b).
#
# 			The number of distinct values in columns a and b is confirmed by viewing the data in columns a and b in table t1,
# 			in which there are five distinct values: (1,1), (1,2), (1,3), (1,4) and (1,5).
#
# 			The counted columns (a,b) are shown in the stat_description column of the result set.
#
# For the secondary index (i1), there are four n_diff% rows. Only two columns are defined for the secondary index (c,d) but there
# are four n_diff% rows for the secondary index because InnoDB suffixes all nonunique indexes with the primary key.
#
# As a result, there are four n_diff% rows instead of two to account for the both the secondary index columns (c,d), and the
# primary key columns (a,b)
#
# 		) Where index_name=i1 and stat_name=n_diff_pfx01, the stat_value is 1, which indicates that there is a single distinct value
# 			in the first column of the index (column c)
#
# 			The number of distinct values in column c is confirmed by viewing the data in column c in table t1, in which there is a single
# 			distinct value: (10).
#
# 			The counted column (c) is shown in the stat_description column of the result set.
#
# 		) Where index_name=i1 and stat_name=n_diff_pfx02, the stat_value is 2, which indicates that there are two distinct values in the first
# 			two columns of the index (c,d).
#
# 			The number of distinct values in columns c and d is confirmed by viewing the data in columns c and d in table t1, in which there
# 			are two distinct values: (10, 11) and (10,12).
#
# 			The counted columns (c,d) are shown in the stat_description column of the result set.
#
# 		) 	Where index_name=i1 and stat_name=n_diff_pfx03, the stat_value is 2, which indicates that there are two distinct values in the first
# 			three columns of the index (c,d,a).
#
# 			The number of distinct values in columns c,d and a is confirmed by viewing the data in column c,d and a in table t1, in which there
# 			are two distinct values:
#
# 				(10,11,1) and (10,12,1).
#
# 			The counted columns (c,d,a) are shown in the stat_description column of the result set.
#
# 		) Where index_name=i1 and stat_name=n_diff_pfx04, the stat_value is 5, which indicates that there are five distinct values in
# 			the four columns of the index (c,d,a,b)
#
# 			The number of distinct values in columns c,d,a and b is confirmed by viewing the data in columns c,d,a and b in table t1,
# 			in which there are five distinct values:
#
# 				(10,11,1,1)
#
# 				(10,11,1,2)
#
# 				(10,11,1,3)
#
# 				(10,12,1,4)
#
# 				(10,12,1,5)
#
# 			The counted columns (c,d,a,b) are shown in the stat_description column of the result set.
#
# For the unique index (i2uniq), there are two n_diff% rows.
#
# 		) Where index_name=i2uniq and stat_name=n_diff_pfx01, the stat_value is 2, which indicates that there are two distinct
# 			values in the first column of the index (column e).
#
# 			The number of distinct values in column e is confirmed by viewing the data in column e in table t1, in which there are
# 			two distinct values:
#
# 				(100)
#
# 				(200)
#
# 			The counted column (e) is shown in the stat_description column of the result set.
#
# 		) Where index_name=i2uniq and stat_name=n_diff_pfx02, the stat_value is 5, which indicates that there are five distinct
# 			values in the two columns of the index (e,f).
#
# 			The number of distinct values in columns e and f is confirmed by viewing the data in columns e and f in table t1, in which
# 			there are five distinct values:
#
# 				(100,101)
#
# 				(200,102)
#
# 				(100,103)
#
# 				(200,104)
#
# 				(100,105)
#
# 			The counted columns (e,f) are shown in the stat_description column of the result set.
#
# 15.8.10.1.7 RETRIEVING INDEX SIZE USING THE INNODB_INDEX_STATS TABLE
#
# The size of indexes for tables, partitions, or subpartitions can be retrieved using the innodb_index_stats table.
#
# In the following example, index sizes are retrieved for table t1.
#
# For a definition of table t1 and corresponding index statistics, see SECTION 15.8.10.1.6, "InnoDB PERSISTENT STATISTICS TABLES EXAMPLE"
#
# 		mysql> SELECT SUM(stat_value) pages, index_name,
# 				 SUM(stat_value)*@@innodb_page_size size
# 				FROM mysql.innodb_index_stats WHERE table_name='t1'
# 				AND stat_name = 'size' GROUP BY index_name;
# 		+------+---------------+------------+
# 		| pages| index_name 	  | size 		|
# 		+------+---------------+------------+
# 		| 1 	 | PRIMARY 		  | 16384 		|
# 		| 1 	 | i1 			  | 16384 		|
# 		| 1 	 | i2uniq 		  | 16384 		|
# 		+------+---------------+------------+
#
# For partitions or subpartitions, the same query with a modified WHERE clause can be used to retrieve index sizes.
#
# For example, the following query retrieves index sizes for partitions of table t1:
#
# 		mysql> SELECT SUM(stat_value) pages, index_name,
# 				 SUM(stat_value)*@@innodb_page_size size
# 				 FROM mysql.innodb_index_stats WHERE table_name LIKE 't1#P%'
# 				 AND stat_name = 'size' GROUP BY index_name;
#
# 15.8.10.2 CONFIGURING NON-PERSISTENT OPTIMIZER STATISTICS PARAMETERS
#
# This section describes how to configure non-persistent optimizer statistics. Optimizer statistics are not persisted
# to disk when innodb_stats_persistent=OFF or when individual tables are created or altered with STATS_PERSISTENT=0.
#
# Instead, statistics are stored in memory, and are lost when the server is shut down. Statistics are also updated
# periodically by certain operations and under certain conditions.
#
# Optimizer statistics are persisted to disk by default, enabled by the innodb_stats_persistent configuration option.
#
# For information about persistent optimizer statistics, see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# OPTIMIZER STATISTICS UPDATES
#
# Non-persistent optimizer statistics are updated when:
#
# 		) Running ANALYZE_TABLE
#
# 		) Running SHOW_TABLE_STATUS, SHOW_INDEX, or querying the INFORMATION_SCHEMA.TABLES or INFORMATION_SCHEMA.STATISTICS tables
# 			with the innodb_stats_on_metadata option enabled.
#
# 			The default setting for innodb_stats_on_metadata is OFF. Enabling innodb_stats_on_metadata may reduce access speed
# 			for schemas that have a large number of tables or indexes, and reduce stability of execution plans for queries that
# 			involve InnoDB tables.
#
# 			innodb_stats_on_metadata is configured globally using a SET statement.
#
# 				SET GLOBAL innodb_stats_on_metadata=ON
#
# 			NOTE:
#
# 				innodb_stats_on_metadata only applies when optimizer statistics are configured to be non-persistent (when innodb_stats_persistent
# 				is disabled)
#
# 		) Starting a mysql client with the --auto-rehash option enabled, which is the default.
#
# 			The auto-rehash option causes all InnoDB tables to be opened, and the open table operations cause statistics to be recalculated.
#
# 			To improve the start up time of the mysql client and to updating statistics, you can turn off auto-rehash using the --disable-auto-rehash
# 			option.
#
# 			The auto-rehash feature enables automatic name completion of database, table, and column names for interactive users.
#
# 		) A table is first opened.
#
# 		) InnoDB detects that 1/16 of table has been modified since the last time statistics were updated.
#
# CONFIGURING THE NUMBER OF SAMPLED PAGES
#
# The MySQL query optimizer uses estimated statistics about key distributions to choose the indexes for an execution plan, based on the relative
# selectivity of the index.
#
# When InnoDB updates optimizer statistics, it samples random pages from each index on a table to estimate the cardinality of the index.
#
# (This technique is known as random dives)
#
# To give you control over the quality of the statistics estimate (and thus better information for the query optimizer), you can change the number
# of sampled pages using the parameter innodb_stats_transient_sample_pages.
#
# The default number of sampled pages is 8, which could be insufficient to produce an accurate estimate, leading to poor index choices
# by the query optimizer.
#
# This technique is especially important for large tables and tables used in joins. Unnecessary full table scans for such tables can
# be a substantial performance issue.
#
# See SECTION 8.2.1.22, "AVOIDING FULL TABLE SCANS" for tips on tuning such queries. innodb_stats_transient_sample_pages is a global
# parameter that can be set at runtime.
#
# The value of innodb_stats_transient_sample_pages affects the index sampling for all InnoDB tables and indexes when innodb_stats_persistent=0.
#
# Be aware of the following potentially significant impacts when you change the index sample size:
#
# 		) Small values like 1 or 2 can result in inaccurate estimates of cardinality
#
# 		) Increasing the innodb_stats_transient_sample_pages value might require more disk reads. Values much larger than 8 (say, 100),
# 			can cause a significant slowdown in the time it takes to open a table or execute SHOW TABLE STATUS.
#
# 		) The optimizer might choose very different query plans based on different estimates of index selectivity.
#
# Whatever value of innodb_stats_transient_sample_pages works best for a system, set the option and leave it at that value.
#
# Choose a value that results in reasonably accurate estimates for all tables in your database without requiring excessive I/O.
#
# Because the statistics are automatically recalculated at various times other than on execution of ANALYZE_TABLE, it does not make
# sense to increase the index sample size, run ANALYZE_TABLE, then decrease sample size again.
#
# Smaller tables generally require fewer index samples than larger tables. If your database has many large tables, consider using
# a higher value for innodb_stats_transient_sample_pages than if you have mostly smaller tables.
#
# 15.8.10.3 ESTIMATING ANALYZE TABLE COMPLEXITY FOR INNODB TABLES
#
# ANALYZE_TABLE complexity for InnoDB tables is dependent on:
#
# 		) The number of pages sampled, as defined by innodb_stats_persistent_sample_pages
#
# 		) The number of indexed columns in a table
#
# 		) The number of partitions. If a table has no partitions, the number of partitions is considered to be 1.
#
# Using these parameters, an approximate formula for estimating ANALYZE_TABLE complexity would be:
#
# 		The value of innodb_stats_persistent_sample_pages * number of indexed columns in a table * the number of partitions
#
# Typically, the greater the resulting value, the greater the execution time for ANALYZE_TABLE
#
# NOTE:
#
# 		innodb_stats_persistent_sample_pages defines the number of pages sampled at a global level.
#
# 		To set the number of pages sampled for an individual table, use the STATS_SAMPLE_PAGES option with
# 		CREATE_TABLE or ALTER_TABLE.
#
# 		For more information, see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 		If innodb_stats_persistent=OFF, the number of pages sampled is defined by innodb_stats_transient_sample_pages.
#
# 		See SECTION 15.8.10.2, "CONFIGURING NON-PERSISTENT OPTIMIZER STATISTICS PARAMETERS" for additional information.
#
# For a more in-depth approach to estimating ANALYZE TABLE complexity, consider the following example.
#
# In Big O notation, ANALYZE_TABLE complexity is described as:
#
# 		O(n_sample
# 			* (n_cols_in_uniq_i
# 				+ n_cols_in_non_uniq_i
# 				+ n_cols_in_pk * (1 + n_non_uniq_i))
# 			* n_part)
#
# where:
#
# 		) n_sample is the number of pages sampled (defined by innodb_stats_persistent_sample_pages)
#
# 		) n_cols_in_uniq_i is total number of all columns in all unique indexes (not counting the primary key columns)
#
# 		) n_cols_in_non_uniq_i is the total number of all columns in all nonunique indexes
#
# 		) n_cols_in_pk is the number of columns in the primary key (if a primary key is not defined, InnoDB creates a single column primary key internally)
#
# 		) n_non_uniq_i is the number of nonunique indexes in the table
#
# 		) n_part is the number of partitions. If no partitions are defined, the table is considered to be a single partition.
#
# Now, consider the following table (table t), which has a primary key (2 columns), a unique index (2 columns), and two nonunique
# indexes (two columns each):
#
# 		CREATE TABLE t (
# 			a INT,
# 			b INT,
# 			c INT,
# 			d INT,
# 			e INT,
# 			f INT,
# 			g INT,
# 			h INT,
# 			PRIMARY KEY (a, b),
# 			UNIQUE KEY i1uniq (c, d),
# 			KEY i2nonuniq (e, f),
# 			KEY i3nonuniq (g, h)
# 		);
#
# For the column and index data required by the algorithm described above, query the mysql.innodb_index_stats persistent index
# statistics table for table t.
#
# The n_diff_pfx% statistics show the columns that are counted for each index. For example, columns a and b are counted for
# the primary key index.
#
# For the nonunique indexes, the primary key columns (a,b) are counted in addition to the user defined columns.
#
# NOTE:
#
# 		For additional information about the InnoDB persistent statistics tables, see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# mysql> SELECT index_name, stat_name, stat_description
# 			FROM mysql.innodb_index_stats WHERE
# 			database_name='test' AND
# 			table_name='t' AND
# 			stat_name like 'n_diff_pfx%';
# 			+--------------+---------------+-----------------------+
# 			| index_name 	| stat_name 	 | stat_description 	    |
# 			+--------------+---------------+-----------------------+
# 			| PRIMARY 		| n_diff_pfx01  | a 							 |
# 			| PRIMARY 		| n_diff_pfx02  | a,b 						 |
# 			| i1uniq 		| n_diff_pfx01  | c 							 |
# 			| i1uniq 		| n_diff_pfx02  | c,d 						 |
# 			| i2nonuniq 	| n_diff_pfx01  | e 							 |
# 			| i2nonuniq 	| n_diff_pfx02  | e,f 						 |
# 			| i2nonuniq    | n_diff_pfx03  | e,f,a 					 |
# 			| i2nonuniq 	| n_diff_pfx04  | e,f,a,b 					 |
# 			| i3nonuniq 	| n_diff_pfx01  | g 							 |
# 			| i3nonuniq 	| n_diff_pfx02  | g,h 						 |
# 			| i3nonuniq 	| n_diff_pfx03  | g,h,a 					 |
# 			| i3nonuniq 	| n_diff_pfx04  | g,h,a,b 					 |
# 			+--------------+---------------+-----------------------+
#
# Based on the index statistics data shown above and the table definition, the following values can be determined:
#
# 		) n_cols_in_uniq_i, the total number of all columns in all unique indexes not counting the primary key columns, is 2 (c and d)
#
# 		) n_cols_in_non_uniq_i, the total number of all columns in all nonunique indexes, is 4 (e,f,g and h)
#
# 		) n_cols_in_pk, the number of columns in the primary key, is 2 (a and b)
#
# 		) n_non_uniq_i, the number of nonunique indexes in the table, is 2 (i2nonuniq and i3nonuniq))
#
# 		) n_part, the number of partitions, is 1.
#
# You can now calculate innodb_stats_persistent_sample_pages * (2 + 4 + 2 * (1 + 2)) * 1 to determine the number of leaf pages
# that are scanned.
#
# With innodb_stats_persistent_sample_pages set to the default value of 20, and with a default page size of 16 KiB (innodb_page_size=16384),
# you can then estimate that 20 * 12 * 16384 bytes are read for table t, or about 4 MiB.
#
# NOTE:
#
# 		All 4 MiB may not be read from disk, as some leaf pages may already be cached in the buffer pool.
#
# 15.8.11 CONFIGURING THE MERGE THRESHOLD FOR INDEX PAGES
#
# You can configure the MERGE_THRESHOLD value for index pages. If the "page-full" percentage for an index page falls below
# the MERGE_THRESHOLD value when a row is deleted or when a row is shortened by an UPDATE operation, InnoDB attempts to merge
# the index page with a neighboring index page.
#
# The default MERGE_THRESHOLD value is 50, which is the previously hardcoded value. The minimum MERGE_THRESHOLD value is 1 and
# the maximum value is 50.
#
# When the "page-full" percentage for an index page falls below 50%, which is the default MERGE_THRESHOLD setting, InnoDB
# attempts to merge the index page with a neighboring page.
#
# If both pages are close to 50% full, a page split can occur soon after the pages are merged.
#
# If this merge-split behavior occurs frequently, it can have an adverse affect on performance.
#
# To avoid frequent merge-splits, you can lower the MERGE_THRESHOLD value so that InnoDB attempts page merges
# at a lower "page-full" percentage.
#
# Merging pages at a lower page-full percentage leaves more room in index pages and helps reduce merge-split behavior.
#
# The MERGE_THRESHOLD for index pages can be defined for a table or for individual indexes. A MERGE_THRESHOLD value
# defined for an individual index takes priority over a MERGE_THRESHOLD value defined for the table.
#
# If undefined, the MERGE_THRESHOLD value defaults to 50.
#
# SETTING MERGE_THRESHOLD FOR A TABLE
#
# You can set the MERGE_THRESHOLD value for a table using the table_option COMMENT clause of the CREATE_TABLE statement.
#
# For example:
#
# 		CREATE TABLE t1 (
# 			id INT,
# 			KEY id_index (id)
# 		) COMMENT='MERGE_THRESHOLD=45';
#
# You can also set the MERGE_THRESHOLD value for an existing table using the table_option COMMENT clause with ALTER_TABLE:
#
# 		CREATE TABLE t1 (
# 			id INT,
# 			KEY id_index (id)
# 		);
#
# 		ALTER TABLE t1 COMMENT='MERGE_THRESHOLD=40';
#
# SETTING MERGE_THRESHOLD FOR INDIVIDUAL INDEXES
#
# To set the MERGE_THRESHOLD value for an individual index, you can use the index_option COMMENT clause with
# CREATE_TABLE, ALTER_TABLE or CREATE_INDEX, as shown in the following examples:
#
# 		) Setting MERGE_THRESHOLD for an individual index using CREATE_TABLE:
#
# 			CREATE TABLE t1 (
# 				id INT,
# 				KEY id_index (id) COMMENT 'MERGE_THRESHOLD=40'
# 			);
#
# 		) Setting MERGE_THRESHOLD for an individual index using ALTER_TABLE:
#
# 			CREATE TABLE t1 (
# 				id INT,
# 				KEY id_index (id)
# 			);
#
# 			ALTER TABLE t1 DROP KEY id_index;
# 			ALTER TABLE t1 ADD KEY id_index (id) COMMENT 'MERGE_THRESHOLD=40';
#
# 		) Setting MERGE_THRESHOLD for an individual index using CREATE_INDEX:
#
# 			CREATE TABLE t1 (id INT);
# 			CREATE INDEX id_index ON t1 (id) COMMENT 'MERGE_THRESHOLD=40';
#
# NOTE:
#
# 		You cannot modify the MERGE_THRESHOLD value at the index level for GEN_CLUST_INDEX, which is the clustered
# 		index created by InnoDB when an InnoDB table is created without a primary key or unique key index.
#
# 		You can only modify the MERGE_THRESHOLD value for GEN_CLUST_INDEX by setting MERGE_THRESHOLD for the table.
#
# QUERYING THE MERGE_THRESHOLD VALUE FOR AN INDEX
#
# The current MERGE_THRESHOLD value for an index can be obtained by querying the INNODB_INDEXES table.
#
# For example:
#
# 		mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_INDEXES WHERE NAME='id_index' \G
# 		************************** 1. row ****************************
# 						INDEX_ID: 91
# 							NAME : id_index
# 						TABLE_ID: 68
# 						   TYPE : 0
# 						N_FIELDS: 1
# 						PAGE_NO : 4
# 						   SPACE: 57
# 			  MERGE_THRESHOLD: 40 		
#
# You can use SHOW_CREATE_TABLE to view the MERGE_THRESHOLD value for a table, if explicitly defined using the
# table_option COMMIT clause:
#
# 		mysql> SHOW CREATE TABLE t2 \G
# 		*********************** 1. row ****************************
# 					Table: t2
# 			Create Table: CREATE TABLE `t2` (
# 				`id` int(11) DEFAULT NULL,
# 			KEY `id_index` (`id`) COMMENT 'MERGE_THRESHOLD=40'
# 			) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
#
# NOTE:
#
# 		A MERGE_THRESHOLD value defined at the index level takes priority over a MERGE_THRESHOLD value defined
# 		for the table.
#
# 		If undefined, MERGE_THRESHOLD defaults to 50% (MERGE_THRESHOLD=50, which is the previously harcoded value)
#
# Likewise, you can use SHOW_INDEX to view the MERGE_THRESHOLD value for an index, if explicitly defined using the
# index_option COMMENT clause:
#
# 		mysql> SHOW INDEX FROM t2 \G
# 		************************** 1. row ************************
# 					Table: t2
# 			Non_unique : 1
# 			Key_name   : id_index
# 		  Seq_in_index: 1
# 			Column_name: id
# 			Collation  : A
# 			Cardinality: 0
# 				Sub_part: NULL
# 				Packed  : NULL
# 				 	 Null: YES
# 			Index_type : BTREE
# 				Comment : 
# 		 Index_comment: MERGE_THRESHOLD=40
#
# MEASURING THE EFFECT OF MERGE_THRESHOLD SETTINGS
#
# The INNODB_METRICS table provides two counters that can be used to measure the effect of a MERGE_THRESHOLD setting on index page merges.
#
# 		mysql> SELECT NAME, COMMENT FROM INFORMATION_SCHEMA.INNODB_METRICS
# 				 WHERE NAME LIKE '%index_page_merge%';
# 		+------------------------------+----------------------------------------+
# 		| NAME 								 | COMMENT 								         |
# 		+------------------------------+----------------------------------------+
# 		| index_page_merge_attempts 	 | Number of index page merge attempts    |
# 		| index_page_merge_successful  | Number of successful index page merges |
# 		+------------------------------+----------------------------------------+
#
# When lowering the MERGE_THRESHOLD value, the objectives are:
#
# 		) A smaller number of page merge attempts and successful page merges
#
# 		) A similar number of page merge attempts and successful page merges
#
# A MERGE_THRESHOLD setting that is too small could result in large data files due to an excessive amount of empty page space.
#
# For information about using INNODB_METRICS counters, see SECTION 15.14.6, "InnoDB INFORMATION_SCHEMA METRICS TABLE"
#
# 15.8.12 ENABLING AUTOMATIC CONFIGURATION FOR A DEDICATED MYSQL SERVER
#
# When innodb_dedicated_server is enabled, InnoDB automatically configures the following variables:
#
# 		) innodb_buffer_pool_size
#
# 		) innodb_log_file_size
#
# 		) innodb_log_files_in_group (as of MySQL 8.0.14)
#
# 		) innodb_flush_method
#
# Only consider enabling innodb_dedicated_server if the MySQL instance resides on a dedicated server where it can use all available
# system resources.
#
# For example, consider enabling if you run MySQL Server in a Docker container or dedicated VM. Enabling innodb_dedicated_server is
# not recommended if the MySQL instance shares system resources with other applications.
#
# The information that follows describes how each variable is automatically configured.
#
# 		) innodb_buffer_pool_size
#
# 			Buffer pool size is configured according to the amount of memory detected on the server.
#
# 		TABLE 15.9 Automatically CONFIGURED BUFFER POOL SIZE
#
# 			DETECTED SERVER MEMORY 				BUFFER POOL SIZE
#
# 			< 1GB 									128MiB (the default value)
#
# 			1GB to 4GB 								detected server memory*0.5
#
# 			> 4GB 									detected server memory*0.75
#
# 		) innodb_log_file_size
#
# 			As of MySQL 8.0.14, log file size is configured according to the automatically configured buffer pool size.
#
# 		TABLE 15.10 Automatically Configured Log File Size
#
# 			BUFFER POOL SIZE 			LOG FILE SIZE
#
# 			< 8GB 						512MiB
#
# 			8GB to 128GB 				1024MiB
#
# 			>128GB 						2048MiB
#
# 		NOTE:
#
# 			Prior to MySQL 8.0.14, the innodb_log_file_size variable was automatically configured according to the amount
# 			of memory detected on the server, as shown below:
#
# 				TABLE 15.11 AUTOMATICALLY CONFIGURED LOG FILE SIZE (MySQL 8.0.13 AND EARLIER)
#
# 					DETECTED SERVER MEMORY 		LOG FILE SIZE
#
# 						< 1GB 						48MiB (the default value)
#
# 						<= 4GB 						128MiB
# 
# 						<= 8GB 						512MiB
#
# 						<= 16GB 						1024MiB
#
# 						> 16GB 						2048MiB
#
# 		) innodb_log_files_in_group
#
# 			The number of log files is configured according to the automatically configured buffer pool size (in gigabytes).
#
# 			Automatic configuration of the innodb_log_files_in_group variable was added in MySQL 8.0.14
#
# 				TABLE 15.12 AUTOMATICALLY CONFIGURED NUMBER OF LOG FILES
#
# 					BUFFER POOL SIZE 					NUMBER OF LOG FILES
#
# 					< 8GB 								ROUND(buffer pool size)
#
# 					8GB to 128GB 						ROUND(buffer pool size * 0.75)
#
# 					> 128GB 								64
#
# 		) innodb_flush_method
#
# 			The flush method is set to O_DIRECT_NO_FSYNC when innodb_dedicated_server is enabled.
#
# 			If the O_DIRECT_NO_FSYNC setting is not available, the default innodb_flush_method setting
# 			is used.
#
# 			InnoDB uses O_DIRECT during flushing I/O, but skips the fsync() system call after each write operation.
#
# 				WARNING:
#
# 					Prior to MySQL 8.0.14, this setting is not suitable for file systems such as XFS and EXT4, which require an fsync()
# 					system call to synchronize file system metadata changes.
#
# 					As of MySQL 8.0.14, fsync() is called after creating a new file, after increasing file size, and after closing a file, to
# 					ensure that file system metadata changes are synchronized.
#
# 					The fsync() system call is still skipped after each write operation.
#
# 					On storage devices with cache, data loss is possible if data files and redo log files reside on different storage devices,
# 					and a crash occurs before data file writes are flushed from the device cache.
#
# 					If you use or intend to use different storage devices for redo logs and data files, use O_DIRECT instead.
#
# 			If an automatically configured option is configured explicitly in an option file or elsewhere, the explicitly specified setting
# 			is used, and a startup warning similar to this is printed to stderr:
#
# 				[Warning] [00000000] InnoDB: Option innodb_dedicated_server is ignored for innodb_buffer_pool_size because innodb_buffer_pool_size=134217728
# 				is specified explicitly.
#
# 			Explicit configuration of one option does not prevent the automatic configuration of other options.
#
# 			For example, if innodb_dedicated_server is enabled and innodb_buffer_pool_size is configured explicitly in an option file,
# 			innodb_log_file_size and innodb_log_files_in_group are automatically configured based on the implicit buffer pool size
# 			that is calculated according to the amount of memory detected on the server.
#
# 			Automatically configured settings are evaluated and reconfigured if necessary each time the MySQL server is started.
#
# 15.9 InnoDB TABLE AND PAGE COMPRESSION
#
# 15.9.1 InnoDB TABLE COMPRESSION
# 15.9.2 InnoDB PAGE COMPRESSION
#
# This section provides information about the InnoDB table compression and InnoDB page compression features.
#
# The page compression feature is also referred to as transparent page compression.
#
# Using the compression features of InnoDB, you can create tables where the data is stored in compressed form.
# Compression can help to improve both raw performance and scalability.
#
# The compression means less data is transferred between disk and memory, and takes up less space on disk and in memory.
#
# The benefits are amplified for tables with secondary indexes, because index data is compressed also. Compression
# can be especially important for SSD storage devices, because they tend to have lower capacity than HDD devices.
#
# 15.9.1 INNODB TABLE COMPRESSION
#
# 15.9.1.1 Overview of Table Compression
# 15.9.1.2 Creating Compressed Tables
# 15.9.1.3 Tuning Compression For InnoDB Tables
# 15.9.1.4 Monitoring InnoDB Table Compression at Runtime
# 15.9.1.5 How Compression Works for InnoDB Tables
# 15.9.1.6 Compression for OLTP Workloads
# 15.9.1.7 SQL Compression Syntax Warnings and Errors
#
# This section describes InnoDB table compression, which is supported with InnoDB tables that reside in file_per_table
# tablespaces or general tablespaces. Table compression is enabled using the ROW_FORMAT=COMPRESSED attribute with 
# CREATE_TABLE or ALTER_TABLE.
#
# 15.9.1.1 OVERVIEW OF TABLE COMPRESSION
#
# Because processors and  cache memories have increased in speed more than disk storage devices, many workloads are disk-bound.
# Data compression enables smaller database size, reduced I/O and improved throughput, at the small cost of increased CPU utilization.
#
# Compression is especially valuable for read-intensive applications, on systems with enough RAM to keep frequently used data in memory.
#
# An InnoDB table created with ROW_FORMAT=COMPRESSED can use a smaller page size on disk than the configured innodb_page_size value.
#
# Smaller pages require less I/O to read from and write to disk, which is especially valuable for SSD devices.
#
# The compressed page size is specified through the CREATE_TABLE or ALTER_TABLE KEY_BLOCK_SIZE parameter. The different page size requires
# that the table be placed in a file-per-table tablespace or general tablespace rather than in the system tablespace, as the system
# tablespace cannot store compressed tables. 
#
# For more information, see Section 15.6.3.2, "File-Per-Table Tablespaces", and Section 15.6.3.3, "General Tablespaces"
#
# The level of compression is the same regardless of the KEY_BLOCK_SIZE value. As you specify smaller values for KEY_BLOCK_SIZE, you get
# the I/O benefits of increasingly smaller pages.
#
# But if you specify a value that is too small, there is additional overhead to reorganize the pages when data values cannot be compressed
# enough to fit multiple rows in each page.
#
# There is a hard limit on how small KEY_BLOCK_SIZE can be for a table, based on the lengths of the key columns for each of its indexes.
#
# Specify a value that is too small, and the CREATE_TABLE or ALTER_TABLE statement fails.
#
# In the buffer pool, the compressed data is held in small pages, with a page size based on the KEY_BLOCK_SIZE value. For extracting
# or updating the column values, MySQL also creates an uncompressed page in the buffer pool with the uncompressed data.
#
# Within the buffer pool, any updates to the uncompressed page are also re-written back to the equivalent compressed page.
#
# You might need to size your buffer pool to accomodate the additional data of both compressed and uncompressed pages, although
# the uncompressed pages are evicted from the buffer pool when space is needed, and then uncompressed again on the next access.
#
# 15.9.1.2 CREATING COMPRESSED TABLES
#
# Compressed tables can be created in the file-per-table tablespaces or in general tablespaces. Table compression is not available
# for the InnoDB system tablespace. The system tablespace (space 0, the .ibdata files) can contain user-created tables, but it also
# contains internal system data, which is never compressed.
#
# Thus, compression applies only to tables (and indexes) stored in file-per-table or general tablespaces.
#
# CREATING A COMPRESSED TABLE IN FILE-PER-TABLE TABLESPACE
#
# To create a compressed table in a file-per-table tablespace, innodb_file_per_table must be enabled (the default). You can set this
# parameter in the MySQL configuration file (my.cnf or my.ini) or dynamically, using a SET statement.
#
# After the innodb_file_per_table option is configured, specify the ROW_FORMAT=COMPRESSED clause or KEY_BLOCK_SIZE clause, or both,
# in a CREATE_TABLE or ALTER_TABLE statement to create a compressed table in a file-per-table tablespace.
#
# For example, you might use the following statements:
#
# 		SET GLOBAL innodb_file_per_table=1;
# 		CREATE TABLE t1
# 			(c1 INT PRIMARY KEY)
# 			ROW_FORMAT=COMPRESSED
# 			KEY_BLOCK_SIZE=8;
#
# CREATING A COMPRESSED TABLE IN A GENERAL TABLESPACE
#
# To create a compressed table in a general tablespace, FILE_BLOCK_SIZE must be defined for the general tablespace, which is specified
# when the tablespace is created.
#
# The FILE_BLOCK_SIZE value must be a valid compressed page size in relation to the innodb_page_size value, and the page size of the
# compressed table, defined by the CREATE_TABLE or ALTER_TABLE KEY_BLOCK_SIZE clause, must be equal to FILE_BLOCK_SIZE/1024.
#
# For example, if innodb_page_size=16384 and FILE_BLOCK_SIZE=8192, the KEY_BLOCK_SIZE of the table must be 8.
#
# For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# The following example demonstrates creating a general tablespace and adding a compressed table. The example assumes a default
# innodb_page_size of 16K.
#
# The FILE_BLOCK_SIZE of 8192 requires that the compressed table have a KEY_BLOCK_SIZE of 8.
#
# 		mysql> CREATE TABLESPACE `ts2` ADD DATAFILE 'ts2.ibd' FILE_BLOCK_SIZE = 8192 Engine=InnoDB;
#
# 		mysql> CREATE TABLE t4 (c1 INT PRIMARY KEY) TABLESPACE ts2 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8;
#
# NOTES:
#
# 		) As of MySQL 8.0, the tablespace file for a compressed table is created using the physical page size instead of
# 			the InnoDB page size, which makes the initial size of a tablespace file for an empty compressed table smaller
# 			than in previous MySQL releases.
#
# 		) If you specify ROW_FORMAT=COMPRESSED, you can omit KEY_BLOCK_SIZE; the KEY_BLOCK_SIZE setting defaults to half the innodb_page_size value.
#
# 		) If you specify a valid KEY_BLOCK_SIZE value, you can omit ROW_FORMAT=COMPRESSED; compression is enabled automatically.
#
# 		) To determine the best value for KEY_BLOCK_SIZE, typically you create several copies of the same table with different values for this clause,
# 			then measure the size of the resulting .ibd files and see how well each performs with a realistic workload.
#
# 			For general tablespaces, keep in mind that dropping a table does not reduce the size of the general tablespace .ibd file, nor does
# 			it return disk space to the OS.
#
# 			For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# 		) The KEY_BLOCK_SIZE value is treated as a hint; a different size could be used by InnoDB if necessary. For file-per-table tablespaces,
# 			the KEY_BLOCK_SIZE can only be less than or equal to the innodb_page_size value.
#
# 			If you specify a value greater than the innodb_page_size value, the specified value is ignored, a warning is issued, 
# 			and KEY_BLOCK_SIZE is set to half of the innodb_page_size value. If innodb_strict_mode=ON, specifying an invalid KEY_BLOCK_SIZE
# 			value returns an error.
#
# 			For general tablespaces, valid KEY_BLOCK_SIZE values depend on the FILE_BLOCK_SIZE setting of the tablespace.
#
# 			For more information, see SECTION 15.6.3.3, "General Tablespaces"
#
# 		) InnoDB supports 32kb and 64kb page sizes but these page sizes do not support compression. For more information, refer to the innodb_page_size documentation.
#
# 		) The default uncompressed size of InnoDB data pages is 16KB. Depending on the combination of option values, MySQL uses a page size of 1kb,2kb,4kb, 8kb or 16kb
# 			for the tablespace data file (.ibd file)
#
# 			The actual compression algorithm is not affected by the KEY_BLOCK_SIZE value; the value determines how large each compressed chunk is, which in turn
# 			affects how many rows can be packed into each compressed page.
#
# 		) When creating a compressed table in a file-per-table tablespace, setting KEY_BLOCK_SIZE equal to the InnoDB page size does not typically
# 			result in much compression.
#
# 			For example, setting KEY_BLOCK_SIZE=16 typically would not result in much compression, since the normal InnoDB page size is 16kb.
# 			This setting may still be useful for tables with many long BLOB, VARCHAR, or TEXT columns, because such values often do compress well,
# 			and might therefore require fewer overflow pages as described in SECTION 15.9.1.5, "HOW COMPRESSION WORKS FOR INNODB TABLES"
#
# 			For general tablespaces, a KEY_BLOCK_SIZE value equal to the InnoDB page size is not permitted.
#
# 			For more information, see SECTION 15.6.3.3, "General Tablespaces"
#
# 		) All indexes of a table (including the clustered index) are compressed using the same page size, as specified in the CREATE TABLE or ALTER TABLE
# 			statement.
#
# 			Table attributes such as ROW_FORMAT and KEY_BLOCK_SIZE are not part of the CREATE INDEX syntax for InnoDB tables, and are ignored if they
# 			are specified (although, if specified, they will appear in the output of the SHOW_CREATE_TABLE statement)
#
# 		) For performance-related configuration options, see SECTION 15.9.1.3, "TUNING COMPRESSION FOR INNODB TABLES"
#
# RESTRICTIONS ON COMPRESSED TABLES
#
# 		) Compressed tables cannot be stored in the InnoDB system tablespace.
#
# 		) General tablespaces can contain multiple tables, but compressed and uncompressed tables cannot coexist within the same general tablespace.
#
# 		) Compression applies to an entire table and all its associated indexes, not to individual rows, despite the clause name ROW_FORMAT.
#
# 		) InnoDB does not support compressed temporary tables. When innodb_strict_mode is enabled (the default), CREATE_TEMPORARY_TABLE returns errors
# 			if ROW_FORMAT=COMPRESSED or KEY_BLOCK_SIZE is specified.
#
# 			If innodb_strict_mode is disabled, warnings are issued and the temporary table is created using a non-compressed row format.
#
# 			The same restrictions apply to ALTER_TABLE operations on temporary tables.
#
# 15.9.1.3 TUNING COMPRESSION FOR INNODB TABLES
#
# Most often, the internal optimizations described in InnoDB Data Storage and Compression ensure that the system runs well with compressed data.
# However, because the efficiency of compression depends on the nature of your data, you can make decisions that affect the performance of compressed tables:
#
# 		) Which tables to compress
#
# 		) What compressed page size to use
#
# 		) Whether to adjust the size of the buffer pool based on run-time performance characteristics, such as the amount of time the system spends
# 			compressing and uncompressing data.
#
# 			Whether the workload is more like a data warehouse (primarily queries) or an OLTP system (mix of queries and DML)
#
# 		) If the system performs DML operations on compressed tables, and the way the data is distributed leads to expensive compression failures
# 			at runtime, you might adjust additional advanced configuration options.
#
# Use the guidelines in this section to help make those architechtural and configuration choices. When you are ready to conduct long-term
# testing and put compressed tables into production, see SECTION 15.9.1.4, "MONITORING INNODB TABLE COMPRESSION AT RUNTIME" for ways to verify
# the effectiveness of those choices under real-world conditions.
#
# When to Use Compression
#
# In general, compression works best on tables that include a reasonable number of character string columns and where the data is read far more
# often than it is written.
#
# Because there are no guaranteed ways to predict whether or not compression benefits a particular situation, always test with a specific
# workload and data set running on a representative configuration. Consider the following factors when deciding which tables to compress.
#
# DATA CHARACTERISTICS AND COMPRESSION
#
# A key determinant of the efficiency of compression in reducing the size of data files is the nature of the data itself.
#
# Recall that compression works by identifying repeated strings of bytes in a block of data. Completely randomized data is
# the worst case.
#
# Typical data often has repeated values, and so compresses effectively. Character strings often compress well, whether defined
# in CHAR, VARCHAR, TEXT, or BLOB columns.
#
# On the other hand, tables containing mostly binary data (integers or floating point numbers) or data that is previously
# compressed (for example JPEG or PNG images) may not generally compress well, significantly or at all.
#
# You choose whether to turn on compression for each InnoDB table. A table and all of its indexes use the same (compressed)
# page size. It might be that the primary key (clustered) index, which contains the data for all columns of a table, compresses
# more effectively than the secondary indexes.
#
# For those cases where there are long rows, the use of compression might result in long column values being stored "off-page", 
# as discussed in DYNAMIC ROW FORMAT.
#
# Those overflow pages may compress well. Given these considerations, for many applications, some tables compress more effectively
# than others, and you might find that your workload performs best only with a subset of tables compressed.
#
# To determine whether or not to compress a particular table, conduct experiments.
#
# You can get a rough estimate of how efficiently your data can be compressed by using a utility that implements LZ77 compression
# (such as gzip or Winzip) on a copy of the .ibd file for an uncompressed table.
#
# You can expect less compression from a MySQL compressed table than from file-based compression tools, because MySQL compresses data
# in chunks based on the page size, 16KB by default.
#
# In addition to user data, the page format includes some internal system data that is not compressed.
#
# File-based compression utilities can examine much larger chunks of data, and so might find more repeated strings in a huge file
# than MySQL can find in an individual page.
#
# Another way to test compression on a specific table is to copy some data from your uncompressed table to a similar; compressed
# table (having all the same indexes) in a file-per-table tablespace and look at the size of the resulting .ibd files.
#
# For example:
#
# 		USE test;
# 		SET GLOBAL innodb_file_per_table=1;
# 		SET GLOBAL autocommit=0;
#
# 		-- Create an uncompressed table with a million or two rows --
# 		CREATE TABLE big_table AS SELECT * FROM information_schema.columns;
# 		INSERT INTO big_table SELECT * FROM big_table; - x 10 -
# 		COMMIT;
# 		ALTER TABLE big_table ADD id int unsigned NOT NULL PRIMARY KEY auto_increment;
#
# 		SHOW CREATE TABLE big_table\G
# 	
# 		select count(id) from big_table;
#
# 		-- Check how much space is needed for the uncompressed table --
# 		\! ls -l data/test/big_table.ibd
#
# 		CREATE TABLE key_block_size_4 LIKE big_table;
# 		ALTER TABLE key_block_size_4 key_block_size=4 row_format=compressed;
#
# 		INSERT INTO key_block_size_4 SELECT * FROM big_table;
# 		commit;
#
# 		--Check how much space is needed for a compressed table
# 		-- with particular compression settings
# 		\! ls -l data/test/key_block_size_4.ibd
#
# This experiment produced the following numbers, which of course could vary considerably depending on your table structure and data:
#
# 		-rw-rw----  	1 cirrus 	staff 	310378496 	Jan 	9 13:44 data/test/big_table.ibd
# 		-rw-rw---- 		1 cirrus 	staff 	83886080 	Jan 	9 15:10 data/test/key_block_size_4.ibd
#
# To see whether compression is efficient for your particular workload:
#
# 		) For simple tests, use a MySQL instance with no other compressed tables and run queries against the INFORMATION_SCHEMA.INNODB_CMP table
#
# 		) For more elaborate tests involving workloads with multiple compressed tables, run queries against the INFORMATION_SCHEMA.INNODB_CMP_PER_INDEX table.
#
# 			Because the statistics in the INNODB_CMP_PER_INDEX table are expensive to collect, you must enable the configuration option
# 			innodb_cmp_per_index_enabled before querying that table, and you might restrict such testing to a development server or a 
# 			non-critical slave server.
#
# 		) Run some typical SQL statements against the compressed table you are testing.
#
# 		) Examine the ratio of successful compression operations to overall compression operations by querying the INFORMATION_SCHEMA.INNODB_CMP or
# 			INFORMATION_SCHEMA.INNODB_CMP_PER_INDEX table, and comparing COMPRESS_OPS to COMPRESS_OPS_OK.
#
# 		) If a high percentage of compression operations complete successfully, the table might be a good candidate for compression.
#
# 		) If you get a high proportion of compression failures, you can adjust innodb_compression_level, innodb_compression_failure_threshold_pct,
# 			and innodb_compression_pad_pct_max options as described in SECTION 15.9.1.6, "COMPRESSION FOR OLTP WORKLOADS" and try further tests.
#
# DATABASE COMPRESSION VERSUS APPLICATION COMPRESSION
#
# Decide whether to compress data in your application or in the table; do not use both types of compression for the same data.
# When you compress the data in the application and store the results in a compressed table, extra space savings are extremely
# unlikely, and the double compression just wastes CPU cycles.
#
# COMPRESSING IN THE DATABASE
#
# When enabled, MySQL table compression is automatic and applies to all columns and index values. The columns can still be tested with
# operators such as LIKE, and sort operations can still use indexes even when the index values are compressed.
#
# Because indexes are often a significant fraction of the total size of a database, compression could result in significant savings
# in storage, I/O or processor time.
#
# The compression and decompression operations happen on the database server, which likely is a powerful system that is sized
# to handle the expected load.
#
# COMPRESSING IN THE APPLICATION
#
# If you compress data such as text in your application, before it is inserted into the database, You might save overhead for data that
# does not compress well by compressing some columns and not others.
#
# This approach uses CPU cycles for compression and uncompression on the client machine rather than the database server, which might be
# appropriate for a distributed application with many clients, or where the client machine has spare CPU cycles.
#
# HYBRID APPROACH
#
# Of course, it is possible to combine these approaches.
#
# For some applications, it may be appropriate to use some compressed tables and some uncompressed tables. It may be best to externally
# compress some data (and store it in uncompressed tables) and allow MySQL to compress (some of) the other tables in the application.
#
# As always, up-front design and real-life testing are valuable in reaching the right decision.
#
# WORKLOAD CHARACTERISTICS AND COMPRESSION
#
# In addition to choosing which tables to compress (and the page size), the workload is another key determinant of performance.
#
# If the application is dominated by reads, rather than updates, fewer pages need to be reorganized and recompressed after the index
# page runs out of room for the per-page "modification log" that MySQL maintains for compressed data.
#
# If the updates predominantly change non-indexed columns or those containing BLOBS or large strings that happen to be stored "off-page",
# the overhead of compression may be acceptable.
#
# If the only changes to a table are INSERTS that use a monotonically increasing primary key, and there are few secondary indexes, there
# is little need to reorganize and recompress index pages. Since MySQL can "delete-mark" and delete rows on compressed pages "in place"
# by modifying uncompressed data, DELETE operations on a table are relatively efficient.
#
# For some environments, the time it takes to load data can be as important as run-time retrieval. Especially in data warehouse environments,
# many tables may be read-only or read-mostly. In those cases, it might or might not be acceptable to pay the price of compression in terms
# of increased load time, unless the resulting savings in fewer disk reads or in storage cost is significant.
#
# Fundamentally, compression works best when the CPU time is available for compressing and uncompressing data. Thus, if your workload
# is I/O bound, rather than CPU-bound, you might find that compression can improve overall performance. When you test your application performance
# with different compression configurations, test on a platform similar to the planned configuration of the production system.
#
# CONFIGURATION CHARACTERISTICS AND COMPRESSION
#
# Reading and writing database pages from and to disk is the slowest aspect of system performance. Compression attempts to reduce I/O by using
# CPU time to compress and uncompress data, and is most effective when I/O is a relatively scarce resource compared to processor cycles.
#
# This is often especially the case when running in a multi-user environment with fast, multi-core CPUs. When a page of a compressed table
# is in memory, MySQL often uses additional memory, typically 16kb, in the buffer pool for an uncompressed copy of the page.
#
# The adaptive LRU algorithm attempts to balance the use of memory between compressed and uncompressed pages to take into account whether
# the workload is running in an I/O-bound or CPU-bound manner.
#
# Still, a configuration with more memory dedicated to the buffer pool tends to run better when using compressed tables than a configuration
# where memory is highly constrained.
#
# CHOOSING THE COMPRESSED PAGE SIZE
#
# The optimal setting of the compressed page size depends on the type and distribution of data that the table and its indexes contain.
# The compressed page size should always be bigger than the maximum record size, or operations may fail as noted in COMPRESSION OF B-TREE PAGES.
#
# Setting the compressed page size too large wastes some space, but the pages do not have to be compressed as often. If the compressed page size
# is set too small, inserts or updates may require time-consuming recompression, and the B-tree nodes may have to be split more frequently,
# leading to bigger data files and less efficient indexing.
#
# Typically, you set the compressed page size to 8K or 4K bytes. Given that the maximum row size of an InnoDB table is around 8K, KEY_BLOCK_SIZE=8
# is usually a safe choice.
#
# 15.9.1.4 MONITORING INNODB TABLE-COMPRESSION AT RUNTIME
#
# Overall application performance, CPU and I/O utilization and the size of disk files are good indicators of how effective compression is for your
# application.
#
# This section builds on the performance tuning advice from SECTION 15.9.1.3, "TUNING COMPRESSION FOR INNODB TABLES", and shows how to find problems
# that might not turn up during initial testing.
#
# To dig deeper into performance considerations for compressed tables, you can monitor compression performance at runtime using the information Schema
# tables described in EXAMPLE 15.1, "USING THE COMPRESSION INFORMATION SCHEMA TABLES"
#
# These tables reflect the internal use of memory and the rates of compression used overall.
#
# The INNODB_CMP table reports information about compression activity for each compressed page size (KEY_BLOCK_SIZE) in use.
#
# The information in these tables is system-wide: it summarizes the compression statistics across all compressed tables in your database.
#
# You can use this data to help decide whether or not to compress a table by examining these tables when no other compressed tables are being
# accessed.
#
# It involves relatively low overhead on the server, so you might query it periodically on a production server to check the overall efficiency
# of the compression feature.
#
# The INNODB_CMP_PER_INDEX table reports information about compression activity for individual tables and indexes. This information is more targeted
# and more useful for evaluating compression efficiency and diagnosing performance issues one table or index at a time.
#
# (Because that each InnoDB table is represented as a clustered index, MySQL does not make a big distinction between tables and indexes in this context).
#
# The INNODB_CMP_PER_INDEX table does involve substantial overhead, so it is more suitable for development servers, where you can compare the effects of
# different workloads, data, and compression settings in isolation.
#
# To guard against imposing this monitoring overhead by accident, you must enable the innodb_cmp_per_index_enabled configuration option before you can
# query the INNODB_CMP_PER_INDEX table.
#
# The key statistics to consider are the number of, and amount of time spent performing, compression and uncompression operations. Since MySQL splits B-Tree
# nodes when they are too full to contain the compressed data following a modification, compare the number of "successful" compression operations with the
# number of such operations overall.
#
# Based on the information in the INNODB_CMP and INNODB_CMP_PER_INDEX tables and overall application performance and hardware resource utilization, you might
# make changes in your hardware configuration, adjust the size of the buffer pool, choose a different page size, or select a different set of tables to compress.
#
# If the amount of CPU time required for compressing and uncompressing is high, changing to faster or multi-core CPUs can help improve performance with the
# same data, application workload and set of compressed tables.
#
# Increasing the size of the buffer pool might also help performance, so that more uncompressed pages can stay in memory, reducing the need to uncompress pages
# that exist in memory only in compressed form.
#
# A large number of compression operations overall (compared to the number of INSERT, UPDATE and DELETE operations in your application and the size of the database)
# could indicate that some of your compressed tables are being updated too heavily for effective compression. If so, choose a larger page size, or be more selective
# about which tables you compress.
#
# If the number of "successful" compression operations (COMPRESS_OPS_OK) is a high percentage of the total number of compression operations (COMPRESS_OPS), then the
# system is likely performing well.
#
# If the ratio is low, then MySQL is reorganizing, recompressing, and splitting B-tree nodes mroe often than is desirable. In this case, avoid
# compressing some tables, or increase KEY_BLOCK_SIZE for some of the compressed tables.
#
# You might turn off compression for tables that cause the number of "compressiong failures" in your application to be more than 1% or 2% of the total.
#
# (Such a failure ratio might be acceptable during a temporary operation such as a data load)
#
# 15.9.1.5 HOW COMPRESSION WORKS FOR INNODB TABLES
#
# This section describes some internal implementation details about compression for InnoDB tables. The information presented here may be
# helpful in tuning for performance, but is not necessary to know for basic use of compression.
#
# COMPRESSION ALGORITHMS
#
# Some OS implement compression at the file system level. Files are typically divided into fixed-size blocks that are compressed into variable-size
# blocks, which easily leads into fragmentation.
#
# Every time something inside a block is modified, the whole block is recompressed before it is written to disk. These properties make this compression
# technique unsuitable for use in an update-intensive database system.
#
# MySQL implements compression with the help of the well-known zlib library, which implements the LZ77 compression algorithm.
#
# This compression algorithm is mature, robust, and efficient in both CPU utilization and in reduction of data size. The algorithm is "lossless",
# so that hte original uncompressed data can always be reconstructed from the compressed form.
#
# LZ77 compression works by finding sequences of data that are repeated within the data to be compressed.
#
# THe patterns of values in your data determine how well it compresses, but typical user data often compresses by 50% or more.
#
# NOTE:
#
# 		InnoDB supports the zlib library up to version 1.2.11, which is the version bundled with MySQL 8.0
#
# Unlike compression performed by an application, or compression features of some other database management systems, InnoDB compression
# applies both ot user data and to indexes.
#
# In many cases, indexes can constitute 40-50% or more of the total database size, so this difference is significant.
#
# When compression is working well for a data set, the size of the InnoDB data files (the file-per-table tablespace or general tablespace .ibd files)
# is 25% to 50% of the uncompressed size or possibly smaller.
#
# Depending on the workload, this smaller database can in turn lead to a reduction in I/O, and an increase in throughput, at a modest cost in terms
# of increased CPU utilization.
#
# You can adjust the balance between compression level and CPU overhead by modifying the innodb_compression_level configuration option.
#
# InnoDB DATA STORAGE AND COMPRESSION
#
# All user data in InnoDB tables is stored in pages comprising a B-TREE index (the clustered index).
#
# In some other database systems, this type of index is called an "index-organized table". Each row in the index node contains
# the values of the (user-specified or system-generated) primary key and all the other columns of the table.
#
# Secondary indexes in InnoDB tables are also B-TREES, containing pairs of values: the index key and a pointer to a row in the clustered index.
#
# The pointer is in fact the value of the primary key of the table, which is used to access the clustered index if columns other than the index
# key and primary key are required.
#
# Secondary index records must always fit on a single B-TREE page.
#
# The compression of B-tree nodes (of both clustered and secondary indexes) is handled differently from compression of overflow pages used to
# store long VARCHAR, BLOB or TEXT columns, as explained in the following sections.
#
# COMPRESSION OF B-TREE PAGES
#
# Because they are frequently updated, B-TREE pages require special treatment. It is important to minimize the number of times B-tree nodes
# are split, as well as to minimize the need to uncompress and recompress their content.
#
# One technique MySQL uses is to maintain some system information in the B-tree node in uncompressed form, thus facilitating certain in-place
# updates.
#
# For example, this allows rows to be delete-marked and deleted without any compression operation.
#
# In addition, MySQL attempts to avoid unnecessary uncompression and recompression of index pages when they are changed. Within each B-tree page,
# the system keeps an uncompressed "modification log" to record changes made to the page.
#
# Updates and inserts of small records may be written to this modification log without requiring the entire page to be completely reconstructed.
#
# When the space for the modification log runs out, InnoDB uncompresses the page, applies the changes and recompresses the page. If recompression
# fails (a situation known as a compression failure), the B-tree nodes are split and the process is repeated until the update or insert succeeds.
#
# To avoid frequent compression failures in write-intensive workloads, such as for OLTP applications, MySQL sometimes reserves some empty space
# (padding) in the page, so that the modification log fills up sooner and the page is recompressed while there is still enough room to avoid splitting it.
#
# The amount of padding space left in each page varies as the system keeps track of the frequency of page splits.
#
# ON a busy server doing frequent writes to compressed tables, you can adjust the innodb_compression_failure_threshold_pct, and
# innodb_compression_pad_pct_max configuration options to fine-tune this mechanism.
#
# Generally, MySQL requires that each B-tree page in an InnoDB table can accomodate at least two records. For compressed tables, this requirement
# has been relaxed.
#
# Leaf pages of B-tree nodes (whether of the primary key or secondary indexes) only need to accomodate one record, but that record must fit,
# in uncompressed form, in the per-page modificaiton log.
#
# If innodb_strict_mode is ON, MySQL checks the maximum row size during CREATE_TABLE or CREATE_INDEX.
#
# If the row does not fit, the following error message is issued: ERROR HY00: Too big row.
#
# If you create a table when innodb_strict_mode is OFF, and a subsequent INSERT or UPDATE statement attempts to create an index entry that does
# not fit in the size of the compressed page, the operaiton fails with ERROR 42000: Row size too large.
#
# (This error message does not name the index for which the record is too large, or mention the length of the index record or the maximum record size
# on that particular index page)
#
# To solve this problem, rebuild the table with ALTER_TABLE and select a larger compressed page size (KEY_BLOCK_SIZE), shorten any column prefix indexes,
# or disable compression entirely with ROW_FORMAT=DYNAMIC or ROW_FORMAT=COMPACT.
#
# innodb_strict_mode is not applicable to general tablespaces, which also support compressed tables. Tablespace management rules for general tablespaces
# are strictly enforced independently of innodb_strict_mode.
#
# For more information, see SECTION 13.1.21, "CREATE TABLESPACE SYNTAX"
#
# COMPRESSING BLOB, VARCHAR AND TEXT COLUMNS
#
# In an InnoDB table, BLOB, VARCHAR and TEXT columns that are not part of the primary key may be stored on separately allocated overflow pages.
#
# We refer to these columns as off-page columns. Their values are stored on singly-linked lists of overflow pages.
#
# For tables created in ROW_FORMAT=DYNAMIC or ROW_FORMAT=COMPRESSED, the values of BLOB, TEXT, or VARCHAR columns may be stored fully off-page, depending
# on their length and the length of the entire row.
#
# For columns that are stored off-page, the clustered index record only contains 20-byte pointers to the overflow pages, one per column.
# 
# Whether any columns are stored off-page depends on the page size and the total size of the row. When the row is too long to fit entirely within the page
# of the clustered index, MySQL chooses the longest columns for off-page storage until the row fits on the clustered index page.
#
# As noted above, if a row does not fit by itself on a compressed page, an error occurs.
#
# NOTE:
#
# 		For tables created in ROW_FORMAT=DYNAMIC or ROW_FORMAT=COMPRESSED, TEXT and BLOB columns that are less than or equal to 40 bytes
# 		are always stored in-line.
#
# Tables that use ROW_FORMAT=REDUNDANT and ROW_FORMAT=COMPACT store the first 768 bytes of BLOB, VARCHAR and TEXT columns in the clustered index
# record along with the primary key.
#
# The 768-byte prefix is followed by a 20-byte pointer to the overflow pages that contain the rest of the column value.
#
# When a table is in COMPRESSED format, all data written to overflow pages is compressed "as is"; that is, MySQL applies the zlib compression algorithm
# to the entire data item.
#
# Other than the data, compressed overflow pages contain an uncompressed header and trailer comprising a page checksum and a link to the next overflow
# page, among other things.
#
# Therefore, very significant storage savings can be obtained for longer BLOB, TEXT or VARCHAR columns if the data is highly compressible, as is often the 
# case with text data.
#
# Image data, such as JPEG, is typically already compressed and does not benefit much from being stored in a compressed table; the double compression
# can waste CPU cycles forl ittle or no space savings.
#
# The overflow pages are of the same size as other pages. A row containing ten columns stored off-page occupies ten overflow pages, even if the total length
# of the columns is only 8K bytes.
#
# In an uncompressed table, ten uncompressed overflow pages occupy 160K bytes. In a compressed table with an 8K page size, they occupy only 80K bytes.
# Thus, it is often more efficient to use compressed table format for tables with long column values.
#
# For file-per-table tablespaces, using a 16K compressed page size can reduce storage and I/O costs for BLOB, VARCHAR or TEXT columns, because such data
# often compress well, and might therefore require fewer overflow pages, even though the B-tree nodes themselves takes as many pages as in the uncompressed
# form.
#
# General tablespaces do not support a 16K compressed page size (KEY_BLOCK_SIZE). For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# COMPRESSION AND THE INNODB BUFFER POOL
#
# In a compressed InnoDB table, every compressed page (whether 1K,2K, 4K or 8K) corresponds to an uncompressed page of 16K bytes (or a smaller size if
# innodb_page_size is set).
#
# To access the data in a page, MySQL reads the compressed page from disk if it is not already in the buffer pool, then uncompresses the page to
# its original form.
#
# This section describes how InnoDB manages the buffer pool with respect to pages of compressed tables.
#
# To minimize I/O and to reduce the need ot uncompress a page, at times the buffer pool contains both the compressed and uncompressed form of a database
# page.
#
# to make room for other required database pages, MySQL can evict from the buffer pool an uncompressed page, while leaving the compressed page in memory.
# Or, if a page has not been accessed in a while, the compressed form of the page might be written to disk, to free space for other data.
#
# Thus, at any given time, the buffer pool might contain both the compressed and uncompressed forms of the page, or only the compressed form of the page,
# or neither.
#
# MySQL keeps track of which pages to keep in memory and which to evict using a least-recently used (LRU) list, so that hot (frequently used) data tends to
# stay in memory.
#
# When compressed tables are accessed, MySQL uses an adaptive LRU algorithm to achieve an appropriate balance of compressed and uncompressed pages in
# memory.
#
# This adaptive algorithm is sensitive to whether the system is running in an I/O-bound or CPU-bound manner. The goal is to avoid spending too much processing
# time uncompressing pages when the CPU is busy, and to avoid doing excess I/O when the CPU has spare cycles that can be used for uncompressing compressed
# pages (that may already be in memory).
#
# When the system is I/O-bound, the algorithm prefers to evict the uncompressed copy of a page rather than both copies, to make more room for other disk pages
# to become memory resident.
#
# When the system is CPU-Bound, MySQL prefers to evict both the compressed and uncompressed page, so that more memory cna be used for "Hot" pages and reducing
# the need to uncompress data in memory only in compressed form.
#
# COMPRESSION AND THE INNODB REDO LOG FILES
#
# Before a compressed page is written to a data file, MySQL writes a copy of the page to the redo log (if it has been recompressed since the last time it was written
# to the database)
#
# This is done to ensure that redo logs are usable for crash recovery, even in the unlikely case that the zlib library is upgraded and that change introduces a 
# compatibility problem with the compressed data. Therefore, some increase in teh size of log files, or a need for more frequent checkpoints, can be expected
# when using compression.
#
# The amount of increase in the log file size or checkpoint frequency depends on the number of times compressed pages are modified in a way that requires
# reorganization and recompression.
#
# To create a compressed table in a file-per-table tablespace, innodb_file_per_table must be enabled. There is no dependence on the innodb_file_per_table setting
# when creating a compressed table in a general tablespace. For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# 15.9.1.6 COMPRESSION FOR OLTP WORKLOADS
#
# Traditionally, the InnoDB compression feature was recommended primarily for read-only or read-mostly workloads, such as in a data warehouse configuration.
#
# The rise of SSD storage devices, which are fast but small and expensive, make compression attractive also for OLTP workloads: High-traffic, interactive
# websites can reduce their storage requirements and their I/O operations per second (IOPS) by using compressed tables with applications that do frequent
# INSERT, UPDATE and DELETE operations.
#
# These configuration options let you adjust the way compression works for a particular MySQL instance, with an emphasis on performance and scalability
# for write-intensive operations:
#
# 		) innodb_compression_level lets you turn the degree of compression up or down. A higher value lets you fit more data onto a storage device, at the expense
# 			of more CPU overhead during compression.
#
# 			A lower value lets you reduce CPU overhead when storage space is not critical, or you expect the data is not especially compressible.
#
# 		) innodb_compression_failure_threshold_pct specifies a cutoff point for compression failures during updates to a compressed table.
#
# 			When this threshold is passed, MySQL begins to leave additional free space within each new compressed page, dynamically adjusting the amount
# 			of free space up to the percentage of page size specified by innodb_compression_pad_pct_max
#
# 		) innodb_compression_pad_pct_max lets you adjust the maximum amount of space reserved within each page to record changes to compressed rows, without
# 			needing to compress the entire page again.
#
# 			The higher the value, the more changes can be recorded without recompressing the page. MySQL uses a variable amount of free space for the pages within
# 			each compressed table, only when a designated percentage of compression operations "fail" at runtime, requiring an expensive operation to split the
# 			compressed page.
#
# 		) innodb_log_compressed_pages lets you disable writing of images of re-compressed pages to the redo log. Re-compression may occur when changes are made to
# 			compressed data.
#
# 			This option is enabled by default to prevent corruption that could occur if a different version of the zlib compression algorithhm is used during
# 			recovery.
#
# 			If you are certain that the zlib version will not change, disable innodb_log_compressed_pages to reduce redo log generation for workloads that modify
# 			compressed data.
#
# Because working with compressed data sometimes involves keeping both compressed and uncompressed versions of a page in memory at the same time, when using compression
# with an OLTP-style workload, be prepared to increase the value of the innodb_buffer_pool_size configuration option.
#
# 15.9.1.7 SQL COMPRESSION SYNTAX WARNINGS AND ERRORS
#
# This section describes syntax warnings and errors that you may encounter when using the table compression feature with file-per-table tablespaces
# and general tablespaces.
#
# SQL COMPRESSION SYNTAX WARNINGS AND ERRORS FOR FILE-PER-TABLE TABLESPACES
#
# When innodb_strict_mode is enabled (the default), specifying ROW_FORMAT=COMPRESSED or KEY_BLOCK_SIZE in CREATE_TABLE or ALTER_TABLE statements
# produces the following error if innodb_file_per_table is disabled.
#
# 		ERROR 1031 (HY000): Table storage engine for 't1' doesn't have this option
#
# NOTE:
#
# 		The table is not created if the current configuration does not permit using compressed tables.
#
# When innodb_strict_mode is disabled, specifying ROW_FORMAT=COMPRESSED or KEY_BLOCK_SIZE in CREATE_TABLE or ALTER_TABLE statements
# produces the following warnings if innodb_file_per_table is disabled.
#
# 		mysql> SHOW WARNINGS;
# 		+----------+----------+---------------------------------------------------------------+
# 		| Level 	  | Code 	 | Message 												     				     |
# 		+----------+----------+---------------------------------------------------------------+
# 		| Warning  | 1478 	 | InnoDB: KEY_BLOCK_SIZE requires innodb_file_per_table.    	  |
# 		| Warning  | 1478 	 | InnoDB: ignoring KEY_BLOCK_SIZE=4 								     |
# 		| Warning  | 1478 	 | InnoDB: ROW_FORMAT=COMPRESSED requires innodb_file_per_table  |
# 		| Warning  | 1478 	 | InnoDB: assuming ROW_FORMAT=DYNAMIC. 								  |
# 		+----------+----------+---------------------------------------------------------------+
#
# NOTE:
#
# 		These messages are only warnings, not errors, and the table is created without compression, as if the options were not specified.
#
# The "non-strict" behavior lets you import a mysqldump file into a database that does not support compressed tables, even if the source
# database contained compressed tables.
#
# In that case, MySQL creates the table in ROW_FORMAT=DYNAMIC instead of preventing the operation.
#
# To import the dump file into a new database, and have the tables re-created as they exist in the original database, ensure the server
# has the proper setting for the innodb_file_per_table configuration parameter.
#
# The attribute KEY_BLOCK_SIZE is permitted only when ROW_FORMAT is specified as COMPRESSED or is omitted. Specifying a KEY_BLOCK_SIZE with
# any other ROW_FORMAT generates a warning that you can view with SHOW WARNINGS.
#
# However, the table is non-compressed; the specified KEY_BLOCK_SIZE is ignored).
#
# +--------------+---------------+----------------------------------------------------------------+
# | Level 		  | Code 			| Message 														  				  |
# +--------------+---------------+----------------------------------------------------------------+
# | Warning 	  | 1478 		 	| InnoDB: ignoring KEY_BLOCK_SIZE=n unless ROW_FORMAT=COMPRESSED |
# +--------------+---------------+----------------------------------------------------------------+
#
# If you are running with innodb_strict_mode enabled, the combination of a KEY_BLOCK_SIZE with any ROW_FORMAT
# other than COMPRESSED generates an error, not a warning, and the table is not created.
#
# TABLE 15.13, "ROW_FORMAT AND KEY_BLOCK_SIZE OPTIONS" provides an overview of the ROW_FORMAT and KEY_BLOCK_SIZE options that
# are used with CREATE_TABLE or ALTER_TABLE.
#
# TABLE 15.13 ROW_FORMAT AND KEY_BLOCK_SIZE OPTIONS
#
# 		+----------------------+------------------------------------------------+--------------------------------------------------------------------+
# 		| OPTION 			     | USAGE NOTES 												| DESCRIPTION 																			|
# 		+----------------------+------------------------------------------------+--------------------------------------------------------------------+
# 		| ROW_FORMAT=REDUNDANT | Storage format used prior to MySQL 5.0.3 		| Less efficient than ROW_FORMAT=COMPACT, for backward compatibility.|
# 		+----------------------+------------------------------------------------+--------------------------------------------------------------------+
# 		| ROW_FORMAT=COMPACT   | Default storage format since MySQL 5.0.3 		| Stores a prefix of 768 bytes of long column values in the clustered|
# 		| 							  | 																| index page, with the remaining bytes stored in an overflow page    |
# 		+----------------------+------------------------------------------------+--------------------------------------------------------------------+
# 		| ROW_FORMAT=DYNAMIC   | 																| Store values within the clustered index page if they fit; if not,  |
# 		| 							  | 																| stores only a 20-byte pointer to an overflow page (no prefix) 	   |
# 		+----------------------+------------------------------------------------+--------------------------------------------------------------------+
# 		| ROW_FORMAT=COMPRESSED| 																| Compresses the table and indexes using zlib 								|
# 		+----------------------+------------------------------------------------+--------------------------------------------------------------------+
# 		| KEY_BLOCK_SIZE=n 	  | 																| Specifies compressed page size of 1,2,4,8 or 16 kilobytes; implies |
# 		| 							  | 																| ROW_FORMAT=COMPRESSED. For general tablespaces, a KEY_BLOCK_SIZE   |
# 		| 							  | 															   | value equal to the InnoDB page size is not permitted. 					|
# 		+----------------------+------------------------------------------------+--------------------------------------------------------------------+
#
# TABLE 15.14, "CREATE/ALTER TABLE WARNINGS AND ERRORS WHEN INNODB STRICT MODE IS OFF" summarizes error conditions that occur with certain combinations
# of configuration parameters and options on the CREATE_TABLE or ALTER_TABLE statements, and how the options appear in the output of SHOW TABLE STATUS.
#
# When innodb_strict_mode is OFF, MySQL creates or alters the table, but ignores certain settings as shown below. You can see the warning messages in
# the MySQL error log.
#
# When innodb_strict_mode is ON, these specified combinations of options generate errors, and the table is not created or altered. To see the full
# description of the error condition, issue the SHOW ERRORS statement: example:
#
# 		mysql> CREATE TABLE x (id INT PRIMARY KEY, c INT)
#
# 		-> ENGINE=INNODB_KEY_BLOCK_SIZE=33333;
#
# 		ERROR 1005 (HY000): Can't create table 'test.x' (errno: 1478)
#
# 		mysql> SHOW ERRORS;
# 		+--------------+-------------+----------------------------------------------------+
# 		| Level 			| Code 		  | Message 														 |
# 		+--------------+-------------+----------------------------------------------------+
# 		| Error 			| 1478 		  | InnoDB: invalid KEY_BLOCK_SIZE=33333. 				 |
# 		| Error 			| 1005 		  | Can't create table 'test.x' (errno: 1478) 			 |
# 		+--------------+-------------+----------------------------------------------------+
#
# TABLE 15.14 CREATE/ALTER TABLE WARNINGS AND ERRORS WHEN INNODB STRICT MODE IS OFF
#
# +-----------------------------------------------------+---------------------------------------------------+-----------------------------------------------------+
# | SYNTAX 															  | WARNING OR ERROR CONDITION  								| RESULTING ROW_FORMAT, AS SHOWN IN SHOW TABLE STATUS |
# +-----------------------------------------------------+---------------------------------------------------+-----------------------------------------------------+
# | ROW_FORMAT=REDUNDANT 										  | None 															| REDUNDANT 														|
# +-----------------------------------------------------+---------------------------------------------------+-----------------------------------------------------+
# | ROW_FORMAT=COMPACT 											  | None 															| COMPACT 															|
# +-----------------------------------------------------+---------------------------------------------------+-----------------------------------------------------+
# | ROW_FORMAT=COMPRESSED or ROW_FORMAT=DYNAMIC or 	  | Ignored for file-per-table tablespaces unless 		| the default row format for file-per-table 				|
# | KEY_BLOCK_SIZE is specified 								  | innodb_file_per_table is enabled. General 			| tablespaces; the specified row format for general   |
# |  																	  | tablespaces support all row formats. See Section  | tablespaces 														|
# | 																	  | 15.6.3.3, "GENERAL TABLESPACES" 						| 																		|
# +-----------------------------------------------------+---------------------------------------------------+-----------------------------------------------------+
# | Invalid KEY_BLOCK_SIZE is specified (not 1,2,4,     | KEY_BLOCK_SIZE is ignored 								| the specified row format, or the default row format |
# | 8 or 16 														  |  																	| 																		|
# +-----------------------------------------------------+---------------------------------------------------+-----------------------------------------------------+
# | ROW_FORMAT=COMPRESSED and valid KEY_BLOCK_SIZE are  | None; KEY_BLOCK_SIZE specified is used 				| COMPRESSED 														|
# | specified 														  |  																	| 																		|
# +-----------------------------------------------------+---------------------------------------------------+-----------------------------------------------------+
# | KEY_BLOCK_SIZE is specified with REDUNDANT, COMPACT | KEY_BLOCK_SIZE is ignored 								| REDUNDANT, COMPACT or DYNAMIC 								|
# | or DYNAMIC row format 										  | 																	| 																		|
# +-----------------------------------------------------+---------------------------------------------------+-----------------------------------------------------+
# | ROW_FORMAT is not one of REDUNDANT, COMPACT, DYNAMIC| Ignored if recognized by the MySQL parser. Other- | the default row format or N/A 								|
# | or COMPRESSED 												  | wise, an error is issued. 								| 																		|
# +-----------------------------------------------------+---------------------------------------------------+-----------------------------------------------------+
#
# When innodb_strict_mode is ON, MySQL rejects invalid ROW_FORMAT or KEY_BLOCK_SIZE parameters and issues errors.
#
# Strict mode is ON by default. When innodb_strict_mode is OFF, MySQL issues warnings instead of errors for ignored invalid parameters.
#
# It is not possible to see the chosen KEY_BLOCK_SIZE using SHOW TABLE STATUS. The statement SHOW CREATE TABLE displays the KEY_BLOCK_SIZE (even if
# it was ignored when creating the table). The real compressed page size of the table cannot be displayed by MySQL.
#
# SQL COMPRESSION SYNTAX WARNINGS AND ERRORS FOR GENERAL TABLESPACES
#
# 		) If FILE_BLOCK_SIZE was not defined for the general tablespace when the tablespace was created, the tablespace cannot contain compressed tables.
#
# 			If you attempt to add a compressed table, an error is returned, as shown in the following example:
#
# 				mysql> CREATE TABLESPACE `ts1` ADD DATAFILE 'ts1.ibd' Engine=InnoDB;
#
# 				mysql> CREATE TABLE t1 (c1 INT PRIMARY KEY) TABLESPACE ts1 ROW_FORMAT=COMPRESSED
# 						 KEY_BLOCK_SIZE=8;
# 				ERROR 1478 (HY000): InnoDB: Tablespace `ts1` cannot contain a COMPRESSED table
#
# 		) Attempting to add a table with an invalid KEY_BLOCK_SIZE to a general tablespace returns an error, as shown in the following example:
#
# 				mysql> CREATE TABLESPACE `ts2` ADD DATAFILE 'ts2.ibd' FILE_BLOCK_SIZE = 8192 Engine=InnoDB;
#
# 				mysql> CREATE TABLE t2 (c1 INT PRIMARY KEY) TABLESPACE ts2 ROW_FORMAT=COMPRESSED
# 						 KEY_BLOCK_SIZE=4;
# 				ERROR 1478 (HY000): InnoDB: Tablespace `ts2` uses block size 8192 and cannot contain
# 				a table with physical page size 4096
#
# 			For general tablespaces, the KEY_BLOCK_SIZE of the table must be equal to the FILE_BLOCK_SIZE of the tablespace divided by 1024.
#
# 			For example, if the FILE_BLOCK_SIZE of the tablespace is 8192, the KEY_BLOCK_SIZE of the table must be 8.
#
# 		) Attempting to add a table with an uncompressed row format to a general tablespace configured to store compressed table returns an error,
# 			as shown in the following example:
#
# 				mysql> CREATE TABLESPACE `ts3` ADD DATAFILE 'ts3.ibd' FILE_BLOCK_SIZE = 8192 Engine=InnoDB;
#
# 				mysql> CREATE TABLE t3 (c1 INT PRIMARY KEY) TABLESPACE ts3 ROW_FORMAT=COMPACT;
# 				ERROR 1478 (HY000): InnoDB: Tablespace `ts3` uses block size 8192 and cannot contain
# 				a table with physical page size 16384
#
# innodb_strict_mode is not applicable to general tablespaces. Tablespace management rules for general tablespaces are strictly enforced
# independently of innodb_strict_mode. For more information, see SECTION 13.1.21, "CREATE TABLESPACE SYNTAX"
#
# For more information about using compressed tables with general tablespaces, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# 15.9.2 InnoDB PAGE COMPRESSION
#
# InnoDB supports page-level compression for tables that reside in file-per-table tablespaces. This feature is referred to as TRANSPARENT PAGE COMPRESSION.
#
# Page compression is enabled by specifying the COMPRESSION attribute with CREATE_TABLE or ALTER_TABLE. Supported compression algorithms include Zlib and LZ4.
#
# SUPPORTED PLATFORMS
#
# Page compression requires sparse file and hole punching support. Page compression is supported on Windows with NTFS, and on the following
# subset of MySQL supported Linux platforms where the kernel level provides hole punching support:
#
# 		) RHEL 7 and derived distributions that use kernel version 3.10.0-123 or higher
#
# 		) OEL 5.10 (UEK2) kernel version 2.6.39 or higher
#
# 		) OEL 6.5 (UEK3) kernel version 3.8.13 or higher
#
# 		) OEL 7.0 kernel version 3.8.13 or higher
#
# 		) SLE11 kernel version 3.0-x
#
# 		) SLE12 kernel version 3.12-x
#
# 		) OES11 kernel version 3.0-x
#
# 		) Ubuntu 14.0.4 LTS kernel version 3.13 or higher
#
# 		) Ubuntu 12.0.4 LTS kernel version 3.2 or higher
#
# 		) Debian 7 kernel version 3.2 or higher
#
# NOTE:
#
# 		All of the available file systems for a given Linux distribution may not support hole punching.
#
# HOW PAGE COMPRESSION WORKS
#
# When a page is written, it is compressed using the specified compression algorithm. The compressed data is written
# to disk, where the hole punching mechanism releases empty blocks from the end of the page.
#
# If compression fails, data is written out as-is.
#
# HOLE PUNCH SIZE ON LINUX
#
# On Linux systems, the file system block size is the unit size for hole punching. Therefore, page compression only works if page data
# can be compressed to a size that is less than or equal to the InnoDB page size minus the file system block size.
#
# For example, if innodb_page_size=16K and the file system block size is 4K, page data must compress to less than or equal
# to 12K to make hole punching possible.
#
# HOLE PUNCH SIZE ON WINDOWS
#
# On Windows Systems, the underlying infrastructure for sparse files is based on NTFS compression. Hole punching size is the NTFS
# compression unit, which is 16 times the NTFS cluster size.
#
# Cluster sizes and their compression units are shown in the following table:
#
# 		TABLE 15.15 Windows NTFS Cluster Size and Compression Units
#
# 			CLUSTER SIZE 		COMPRESSION UNIT
#
# 			512 Bytes 			8 kb
#
# 			1 kb 					16 kb
#
# 			2 kb 					32 kb
#
# 			4 kb 					64 kb
#
# Page compression on Windows systems only works if page data can be compressed to a size that is less than or equal to the
# InnoDB page size minus the compression unit size.
#
# The default NTFS cluster size is 4kb, for which the compression unit size is 64KB. This means that page compression has no benefit
# for an out-of-box Windows NFTS configuration, as the maximum innodb_page_size is also 64KB.
#
# For page compression to work on Windows, the file system must be created with a cluster size smaller than 4K, and the innodb_page_size
# must be at least twice the size of the compression unit. For example, for page compression to work on Windows, you could build the file
# system with a cluster size of 512 bytes (which has a compression unit of 8kb) and initialize InnoDB with an innodb_page_size value of
# 16K or greater.
#
# ENABLING PAGE COMPRESSION
#
# To enable page compression, specify the COMPRESSION attribute in the CREATE_TABLE statement. For example:
#
# 		CREATE TABLE t1 (c1 INT) COMPRESSION="zlib";
#
# You can also enable page compression in an ALTER_TABLE statement. However, ALTER_TABLE_..._COMPRESSION only updates the tablespace
# compression attribute.
#
# Writes to the tablespace that occur after setting the new compression algorithm use the new setting, but to apply the new compression
# algorithm to existing pages, you must rebuild the table using OPTIMIZE_TABLE.
#
# 		ALTER TABLE t1 COMPRESSION="zlib";
# 		OPTIMIZE TABLE t1;
#
# DISABLING PAGE COMPRESSION
#
# To disable page compression, set COMPRESSION=None using ALTER_TABLE. Writes to the tablespace that occur after setting COMPRESSION=None
# no longer use page compression.
#
# To uncompress existing pages, you must rebuild the table using OPTIMIZE_TABLE after setting COMPRESSION=None.
#
# 		ALTER TABLE t1 COMPRESSION="None";
# 		OPTIMIZE TABLE t1;
#
# PAGE COMPRESSION METADATA
#
# Page compression metadata is found in the INFORMATION_SCHEMA.INNODB_TABLESPACES table, in the following columns:
#
# 		) FS_BLOCK_SIZE: The file system block size, which is the unit size used for hole punching
#
# 		) FILE_SIZE: The apparent size of the file, which represents the maximum size of the file, uncompressed.
#
# 		) ALLOCATED_SIZE: The actual size of the file, which is the amount of space allocated on disk.
#
# 	NOTE:
#
# 		On Unix-like systems, ls -l tablespace_name.ibd shows the apparent file size (equivalent to FILE_SIZE) in bytes.
#
# 		To view the actual amount of space allocated on disk (equivalent to ALLOCATED_SIZE), use du --block-size=1 tablespace_name.ibd
# 	
# 		The --block-size=1 option prints the allocated space in bytes instead of blocks, so that it can be compared to ls -l output.
#
# 		Use SHOW_CREATE_TABLE to view the current page compression setting (Zlib, Lz4, or None). A table may contain a mix of pages
# 		with different compression settings.
#
# In the following example, page compression metadata for the employees table is retrieved from the INFORMATION_SCHEMA.INNODB_TABLESPACES table.
#
# 		# Create the employees table with Zlib page compression
#
# 		CREATE TABLE employees (
# 			emp_no 		INT 				NOT NULL,
# 			birth_date  DATE 				NOT NULL,
# 			first_name 	VARCHAR(14) 	NOT NULL,
# 			last_name 	VARCHAR(16) 	NOT NULL,
# 			gender 		ENUM ('M','F') NOT NULL,
# 			hire_date 	DATE 				NOT NULL,
# 			PRIMARY KEY (emp_no)
# 		) COMPRESSION="zlib";
#
# 		# Insert data (not shown)
#
# 		# Query page compression metadata in INFORMATION_SCHEMA.INNODB_TABLESPACES
#
# 		mysql> SELECT SPACE, NAME, FS_BLOCK_SIZE, FILE_SIZE, ALLOCATED_SIZE FROM
# 				 INFORMATION_SCHEMA.INNODB_TABLESPACES WHERE NAME='employees/employees'\G
# 		*********************** 1. row ****************************************
# 		SPACE: 	45
# 		NAME: employees/employees
# 		FS_BLOCK_SIZE: 4096
# 		FILE_SIZE: 23068672
# 		ALLOCATED_SIZE: 19415040
#
# Page compression metadata for the employees table shows that the apparent file size is 23068672 bytes while the actual
# file size (with page compression) is 19415040 bytes.
#
# The file system block size is 4096 bytes, which is the block size used for hole punching.
#
# PAGE COMPRESSION LIMITATIONS AND USAGE NOTES
#
# 		) Page compression is disabled if the file system block size (or compression unit size on Windows) * 2 > innodb_page_size
#
# 		) Page compression is not supported for tables that reside in shared tablespaces, which include the system tablespace, temporary tablespaces, and general tablespaces.
#
# 		) Page compression is not supported for undo log tablespaces.
#
# 		) Page compression is not supported for redo log pages.
#
# 		) R-tree pages, which are used for spatial indexes, are not compressed.
#
# 		) Pages that belong to compressed tables (ROW_FORMAT=COMPRESSED) are left as-is.
#
# 		) During recovery, updated pages are written out in an uncompressed form.
#
# 		) Loading a page-compressed tablespace on a server that does not support the compression algorithm that was used causes an I/O error.
#
# 		) Before downgrading to an earlier version of MySQL that does not support page compression, uncompress the tables that use the page compression
# 			feature.
#
# 			To uncompress a table, run ALTER_TABLE_..._COMPRESSION=None and OPTIMIZE_TABLE
#
# 		) Page-compressed tablespaces can be copied between Linux and Windows servers if the compression algorithm that was used is available on both servers.
#
# 		) Preserving page compression when moving a page-compressed tablespace file from one host to another requires a utility that preserves sparse files.
#
# 		) Better page compression may be achieved on Fusion-io hardware with NVMFS than on other platforms, as NVMFS is designed to take advantage of punch
# 			hole functionality.
#
# 		) Using the page compression feature with a large InnoDB page size and relatively small file system block size could result in write amplification.
#
# 		  For example, a maximum InnoDB page size of 64KB with a 4KB file system block size may improve compression but may also increase demand on the buffer
# 			pool, leading to increased I/O and potential write amplification.
#
# 15.10 InnoDB ROW FORMATS
#
# The row format of a table determines how its rows are physically stored, which in turn can affect the performance of queries and DML operations.
#
# As more rows fit into a single disk page, queries and index lookups can work faster, less cache memory is required in the buffer pool, and less
# I/O is required to write out updated values.
#
# The data in each table is divided into pages. The pages that make up each table are arranged in a tree data structure called a B-tree index.
#
# Table data and secondary indexes both use this type of structure. The B-tree index that represents an entire table is known as the clustered
# index, which is organized according to the primary key columns.
#
# The nodes of a clustered index data structure contain the values of all columns in the row. The nodes of a secondary index structure contain
# the values of index columns and primary key columns.
#
# Variable-length columns are an exception to the rule that column values are stored in B-tree index nodes. Variable-length columns that are too
# long to fit on a B-tree page are stored on separately allocated disk pages called overflow pages.
#
# Such columns are referred to as off-page columns. The values of off-page columns are stored in singly-linked lists of overflow pages, with
# each such column having its own list of one or more overflow pages.
#
# Depending on column length, all or a prefix of variable-length column values are stored in the B-tree to avoid wasting storage and having
# to read a separate page.
#
# The InnoDB storage engine supports four row formats: REDUNDANT, COMPACT, DYNAMIC and COMPRESSED.
#
# 		TABLE 15.16 InnoDB ROW FORMAT OVERVIEW
#
# 			ROW FORMAT 			COMPACT STORAGE CHARACTERISTICS 		ENHANCED VARIABLE-LENGTH COLUMN STORAGE 		LARGE INDEX KEY PREFIX SUPPORT 	COMPRESSION SUPPORT 		SUPPORTED TABLESPACE TYPES
#
# 			REDUNDANT 							NO 											No 													No 									No 					system, file-per-table, general
#
# 			COMPACT 								Yes 											No 													No 									No 					system, file-per-table, general
#
# 			DYNAMIC 								Yes 											Yes 													Yes 									No 					system, file-per-table, general
#
# 			COMPRESSED 							Yes 											Yes 													Yes 									Yes 					file-per-table, general
#
# The topics that follow describe row format storage characteristics and how to define and determine the row format of a table.
#
# 		) REDUNDANT ROW FORMAT
#
# 		) COMPACT ROW FORMAT
# 	
# 		) DYNAMIC ROW FORMAT
#
# 		) COMPRESSED ROW FORMAT
#
# 		) DEFINING THE ROW FORMAT OF A TABLE
#
# 		) DETERMINING THE ROW FORMAT OF A TABLE
#
# REDUNDANT ROW FORMAT
#
# The REDUNDANT format provides compatibility with older versions of MySQL.
#
# Tables that use the REDUNDANT row format stores the first 768 bytes of variable-length column values (VARCHAR, VARBINARY, and BLOB and TEXT types) in the index
# record within the B-tree node, with the remainder stored on overflow pages.
#
# Fixed-length columns greater than or equal to 768 bytes are encoded as variable-length columns, which can be stored off-page. For example, a CHAR(255) column can
# exceed 768 bytes if the maximum byte length of the character set is greater than 3, as it is with utf8mb4.
#
# If the value of a column is 768 bytes or less, an overflow page is not used, and some savings in I/O may result, since the value is stored entirely in the B-tree node.
#
# This works well for relatively short BLOB column values, but may cause B-tree nodes to fill with data rather than key values, reducing their efficiency. Tables with many
# BLOB columns could cause B-tree nodes to become too full, and contain too few rows, making the entire index less efficient than if rows were shorter or column values
# were stored off-page.
#
# REDUNDANT ROW FORMAT STORAGE CHARACTERISTICS
#
# The REDUNDANT row format has the following storage characteristics:
#
# 		) Each index record contains a 6-byte header. The header is used to link together consecutive records, and for row-level locking.
#
# 		) Records in the clustered index contain fields for all user-defined columns. In addition, there is a 6-byte transaction ID field and a 7-byte roll pointer field.
#
# 		) If no primary key is defined for a table, each clustered index record also contains a 6-byte row ID field.
#
# 		) Each secondary index record contains all the primary key columns defined for the clustered index key that are not in the secondary index.
#
# 		) A record contains a pointer to each field of the record. If the total length of the fields in a record is less than 128 bytes, the pointer is one
# 			byte; otherwise, two bytes.
#
# 			The array of pointers is called the record directory. The area where the pointers point is the data part of the record.
#
# 		) Internally, fixed-length character columns such as CHAR(10) in stored in fixed-length format. Trailing spaces are not truncated from VARCHAR columns.
#
# 		) Fixed-length columns greater than or equal to 768 bytes are encoded as variable-length columns, which can be stored off-page. For example, a CHAR(255)
# 			column can exceed 768 bytes if the maximum byte length of the character set is greater than 3, as it is with utf8mb4.
#
# 		) An SQL NULL value reserves one or two bytes in the record directory. An SQL NULL value reserves zero bytes in teh data part of the record if stored in a 
# 			variable-length column.
#
# 			For a fixed-length column, the fixed length of the column is reserved in the data part of the record. Reserving fixed space for NULL values permits columns
# 			to be updated in place from NULL to non-NULL values without causing index page fragmentation.
#
# COMPACT ROW FORMAT
#
# The COMPACT row format reduces row storage space by about 20% compared to the REDUNDANT row format, at the cost of increasing CPU use for some operations.
#
# If your workload is a typical one that is limited by cache hit rates and disk speed, COMPACT format is likely to be faster. If the workload is limited by
# CPU speed, compact format might be slower.
#
# Tables that use the COMPACT row format store the first 768 bytes of variable-length column values (VARCHAR, VARBINARY, and BLOB and TEXT types) in the
# index record within the B-TREE node, with the remainder stored on overflow pages.
#
# Fixed-length columns greater than or equal to 768 bytes are encoded as variable-length columns, which can be stored off-page. For example, a CHAR(255)
# column can exceed 768 bytes if the maximum byte length of the character set is greater than 3, as it is with utf8mb4.
#
# If the value of a column is 768 bytes or less, an overflow page is not used, and some savings in I/O may result, since the value is stored entirely in
# the B-tree node.
#
# This works well for relatively short BLOB column values, but may cause B-tree nodes to fill with data rather than key values, reducing their efficiency.
#
# Tables with many BLOB columns could cause B-tree nodes to become too full, and contain too few rows, making the entire index less efficient than if
# rows were shorter or column values were stored off-page.
#
# COMPACT ROW FORMAT STORAGE CHARACTERISTICS
#
# The COMPACT row format has the following storage characteristics:
#
# 		) Each index record contains a 5-byte header that may be preceded by a variable-length header. The header is used to link together consecutive
# 			records, and for row-level locking.
#
# 		) The variable-length part of the record header contains a bit vector for indicating NULL columns. If the number of columns in the index that
# 			can be NULL is N, the bit vector occupies CEILING(N/8) bytes. (For example, if there are anywhere from 9 to 16 columns that can be NULL,
# 			the bit vector uses two bytes)
#
# 			Columns that are NULL do not occupy space other than the bit in this vector. The variable-length part of the header also contains the lengths
# 			of variable-length columns.
#
# 			Each length takes one or two bytes, depending on the maximum length of the column. If all columns in the index are NOT NULL and have a fixed
# 			length, the record header has no variable-length part.
#
# 		) For each non-NULL variable length field, the record header contains the length of the column in one or two bytes.
#
# 			Two bytes are only needed if part of the column is stored externally in overflow pages or the maximum length exceeds 255 bytes and the
# 			actual length exceeds 127 bytes.
#
# 			For an externally stored column, the 2-byte length indicates the length of the internally stored part plus the 20-byte pointer to the
# 			externally stored part.
#
# 			The intenral part is 768 bytes, so the length is 768+20. The 20-byte pointer stores the true length of the column.
#
# 		) The record header is followed by the data contents of non-NULL columns.
#
# 		) Records in the clustered index contain fields for all user-defined columns. In addition, there is a 6-byte transaction ID field and a 7-byte
# 			roll pointer field.
#
# 		) If no primary key is defined for a table, each clustered index record also contains a 6-byte row ID field.
#
# 		) Each secondary index record contains all the primary key columns defined for the clustered index key that are not in the secondary index.
#
# 			If any of the primary key columns are variable length, the record header for each secondary index has a variable-length part to record
# 			their lengths, even if the secondary index is defined on fixed-length columns.
#
# 		) Internally, for nonvariable-length character sets, fixed-length character columns such as CHAR(10) are stored in a fixed-length format.
#
# 			Trailing spaces are not truncated from VARCHAR columns.
#
# 		) Internally, for variable-length character sets such as utf8mb3 and utf8mb4, InnoDB attempts to store CHAR(N) in N bytes by trimming
# 			trailing spaces.
#
# 			If the byte length of a CHAR(N) column value exceeds N bytes, trailing spaces are trimmed to a minimum of the column value byte length.
#
# 			The maximum length of a CHAR(N) column is the maximum character byte length x N.
#
# 			A minimum of N bytes is reserved for CHAR(N). Reserving the minimum space N in many cases enables column updates to be done in place
# 			without causing index page fragmentation.
#
# 			By comparison, CHAR(N) columns occupy the maximum character byte length x N when using the REDUNDANT row format.
#
# 			Fixed-length columns greater than or equal to 768 bytes are encoded as variable-length fields, which can be stored off-page.
#
# 			For example, a CHAR(255) column can exceed 768 bytes if the maximum byte length of the character set is greater than 3,
# 			as it is with utf8mb4.
#
# DYNAMIC ROW FORMAT
#
# The DYNAMIC row format offers the same storage characteristics as the COMPACT row format but adds enhanced storage capabilities for long
# variable-length columns and supports large index key prefixes.
#
# When a table is created with ROW_FORMAT=DYNAMIC, InnoDB can store long variable-length column values (for VARCHAR, VARBINARY and BLOB and TEXT types)
# fully off-page, with the clustered index record containing only a 20-byte pointer to the overflow page.
#
# Fixed-length fields greater than or equal to 768 bytes are encoded as variable-length fields. For example, a CHAR(255) column can exceed 768 bytes
# if the maximum byte length of the character set is greater than 3, as it is with utf8mb4.
#
# Whether columns are stored off-page depends on the page size and the total size of the row. When a row is too long, the longest columns are chosen
# for off-page storage until the clustered index record fits on the B-tree page.
#
# TEXT and BLOB columns that are less than or equal to 40 bytes are stored in line.
#
# The DYNAMIC row format maintains the efficiency of storing the entire row in the index node if it fits (as do the COMPACT and REDUNDANT formats),
# but the DYNAMIC row format avoids the problem of filling B-tree nodes with a large number of data bytes of long columns.
#
# The DYNAMIC row format is based on the idea that if a portion of a long data value is stored off-page, it is usually most efficient to store the
# entire value off-page.
#
# With DYNAMIC format, shorter columns are likely to remain in the B-tree node, minimizing the number of overflow pages required for a given row.
#
# The DYNAMIC row format supports index key prefixes up to 3072 bytes.
#
# Tables that use the DYNAMIC row format can be stored in the system tablespace, file-per-table tablesapces, and general tablesapces.
#
# To store DYNAMIC tables in the system tablespace, either disable innodb_file_per_table and use a regular CREATE TABLE or ALTER TABLE statement,
# or use the TABLESPACE [=] innodb_system table option with CREATE TABLE or ALTER TABLE.
#
# The innodb_file_per_table variable is not applicable to general tablespaces, nor is it applicable when using the TABLESPACE [=] innodb_system
# table option to store DYNAMIC tables in the system tablespace.
#
# DYNAMIC ROW FORMAT STORAGE CHARACTERISTICS
#
# The DYNAMIC row format is a variation of the COMPACT row format. For storage characteristics, see COMPACT ROW FORMAT STORAGE CHARACTERISTICS.
#
# COMPRESSED ROW FORMAT
#
# The COMPRESSED row format offers the same storage characteristics and capabilities as the DYNAMIC row format but adds support for table
# and index data compression.
#
# The COMPRESSED row format uses similar internal details for off-page storage as the DYNAMIC row format, with additional storage and performance
# considerations from the table and index data being compressed and using smaller page sizes.
#
# With the COMPRESSED row format, the KEY_BLOCK_SIZE option controls how much column data is stored in the clustered index, and how much is
# placed on overflow pages.
#
# For more information about the COMPRESSED row format, see SECTION 15.9, "InnoDB TABLE AND PAGE COMPRESSION"
#
# The COMPRESSED row format supports index key prefixes up to 3072 bytes.
#
# Tables that use the COMPRESSED row format can be created in file-per-table tablespaces or general tablespaces. The system tablespace
# does not support the COMPRESSED row format.
#
# To store a COMPRESSED table in a file-per-table tablespace, the innodb_file_per_table variable must be enabled.
#
# The innodb_file_per_table variable is not applicable to general tablespaces. General tablespaces support all row formats
# with the caveat that compressed and uncompressed tables cannot coexist in the same general tablespace due to different
# physical page sizes. For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# COMPRESSED ROW FORMAT STORAGE CHARACTERISTICS
#
# The COMPRESSED row format is a variation of the COMPACT row format. For storage characteristics, see COMPACT ROW FORMAT STORAGE CHARACTERISTICS.
#
# DEFINING THE ROW FORMAT OF A TABLE
#
# The default row format for InnoDB tables is defined by innodb_default_row_format variable, which has a default value of DYNAMIC.
#
# The default row format is used when the ROW_FORMAT table option is not defined explicitly or when ROW_FORMAT=DEFAULT is specified.
#
# The row format of a table can be defined explicitly using the ROW_FORMAT table option in a CREATE_TABLE or ALTER_TABLE statement.
#
# For example:
#
# 		CREATE TABLE t1 (c1 INT) ROW_FORMAT=DYNAMIC;
#
# An explicitly defined ROW_FORMAT setting overrides the default row format. Specifying ROW_FORMAT=DEFAULT is equivalent to using
# the implicit default.
#
# The innodb_default_row_format variable can be set dynamically:
#
# 		mysql> SET GLOBAL innodb_default_row_format=DYNAMIC;
#
# Valid innodb_default_row_format options include DYNAMIC, COMPACT and REDUNDANT. The COMPRESSED row format, which is not supported
# for use in the system tablespace, cannot be defined as the default.
#
# It can only be specified explicitly in a CREATE_TABLE or ALTER_TABLE statement. Attempting to set the innodb_default_row_format
# variable to COMPRESSED returns an error:
#
# 		mysql> SET GLOBAL innodb_default_row_format=COMPRESSED;
# 		ERROR 1231 (42000): Variable 'innodb_default_row_format'
# 		can't be set to the value of 'COMPRESSED'
#
# Newly created tables use the row format defined by teh innodb_default_row_format variable when a ROW_FORMAT option is not specified
# explicitly, or when ROW_FORMAT=DEFAULT is used. For example, the following CREATE_TABLE statements use the row format defined by the
# innodb_default_row_format variable.
#
# 		CREATE TABLE t1 (c1 INT);
#
# 		CREATE TABLE t2 (c1 INT) ROW_FORMAT=DEFAULT;
#
# When a ROW_FORMAT option is not specified explicitly, or when ROW_FORMAT=DEFAULT is used, an operation that rebuilds a table silently
# changes the row format of the table to the format defined by the innodb_default_row_format variable.
#
# Table-rebuilding operations include ALTER_TABLE operations that use ALGORITHM=COPY or ALGORITHM=INPLACE where table rebuilding is required.
# See SECTION 15.12.1, "ONLINE DDL OPERATIONS" for more information.
#
# OPTIMIZE_TABLE is also a table-rebuilding operation.
#
# The following example demonstrates a table-rebuilding operation that silently changes the row format of a table created without an
# explicitly defined row format.
#
# 		mysql> SELECT @@innodb_default_row_format;
# 		+----------------------------+
# 		| @@innodb_default_row_format|
# 		+----------------------------+
# 		| dynamic 						  |
# 		+----------------------------+
#
# 		mysql> CREATE TABLE t1 (c1 INT);
#
# 		mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_TABLES WHERE NAME LIKE 'test/t1' \G
# 		******************************* 1. row *******************************
# 				TABLE_ID: 54
# 					NAME : test/t1
# 					FLAG : 33
# 				 N_COLS : 4
# 				  SPACE : 35
# 			ROW_FORMAT : Dynamic
# 		 ZIP_PAGE_SIZE: 0
# 		  SPACE_TYPE  : Single
#
# 		mysql> SET GLOBAL innodb_default_row_format=COMPACT;
#
# 		mysql> ALTER TABLE t1 ADD COLUMN (c2 INT);
#
# 		mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_TABLES WHERE NAME LIKE 'test/t1' \G
# 		******************************* 1. row ********************************
# 				TABLE_ID: 55
# 					NAME : test/t1
# 					FLAG : 1
# 				N_COLS  : 5
# 					SPACE: 36
# 			ROW_FORMAT : Compact
# 		 ZIP_PAGE_SIZE: 0
# 			SPACE_TYPE : Single
#
# Consider the following potential issues before changing the row format of existing tables from REDUNDANT or COMPACT to DYNAMIC.
#
# 		) The REDUNDANT and COMPACT row formats support a maximum index key prefix length of 767 bytes whereas DYNAMIC and COMPRESSED row
# 			formats support an index key prefix length of 3072 bytes.
#
# 			In a replication environment, if the innodb_default_row_format variable is set to DYNAMIC on the master, and set to COMPACT
# 			on the slave, the following DDL statement, which does not explicitly define a row format, succeeds on the master but fails
# 			on the slave:
#
# 				CREATE TABLE t1 (c1 INT PRIMARY KEY, c2 VARCHAR(5000), KEY i1(c2(3070)));
#
# 			For related information, see SECTION 15.6.1.6, "LIMITS ON INNODB TABLES"
#
# 		) Importing a table that does not explicitly define a row format results in a schema mismatch error if the innodb_default_row_format
# 			setting on the source server differs from the setting on the destination server.
#
# 			For more information, refer to the limitations outlined in SECTION 15.6.3.7, "COPYING TABLESPACES TO ANOTHER INSTANCE"
#
# DETERMINING THE ROW FORMAT OF A TABLE
#
# To determine the row format of a table, use SHOW_TABLE_STATUS:
#
# 		mysql> SHOW TABLE STATUS IN test1\G
# 		************************ 1. row *************************
# 						Name: t1
# 					 Engine: InnoDB
# 					Version: 10
# 			Row_format   : Dynamic
# 						Rows: 0
# 		  Avg_row_length: 0
# 			Data_length  : 16384
# 		Max_data_length : 0
# 			Index_length : 16384
# 				Data_free : 0
# 		Auto_increment  : 1
# 			Create_time  : 2016-09-14 16:29:38
# 			Update_time  : NULL
# 			Check_time   : NULL
# 			Collation 	 : utf8mb4_0900_ai_ci
# 			 	Checksum  : NULL
# 		Create_options  :
# 				Comment   :
#
# Alternatively, query the INFORMATION_SCHEMA.INNODB_TABLES table:
#
# 		mysql> SELECT NAME, ROW_FORMAT FROM INFORMATION_SCHEMA.INNODB_TABLES WHERE NAME='test1/t1';
# 		+-----------------+----------------+
# 		| NAME 				| ROW_FORMAT 	  |
# 		+-----------------+----------------+
# 		| test1/t1 			| Dynamic 		  |
# 		+-----------------+----------------+
#
# 15.11 InnoDB DISK I/O AND FILE SPACE MANAGEMENT
#
# 15.11.1 InnoDB DISK I/O
# 15.11.2 FILE SPACE MANAGEMENT
# 15.11.3 InnoDB CHECKPOINTS
# 15.11.4 DEFRAGMENTING A TABLE
# 15.11.5 RECLAIMING DISK SPACE WITH TRUNCATE TABLE
#
# As a DBA, you must manage disk I/O to keep the I/O subsystem from becoming saturated, and manage disk space to avoid filling up storage
# devices.
#
# The ACID design model requires a certain amount of I/O that might seem redundant, but helps to ensure data reliability. Within these
# constraints, InnoDB tries to optimize the database work and the organization of disk files to minimize the amount of disk I/O.
#
# Sometimes, I/O is postponed until the database is not busy, or until everything needs to be brought to a consistent state, such as
# during a database restart after a fast shutdown.
#
# This section discusses the main considerations for I/O and disk space with the default kind of MySQL tables (also known as InnoDB tables):
#
# 		) Controlling the amount of background I/O used to improve query performance.
#
# 		) Enabling or disabling features that provide extra durability at the expense of additional I/O
#
# 		) Organizing tables into many small files, a few larger files, or a combination of both.
#
# 		) Balancing the size of redo log files against the I/O activity that occurs when the log files become full.
#
# 		) How to reorganize a table for optimal query performance.
#
# 15.11.1 InnoDB DISK I/O
#
# InnoDB uses asynch disk I/O where possible, by creating a number of threads to handle I/O operations, while permitting other database
# operations to proceed while the I/O is still in progress.
#
# On Linux and Windows platforms, InnoDB uses the available OS and library functions to perform "native" asynch I/O. On other platforms,
# InnoDB still uses I/O threads, but the threads may actually wait for I/O requests to complete; this technique is known as "simulated"
# asynch I/O.
#
# READ-AHEAD
#
# If InnoDB can determine there is a high probability that data might be needed soon, it performs read-ahead operations to bring that data
# into the buffer pool so that it is available in memory.
#
# Making a few large read requests for continguous data can be more efficient than making several small, spread-out requests.
#
# There are two read-ahead heuristics in InnoDB:
#
# 		) In sequential read-ahead, if InnoDB notices that the access pattern to a segment in the tablespace is sequential, it posts in advance
# 			a batch of reads of database pages to the I/O system.
#
# 		) In random read-ahead, if InnoDB notices that some area in a tablespace seems to be in the process of being fully read into the buffer pool,
# 			it posts the remaining reads to the I/O system.
#
# For information about configuring read-ahead heuristics, see SECTION 15.8.3.4, "CONFIGURING INNODB BUFFER POOL PREFETCHING (READ-AHEAD)"
#
# DOUBLEWRITE BUFFER
#
# InnoDB uses a novel file flush technique involving a structure called the doublewrite buffer, which is enabled by default in most cases
# (innodb_doublewrite=ON)
#
# It adds safety to recovery following a crash or power outage, and improves performance on most varities of Unix by reducing the need for
# fsync() operations.
#
# Before writing pages to a data file, InnoDB first writes them to a continguous tablespace area called the doublewrite buffer. Only after the
# write and the flush to the doublewrite buffer has completed does InnoDB write hte pages to their proper positions in the data file.
#
# If there is an operating system, storage subsystem, or mysqld process crash in the middle of a page write (causing a torn page condition),
# InnoDB can later find a good copy of the page from the doublewrite buffer during recovery.
#
# If system tablespace files ("ibdata files") are located on Fusion-io devices that support atomic writes, doublewrite buffering is automatically
# disabled and Fusion-io atomic writes are used for all data files.
#
# Because the doublewrite buffer setting is global, doublewrite buffering is also disabled for data files residing on non-Fusion-io hardware.
#
# This feature is only supported on Fusion-io hardware and is only enabled for Fusion-io NVMFS on Linux. To take full advantage of this feature,
# an innodb_flush_method setting of O_DIRECT is recommended.
#
# 15.11.2 FILE SPACE MANAGEMENT
#
# The data files that you define in the configuration file using the innodb_data_file_path configuration option form the InnoDB system tablespace.
#
# The files are logically concatenated to form the system tablespace. There is no striping in use. You cannot define where within the system 
# tablespace your tables are allocated. In a newly created system tablespace, InnoDB allocates space starting from the first data file.
#
# To avoid the issues that come with storing all tables and indexes inside the system tablespace, you can enable the innodb_file_per_table
# configuration option (the default), which stores each newly created table in a separate tablespace file (with extension .ibd)
#
# For tables stored this way, there is less fragmentation within the disk file, and when the table is truncated, the space is returned to
# to the operating system rather than still being reserved by InnoDB within the system tablespace.
#
# For more information, see SECTION 15.6.3.2, "FILE-PER-TABLE TABLESPACES"
#
# You can also store tables in general tablespaces. General tablespaces are shared tablespaces created using CREATE_TABLESPACE syntax.
#
# They can be created outside of the MySQL data directory, are capable of holding multiple tables, and support tables of all row formats.
# For more information, see SECTION 15.6.3.3, "GENERAL TABLESPACES"
#
# PAGES, EXTENTS, SEGMENTS AND TABLESPACES
#
# Each tablespace consists of database pages. Every tablespace in a MySQL instance has the same page size. By default, all tablespaces have
# a page size of 16KB, you can reduce the page size to 8kb or 4kb by specifying the innodb_page_size option when you create the MySQL instance.
#
# You can also increase the page size to 32kb or 64kb.
#
# For more information, refer to the innodb_page_size documentation.
#
# The pages are grouped into extents of size 1MB for pages up to 16kb in size (64 consecutive 16kb pages, or 128 8kb pages, or 256 4kb pages)
#
# For a page size of 32kb, extent size is 2MB. For page size of 64kb, extent size is 4MB. The "files" inside a tablespace are called segments
# in InnoDB. (These segments are different from the rollback segment, which actually contains many tablespace segments)
#
# When a segment grows inside the tablespace, InnoDB allocates the first 32 pages to it one at a time. After that, InnoDB starts to allocate
# whole extents to the segment.
#
# InnoDB can add up to 4 extents at a time to a large segment to ensure good sequentiality of data.
#
# Two segments are allocated for each index in InnoDB. One is for nonleaf nodes of the B-tree, the other is for the leaf nodes.
#
# Keeping the leaf nodes contigous on disk enables better sequential I/O operations, because these leaf nodes contain the actual table data.
#
# Some pages in the tablespace contain bitmaps of other pages, and therefore a few extents in an InnoDB tablespace cannot be allocated to
# segments as a whole, but only as individual pages.
#
# When you ask for available free space in the tablespace by issuing a SHOW_TABLE_STATUS statement, InnoDB reports the extents that are definetly
# free in the tablespace.
#
# InnoDB always reserves some extents for cleanup and other internal purposes; these reserved extents are not included in the free space.
#
# When you delete data from a table, InnoDB contracts the corresponding B-tree indexes. Whether the freed space becomes available for other users
# depends on whether the pattern of deletes frees individual pages or extents to the tablespace.
#
# Dropping a table or deleting all rows from it is guarnateed to release the space to other users, but remember that deleted rows are physically
# removed only by the purge operation, which happens automatically some time after they are no longer needed for transaction rollbacks or consistent
# reads.
#
# (see SECTION 15.3, "InnoDB MULTI-VERSIONING")
#
# HOW PAGES RELATE TO TABLE ROWS
#
# The maximum row length is slightly less than half a database page for 4kb, 8kb, 16kb and 32kb innodb_page_size settings. For example, the maximum
# row length is slightly less than 8kb for the default 16kb InnoDB page size.
#
# For 64kb pages, the maximum row length is slightly less than 16kb.
#
# If a row does not exceed the maximum row length, all of it is stored locally within the page. If a row exceeds the maximum row length, variable-length columns
# are chosen for external off-page storage until the row fits within the maximum row length limit.
#
# External off-page storage for variable-length columns differs by row format:
#
# 		) COMPACT and REDUNDANT Row Formats
#
# 			When a variable length column is chosen for external off-page storage, InnoDB stores the first 768 bytes locally in the row, and the rest externally
# 			into overflow pages.
#
# 			Each such column has its own list of overflow pages. The 768-byte prefix is accompanied by a 20-byte value that stores the true length of the column
# 			and points into the overflow list where the rest of the value is stored.
#
# 			See SECTION 15.10, "InnoDB ROW FORMATS"
#
# 		) DYNAMIC and COMPRESSED ROW FORMATS
#
# 			When a variable-length column is chosen for external off-page storage, InnoDB stores a 20-byte pointer locally in the row, and the rest
# 			externally into overflow pages.
#
# 			See SECTION 15.10, "InnoDB ROW FORMATS"
#
# LONGBLOB and LONGTEXT columns must be less than 4GB and the total row length, including BLOB and TEXT columns, must be less than 4Gb.
#
# 15.11.3 InnoDB CHECKPOINTS
#
# Making your log files very large may reduce disk I/O during checkpointing. It often makes sense to set the total size of the log files
# as large as the buffer pool or even larger.
#
# HOW CHECKPOINT PROCESSING WORKS
#
# InnoDB implements a checkpoint mechanism known as fuzzy checkpointing. InnoDB flushes modified database pages from the buffer pool in small batches.
#
# There is no need to flush the buffer pool in one single batch, which would disrupt processing of user SQL statements during the checkpointing process.
#
# During crash recovery, INnoDB looks for a checkpoint label written to the log files. It knows that all modifications to the database before the label
# are present in teh disk image of the database.
#
# Then InnoDB scans the log file forward from the checkpoint, applying the logged modifications to the database.
#
# 15.11.4 DEFRAGMENTING A TABLE
#
# Random insertions into or deletions from a secondary index can cause the index to become fragmented. Fragmentation means that the physical ordering
# of the index pages on the disk is not close to the index ordering of the records on the pages, or that there are many unused pages in the 64-page
# blocks that were allocated to the index.
#
# One symptom of fragmentation is that a table takes more space than it "should" take. How much that is exactly, is difficult to determine. All InnoDB
# data and indexes are stored in B-trees, and their fill factor may vary from 50% to 100%. Another symptom of fragmentation is that a table scan such
# as this takes more time than it "should" take:
#
# 		SELECT COUNT(*) FROM t WHERE non_indexed_column <> 12345;
#
# The preceding query requires MySQL to perform a full table scan, the slowest type of query for a large table.
#
# To speed up index scans, you can periodically perform a "null" ALTER_TABLE operation, which causes MySQL to rebuild the table:
#
# 		ALTER TABLE tbl_name ENGINE=INNODB
#
# You can also use ALTER_TABLE_tbl_name_FORCE to perform a "null" alter operation that rebuilds the table.
#
# Both ALTER_TABLE_tbl_name_ENGINE=INNODB and ALTER_TABLE_tbl_name_FORCE use online DDL. For more information, see SECTION 15.12, "InnoDB and Online DDL"
#
# Another way to perform a defragmentation operation is to use mysqldump to dump the table to a text file, drop the table, and reload it from the dump file.
#
# If the insertions into an index are always ascending and records are deleted only from the end, the InnoDB filespace management algorithm guarantees
# that fragmentation in the index does not occur.
#
# 15.11.5 RECLAIMING DISK SPACE WITH TRUNCATE TABLE
#
# To reclaim operating system disk space when truncating an InnoDB table, the table must be stored in its own .ibd file.
#
# For a table to be stored in its own .ibd file, innodb_file_per_table must be enabled when the table is created. Additionally, there cannot
# be a foreign key constraint between the table being truncated and other tables, otherwise the TRUNCATE TABLE operation fails.
#
# A foreign key constraint between two columns in the same table, however, is permitted.
#
# When a table is truncated, it is dropped and re-created in a new .ibd file, and the freed space is returned to the operating system.
# This is in contrast to truncating InnoDB tables that are stored within the InnoDB system tablespace (tables created when innodb_file_per_table=OFF),
# and tables stored in shared general tablespaces, where only InnoDB can use the freed space after the table is truncated.
#
# The ability to truncate tables and return disk space to the operating system also means that physical backups can be smaller.
#
# Truncating tables that are stored in the system tablespace (tables created when innodb_file_per_table=OFF) or in a general tablespace leaves
# blocks of unused space in the tablespace.
#
# 15.12 InnoDB AND ONLINE DDL
#
# 15.12.1 ONLINE DDL OPERATIONS
# 15.12.2 ONLINE DDL PERFORMANCE AND CONCURRENCY
# 15.12.3 ONLINE DDL SPACE REQUIREMENTS
# 15.12.4 SIMPLIFYING DDL STATEMENTS WITH ONLINE DDL
# 15.12.5 ONLINE DDL FAILURE CONDITIONS
# 15.12.6 ONLINE DDL LIMITATIONS
#
# The online DDL feature provides support for instant and in-place table alterations and concurrent DML. Benefits of this feature include:
#
# 		) Improved responsiveness and availability in busy production environments, where making a table unavailable for minutes or hours is not practical.
#
# 		) For in-place operations, the ability to adjust the balance between performance and concurrency during DDL operations using the LOCK clause. See THE LOCK CLAUSE.
#
# 		) Less disk space usage and I/O overhead than the table-copy method.
#
# NOTE:
#
# 		ALGORITHM=INSTANT support is available for ADD COLUMN and other operations in MySQL 8.0.12
#
# Typically, you do not need to do anything special to enable online DDL. By default, MySQL performs the operation instantly or in place,
# as permitted, with as little locking as possible.
#
# You can control aspects of a DDL operation using the ALGORITHM and LOCK clauses of the ALTER_TABLE statement. These clauses are placed at the end
# of the statement, separated from the table and column specifications by commas. For example:
#
# 		ALTER TABLE tbl_name ADD PRIMARY KEY (column), ALGORITHM=INPLACE, LOCK=NONE;
#
# The LOCK clause may be used for operations that are performed in place and is useful for fine-tuning the degree of concurrent access to the table during operations.
#
# Only LOCK=DEFAULT is supported for operations that are performed instantly. The ALGORITHM clause is primarily intended for performance comparisons and as a fallback
# to the older table-copying behavior in case you encounter any issues. For example:
#
# 		) To avoid accidentally making the table unavailable for reads, writes, or both, during an in-place ALTER_TABLE operation, specify a clause on the ALTER_TABLE statement
# 			such as LOCK=NONE (permit reads and writes) or LOCK=SHARED (permit reads). The operation halts immediately if the requested level of concurrency is not available.
#
# 		) To compare performance between algorithms, run a statement with ALGORITHM=INSTANT, ALGORITHM=INPLACE and ALGORITHM=COPY. You can also run a statement with the old_alter_table
# 			configuration option enabled to force the use of ALGORITHM=COPY
#
# 		) To avoid tying up the server with an ALTER_TABLE operation that copies the table, include ALGORITHM=INSTANT or ALGORITHM=INPLACE. The statement halts immediately
# 			if it cannot use the specified algorithm.
#
# 15.12.1 ONLINE DDL OPERATIONS
#
# Online support details, syntax examples, and usage notes for DDL operations are provided under the following topics in this section.
#
# 		) INDEX OPERATIONS
#
# 		) PRIMARY KEY OPERATIONS
#
# 		) COLUMN OPERATIONS
#
# 		) GENERATED COLUMN OPERATIONS
#
# 		) FOREIGN KEY OPERATIONS
#
# 		) TABLE OPERATIONS
#
# 		) TABLESPACE OPERATIONS 
#
# 		) PARTITIONING OPERATIONS
#
# INDEX OPERATIONS
#
# The following table provides an overview of online DDL support for index operations. An asterisk indicates additional information, an exception or a dependency.
#
# For details, see SYNTAX AND USAGE NOTES.
#
# 		TABLE 15.17 ONLINE DDL SUPPORT FOR INDEX OPERATIONS
#
# 			OPERATION 					INSTANT 			IN PLACE 				REBUILDS TABLE 			PERMITS CONCURRENT DML 			ONLY MODIFIES METADATA
#
# 			Creating or adding a 	No 				Yes 						No 							Yes 									No
# 			secondary index
#
# 			Dropping an index 		No 				Yes 						No 							Yes 									Yes
#
# 			Renaming an index 		No 				Yes 						No 							Yes 									Yes
#
# 			Adding a FULLTEXT index No 				Yes* 						No* 							No 									No
#
# 			Adding a SPATIAL index  No 				Yes 						No 							No 									No
#
# 			Changing the index type Yes 				Yes 						No 							Yes 									Yes
#
# SYNTAX AND USAGE NOTES
#
# 	) Creating or adding a secondary index
#
# 			CREATE INDEX name ON table (col_list);
#
# 			ALTER TABLE tbl_name ADD INDEX name (col_list);
#
# 		The table remains available for read and write operations while the index is being created. The CREATE_INDEX statement only finishes
# 		after all transactions that are accessing the table are completed, so that the initial state of the index reflects the most recent
# 		contents of the table.
#
# 		Online DDL support for adding secondary indexes means that you can generally speed the overall process of creating and loading a table
# 		and associated indexes by creating the table without secondary indexes, then adding secondary indexes after the data is loaded.
#
# 		A newly created secondary index contains only the committed data in the table at the time in the CREATE_INDEX or ALTER_TABLE statement
# 		finishes executing. It does not contain any uncommitted values, old versions of values, or values marked for deletion but not yet
# 		removed from the old index.
#
# 		Some factors affect the performance, space usage, and semantics of this operation. For details, see SECTION 15.12.6, "ONLINE DDL LIMITATIONS"
#
# 	) Dropping an index
#
# 		DROP INDEX name ON table;
#
# 		ALTER TABLE tbl_name DROP INDEX name;
#
# 		The table remains available for read and write operations while the index is being dropped. The DROP_INDEX statement only finishes
# 		after all transactions that are accessing the table are completed, so that the initial state of the index reflects the most recent
# 		contents of the table.
#
# 	) Renaming an index
#
# 		ALTER TABLE tbl_name RENAME INDEX old_index_name TO new_index_name, ALGORITHM=INPLACE, LOCK=NONE;
#
# 	) Adding a FULLTEXT index
#
# 			CREATE FULLTEXT INDEX name ON table(column);
#
# 		Adding the first FULLTEXT index rebuilds the table if there is no user-defined FTS_DOC_ID column. Additional FULLTEXT indexes may
# 		be added without rebuilding the table.
#
# 	) Adding a SPATIAL index
#
# 			CREATE TABLE geom (g GEOMETRY NOT NULL);
# 			ALTER TABLE geom ADD SPATIAL INDEX(g), ALGORITHM=INPLACE, LOCK=SHARED;
#
# 		Adding the first FULLTEXT index rebuilds the table if there is no user-defined FTS_DOC_ID column. Additional FULLTEXT indexes may
# 		be added without rebuilding the table.
#
# ) Changing the index type (USING {BTREE | HASH})
#
# 		ALTER TABLE tbl_name DROP INDEX i1, ADD INDEX i1(key_part,...) USING BTREE, ALGORITHM=INSTANT;
#
# PRIMARY KEY OPERATIONS
#
# The following table provides an overview of online DDL support for primary key operations. An asterisk indicates additional information,
# an exception, or a dependency.
#
# See SYNTAX AND USAGE NOTES.
#
# 		TABLE 15.18 ONLINE DDL SUPPORT FOR PRIMARY KEY OPERATIONS
#
# 			OPERATION 					INSTANT 			IN PLACE 			REBUILDS TABLE 			PERMITS CONCURRENT DML 				ONLY MODIFIES METADATA
#
# 			Adding a primary key 	No 				Yes* 					Yes* 							Yes 										No
#
# 			Dropping a primary key 	No 				No 					Yes 							No 										No
#
# 			Dropping a primary key 	No 				Yes 					Yes 							Yes 										No
# 			and adding another 
#
# SYNTAX AND USAGE NOTES
#
# 	) Adding a primary key
#
# 			ALTER TABLE tbl_name ADD PRIMARY KEY (column), ALGORITHM=INPLACE, LOCK=NONE;
#
# 		Rebuilds the table in place. Data is reorganized substantially, making it an expensive operation. ALGORITHM=INPLACE is not permitted
# 		under certain conditions if columns have to be converted to NOT NULL.
#
# 		Restructuring the clustered index always requires copying of table data. Thus, it is best to define the primary key when you create a table,
# 		rather than issuing ALTER TABLE ... ADD PRIMARY KEY later.
#
# 		When you create a UNIQUE or PRIMARY KEY index, MySQL must do some extra work. For UNIQUE index, MySQL checks that the table contains no duplicate
# 		values for the key.
#
# 		For a PRIMARY KEY index, MySQL also checks that none of the PRIMARY KEY columns contains a NULL.
#
# 		When you add a primary key using the ALGORITHM=COPY clause, MySQL converts NULL values in the associated columns to default values: 0 for numbers,
# 		an empty string for character-based columns and BLOBs, and 0000-00-00 00:00:00 for DATETIME.
#
# 		This is a non-standard behavior that Oracle recommends you do not rely on.
#
# 		Adding a primary key using ALGORITHM=INPLACE is only permitted when the SQL_MODE setting includes the strict_trans_tables or strict_all_tables flags;
# 		when the SQL_MODE setting is strict, ALGORITHM=INPLACE is permitted, but the statement can still fail if the requested primary key columns contain
# 		NULL values.
#
# 		The ALGORITHM=INPLACE behavior is more standard-compliant.
#
# 		If you create a table without a primary key, InnoDB chooses one for you, which can be the first UNIQUE key defined on NOT NULL columns, or a system-generated
# 		key.
#
# 		To avoid uncertainty and the potential space requirement for an extra hidden column, specify the PRIMARY KEY clause as part of the CREATE_TABLE statement.
#
# 		MySQL creates a new clustered index by copying the existing data from the original table to a temporary table that has the desired index structure.
# 		Once the data is completely copied to the temporary table, the original table is renamed with a different temporary table name.
#
# 		The temporary table comprising the new clustered index is renamed with the name of the original table, and the original table is dropped from the database.
#
# 		The online performance enhancements that apply to operations on secondary indexes do not apply to the primary key index. The rows of an InnoDB table are
# 		stored in a clustered index organized based on the primary key, forming what some database systems call an "index-organized table".
#
# 		Because the table structure is closely tied to the primary key, redefining the primary key still requires copying the data.
#
# 		When an operation on the primary key uses ALGORITHM=INPLACE, even though the data is still copied, it is more efficient than using ALGORITHM=COPY because:
#
# 			) No undo logging or associated redo logging is required for ALGORITHM=INPLACE. These operations add overhead to DDL statements that use ALGORITHM=COPY
#
# 			) The secondary index entries are pre-sorted, and so can be loaded in order.
#
# 			) The change buffer is not used, because there are no random-access inserts into the secondary indexes.
#
# 	) Dropping a primary key
#
# 			ALTER TABLE tbl_name DROP PRIMARY KEY, ALGORITHM=COPY;
#
# 	  Only ALGORITHM=COPY supports dropping a primary key without adding a new one in the same ALTER TABLE statement.
#
# 	) Dropping a primary key and adding another
#
# 		ALTER TABLE tbl_name DROP PRIMARY KEY, ADD PRIMARY KEY (column), ALGORITHM=INPLACE, LOCK=NONE;
#
# 		Data is reorganized substantially, making it an expensive operation.
#
# COLUMN OPERATIONS
#
# The following table provides an overview of online DDL support for column operations. An asterisk indicates additional information, an exception,
# or a dependency.
#
# For details, see SYNTAX AND USAGE NOTES.
#
# 		TABLE 15.19 ONLINE DDL SUPPORT FOR COLUMN OPERATIONS
#
# 			OPERATION 			INSTANT 		IN PLACE 		REBUILDS TABLE 		PERMITS CONCURRENT DML 		ONLY MODIFIES METADATA
#
# 			Adding a column 	Yes* 			Yes 				No* 						Yes* 								No
#
# 			Dropping a column No 			Yes 				Yes 						Yes 								No
#
# 			Renaming a column No 			Yes 				No 						Yes* 								Yes
#
# 			Reordering cols 	No 			Yes 				Yes 						Yes 								No
#
# 			Setting a col 		Yes 			Yes 				No 						Yes 								Yes 
# 			def. value
#
# 			Changing the 		No 			No 				Yes 						No 								No
# 			col data type
#
# 			Ext. VARCHAR 		No 			Yes 				No 						Yes 								Yes
# 			column size
#
# 			Dropping the 		Yes 			Yes 				No 						Yes 								Yes
# 			col def. val 
#
# 			Changing the 		No 			Yes 				No 						Yes 								No*
# 			auto-inc.val
#
# 			Making a col. 		No 			Yes 				Yes* 						Yes 								No
# 			NULL
#
# 			Making a col. 		No 			Yes* 				Yes* 						Yes 								No
# 			NOT NULL
#
# 			Mod. the def. 		Yes 			Yes 				No 						Yes 								Yes
# 			of an ENUM or SET
#
# 
# SYNTAX AND USAGE NOTES
#
# 		) Adding a column
#
# 				ALTER TABLE tbl_name ADD COLUMN column_name column_definition, ALGORITHM=INSTANT;
#
# 			The following limitations apply when the INSTANT algorithm is used to add a column:
#
# 				) Adding a column cannot be combined in the same statement with other ALTER TABLE actions that do not support ALGORITHM=INSTANT
#
# 				) A column can only be added as the last column of the table. Adding a column to any other positon among other columns is not supported.
#
# 				) Columns cannot be added to tables that use ROW_FORMAT=COMPRESSED
#
# 				) Columns cannot be added to tables that include a FULLTEXT index.
#
# 				) Columns cannot be added to temporary tables. Temporary tables only support ALGORITHM=COPY
#
# 				) Columns cannot be added to tables that reside in the data dictionary tablespace.
#
# 				) Row size limits are not evaluated when adding a column. However, row size limits are checked during DML operations
# 					that insert and update rows in the table.
#
# 			Multiple columns may be added in the same ALTER_TABLE statement. For example:
#
# 				ALTER TABLE t1 ADD COLUMN c2 INT, ADD COLUMN c3 INT, ALGORITHM=INSTANT;
#
# 			INFORMATION_SCHEMA.INNODB_TABLES and INFORMATION_SCHEMA.INNODB_COLUMNS provide metadata for instantly added columns.
#
# 			INFORMATION_SCHEMA.INNODB_TABLES.INSTANT_COLS shows number of columns in the table prior to adding the first instant column.
#
# 			INFORMATION_SCHEMA.INNODB_COLUMNS.HAS_DEFAULT and DEFAULT_VALUE provide metadata about default values for instantly added columns.
#
# 			Concurrent DML is not permitted when adding an auto-increment column. Data is reorganized substantially, making it an expensive
# 			operation.
#
# 			At a minimum, ALGORITHM=INPLACE, LOCK=SHARED is required.
#
# 			The table is rebuilt if ALGORITHM=INPLACE is used to add a column.
#
# 		) Dropping a column
#
# 				ALTER TABLE tbl_name DROP COLUMN column_name, ALGORITHM=INPLACE, LOCK=NONE;
#
# 			Data is reorganized substantially, making it an expensive operation.
#
# 		) Renaming a column
#
# 				ALTER TABLE tbl CHANGE old_col_name new_col_name data_type, ALGORITHM=INPLACE, LOCK=NONE;
#
# 			To permit concurrent DML, keep the same data type and only change the column name.
#
# 			When you keep the same data type and [NOT] NULL attribute, only changing the column name, the operation can always be
# 			performed online.
#
# 			You can also rename a column that is part of a foreign key constraint. The foreign key definition is automatically updated to use
# 			the new column name. Renaming a column participating in a foreign key only works with ALGORITHM=INPLACE.
#
# 			If you use the ALGORITHM=COPY clause, or some other condition causes the command to use ALGORITHM=COPY behind the scenes,
# 			the ALTER TABLE statement fails.
#
# 			ALGORITHM=INPLACE is not supported for renaming a generated column.
#
# 		) Reordering columns
#
# 			To reorder columns, use FIRST or AFTER in CHANGE or MODIFY operations.
#
# 				ALTER TABLE tbl_name MODIFY COLUMN col_name column_definition FIRST, ALGORITHM=INPLACE, LOCK=NONE;
#
# 			Data is reorganized substantially, making it an expensive operation.
#
# 		) Changing the column data type
#
# 				ALTER TABLE tbl_name CHANGE c1 c1 BIGINT, ALGORITHM=COPY;
#
# 			Changing the column data type is only supported with ALGORITHM=COPY
#
# 		) Extending VARCHAR column size
#
# 				ALTER TABLE tbl_name CHANGE COLUMN c1 c1 VARCHAR(255), ALGORITHM=INPLACE, LOCK=NONE;
#
# 			The number of length bytes required by a VARCHAR column must remain the same. For VARCHAR columns of 0 to 255 bytes in size,
# 			one length bytes is required to encode the value.
#
# 			For VARCHAR columns of 256 bytes in size or more, two length bytes are required. As a result, in-place ALTER_TABLE only supports
# 			increasing VARCHAR column size from 0 to 255 bytes, or from 256 bytes to a greater size.
#
# 			In-place ALTER_TABLE does not support increasing the size of a VARCHAR column from less than 256 bytes to a size equal to or greater
# 			than 256 bytes.
#
# 			In this case, the number of required length bytes changes from 1 to 2, which is only supported by a table copy (ALGORITHM=COPY)
#
# 			For example, attempting to change VARCHAR column size for a single byte character set from VARCHAR(255) to VARCHAR(256) using
# 			in-place ALTER_TABLE returns this error:
#
# 				ALTER TABLE tbl_name ALGORITHM=INPLACE, CHANGE COLUMN c1 c1 VARCHAR(256);
# 				ERROR 0A000: ALGORITHM=INPLACE is not supported. Reason: Cannot change column
# 				type INPLACE. Try ALGORITHM=COPY.
#
# 			NOTE:
#
# 				The byte length of a VARCHAR column is dependant on the byte length of the character set.
#
# 			Decreasing VARCHAR size using in-place ALTER_TABLE is not supported. Decreasing VARCHAR size requires a table
# 			copy (ALGORITHM=COPY)
#
# 		) Setting a column default value
#
# 				ALTER TABLE tbl_name ALTER COLUMN col SET DEFAULT literal, ALGORITHM=INSTANT;
#
# 			Only modifies table metadata. Default column values are stored in the data dictionary.
#
# 		) Dropping a column default value
#
# 				ALTER TABLE tbl ALTER COLUMN col DROP DEFAULT, ALGORITHM=INSTANT;
#
# 		) Changing the auto-increment value
#
# 				ALTER TABLE table AUTO_INCREMENT=next_value, ALGORITHM=INPLACE, LOCK=NONE;
#
# 			Modifies a value stored in memory, not the data file.
#
# 			In a distributed system using replication or sharding, you sometimes reset the auto-increment counter for a table to a specific value.
#
# 			The next row inserted into the table uses the specified value for its auto-increment column. You might also use this technique in data
# 			warehousing environment where you periodically empty all the tables and reloads them, and restart the auto-increment sequence from 1.
#
# 		) Making a column NULL
#
# 				ALTER TABLE tbl_name MODIFY COLUMN column_name data_type NULL, ALGORITHM=INPLACE, LOCK=NONE;
#
# 			Rebuilds the table in place. Data is reorganized substantially, making it an expensive operation.
#
# 		) Making a column NOT NULL
#
# 				ALTER TABLE tbl_name MODIFY COLUMN column_name data_type NOT NULL, ALGORITHM=INPLACE, LOCK=NONE;
#
# 			Rebuilds the table in place. STRICT_ALL_TABLES or STRICT_TRANS_TABLES SQL_MODE is required for the operation to succeed.
#
# 			The operation fails if the column contains NULL values.
#
# 			The server prohibits changes to foreign key columns that have the potential to cause loss of referential integrity.
#
# 			See SECTION 13.1.9, "ALTER TABLE SYNTAX"
#
# 			Data is reorganized substantially, making it an expensive operation.
#
# 		) Modifying the definition of an ENUM or SET column
#
# 				CREATE TABLE t1 (c1 ENUM('a', 'b', 'c'));
# 				ALTER TABLE t1 MODIFY COLUMN c1 ENUM('a', 'b', 'c', 'd'), ALGORITHM=INSTANT;
#
# 			Modifying the definition of an ENUM or SET column by adding new enumeration or set members to the end of the list of valid
# 			member values may be performed instantly or in place, as long as the storage size of the data type does not change.
#
# 			For example, adding a member to a SET column that has 8 members changes the required storage per value from 1 byte to 2 bytes;
# 			this requires a table copy.
#
# 			Adding members in the middle of the list causes renumbering of existing members, which requires a table copy.
#
# GENERATED COLUMN OPERATIONS
#
# The following table provides an overview of online DDL support for generated column operations. For details, see SYNTAX and USAGE NOTES.
#
# 		TABLE 15.20 ONLINE DDL SUPPORT FOR GENERATED COLUMN OPERATIONS
#
# 			OPERATION 						INSTANT 			IN PLACE 			REBUILDS TABLE 		PERMITS CONCURRENT DML 		ONLY MODIFIES METADATA
#
#			Adding a STORED column 		No 				No 					Yes 						No 								No
#
# 			Modifying STORED col. 		No 				No 					Yes 						No 								No
# 			order	
#
# 			Dropping a STORED col 		No 				Yes 					Yes 						Yes 								No
#
# 			Adding a VIRTUAL col 		Yes 				Yes 					No 						Yes 								Yes
#
# 			Modifying VIRTUAL col 		No 				No 					Yes 						No 								No
# 			order
#
# 			Dropping a VIRTUAL col 		Yes 				Yes 					No 						Yes 								Yes
#
# SYNTAX AND USAGE NOTES
#
# 		) Adding a STORED column
#
# 				ALTER TABLE t1 ADD COLUMN (c2 INT GENERATED ALWAYS AS (c1 + 1) STORED), ALGORITHM=COPY;
#
# 			ADD COLUMN is not an in-place operation for stored columns (done without using a temporary table) because the expression must be evaluated
# 			by the server.
#
# 		) Modifying STORED column order
#
# 				ALTER TABLE t1 MODIFY COLUMN c2 INT GENERATED ALWAYS AS (c1 + 1) STORED FIRST, ALGORITHM=COPY;
#
# 			Rebuilds the table in place.
#
# 		) Dropping a STORED column
#
# 				ALTER TABLE t1 DROP COLUMN c2, ALGORITHM=INPLACE, LOCK=NONE;
#
# 			Rebuilds the table in place.
#
# 		) Adding a VIRTUAL column
#
# 				ALTER TABLE t1 ADD COLUMN (c2 INT GENERATED ALWAYS AS (c1 + 1) VIRTUAL), ALGORITHM=INSTANT;
#
# 			Adding a virtual column can be performed instantly or in place for non-partitioned tables.
#
# 			Adding a VIRTUAL is not an in-place operation for partitioned tables.
#
# 		) Modifying VIRTUAL column order
#
# 				ALTER TABLE t1 MODIFY COLUMN c2 INT GENERATED ALWAYS AS (c1 + 1) VIRTUAL FIRST, ALGORITHM=COPY;
#
# 		) Dropping a VIRTUAL column:
#
# 				ALTER TABLE t1 DROP COLUMN c2, ALGORITHM=INSTANT;
#
# 			Dropping a VIRTUAL column can be performed instantly or in place for non-partitioned tables.
#
# FOREIGN KEY OPERATIONS
#
# The following table provides an overview of online DDL support for foreign key operations. An asterisk indicates additional information,
# an exception, or a dependency.
#
# For details, see SYNTAX AND USAGE NOTES.
#
# 		TABLE 15.21 ONLINE DDL SUPPORT FOR FOREIGN KEY OPERATIONS
#
# 			OPERATION 			INSTANT 			IN PLACE 		REBUILDS TABLE 		PERMITS CONCURRENT DML 		ONLY MODIFIES METADATA
#
# 			Adding a foreign 	No 				Yes* 				No 						Yes 								Yes
# 			key constraint 
#
# 			Dropping a foreign No 				Yes 				No 						Yes 								Yes
# 			key constraint
#
# SYNTAX AND USAGE NOTES
#
# 		) Adding a foreign key constraint
#
# 			The INPLACE algorithm is supported when foreign_key_checks is disabled. Otherwise, only the COPY algorithm is supported.
#
# 				ALTER TABLE tbl1 ADD CONSTRAINT fk_name FOREIGN KEY index (col1)
# 					REFERENCES tbl2(col2) referential_actions;
#
# 		) Dropping a foreign key constraint
#
# 				ALTER TABLE tbl DROP FOREIGN KEY fk_name;
#
# 			Dropping a foreign key can be performed online with the foreign_key_checks option enabled or disabled.
#
# 			If you do not know the names of the foreign key constraints on a particular table, issue the following statement and find the
# 			constraint name in the CONSTRAINT clause for each foreign key:
#
# 				SHOW CREATE TABLE table\G
#
# 			Or, query the INFORMATION_SCHEMA.TABLE_CONSTRAINTS table and use the CONSTRAINT_NAME and CONSTRAINT_TYPE columns to identify the
# 			foreign key names.
#
# 			You can also drop a foreign key and its associated index in a single statement:
#
# 				ALTER TABLE table DROP FOREIGN KEY constraint, DROP INDEX index;
#
# 			NOTE:
#
# 				If foreign keys are already present in the table being altered (that is, it is a child table containing a FOREIGN KEY ... REFERENCE clause),
# 				additional restrictions apply to online DDL operations, even those not directly involving the foreign key columns:
#
# 					) An ALTER_TABLE on the child table could wait for another transaction to commit, if a change to the parent table causes associated changes
# 						in the child table through an ON UPDATE or ON DELETE clause using the CASCADE or SET NULL parameters.
#
# 					) In the same way, if a table is the parent table in a foreign key relationship, even though it does not contain any FOREIGN KEY clauses,
# 						it could wait for the ALTER_TABLE to complete if an INSERT, UPDATE or DELETE statement causes an ON UPDATE or ON DELETE action in the
# 						child table.
#
# TABLE OPERATIONS
#
# The following table provides an overview of online DDL support for table operations. An asterisk indicates additional information, an exception or
# a dependency.
#
# For details, see SYNTAX AND USAGE NOTES.
#
# 		TABLE 15.22 ONLINE DDL SUPPORT FOR TABLE OPERATIONS
#
# 			OPERATION 					INSTANT 		IN PLACE 	REBUILDS TABLE 	PERMITS CONCURRENT DML 		ONLY MODIFIES METADATA
#
# 			Changing the ROW_FORMAT 	No 		Yes 			Yes 					Yes 								No
#
# 			Changing the  					No 		Yes 			Yes 					Yes 								No
# 			KEY_BLOCK_SIZE
#
# 			Setting persistent 			No 		Yes 			No 					Yes 								Yes
# 			table stats
#
# 			Specifying a char set 		No 		Yes 			Yes* 					No 								No
#
# 			Converting a char set 		No 		No 			Yes* 					No 								No
#
# 			Optimizing a table 			No 		Yes* 			Yes 					Yes 								No
#
# 			Rebuilding with the 			No 		Yes* 			Yes 					Yes 								No
# 			FORCE option
#
# 			Performing a null rebuild  No 		Yes* 			Yes 					Yes 								No
#
# 			Renaming a table 				Yes 		Yes 			No 					Yes 								Yes
#
# 
# SYNTAX AND USAGE NOTES
#
# 	) Changing the ROW_FORMAT
#
# 			ALTER TABLE tbl_name ROW_FORMAT = row_format, ALGORITHM=INPLACE, LOCK=NONE;
#
# 		Data is reorganized substantially, making it an expensive operation.
#
# 		For additional information about the ROW_FORMAT option, see TABLE OPTIONS.
#
# 	) Changing the KEY_BLOCK_SIZE
#
# 			ALTER TABLE tbl_name KEY_BLOCK_SIZE = value, ALGORITHM=INPLACE, LOCK=NONE;
#
# 		Data is reorganized substantially, making it an expensive operation.
#
# 		For additional information about the KEY_BLOCK_SIZE option, see TABLE OPTIONS.
#
# 	) Setting persistent table stats options
#
# 			ALTER TABLE tbl_name STATS_PERSISTENT=0, STATS_SAMPLE_PAGES=20, STATS_AUTO_RECALC=1, ALGORITHM=INPLACE, LOCK=NONE;
#
# 		Only modifies table metadata.
#
# 		Persistent stats includes STATS_PERSISTENT, STATS_AUTO_RECALC and STATS_SAMPLE_PAGES.
#
# 		For more information, see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# ) Specifying a character set
#
# 			ALTER TABLE tbl_name CHARACTER SET = charset_name, ALGORITHM=INPLACE, LOCK=NONE;
#
# 		Rebuilds the table if the new character encoding is different.
#
# ) Converting a character set
#
# 			ALTER TABLE tbl_name CONVERT TO CHARACTER SET charset_name, ALGORITHM=COPY;
#
# 		Rebuilds the table if the new character encoding is different.
#
# ) Optimizing a table
#
# 			OPTIMIZE TABLE tbl_name;
#
# 		In-place operation is not supported for tables with FULLTEXT indexes. The operation uses the INPLACE algorithm, but ALGORITHM
# 		and LOCK syntax is not permitted.
#
# ) Rebuilding a table with the FORCE option.
#
# 			ALTER TABLE tbl_name FORCE, ALGORITHM=INPLACE, LOCK=NONE;
#
# 		Uses ALGORITHM=INPLACE as of MySQL 5.6.17. ALGORITHM=INPLACE is not supported for tables with FULLTEXT indexes.
#
# ) Performing a "null" rebuild
#
# 			ALTER TABLE tbl_name ENGINE=InnoDB, ALGORITHM=INPLACE, LOCK=NONE;
#
# 		Uses ALGORITHM=INPLACE as of MySQL 5.6.17. ALGORITHM=INPLACE is not supported for tables with FULLTEXT indexes.
#
# ) Renaming a table
#
# 			ALTER TABLE old_tbl_name RENAME TO new_tbl_name, ALGORITHM=INSTANT;
#
# 		Renaming a table can be performed instantly or in place. MySQL renames files that correspond to the table tbl_name without
# 		making a copy.
#
# 		(You can also use the RENAME_TABLE statement to rename tables. See SECTION 13.1.36, "RENAME TABLE SYNTAX")
#
# 		Privileges granted specifically for the renamed table are not migrated to the new name.
#
# 		They must be changed manually.
#
# TABLESPACE OPERATIONS
#
# The following table provides an overview of online DDL support for tablespace operations. For details, see SYNTAX AND USAGE NOTES.
#
# 		TABLE 15.23 ONLINE DDL SUPPORT FOR TABLESPACE OPERATIONS
#
# 			OPERATION 				INSTANT 			IN PLACE 			REBUILDS TABLE 		PERMITS CONCURRENT DML 			ONLY MODIFIES METADATA
#
# 			Renaming a general 	No 				Yes 					No 						Yes 									Yes
# 			tablespace
#
# 			Enabling or disabling  No 				Yes 					No 						Yes 									No
# 			general tablespace
# 			encryption
#
# 			Enabling or disabling 	 No 			No 					Yes 						No 									No
# 			file-per-table tablsapce
# 			encryption
#
# SYNTAX AND USAGE NOTES
#
# 		) Renaming a general tablespace
#
# 				ALTER TABLESPACE tablespace_name RENAME TO new_tablespace_name;
#
# 			ALTER_TABLESPACE_..._RENAME_TO uses the INPLACE algorithm but does not support the ALGORITHM clause.
#
# 		) Enabling or disabling general tablespace encryption
#
# 				ALTER TABLESPACE tablespace_name ENCRYPTION='Y';
#
# 			ALTER_TABLESPACE_..._ENCRYPTION uses the INPLACE algorithm but does not support the ALGORITHM clause.
#
# 			For related information, see SECTION 15.6.3.9, "InnoDB DATA-AT-REST ENCRYPTION"
#
# 		) Enabling or disabling file-per-table tablespace encryption
#
# 				ALTER TABLE tbl_name ENCRYPTION='Y', ALGORITHM=COPY;
#
# 			For related information, see SECTION 15.6.3.9, "InnoDB DATA-AT-REST ENCRYPTION"
#
# PARTITIONING OPERATIONS
#
# With the exception of some ALTER_TABLE partitioning clauses, online DDL operations for partitioned INnoDB tables follow the same
# rules that apply to regular InnoDB tables.
#
# Some ALTER_TABLE partitioning clauses do not go through the same internal online DDL API as regular non-partitioned InnoDB tables.
#
# As a result, online support for ALTER_TABLE partitioning clauses varies.
#
# The following table shows the online status for each ALTER TABLE partitioning statement. Regardless of the online DDL API that is
# used, MySQL attempts to minimize data copying and locking where possible.
#
# ALTER_TABLE partitioning options that use ALGORITHM=COPY or that only permit "ALGORITHM=DEFAULT, LOCK=DEFAULT", repartition the table
# using the COPY algorithm.
#
# IN other words, a new partitioned table is created with the new partitioning scheme. The newly created table includes any changes applied
# by the ALTER_TABLE statement, and table data is copied into the new table structure.
#
# 		TABLE 15.24 ONLINE DDL SUPPORT FOR PARTITIONING OPERATIONS
#
# 		PARTITIONING CLAUSE 			INSTANT 		IN PLACE 		PERMITS DML 				NOTES
#
# 		PARTITION_BY 					No 			No 				No 							Permits ALGORITHM=COPY, LOCK={DEFAULT|SHARED|EXCLUSIVE}
#
# 		ADD_PARTITION 					No 			Yes* 				Yes* 							ALGORITHM=INPLACE, LOCK={DEFAULT|NONE|SHARED|EXCLUSIVE} is supported
# 																												for RANGE and LIST partitions, ALGORITHM=INPLACE, LOCK={DEFAULT|SHARED|EXCLUSIVE}
# 																												for HASH and KEY partitions, and ALGORITHM=COPY, LOCK={SHARED|EXCLUSIVE} for all partition types.
#
# 																												Does not copy existing data for tables partitioned by RANGE or LIST. Concurrent queries are permitted
# 																												with ALGORITHM=COPY for tables partitioned by HASH or LIST, as MySQL copies the data while holding
# 																												a shared lock.
#
# 		DROP_PARTITION 				No 			Yes* 				Yes* 							ALGORITHM=INPLACE, LOCK={DEFAULT|NONE|SHARED|EXCLUSIVE} is supported. Does not copy data
# 																												for tables partitioned by RANGE or LIST.
#
# 																												DROP PARTTIION with ALGORITHM=INPLACE deletes data stored in the partition and drops the partition.
# 																												However, DROP PARTITION with ALGORITHM=COPY or old_alter_table=ON rebuilds the partitioned table
# 																												and attempts to move data from the dropped partition to another partition with a compatible PARTITION
# 																												/etc/ VALUES definition. Data that cannot be moved to another partition is deleted.
#
# 		DISCARD_PARTITION 			No 			No 				No 							Only permits ALGORITHM=DEFAULT, LOCK=DEFAULT
#
# 		IMPORT_PARTITION 				No 			No 				No 							Only permits ALGORITHM=DEFAULT, LOCK=DEFAULT
#
# 		TRUNCATE_PARTITION 			No 			Yes 				Yes 							Does not copy existing data. It merely deletes rows; it does not alter the definition of the table
# 																												itself, or of any of its partitions.
#
# 		COALESCE_PARTITION 			No 			Yes* 				No 							ALGORITHM=INPLACE, LOCK={DEFAULT|SHARED|EXCLUSIVE} is supported.
#
# 		REORGANIZE_PARTITION 		No 			Yes* 				No 							ALGORITHM=INPLACE, LOCK={DEFAULT|SHARED|EXCLUSIVE} is supported.
#
# 		EXCHANGE_PARTITION 			No 			Yes 				Yes 
#
# 		ANALYZE_PARTITION 			No 			Yes 				Yes
#
# 		CHECK_PARTITION 				No 			Yes 				Yes
#
# 		OPTIMIZE_PARTITION 			No 			No 				No 							ALGORITHM and LOCK clauses are ignored. Rebuilds the entire table. See SECTION 23.3.4, "MAINTENANCE OF PARTITIONS"
#
# 		REBUILD_PARTITION 			No 			Yes* 				No 							ALGORITHM=INPLACE, LOCK={DEFAULT|SHARED|EXCLUSIVE} is supported.
#
# 		REPAIR_PARTITION 				No 			Yes 				Yes 
#
# 		REMOVE_PARTITIONING 			No 			No 				No 							Permits ALGORITHM=COPY, LOCK={DEFAULT|SHARED|EXCLUSIVE}
#
# Non-partitioning online ALTER_TABLE operations on partitioned tables follow the same rules that apply to regular tables. However, ALTER_TABLE
# performs online operations on each table partition, which causes increased demand on system resources due to operations being performed on
# multiple partitions.
#
# For additional information about ALTER_TABLE partitioning clauses, see PARTITIONING OPTIONS, and SECTION 13.1.9.1, "ALTER TABLE PARTITION OPERATIONS".
#
# For information about partitioning in general, see CHAPTER 23, PARTITIONING.
#
# 15.12.2 ONLINE DDL PERFORMANCE AND CONCURRENCY
#
# 	Online DDL improves several aspects of MySQL operation:
#
# 		) Applications that access the table are more responsive because queries and DML operations on the table can proceed while the DDL
# 			operation is in progress.
#
# 			Reduced locking and waiting for MySQL server resources leads to greater scalability, even for operations that are not involved
# 			in the DDL operation.
#
# 		) Instant operations only modify metadata in the data dictionary. No metadata locks are taken on the table, and table data is unaffected,
# 			making operations instaneous. Concurrent DML is unaffected.
#
# 		) Online operations avoid the disk I/O and CPU cycles associated with the table-copy method, which minimizes overall load on the database.
# 			Minimizing load helps maintain good performance and high throughput during the DDL operation.
#
# 		) Online operations read less data into the buffer pool than table-copy operations, which reduces purging of frequently accessed data
# 			from memory.
#
# 			Purging of frequently accessed data can cause a temporary performance dip after a DDL operation.
#
# THE LOCK CLAUSE
#
# By default, MySQL uses as little locking as possible during a DDL operation. The LOCK clause can be specified for in-place operations and some
# copy operations to enforce more restrictive locking, if required.
#
# If the LOCK clause specifies a less restrictive level of locking than is permitted for a particular DDL operation, the statement fails with an
# error.
#
# LOCK clauses are described below, in order of least to most restrictive:
#
# 		) LOCK=NONE:
#
# 			Permits concurrent queries and DML.
#
# 			For example, use this clause for tables involving customer signups or purchases, to avoid making the tables unavailable during lengthy DDL operations.
#
# 		) LOCK=SHARED:
#
# 			Permits concurrent queries but blocks DML.
#
# 			For example, use this clause on data warehouse tables, where you can delay data load operations until the DDL operation is finished, but queries
# 			cannot be delayed for long periods.
#
# 		) LOCK=DEFAULT:
#
# 			Permits as much concurrency as possible (concurrent queries, DML, or both). Omitting the LOCK clause is the same as specifying LOCK=DEFAULT.
#
# 			Use this clause when you know that the default locking level of the DDL statement will not cause availability problems for the table.
#
# 		) LOCK=EXCLUSIVE
#
# 			Blocks concurrent queries and DML.
#
# 			Use this clause if the primary concern is finishing the DDL operation in teh shortest amount of time possible, and concurrent query and DML
# 			access is not necessary.
#
# 			You might also use this clause if hte server is supposed to be idle, to avoid unexpected table accesses.
#
# ONLINE DDL AND METADATA LOCKS
#
# Online DDL operations can be viewed as having three phases:
#
# 		) Phase 1: Initialization
#
# 			In the initialization phase, the server determines how much concurrency is permitted during the operation, taking into account
# 			storage engine capabilities, operations specified in the statement, and user-specified ALGORITHM and LOCK options.
#
# 			During this phase, a shared upgradable metadata lock is taken to protect the current table definition.
#
# 		) Phase 2: Execution
#
# 			In this phase, the statement is prepared and executed. Whether the metadata lock is upgraded to exclusive depends on the factors
# 			assessed in the initialization phase.
#
# 			If an exclusive metadata lock is required, it is only taken briefly during statement preparation.
#
# 		) Phase 3: Commit Table Definition
#
# 			In the commit table definition phase, the metadata lock is upgraded to exclusive to evict the old table definition and commit the new
# 			one.
#
# 			Once granted, the duration of the exclusive metadata lock is brief.
#
# Due to the exclusive metadata lock requirements outlined above, an online DDL operation may have to wait for concurrent transactions that hold
# metadata locks on the table to commit or rollback.
#
# Transactions started before or during the DDL operation can hold metadata locks on the table being altered. In the case of a long running or
# inactive transaction, an online DDL operation can time out waiting for an exclusive metadata lock.
#
# Additionally, a pending exclusive metadata lock requested by an online DDL operation blocks subsequent transactions on the table.
#
# The following example demonstrates an online DDL operation waiting for an exclusive metadata lock, and how a pending metadata lock blocks
# subsequent transactions on the table.
#
# Session 1:
#
# 		mysql> CREATE TABLE t1 (c1 INT) ENGINE=InnoDB;
# 		mysql> START TRANSACTION;
# 		mysql> SELECT * FROM t1;
#
# The session 1 SELECT statement takes a shared metadata lock on table t1.
#
# Session 2:
#
# 		mysql> ALTER TABLE t1 ADD COLUMN x INT, ALGORITHM=INPLACE, LOCK=NONE;
#
# The online DDL operation in session 2, which requires an exclusive metadata lock on table t1 to commit table definition changes,
# must wait for the session 1 transaction to commit or roll back.
#
# Session 3:
#
# 		mysql> SELECT * FROM t1;
#
# The SELECT statement issued in session 3 is blocked waiting for the exclusive metadata lock requested by the ALTER_TABLE operation
# in session 2 to be granted.
#
# You can use SHOW_FULL_PROCESSLIST to determine if transactions are waiting for a metadata lock.
#
# 		mysql> SHOW FULL PROCESSLIST\G
# 		/ETC/
# 		*********************' 2. row ******************************
# 				Id: 5
# 				User: root
# 				Host: localhost
# 				db: test
# 				Command: Query
# 				Time: 44
# 				State: Waiting for table metadata lock
# 				Info: ALTER TABLE t1 ADD COLUMN x INT, ALGORITHM=INPLACE, LOCK=NONE
# 		/ETC/
# 		********************** 4. row ******************************
# 				Id: 7
# 				User: root
# 				Host: localhost
# 				db: test
# 				Command: Query
# 				Time: 5
# 				State: Waiting for table metadata lock
# 				Info: SELECT * FROM t1
# 				4 rows in set (0.00 sec)
#
# Metadata lock information is also exposed through the Performance Schema metadata_locks table, which provides information about metadata lock
# dependencies between sessions, the metadata lock a session is waiting for, and the session that currently holds the metadata lock.
#
# For more information, see SECTION 26.12.12.3, "THE METADATA_LOCKS TABLE"
#
# ONLINE DDL PERFORMANCE
#
# The performance of a DDL operation is largely determined by whether the operation is performed instantly, in place, and whether it rebuilds the table.
#
# To assess the relative performance of a DDL operation, you can compare results using ALGORITHM=INSTANT, ALGORITHM=INPLACE, and ALGORITHM=COPY.
#
# A statement can also be run with old_alter_table enabled to force the use of ALGORITHM=COPY.
#
# For DDL operations that modify table data, you can determine whether a DDL operation performs changes in place or performs a table copy by looking
# at the "rows affected" value displayed after the command finishes.
#
# For example:
#
# 		) Changing the default value of a column (fast, does not affect the table data):
#
# 				Query OK, 0 rows affected (0.07 sec)
#
# 		) Adding an index (takes time, but 0 rows affected shows thta the table is not copied):
#
# 				Query OK, 0 rows affected (21.42 sec)
#
# 		) Changing the data type of a column (takes substantial time and requires rebuilding all the rows of the table):
#
# 				Query OK, 1671168 rows affected (1 min 35.54 sec)
#
# Before running a DDL operation on a large table, check whether the operation is fast or slow as follows:
#
# 		1. Clone the table structure
#
# 		2. Populate the cloned table with a small amount of data
#
# 		3. Run the DDL operation on the cloned table.
#
# 		4. Check whether the "rows affected" value is zero or not. A nonzero value means the operation copies table data, which might require special planning.
#
# 			For example, you might do the DDL operation during a period of scheduled downtime, or on each replication slave server one at a time.
#
# 			NOTE:
#
# 				For a greater understanding of the MySQL processing associated with a DDL operation, examine Performance Schema and INFORMATION_SCHEMA
# 				tables related to InnoDB before and after DDL operations to see the number of physical reads, writes, memory allocations and so on.
#
# 				Performance Schema stage events can be used to monitor ALTER_TABLE progress. See SECTION 15.15.1, "MONITORING ALTER TABLE PROGRESS FOR INNODB
# 				TABLES USING PERFORMANCE SCHEMA"
#
# Because there is some processing work involved with recording the changes made by concurrent DML operations, then applying those changes at the end,
# an online DDL operation could take longer overall than the table-copy mechanism that blocks table access from other sessions.
#
# The reduction in raw performance is balanced against better responsiveness for applications that use the table. When evaluating the techniques for
# changing table structure, consider end-user perception of performance, based on factors such as load times for web pages.
#
# 15.12.3 ONLINE DDL SPACE REQUIREMENTS
#
# Space requirements for in-place online DDL operations are outlined below. Space requirements do not apply to operations that are performed instantly.
#
# 		) Space for temporary log files
#
# 			A temporary log file records concurrent DML when an online DDL operation creates an index or alters a table. The temporary log file is extended
# 			as required by the value of innodb_sort_buffer_size up to a maximum specified by innodb_online_alter_log_max_size.
#
# 			If a temporary log file exceeds the size limit, the online DDL operation fails, and uncommitted concurrent DML operations are rolled back.
#
# 			A large innodb_online_alter_log_max_size setting permits more DML during an online DDL operation, but it also extends the period of time at
# 			the end of the DDL operation when the table is locked to apply logged DML.
#
# 			If the operation takes a long time and concurrent DML modifies the table so much that the size of the temporary log file exceeds the value of
# 			innodb_online_alter_log_max_size, the online DDL operation fails with a DB_ONLINE_LOG_TOO_BIG error.
#
# 		) Space for temporary sort files
#
# 			Online DDL operations that rebuild the table write temporary sort files to the MySQL temporary directory ($TMPDIR on Unix, %TEMP% on Windows,
# 			or the directory specified by --tmpdir) during index creation.
#
# 			Temporary sort files are not created in the directory that contains the original table. Each temporary sort file is large enough to hold
# 			all secondary index columns plus the primary key columns of the clustered index.
#
# 			Temporary sort files are removed as soon as their contents are merged into the final table or index.
#
# 			Temporary sort files may require space equal to the amount of data in the table plus indexes. An online DDL operation that
# 			rebuilds the table reports an error if it uses all of the available disk space on the file system where the data directory resides.
#
# 			If the MySQL temporary directory is not large enough to hold the sort files, set tmpdir to a different directory. Alternatively, define
# 			a separate temporary directory for online DDL operations using innodb_tmpdir.
#
# 			This option was introduced to help avoid temporary directory overflows that could occur as a result of large temporary sort files.
#
# 		) Space for an intermediate table file
#
# 			Some online DDL operations that rebuild the table create a temporary intermediate table file in the same directory as the original table.
#
# 			An intermediate table file may require space equal to the size of the original table. Intermediate table file names begin with #sql-ib
# 			prefix and only appear briefly during the online DDL operation.
#
# 			The innodb_tmpdir option is not applicable to intermediate table files.
#
# 15.12.4 SIMPLIFYING DDL STATEMENTS WITH ONLINE DDL
#
# Before the introduction of online DDL, it was common practice to combine many DDL operations into a single ALTER_TABLE statement.
# Because each ALTER_TABLE statement involved copying and rebuilding the table, it was more efficient to make several changes to the
# same table at once, since those changes could all be done with a single rebuild operation for the table.
#
# The downside was that SQL code involving DDL operations was harder to maintain and to reuse in different scripts.
#
# if the specific changes were different each time, you might have to construct a new complex ALTER_TABLE for each slightly different scenario.
#
# For DDL operations that can be done online, you can separate them into individual ALTER_TABLE statements for easier scripting and maintenance,
# without sacrificing efficiency.
#
# For example, you might take a complicated statement such as:
#
# 		ALTER TABLE t1 ADD INDEX i1(c1), ADD UNIQUE INDEX i2(c2),
# 			CHANGE c4_old_name c4_new_name INTEGER UNSIGNED;
#
# and break it down into simpler parts that can be tested and performed independently, such as:
#
# 		ALTER TABLE t1 ADD INDEX i1(c1);
# 		ALTER TABLE t1 ADD UNIQUE INDEX i2(c2);
# 		ALTER TABLE t1 CHANGE c4_old_name c4_new_name INTEGER UNSIGNED NOT NULL;
#
# You might still use multi-part ALTER_TABLE statements for:
#
# 		) Operations that must be performed in a specific sequence, such as creating an index followed by a foreign key constraint that uses that index.
#
# 		) Operations all using the same specific LOCK clause, that you want to either succeed or fail as a group.
#
# 		) Operations that cannot be performed online, that is, that still use the table-copy method.
#
# 		) Operations for which you specify ALGORITHM=COPY or old_alter_table=1, to force the table-copying behavior if needed for precise backward-compatibility
# 			in specialized scenarios.
#
# 15.12.5 ONLINE DDL FAILURE CONDITIONS
#
# The failure of an online DDL operation is typically due to one of the following conditions:
#
# 		) An ALGORITHM clause specifies an algorithm that is not compatible with the particular type of DDL operation or storage engine
#
# 		) A LOCK clause specifies a low degree of locking (SHARED or NONE) that is not compatible with the particular type of DDL operation
#
# 		) A timeout occurs while waiting for an exclusive lock on the table, which may be needed briefly during the initial and final phases
# 			of the DDL operation.
#
# 		) The tmpdir or innodb_tmpdir file system runs out of disk space, while MySQL writes temporary sort files on disk during index creation.
#
# 			For more information, see SECTION 15.12.3, "ONLINE DDL SPACE REQUIREMENTS"
#
# 		) The operation takes a long time and concurrent DML modifies the table so much that the size of the temporary online log exceeds the value of the
# 			innodb_online_alter_log_max_size configuration option.
#
# 			This condition causes a DB_ONLINE_LOG_TOO_BIG error.
#
# 		) Concurrent DML makes changes to the table that are allowed with the original table definition, but not with the new one.
#
# 			The operation only fails at the very end, when MySQL tries to apply all the changes from concurrent DML statements. For example,
# 			you might insert duplicate values into a column while a unique index is being created, or you might insert NULL values into a column
# 			while creating a primary key index on that column.
#
# 			The changes made by the concurrent DML take precedence, and the ALTER_TABLE operation is effectively rolled back.
#
# 15.12.6 ONLINE DDL LIMITATIONS
#
# The following limitations apply to online DDL operations:
#
# 		) The table is copied when creating an index on a TEMPORARY TABLE
#
# 		) The ALTER_TABLE clause LOCK=NONE is not permitted if there are ON /etc/ CASCADE or ON /ETC/ SET NULL constraints on the table.
#
# 		) Before an in-place online DDL operation can finish, it must wait for transactions that hold metadata locks on the table to commit
# 			or roll back.
#
# 			An online DDL operation may briefly require an exclusive metadata lock on the table during its execution phase, and always requires
# 			one in the final phase of the operation when updating the table definition.
#
# 			Consequently, transactions holding metadata locks on the table can cause an online DDL operation to block. The transactions that hold
# 			metadata locks on the table may have been started before or during the online DDL operation.
#
# 			A long running or inactive transaction that holds a metadata lock on the table can cause an online DDL operation to timeout.
#
# 		) When running an in-place online DDL operation, the thread that runs the ALTER_TABLE statement applies an online log of DML operations
# 			that were run concurrently on the same table from other connection threads.
#
# 			When the DML operations are applied, it is possible to encounter a duplicate key entry error (ERROR 1062 (23000): Duplicate Entry),
# 			even if the duplicate entry is only temporary and would be reverted by a later entry in the online log.
#
# 			This is similar to the idea of a foreign key constraint check in InnoDB in which constraints must hold during a transaction.
#
# 		) OPTIMIZE_TABLE for an InnoDB table is mapped to an ALTER_TABLE operation to rebuild the table and update index statistics and free unused
# 			space in the clustered index.
#
# 			Secondary indexes are not created as efficiently because keys are inserted in the order they appeared in the primary key.
#
# 			OPTIMIZE_TABLE is supported with the addition of online DDL support for rebuilding regular and partitioned InnoDB tables.
#
# 		) Tables created before MySQL 5.6 that include temporal columns (DATE, DATETIME or TIMESTAMP) and have not been rebuilt using
# 			ALGORITHM=COPY do not support ALGORITHM=INPLACE.
#
# 			In this case, an ALTER_TABLE_/ETC/_ALGORITHM=INPLACE operation returns the following error:
#
# 				ERROR 1846 (0A000): ALGORITHM=INPLACE is not supported
# 				Reason: Cannot change column type INPLACE. Try ALGORITHM=COPY
#
# 		) The following limitations are generally applicable to online DDL operations on large tables that involve rebuilding the table:
#
# 			) There is no mechanism to pause an online DDL operation or to throttle I/O or CPU usage for an online DDL operation
#
# 			) Rollback of an online DDL operation can be expensive should the operation fail
#
# 			) Long running DDL operations can cause replication lag. An online DDL operation must finish running on the master before it
# 				is run on the slave. Also, DML that was processed concurrently on the master is only processed on the slave after the DDL
# 				operation on the slave is completed.
#
# For additional information related to running online DDL operations on large tables, see SECTION 15.12.2, "ONLINE DDL PERFORMANCE AND CONCURRENCY"
#
# 15.13 INNODB STARTUP OPTIONS AND SYSTEM VARIABLES
#
# 		) System variables that are true or false can be enabled at server startup by naming them, or disabled by using a --skip- prefix.
#
# 			For example, to enable or disable the InnoDB adaptive hash index, you can use --innodb-adaptive-hash-index or --skip-innodb-adaptive-hash-index
# 			on the command line, or innodb_adaptive_hash_index or skip_innodb_adaptive_hash_index in an option file.
#
# 		) System variables that take a numeric value can be specified as --var-name=value on the command line or as var_name=value in option files.
#
# 		) Many system variables can be changed at runtime (see SECTION 5.1.9.2, "DYNAMIC SYSTEM VARIABLES")
#
# 		) For information about GLOBAL and SESSION variable scope modifiers, refer to the SET statement documentation.
#
# 		) Certain options control the locations and layout of the InnoDB data files. SECTION 15.8.1, "InnoDB STARTUP CONFIGURATION" explains how to use these options.
#
# 		) Some options, which you might not use initially, help tune InnoDB performance characteristics based on machine capacity and your database workload.
#
# 		) For more information on specifying options and system variables, see SECTION 4.2.2, "SPECIFYING PROGRAM OPTIONS"
#
# 	TABLE 15.25 InnoDB OPTTION AND VARIABLE REFERENCE
#
# 		NAME 														CMD-LINE 			OPTION FILE 			SYSTEM VAR 			STATUS VAR 		VAR SCOPE 		DYNAMIC
#
# 		daemon_memcached_enable_binlog 					Yes 					Yes 						Y 						- 					Global 			N
# 		daemon_memcached_engine_lib_name 				Y 						Y 							Y 						- 					Global 			N
# 		daemon_memcached_engine_lib_path 				Y 						Y 							Y 						- 					Global 			N
# 		daemon_memcached_option 							Y 						Y 							Y 						- 					Global 			N
# 		daemon_memcached_r_batch_size 					Y 						Y 							Y 						- 					Global 			N
# 		daemon_memcached_w_batch_size 					Y 						Y 							Y 						- 					Global 			N
# 	
# 		foreign_key_checks 									- 						- 							Y 						- 					Both 				Y
#
# 		ignore-builtin-innodb 								Y 						Y 							Y 						- 					Global 			N
# 		innodb 													Y 						Y 							- 						- 					- 					-
# 		innodb_adaptive_flushing 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_adaptive_flushing_lwm 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_adaptive_hash_index 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_adaptive_hash_index_parts 				Y 						Y 							Y 						- 					Global 			N
#
# 		innodb_adaptive_max_sleep_delay 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_api_bk_commit_interval 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_api_disable_rowlock 						Y 						Y 							Y 						- 					Global 			N
# 		innodb_api_enable_binlog 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_api_enable_mdl 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_api_trx_level 								Y  					Y 							Y 						- 					Global 			Y
# 		innodb_autoextend_increment 						Y 						Y 							Y 						- 					Global 			Y
#
# 		innodb_autoinc_lock_mode 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_available_undo_logs 						- 						- 							- 						Y 					Global 			N
# 		innodb_background_drop_list_empty 				Y 						Y 							Y 						- 					Global 			Y
# 		innodb_buffer_pool_bytes_data 					- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_bytes_dirty 					- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_chunk_size 					Y 						Y 							Y 						- 					Global 			N
# 		innodb_buffer_pool_debug 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_buffer_pool_dump_at_shutdown 			Y 						Y 							Y 						- 					Global 			Y
# 
# 		innodb_buffer_pool_dump_now 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_buffer_pool_dump_pct 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_buffer_pool_dump_status 					- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_filename 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_buffer_pool_in_core_file 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_buffer_pool_instances 						Y 						Y 							Y 						- 					Global 			N
# 		innodb_buffer_pool_load_abort 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_buffer_pool_load_at_startup 				Y 						Y 							Y 						- 					Global 			N
# 		innodb_buffer_pool_load_now 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_buffer_pool_load_status  					- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_pages_data 					- 						- 							- 						Y 					Global 			N
# 
# 		innodb_buffer_pool_pages_dirty 					- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_pages_flushed 				- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_pages_free 					- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_pages_latched 				- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_pages_misc 					- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_pages_total 					- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_read_ahead 					- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_read_ahead_evicted 			- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_read_ahead_rnd 				- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_read_requests 				- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_reads 							- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_resize_status 				- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_size 							Y 						Y 							Y 						- 					Global 			Y
# 
# 		innodb_buffer_pool_wait_free 						- 						- 							- 						Y 					Global 			N
# 		innodb_buffer_pool_write_requests 				- 						- 							- 						Y 					Global 			N
# 		innodb_change_buffer_max_size 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_change_buffering 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_change_buffering_debug 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_checkpoint_disabled 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_checksum_algorithm 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_cmp_per_index_enabled 						Y 						Y 							Y  					- 					Global 			Y
# 		innodb_commit_concurrency 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_compress_debug 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_compression_failure_threhsold_pct 		Y 						Y 							Y 						- 					Global 			Y
# 		innodb_compression_level 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_compression_pad_pct_mqx 					Y 						Y 							Y 						- 					Global 			Y
# 		
# 		innodb_concurrency_tickets 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_data_file_path  								Y 						Y 							Y 						- 					Global 			N
# 		innodb_data_fsyncs 									- 						- 							- 						Y 					Global 			N
# 		innodb_data_home_dir 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_data_pending_fsyncs 						- 						- 							- 						Y 					Global 			N
# 		innodb_data_pending_reads 							- 						- 							- 						Y 					Global 			N
# 		innodb_data_pending_writes 						- 						- 							- 						Y 					Global 			N
# 		innodb_data_read 										- 						- 							- 						Y 					Global 			N
# 		innodb_data_reads 									- 						- 							- 						Y 					Global 			N
# 
# 		innodb_data_writes 									- 						- 							- 						Y 					Global 			N
# 		innodb_data_written 									- 						- 							- 						Y 					Global 			N
# 		innodb_dblwr_pages_written 						- 						- 							- 						Y 					Global 			N
# 		innodb_dblwr_writes 									- 						- 							- 						Y 					Global 			N
# 		innodb_ddl_log_crash_reset_debug 				Y 						Y 							Y 						- 					Global 			Y
# 		innodb_deadlock_detect 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_dedicated_server 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_default_row_format 							Y 						Y 							Y  					- 					Global 			Y
#
# 		innodb_directories 									Y 						Y 							Y 						- 					Global 			N
# 		innodb_disable_sort_file_cache 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_doublewrite 									Y 						Y 							Y  					- 					Global 			N
# 		innodb_fast_shutdown 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_fil_make_page_dirty_debug 				Y 						Y 							Y 						- 					Global 			Y
# 		innodb_file_per_table 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_fill_factor 									Y 						Y 							Y 						- 					Global 			Y
# 		innodb_flush_log_at_timeout 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_flush_log_at_trx_commit 					Y 						Y 							Y 						- 					Global 			Y
#
# 		innodb_flush_method 									Y 						Y 							Y 						- 					Global 			N
# 		innodb_flush_neighbors 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_flush_sync 									Y 						Y 							Y 						- 					Global 			Y
# 		innodb_flushing_avg_loops 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_force_load_corrupted 						Y 						Y 							Y 						- 					Global 			N
# 		innodb_force_recovery 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_fsync_threshold 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_ft_aux_table 									- 						- 							Y 						- 					Global 			Y
# 		innodb_ft_cache_size 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_ft_enable_diag_print 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_ft_enable_stopword 							Y 						Y 							Y 						- 					Both 				Y
#
# 		innodb_ft_max_token_size 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_ft_min_token_size 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_ft_num_word_optimize 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_ft_result_cache_limit 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_ft_server_stopword_table 					Y 						Y 							Y 						- 					Global 			Y
#
# 		innodb_ft_sort_pll_degree 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_ft_total_cache_size 						Y 						Y 							Y 						- 					Global 			N
# 		innodb_ft_user_stopword_table 					Y 						Y 							Y 						- 					Both 				Y
# 		innodb_have_atomic_builtins 						- 						- 							- 						Y 					Global 			N
# 		innodb_io_capacity 									Y 						Y 							Y 						- 					Global 			Y
# 		innodb_io_capacity_max 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_limit_optimistic_insert_debug 			Y 						Y 							Y 						- 					Global 			Y
# 		innodb_lock_wait_timeout 							Y 						Y 							Y 						- 					Both 				Y
# 		innodb_log_buffer_size 								Y 						Y 							Y 						- 					Global 			Varies
#
# 		innodb_log_checkpoint_fuzzy_now 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_log_checkpoint_now 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_log_checksums 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_log_compressed_pages 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_log_file_size 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_log_files_in_group 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_log_group_home_dir 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_log_spin_cpu_abs_lwm 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_log_spin_cpu_pct_hwm 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_log_wait_for_flush_spin_hwm 				Y 						Y 							Y 						- 					Global 			Y
# 		innodb_log_waits 										- 						- 							- 						Y 					Global 			N
# 		innodb_log_write_ahead_size 						Y 						Y 							Y 						- 					Global 			Y
# 		
# 		innodb_log_write_requests 							- 						- 							- 						Y 					Global 			N
# 		innodb_log_writes 									- 						- 							- 						Y 					Global 			N
# 		innodb_lru_scan_depth 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_max_dirty_pages_pct 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_max_dirty_pages_pct_lwm 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_max_purge_lag 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_max_purge_lag_delay 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_max_undo_log_size 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_merge_threshold_set_all_debug 			Y 						Y 							Y 						- 					Global 			Y
# 		innodb_monitor_disable 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_monitor_enable 								Y 						Y 							Y 						- 					Global 			Y
# 
# 		innodb_monitor_reset 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_monitor_reset_all 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_num_open_files 								- 						- 							- 						Y 					Global 			N
# 		innodb_numa_interleave 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_old_blocks_pct 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_old_blocks_time 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_online_alter_log_max_size 				Y 						Y 							Y 						- 					Global 			Y
# 		innodb_open_files 									Y 						Y 							Y 						- 					Global 			N
# 		innodb_optimize_fulltext_only 					Y 						Y 							Y 						- 					Global 			Y
# 
# 		innodb_os_log_fsyncs 								- 						- 							- 						Y 					Global 			N
# 		innodb_os_log_pending_fsyncs 						- 						- 							- 						Y 					Global 			N
# 		innodb_os_log_pending_writes 						- 						- 							- 						Y 					Global 			N
# 		innodb_os_log_written  								- 						- 							- 						Y 					Global 			N
# 		innodb_page_cleaners 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_page_size 										- 						- 							-  					Y 					Global 			N
# 		innodb_page_size 										Y 						Y 							Y 						- 					Global 			N
# 		innodb_pages_created 								- 						- 							- 						Y 					Global 			N
# 		innodb_pages_read 									- 						- 							-  					Y 					Global 			N
# 		innodb_pages_written 								- 						- 							- 						Y 					Global 			N
#
# 		innodb_parallel_read_threads 						Y 						Y 							Y 						- 					Session 			Y
# 		innodb_print_all_deadlocks 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_print_ddl_logs 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_purge_batch_size 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_purge_rseg_truncate_frequency 			Y 						Y 							Y 						- 					Global 			Y
# 		innodb_purge_threads 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_random_read_ahead 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_read_ahead_threshold 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_read_io_threads 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_read_only 										Y 						Y 							Y 						- 					Global 			N
# 		innodb_redo_log_archive_dirs 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_redo_log_encrypt 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_replication_delay 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_rollback_on_timeout 						Y 						Y 							Y 						- 					Global 			N
# 		innodb_rollback_segments 							Y 						Y 							Y 						- 					Global 			Y
#
# 		innodb_row_lock_current_waits 					- 						- 							-- 					Y 					Global 			N
# 		innodb_row_lock_time 								- 						- 							- 						Y 					Global 			N
# 		innodb_row_lock_time_avg 							- 						- 							- 						Y 					Global 			N
# 		innodb_row_lock_time_max  							- 						- 							- 						Y 					Global 			N
# 		innodb_row_lock_waits 								- 						- 							- 						Y 					Global 			N
#
# 		innodb_rows_deleted 									- 						- 							- 						Y 					Global 			N
# 		innodb_rows_inserted 								- 						- 							- 						Y 					Global 			N
# 		innodb_rows_read 										- 						- 							- 						Y 					Global 			N
# 		innodb_rows_updated 									- 						- 							- 						Y 					Global 			N
# 		innodb_saved_page_number_debug 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_scan_directories 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_sort_buffer_size 							Y 						Y 							Y 						- 					Global 			N
# 		innodb_spin_wait_delay 								Y 						Y 							Y 						- 					Global 			Y
#
# 		innodb_spin_wait_pause_multiplier 				Y 						Y 							Y 						- 					Global 			Y
# 		innodb_stats_auto_recalc 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_stats_include_delete_marked 				Y 						Y 							Y 						- 					Global 			Y
# 		innodb_stats_method 									Y 						Y 							Y 						- 					Global 			Y
# 		innodb_stats_on_metadata 							Y 						Y 							Y 						-  				Global 			Y
# 		innodb_stats_persistent 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_stats_persistent_sample_pages 			Y 						Y 							Y 						- 					Global 			Y
# 		innodb_stats_transient_sample_pages 			Y 						Y 							Y 						- 					Global 			Y
#
# 		innodb-status-file 									Y 						Y 							- 						-  				- 					-
# 		innodb_status_output 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_status_output_locks 						Y 						Y 							Y 						- 					Global 			Y
# 		innodb_strict_mode 									Y 						Y 							Y 						- 					Both 				Y
# 		innodb_sync_array_size 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_sync_debug 									Y 						Y 							Y 						- 					Global 			N
# 		innodb_sync_spin_loops 								Y 						Y 							Y 						- 					Global 			Y
# 		innodb_table_locks 									Y 						Y 							Y 						- 					Both 				Y
# 		innodb_temp_data_file_path 						Y 						Y 							Y 						- 					Global 			N
# 		innodb_temp_tablespaces_dir 						Y 						Y 							Y 						- 					Global 			N
# 		innodb_thread_concurrency 							Y 						Y 							Y 						- 					Global 			Y
# 
# 		innodb_thread_sleep_delay 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_tmpdir 											Y 						Y 							Y 						- 					Both 				Y
# 		innodb_truncated_status_writes 					- 						- 							- 						Y 					Global 			N
#
# 		innodb_trx_purge_view_update_only_debug 		Y 						Y 							Y 						- 					Global 			Y
# 		innodb_trx_rseg_n_slots_debug 					Y 						Y 							Y 						- 					Global 			Y
# 		innodb_undo_directory 								Y 						Y 							Y  					- 					Global 			N
# 		innodb_undo_log_encrypt 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_undo_log_truncate 							Y 						Y 							Y 						- 					Global 			Y
# 		innodb_undo_logs 										Y 						Y 							Y 						- 					Global 			Y
# 		innodb_undo_tablespaces 							Y 						Y 							Y 						- 					Global 			Varies
# 		innodb_use_native_aio 								Y 						Y 							Y 						- 					Global 			N
# 		innodb_version 										- 						- 							Y 						- 					Global 			N
# 		innodb_write_io_threads 							Y 						Y 							Y 						- 					Global 			N
# 		unique_checks 											- 						- 							Y 						- 					Both 				Y
#
# InnoDB COMMAND OPTIONS
#
# 		) --ignore-builtin-innodb
#
# 			Property 		Value
#
# 			Cmd line 		--ignore-builtin-innodb[={OFF|ON}]
# 			Deprecated 		Yes (removed in 8.0.3)
# 			Sys var 			ignore_builtin_innodb
# 			Scope 			Global
# 			Dynamic 			N
# 			SET_VAR Hint 	N
# 			Type 				Boolean
#
# 			In earlier versions of MySQL, this option caused the server to behave as if the built-in InnoDB were not present, which enabled
# 			the InnoDB Plugin to be used instead.
#
# 			In MySQL 8.0, InnoDB is the default storage engine and InnoDB Plugin is not used. This option was removed in MySQL 8.0
#
# 		) --innodb[=value]
#
# 			Property 		Value
#
# 			Cmd line 		--innodb[=value]
# 			Deprecated 		Y
# 			Type 				Enumeration
# 			Default 			ON
# 			Valid 			OFF ON FORCE
#
# 			Controls loading of the InnoDB storage engine, if the server was compiled with InnoDB support.
#
# 			THis option has a tristate format, with possible values of OFF, ON or FORCE.
#
# 			See SECTION 5.6.1, "INSTALLING AND UNINSTALLING PLUGINS"
#
# 			To disable InnoDB, use --innodb=OFF or --skip-innodb. IN this case, because the default storage engine is InnoDB,
# 			the server does not start unless you also use --default-storage-engine and --default-tmp-storage-engine to set the
# 			default to some other engine for both permanent and TEMPORARY tables.
#
# 			The InnoDB storage engine can no longer be disabled, and the --innodb=OFF and --skip-innodb options are deprecated and have
# 			no effect. Their use results in a warning.
#
# 			These options will be removed in a future MySQL release.
#
# 		) --innodb-status-file
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-status-file[={OFF|ON}]
# 			Type 				Boolean
# 			Default 			OFF
#
# 			The --innodb-status-file startup option controls whether InnoDB creates a file named innodb_status.pid in the data directory
# 			and writes SHOW_ENGINE_INNODB_STATUS output to it every 15 seconds, approximately.
#
# 			The innodb_status.pid file is not created by default. To create it, start mysqld with the --innodb-status-file option.
#
# 			InnoDB removes the file when the server is shut down normally. If an abnormal shutdown occurs, the status file may have
# 			to be removed manually.
#
# 			The --innodb-status-file option is intended for temporary use, as SHOW_ENGINE_INNODB_STATUS output generation can affect
# 			performance, and the innodb_status.pid file can become quite large over time.
#
# 			For related information, see SECTION 15.16.2, "ENABLING InnoDB MONITORS"
#
# 		) --skip-innodb
#
# 			Disable the InnoDB storage engine. See the description of --innodb.
#
# InnoDB SYSTEM VARIABLES
#
# 		) daemon_memcached_enable_binlog
#
# 			Property 		Value
#
# 			Cmd line 		--daemon-memcached-enable-binlog[={OFF|ON}]
# 			Sys var 			daemon_memcached_enable_binlog
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_VAR Hint 	No
# 			Type 				Boolean
# 			Default 			OFF
#
# 			Enable this option on the master server to use the InnoDB memcached plugin (daemon_memcached) with the MySQL binary log.
#
# 			This option can only be set at server startup. You must also enable the MySQL binary log on the master server using
# 			the --log-bin option.
#
# 			For more information, see SECTION 15.19.7, "THE INNODB MEMCACHED PLUGIN AND REPLICATION"
#
# 		) daemon_memcached_engine_lib_name
#
# 			Property 		Value
#
# 			Cmd line 		--daemon-memcached-engine-lib-name=file_name
# 			Sys var 			daemon_memcached_engine_lib_name
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_VAR Hint 	No
# 			Type 				File name
# 			Default 			innodb_engine.so
#
# 			Specifies the shared library that implements the InnoDB memcached plugin.
#
# 			For more information, see SECTION 15.19.3, "SETTING UP THE INNODB MEMCACHED PLUGIN"
#
# 		) daemon_memcached_engine_lib_path
#
# 			Property 		Value
#
# 			Cmd line 		--daemon-memcached-engine-lib-path=dir_name
# 			Sys var 			daemon_memcached_engine_lib_path
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_VAR Hint 	No
# 			Type 				Dir name
# 			Default 			NULL
#
# 			The path of the directory containing the shared library that implements the InnoDB memcached plugin.
# 			The default value is NULL, representing the MySQL plugin dir.
#
# 			You should not need to modify this parameter unless specifying a memcached plugin for a different storage
# 			engine that is located outside of the MySQL plugin dir.
#
# 			For more information, see SECTION 15.19.3, "SETTING UP THE INNODB MEMCACHED PLUGIN"
#
# 		) daemon_memcached_option
#
# 			Property 		Value
#
# 			Cmd line 		--daemon-memcached-option=options
# 			Sys var 			daemon_memcached_option
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_VAR hint 	No
# 			Type 				String
# 			Default 			-
#
# 			Used to pass space-separated memcached options to the underlying memcached memory object caching daemon on startup.
#
# 			For example, you might change the port that memcached listens on, reduce the maximum number of simultaneous connections,
# 			change the maximum memory size for a key-value pair, or enable debugging messages for the error log.
#
# 			See SECTION 15.19.3, "SETTING UP THE INNODB MEMCACHED PLUGIN" for usage details. For information about memcached options,
# 			refer to the memcached man page.
#
# 		) daemon_memcached_r_batch_size
#
# 			Property 		Value
#
# 			Cmd line 		--daemon-memcached-r-batch-size=#
# 			Sys var 			daemon_memcached_r_batch_size
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_VAR Hint 	No
# 			Type 				Integer
# 			Default 			1
#
# 			Specifies how many memcached read operations (get operations) to perform before doing a COMMIT to start a new transaction.
# 			Counterpart of daemon_memcached_w_batch_size.
#
# 			This value is set to 1 by default, so that any changes made to the table through SQL statements are immediately visible to
# 			memcached operations.
#
# 			You might increase it to reduce the overhead from frequent commits on a system where the underlying table is only being accessed
# 			through the memcached interface.
#
# 			If you set the value too large, the amount of undo or redo data could impose some storage overhead, as with any long-running
# 			transaction.
#
# 			For more information, see SECTION 15.19.3, "SETTING UP THE INNODB MEMCACHED PLUGIN"
#
# 		) daemon_memcached_w_batch_size
#
# 			Property 		Value
#
# 			Cmd line 		--daemon-memcached-w-batch-size=#
# 			Sys var 			daemon_memcached_w_batch_size
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_VAR Hint 	No
# 			Type 				Integer
# 			Default 			1
#
# 			Specifies how many memcached write operations, such as add, set and incr, to perform before doing a COMMIT to start a new transaction.
#
# 			Counterpart of daemon_memcached_r_batch_size
#
# 			This value is set to 1 by default, on the assumption that data being stored is important to preserve in case of an outage and should
# 			immediately be committed.
#
# 			When storing non-critical data, you might increase this value to reduce the overhead from frequent commits; but then the last N-1 
# 			uncommitted write operations could be lost if a crash occurs.
#
# 			For more information, see SECTION 15.19.3, "SETTING UP THE INNODB MEMCACHED PLUGIN"
#
# 		) ignore_builtin_innodb
#
# 			Property 		Value
#
# 			Cmd line 		--ignore-builtin-innodb[={OFF|ON}]
# 			Deprecated 		Yes (removed in 8.0.3)
# 			Sys var 			ignore_builtin_innodb
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_VAR Hint 	No
# 			Type 				Boolean
#
# 			See the description of --ignore-builtin-innodb under "InnoDB Command Options" earlier in this section.
#
# 			This variable was removed in MySQL 8.0
#
# 		) innodb_adaptive_flushing
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-adaptive-flushing[={OFF|ON}]
# 			Sys var 			innodb_adaptive_flushing
# 			Scope 			Global
# 			Dynamic 			Yes
# 			SET_VAR Hint 	No
# 			Type 				Boolean
# 			Default 			ON
#
# 			Specifies whether to dynamically adjust the rate of flushing dirty pages in the InnoDB buffer pool based on the workload.
#
# 			Adjusting the flush rate dynamically is intended to avoid bursts of I/O activity. This setting is enabled by default.
#
# 			See SECTION 15.8.3.5, "CONFIGURING INNODB BUFFER POOL FLUSHING" for more information.
#
# 			For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING InnoDB DISK I/O"
#
# 		) innodb_adaptive_flushing_lwm
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-adaptive-flushing-lwm=#
# 			Sys var 			innodb_adaptive_flushing_lwm
# 			Scope 			Global
# 			Dynamic 			Yes
# 			SET_VAR Hint 	No
# 			Type 				Integer
# 			Default 			10
# 			Min 				0
# 			Max 				70
#
# 			Defines the low water mark representing percentage of redo log capacity at which adaptive flushing is enabled.
#
# 			For more information, see SECTION 15.8.3.6, "FINE-TUNING InnoDB BUFFER POOL FLUSHING"
#
# 		) innodb_adaptive_hash_index
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-adaptive-hash-index[={OFF|ON}]
# 			Sys var 			innodb_adaptive_hash_index
# 			Scope 			Global
# 			Dynamic 			Yes
# 			SET_VAR Hint 	No
# 			Type 				Boolean
# 			Default 			ON
#
# 			Whether the InnoDB adaptive hash index is enabled or disabled. It may be desirable, depending on your workload, to dynamically
# 			enable or disable adaptive hash indexing to improve query performance.
#
# 			Because the adaptive hash index may not be useful for all workloads, conduct benchmarks with it both enabled and disabled,
# 			using realistic workloads. See SECTION 15.5.3, "ADAPTIVE HASH INDEX" for details.
#
# 			This variable is enabled by default. You can modify this parameter using the SET GLOBAL statement, without restarting the server.
#
# 			Changing the setting at runtime requires privileges sufficient to set global system variables. See SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES"
#
# 			You can also use --skip-innodb-adaptive-hash-index at server startup to disable it.
#
# 			Disabling the adaptive hash index empties the hash table immediately. Normal operations can continue while the hash table is emptied, and executing
# 			queries that were using the hash table access the index B-trees directly instead.
#
# 			When the adaptive hash index is re-enabled, the hash table is populated again during normal operation.
#
# 		) innodb_adaptive_hash_index_parts
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-adaptive-hash-index-parts=#
# 			Sys var 			innodb_adaptive_hash_index_parts
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_VAR Hint 	No
# 			Type 				Numeric
# 			Default 			8
# 			Min 				1
# 			Max 				512
#
# 			Partitions the adaptive hash index search system. Each index is bound to a specific partition, with each partition protected by a separate latch.
#
# 			The adaptive hash index search system is partitioned into 8 parts by default. The maximum setting is 512.
#
# 			For related information, see SECTION 15.5.3, "ADAPTIVE HASH INDEX"
#
# 		) innodb_adaptive_max_sleep_delay
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-adaptive-max-sleep-delay=#
# 			Sys var 			innodb_adaptive_max_sleep_delay
# 			Scope 			Global
# 			Dynamic 			Yes
# 			SET_VAR Hint 	No
# 			Type 				Integer
# 			Default 			15000
# 			Min  				0
# 			Max 				1000000
#
# 			Permits InnoDB to automatically adjust the value of innodb_thread_sleep_delay up or down according to the current workload.
# 			Any nonzero value enables automated, dynamic adjustment of the innodb_thread_sleep_delay value, up to the maximum value specified
# 			in the innodb_adaptive_max_sleep_delay option.
#
# 			The value represents the number of microseconds. This option can be useful in busy systems, with greater than 16 InnoDB threads.
#
# 			(In practice, it is most valuable for MySQL systems with hundreds or thousands of simultaneous connections)
#
# 			For more information, see SECTION 15.8.4, "CONFIGURING THREAD CONCURRENCY FOR INNODB"
#
# 		) innodb_api_bk_commit_interval
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-api-bk-commit-interval=#
# 			Sys var 			innodb_api_bk_commit_interval
# 			Scope 			Global
# 			Dynamic 			Yes
# 			SET_VAR Hint 	No
# 			Type 				Integer
# 			Default 			5
# 			Min 				1
# 			Max 				1073741824
#
# 			How often to auto-commit idle connections that use the InnoDB memcached interface, in seconds. For more information, 
# 			see SECTION 15.19.6.4, "CONTROLLING TRANSACTIONAL BEHAVIOR OF THE INNODB MEMCACHED PLUGIN"
#
# 		) innodb_api_disable_rowlock
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-api-disable-rowlock[={OFF|ON}]
# 			Sys var 			innodb_api_disable_rowlock
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_Var Hint 	No
# 			Type 				Boolean
# 			Default 			OFF
#
# 			Use this option to disable row locks when InnoDB memcached performs DML operations. By default, innodb_api_disable_rowlock is disabled,
# 			which means that memcached requests row locks for get and set operations.
#
# 			When innodb_api_disable_rowlock is enabled, memcached requests a table lock instead of row locks.
#
# 			innodb_api_disable_rowlock is not dynamic. It must be specified on the mysqld command line or entered in the MySQL configuration file.
#
# 			Configuration takes effect when the plugin is installed, which occurs when the MySQL server is started.
#
# 			For more information, see SECTION 15.19.6.4, "CONTROLLING TRANSACTIONAL BEHAVIOR OF THE INNODB MEMCACHED PLUGIN"
#
# 		) innodb_api_enable_binlog
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-api-enable-binlog[={OFF|ON}]
# 			Sys var 			innodb_api_enable_binlog
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_VAR Hint 	No
# 			Type 				Boolean
# 			Default 			OFF
#
# 			Lets you use the InnoDB memcached plugin with the MySQL binary log. For more information, see ENABLING THE INNODB MEMCACHED BINARY LOG.
#
# 		) innodb_api_enable_mdl
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-api-enable-mdl[={OFF|ON}]
# 			Sys var 			innodb_api_enable_mdl
# 			Scope 			Global
# 			Dynamic 			No
# 			SET_VAR Hint 	No
# 			Type 				Boolean
# 			Default 			OFF
#
# 			Locks the table used by the InnoDB memcached plugin, so that it cannot be dropped or altered by DDL through the SQL interface.
#
# 			For more information, see SECTION 15.19.6.4, "CONTROLLING TRANSACTIONAL BEHAVIOR OF THE INNODB MEMCACHED PLUGIN"
#
# 		) innodb_api_trx_level
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-api-trx-level=#
# 			Sys var 			innodb_api_trx_level
# 			Scope 			Global
# 			Dynamic 			Yes
# 			SET_VAR Hint 	No
# 			Type 				Integer
# 			Default 			0
#
# 			Controls the transaction isolation level on queries processed by the memcached interface.
#
# 			The constants corresponding to the familiar names are:
#
# 				) 0 = READ_UNCOMMITTED
#
# 				) 1 = READ_COMMITTED
#
# 				) 2 = REPEATABLE_READ
#
# 				) 3 = SERIALIZABLE
#
# 			For more information, see SECTION 15.19.6.4, "CONTROLLING TRANSACTIONAL BEHAVIOR OF THE INNODB MEMCACHED PLUGIN"
#
# 		) innodb_autoextend_increment
#
# 			Property 		Value
#
# 			Cmd line 		--innodb-autoextend-increment=#
# 			Sys var 			innodb_autoextend_increment
# 			Scope 			Global
# 			Dynamic 			Yes
# 			SET_VAR Hint 	No
# 			Type 				Integer
# 			Default 			64
# 			Min 				1
# 			Max 				1000
#
# 			The increment size (in megabytes) for extending the size of an auto-extending InnoDB system tablespace file when it
# 			becomes full.
#
# 			The default value is 64. For related information, see SYSTEM TABLESPACE DATA FILE CONFIGURATION, and RESIZING THE SYSTEM TABLESPACE.
#
# 			The innodb_autoextend_increment setting does not affect file-per-table tablespace files or general tablespace files.
#
# 			These files are auto-extending regardless of the innodb_autoextend_increment setting. The initial extensions are by small amounts,
# 			after which extensions occur in increments of 4MB.
#
# 		) innodb_autoinc_lock_mode
#
# 			Property 			Value
#
# 			Cmd line 			--innodb-autoinc-lock-mode=#
# 			Sys var 				innodb_autoinc_lock_mode
# 			Scope 				Global
# 			Dynamic 				No
# 			SET_VAR Hint 		No
# 			Type 					Integer
# 			Default (>= 8.0.3) 2
# 			Default (<= 8.0.2) 1
# 			Valid 				 0 1 2
#
# 			The lock mode to use for generating auto-increment values. Permissible values are 0, 1, or 2, for traditional, consecutive,
# 			or interleaved, respectively.
#
# 			The default setting is 2 (interleaved) as Of MySQL 8.0, and 1 (consecutive) before that. The change to interleaved lock mode
# 			as the default setting reflects the change from statement-based to row-based replication as the default replication type,
# 			which occurred in MySQL 5.7.
#
# 			Statement-based replication requires the consecutive auto-increment lock mode to ensure that auto-increment values are assigned
# 			in a predictable and repeatable order for a given sequence of SQL statements, whereas row-based replication is not sensitive
# 			to the execution order of SQL statements.
#
# 			For the characteristics of each lock mode, see InnoDB AUTO_INCREMENT LOCK MODES
#
# 		) innodb_background_drop_list_empty
#
# 			Property 			Value
#
# 			Cmd line 			--innodb-background-drop-list-empty[={OFF|ON}]
# 			Sys var 				innodb_background_drop_list_empty
# 			Scope 				Global
# 			Dynamic 				Yes
# 			SET_VAR Hint 		No
# 			Type 					Boolean
# 			Default 				OFF
#
# 			Enabling the innodb_background_drop_list_empty debug option helps avoid test case failures by delaying table creation
# 			until the background drop list is empty.
#
# 			For example, if test case A places table t1 on the background drop list, test case B waits until the background drop list
# 			is empty before creating table t1.
#
# 		) innodb_buffer_pool_chunk_size
#
# 			Property 			Value
#
# 			cmd line 			--innodb-buffer-pool-chunk-size=#
# 			Sys Var 				innodb_buffer_pool_chunk_size
# 			Scope 				Global
# 			Dynamic 				No
# 			SET_VAR Hint 		No
# 			Type 					Integer
# 			Default 				134217728
# 			Min 					1048576
# 			Max 					innodb_buffer_pool_size / innodb_buffer_pool_instances
#
# 			innodb_buffer_pool_chunk_size defines the chunk size for InnoDB buffer pool resizing operations.
#
# 			The innodb_buffer_pool_size parameter is dynamic, which allows you to resize the buffer pool
# 			without restarting the server.
#
# 			To avoid copying all buffer pool pages during resizing operations, the operation is performed in "chunks".
#
# 			By default, innodb_buffer_pool_chunk_size is 128MB (134217728 bytes).
#
# 			The number of pages contained in a chunk depends on the value of innodb_page_size. innodb_buffer_pool_chunk_size
# 			can be increased or decreased in units of 1MB (1048576 bytes)
#
# 			The following conditions apply when altering the innodb_buffer_pool_chunk_size value:
#
# 				) If innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances is larger than the current buffer pool size when the buffer
# 					pool is initialized, innodb_buffer_pool_chunk_size is truncated to innodb_buffer_pool_size / innodb_buffer_pool_instances
#
# 				) Buffer pool size must always be equal to or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances.
#
# 					If you alter innodb_buffer_pool_chunk_size, innodb_buffer_pool_size is automatically rounded to a value that is equal to
# 					or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances.
#
# 					The adjustment occurs when the buffer pool is initialized.
#
# 						IMPORTANT:
#
# 							Care should be taken when changing innodb_buffer_pool_chunk_size, as changing this value can automatically increase
# 							the size of the buffer pool.
#
# 							Before changing innodb_buffer_pool_chunk_size, calculate the effect it will have on innodb_buffer_pool_size to ensure that
# 							the resulting buffer pool size is acceptable.
#
# 					To avoid potential performance issues, the number of chunks (innodb_buffer_pool_size / innodb_buffer_pool_chunk_size) should not exceed 1000.
#
# 					See SECTION 15.8.3.1, "CONFIGURING INNODB BUFFER POOL SIZE" for more information.
#
# 				) innodb_buffer_pool_debug
#
# 					Property 						Value
#
# 					Cmd line 						--innodb-buffer-pool-debug[={OFF|ON}]
# 					Sys Var 							innodb_buffer_pool_debug
# 					Scope 							Global
# 					Dynamic 							No
# 					SET_VAR Hint 					No
# 					Type 								Boolean
# 					Default 							OFF
#
# 					Enabling this option permits multiple buffer pool instances when the buffer pool is less than 1GB in size, ignoring the 1GB minimum
# 					buffer pool size constraint imposed on innodb_buffer_pool_instances.
#
# 					The innodb_buffer_pool_debug option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.
#
# 				) innodb_buffer_pool_dump_at_shutdown
#
# 					Property 						Value
#
# 					Cmd line 						--innodb-buffer-pool-dump-at-shutdown[={OFF|ON}]
# 					Sys var 							innodb_buffer_pool_dump_at_shutdown
# 					Scope 							Global
# 					Dynamic 							Yes
# 					SET_VAR Hint 					No
# 					Type 								Boolean
# 					Default 							ON
#
# 					Specifies whether to record the pages cached in the InnoDB buffer pool when the MySQL server is shut down,
# 					to shorten the warmup process at the next restart.
#
# 					Typically used in combination with innodb_buffer_pool_load_at_startup. The innodb_buffer_pool_dump_pct option defines
# 					the percentage of most recently used buffer pool pages to dump.
#
# 					Both innodb_buffer_pool_dump_at_shutdown and innodb_buffer_pool_load_at_startup are enabled by default.
#
# 					For more information, see SECTION 15.8.3.7, "SAVING AND RESTORING THE BUFFER POOL STATE"
#
# 				) innodb_buffer_pool_dump_now
#
# 					Property 						Value
# 			
# 					Cmd line 						--innodb-buffer-pool-dump-now[={OFF|ON}]
# 					Sys var 							innodb_buffer_pool_dump_now
# 					Scope 							Global
# 					Dynamic 							Yes
# 					SET_VAR Hint 					No
# 					Type 								Boolean
# 					Default 							OFF
#
# 					Immediately records the pages cached in the InnoDB buffer pool. Typically used in combination
# 					with innodb_buffer_pool_load_now.
#
# 					For more information, see SECTION 15.8.3.7, "SAVING AND RESTORING THE BUFFER POOL STATE"
#
# 				) innodb_buffer_pool_dump_pct
#
# 					Property 						Value
#
# 					Cmd line 						--innodb-buffer-pool-dump-pct=#
# 					Sys var 							innodb_buffer_pool_dump_pct
# 					Scope 							Global
# 					Dynamic 							Yes
# 					SET_VAR Hint 					No
# 					Type 								Integer
# 					Default 							25
# 					Min 								1
# 					Max 								100
#
# 					Specifies the percentage of the most recently used pages for each buffer pool to read out and dump.
#
# 					The range is 1 to 100. The default value is 25. For example, if there are 4 buffer pools with 100 pages
# 					each, and innodb_buffer_pool_dump_pct is set to 25, the 25 most recently used pages from each buffer pool are dumped.
#
# 				) innodb_buffer_pool_filename
#
# 					Property 						Value
#
# 					Cmd line 						--innodb-buffer-pool-filename=file_name
# 					Sys var 							innodb_buffer_pool_filename
# 					Scope 							Global
# 					Dynamic 							Yes
# 					SET_VAR hint 					No
# 					Type 								File name
# 					Default value 					ib_buffer_pool
#
# 					Specifies the name of the file that holds the list of tablespace IDs and page IDs produced by innodb_buffer_pool_dump_at_shutdown
# 					or innodb_buffer_pool_dump_now.
#
# 					Tablespace IDs and page IDs are saved in the following format:
#
# 						space, page_id
#
# 					By default, the file is named ib_buffer_pool and is located in the InnoDB data directory.
# 					A non-default location must be specified relative to the data directory.
#
# 					A file name can be specified at runtime, using a SET statement:
#
# 						SET GLOBAL innodb_buffer_pool_filename='file_name';
#
# 					You can also specify a file name at startup, in a startup string or MySQL configuration file.
#
# 					When specifying a file name at startup, the file must exist or InnoDB will return a startup error
# 					indicating that there is no such file or directory.
#
# 					For more information, see SECTION 15.8.3.7, "SAVING AND RESTORING THE BUFFER POOL STATE"
#
# 			) innodb_buffer_pool_in_core_file
#
# 					Property 					Value
#
# 					cmd line 					--innodb-buffer-pool-in-core-file[={OFF|ON}]
# 					Introduced 					8.0.14
# 					Sys var 						innodb_buffer_pool_in_core_file
# 					Scope 						Global
# 					Dynamic 						Yes
# 					SET_VAR Hint 				No
# 					Type 							Boolean
# 					Default 						ON
#
# 					Disabling the innodb_buffer_pool_in_core_file variable reduces the size of core files by excluding InnoDB buffer pool pages.
#
# 					To use this variable, the core_file variable must be enabled and the operating system must support the MADV_DONTDUMP non-POSIX
# 					extension to madvise(), which is supported in Linux 3.4 and later.
#
# 					For more information, see SECTION 15.8.3.8, "EXCLUDING BUFFER POOL PAGES FROM CORE FILES"
#
# 			) innodb_buffer_pool_instances
#
# 					Property 					Value
#
# 					Cmd line 					--innodb-buffer-pool-instances=#
# 					Sys var 						innodb_buffer_pool_instances
# 					Scope 						Global
# 					Dynamic 						No
# 					SET_VAR Hint 				No
# 					Type 							Integer
# 					Default (Other) 			8 (or 1 if innodb_buffer_pool_size < 1GB)
# 					Default (Windows, 32-bit) (autosized)
# 					Min 							1
# 					Max 							64
#
# 					The number of regions that the InnoDB buffer pool is divided into. For systems with buffer pools in the multi-gigabyte range, dividing
# 					the buffer pool into separate instances can improve concurrency, by reducing contention as different threads read and write to cached pages.
#
# 					Each page that is stored in or read from the buffer pool is assigned to one of the buffer pool instances randomly, using a hashing function.
#
# 					Each buffer pool manages its own free lists, flush lists, LRUs, and all other data structures connected to a buffer pool, and is protected
# 					by its own buffer pool mutex.
#
# 					This option only takes effect when setting innodb_buffer_pool_size to 1GB or more. The total buffer size is divided among all the buffer pools.
#
# 					For best efficiency, specify a combination of innodb_buffer_pool_instances and innodb_buffer_pool_size so that each buffer pool instance is at least 1GB.
#
# 					The default value on 32-bit Windows systems depends on the value of innodb_buffer_pool_size, as described below:
#
# 						) If innodb_buffer_pool_size is greater than 1.3GB, the default for innodb_buffer_pool_instances is innodb_buffer_pool_size/128MB, with individual
# 							memory allocation requests for each chunk.
#
# 							1.3GB was chosen as the boundary at which there is significant risk for 32-bit Windows to be unable to allocate the contiguous address space
# 							needed for a single buffer pool.
#
# 						) Otherwise, the default is 1.
#
# 					On all other platforms, the default value is 8 when innodb_buffer_pool_size is greater than or equal to 1GB. Otherwise, the default is 1.
#
# 					For related information, see SECTION 15.8.3.1, "CONFIGURING InnoDB BUFFER POOL SIZE"
#
# 			) innodb_buffer_pool_load_abort
#
# 					Property 				Value
#
# 					Cmd line 				--innodb-buffer-pool-load-abort[={OFF|ON}]
# 					Sys Var 					innodb_buffer_pool_load_abort
# 					Scope 					Global
# 					Dynamic 					Yes
# 					SET_VAR Hint 			No
# 					Type 						Boolean
# 					Default 					OFF
#
# 					Interrupts the process of restoring InnoDB buffer pool contents triggered by innodb_buffer_pool_load_at_startup or innodb_buffer_pool_load_now
#
# 					For more information, see SECTION 15.8.3.7, "SAVING AND RESTORING THE BUFFER POOL STATE"
#
# 			) innodb_buffer_pool_load_at_startup
#
# 					Property 				Value
#
# 					Cmd line 				--innodb-buffer-pool-load-at-startup[={OFF|ON}]
# 					Sys Var 					innodb_buffer_pool_load_at_startup
# 					Scope 					Global
# 					Dynamic 					No
# 					SET_VAR Hint 			No
# 					Type 						Boolean
# 					Default 					ON
#
# 					Specifies that, on MySQL server startup, the InnoDB buffer pool is automatically warmed up by loading the same pages it held
# 					at an earlier time.
#
# 					Typically used in combination with innodb_buffer_pool_dump_at_shutdown
#
# 					Both innodb_buffer_pool_dump_at_shutdown and innodb_buffer_pool_load_at_startup are enabled by default.
#
# 					For more information, see SECTION 15.8.3.7, "SAVING AND RESTORING THE BUFFER POOL STATE"
#
# 			) innodb_buffer_pool_load_now
#
# 					Property 				Value
#
# 					Cmd line 				--innodb-buffer-pool-load-now[={OFF|ON}]
# 					Sys var 					innodb_buffer_pool_load_now
# 					Scope 					Global
# 					Dynamic 					Yes
# 					SET_VAR Hint 			No
# 					Type 						Boolean
# 					Default 					OFF
#
# 					Immediately warms up the InnoDB buffer pool by loading a set of data pages, without waiting for a server restart.
#
# 					Can be useful to bring cache memory back to a known state during benchmarking, or to ready the MySQL server to resume
# 					its normal workload after running queries for reports or maintenance.
#
# 					For more information, see SECTION 15.8.3.7, "SAVING AND RESTORING THE BUFFER POOL STATE"
#
# 			) innodb_buffer_pool_size
#
# 					Property 				Value
#
# 					Cmd line 				--innodb-buffer-pool-size=#
# 					Sys var 					innodb_buffer_pool_size
# 					Scope 					Global
# 					Dynamic 					Yes
# 					SET_VAR Hint 			No
# 					Type 						Integer
# 					Default 					134217728
# 					Min 						5242880
# 					Max value (64-bit) 	2**64-1
# 					Max value (32-bit) 	2**32-1
#
# 					The size in bytes of the buffer pool, the memory area where InnoDB caches table and index data. The default value
# 					is 134217728 bytes (128MB).
#
# 					The maximum value depends on the CPU architechture, the maximum is 4294967295 (2^32-1) on 32-bit systems and
# 					18446744073709551615 (2^64-1) on 64-bit systems.
#
# 					On 32-bit systems, the CPU architechture and operating system may impose a lower practical maximum size than the
# 					stated maximum.
#
# 					When the size of the buffer pool is greater than 1GB, setting innodb_buffer_pool_instances to a value greater than
# 					1 can improve the scalability on a busy server.
#
# 					A larger buffer pool requires less disk I/O to access the same table data more than once. On a dedicated database
# 					server, you might set the buffer pool size to 80% of the machine's physical memory size.
#
# 					Be aware of the following potential issues when configuring buffer pool size, and be prepared to scale back the size
# 					of the buffer pool if necessary.
#
# 						) Competition for physical memory can cause paging in the operating system
#
# 						) InnoDB reserves additional memory for buffers and control structures, so that the total allocated space is approximately,
# 							10% greater than the specified buffer pool size.
#
# 						) Address space for the buffer pool must be contiguous, which can be an issue on Windows Systems with DLLs that load at
# 							specific addresses.
#
# 						) The time to initialize the buffer pool is roughly proportional to its size. On instances with large buffer pools, initialization
# 							time might be significant.
#
# 							To reduce the initialization period, you can save the buffer pool state at server shutdown and restore it at server startup.
#
# 							See SECTION 15.8.3.7, "SAVING AND RESTORING THE BUFFER POOL STATE"
#
# 					When you increase or decrease buffer pool size, the operation is performed in chunks. Chunk size is defined by the innodb_buffer_pool_chunk_size
# 					variable, which has a default value of 128 MB.
#
# 					Buffer pool size must always be equal to or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances.
#
# 					If you alter the buffer pool size to a value that is not equal to or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances,
# 					buffer pool size is automatically adjusted to a value that is equal to or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances.
#
# 					innodb_buffer_pool_size can be set dynamically, which allows you to resize the buffer pool without restarting the server.
#
# 					The Innodb_buffer_pool_resize_status status variable reports the status of online buffer pool resizing operations.
#
# 					See SECTION 15.8.3.1, "CONFIGURING INNODB BUFFER POOL SIZE" for more information.
#
# 					If innodb_dedicated_server is enabled, the innodb_buffer_pool_size value is automatically configured if it is not explicitly
# 					defined.
#
# 					For more information, see SECTION 15.8.12, "ENABLING AUTOMATIC CONFIGURATION FOR A DEDICATED MYSQL SERVER"
#
# 			) innodb_change_buffer_max_size
#
# 					Property 			Value
#	
# 					Cmd line 			--innodb-change-buffer-max-size=#
# 					Sys var 				innodb_change_buffer_max_size
# 					Scope 				Global
# 					Dynamic 				Yes
# 					SET_VAR Hint 		No
# 					Type 					Integer
# 					Default 				25
# 					Min 					0
# 					Max 					50
#
# 					Maximum size for the InnoDB change buffer, as a percentage of the total size of the buffer pool.
#
# 					You might increase this value for a MySQL server with heavy insert, update and delete activity,
# 					or decrease it for a MySQL server with unchanging data used for reporting.
#
# 					For more information, see SECTION 15.5.2, "CHANGE BUFFER" 
#
# 					For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 			) innodb_change_buffering
#
# 					Property 			Value
#
# 					Cmd line 			--innodb-change-buffering=value
# 					Sys var 				innodb_change_buffering
# 					Scope 				Global
# 					Dynamic 				Yes
# 					SET_VAR Hint 		No
# 					Type 					Enumeration
# 					Default 				all
# 					Valid 				none 
# 											inserts
# 											deletes
# 											changes
# 											purges
# 											all
#
# 					Whether InnoDB performs change buffering, an optimization that delays write operations to secondary indexes
# 					so that the I/O operations can be performed sequentially.
#
# 					Permitted values are described in the following table. Values may also be specified numerically.
#
# 					TABLE 15.26 PERMITTED VALUES FOR INNODB_CHANGE_BUFFERING
#
# 						Value 				Numeric 				Description
#
# 						none 					0 						Do not buffer any operations
# 						inserts 				1 						Buffer insert operations
# 						deletes 				2 						Buffer delete marking operations; strictly speaking, the writes that mark index records
# 																		for later deletion during a purge operation.
# 						changes 				3 						Buffer inserts and delete-marking operations
# 						purges 				4 						Buffer the physical deletion operations that happen in the background
# 						all 					5 						The default. Buffer inserts, delete-marking operations, and purges.
#
# 					For more information, see SECTION 15.5.2, "CHANGE BUFFER". For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 			) innodb_change_buffering_debug
#
# 					Property 			Value
#
# 					Cmd line 			--innodb-change-buffering-debug=#
# 					Sys var 				innodb_change_buffering_debug
# 					Scope 				Global
# 					Dynamic 				Yes
# 					SET_VAR Hint 		No
# 					Type 					Integer
# 					Default 				0
# 					Max 					2
#
# 					Sets a debug flag for InnoDB change buffering. A value of 1 forces all changes to the change buffer.
#
# 					A value of 2 causes a crash at merge. A default value of 0 indicates that the change buffering debug
# 					flag is not set.
#
# 					This option is only available when debugging support is compiled in using the WITH_DEBUG CMake option.
#
# 			) innodb_checkpoint_disabled
#
# 					Property 			Value
#
# 					Cmd line 			--innodb-checkpoint-disabled[={OFFON}]
# 					Introduced 			8.0.2
# 					Sys var 				innodb_checkpoint_disabled
# 					Scope 				Global
# 					Dynamic 				Yes
# 					SET_VAR Hint 		No
# 					Type 					Boolean
# 					Default 				OFF
#
# 					This is a debug option that is only intended for expert debugging use.
#
# 					It disables checkpoints so that a deliberate server exit always initiates InnoDB recovery.
#
# 					It should only be enabled for a short interval, typically before running DML operations that write
# 					redo log entries that would require recovery following a server exit.
#
# 					This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.
#
# 			) innodb_checksum_algorithm
#
# 					Property 			Value
#
# 					Cmd line 			--innodb-checksum-algorithm=value
# 					Sys var 				innodb_checksum_algorithm
# 					Scope 				Global
# 					Dynamic 				Yes
# 					SET_VAR Hint 		No
# 					Type 					Enumeration
# 					Default 				crc32
# 					Valid Values 		innodb
# 											crc32
# 											none
# 											strict_innodb
# 											strict_crc32
# 											strict_none
#
# 					Specifies how to generate and verify the checksum stored in the disk blocks of InnoDB tablespaces.
#
# 					The default value for innodb_checksum_algorithm is crc32.
#
# 					Versions of MySQL Enterprise Backup up to 3.8.0 do not support backing up tablespaces that use CRC32 checksums.
# 					MySQL Enterprise Backup adds CRC32 checksum support in 3.8.1, with some limitations.
#
# 					Refer to the MySQL Enterprise Backup 3.8.1 Change History for more information
#
# 					The value innodb is backward-compatible with earlier versions of MySQL. The value crc32 uses an algorithm that is faster
# 					to compute the checksum for every modified block, and to check the checksums for each disk read.
#
# 					It scans blocks 32 bits at a time, which is faster than the InnoDB checksum algorithm, which scans blocks 8 bits at
# 					a time.
#
# 					The value none writes a constant value in the checksum field rather than computing a value based on the block data.
#
# 					The blocks in a tablespace can use a mix of old, new, and no checksum values, being updated gradually as the data is
# 					modified; once blocks in a tablespace are modified to use the crc32 algorithm, the associated tables cannot be read
# 					by earlier versions of MySQL
#
# 					The strict form of a checksum algorithm reports an error if it encounters a valid but non-matching checksum value in
# 					a tablespace.
#
# 					It is recommended that you only use strict settings in a new instance, to set up tablespaces for the first time.
#
# 					Strict settings are somewhat faster, because they do not need to compute all checksum values during disk reads.
#
# 					The following table shows the difference between the none, innodb and crc32 option values, and their strict counterparts.
#
# 					none, innodb, and crc32 write the specified type of checksum value into each data block, but for compatibility accept other
# 					checksum values when verifying a block during a read operation.
#
# 					Strict settings also accept valid checksum values but print an error message when a valid non-matching checksum value is
# 					encountered. Using the strict form can make verification faster if all InnoDB data files in an instance are created
# 					under an identical innodb_checksum_algorithm value.
#
# 						TABLE 15.27 PERMITTED INNODB_CHECKSUM_ALGORITHM VALUES
#
# 						Value 			GENERATED CHECKSUM (when writing) 												PERMITTED CHECKSUMS (when reading)
#
# 						None 				A constant number 																	any of the checksums generated by none, innodb or crc32.
#
# 						innodb 			A checksum calculated in SW, using the original algo from InnoDB  	Any of the checksums generated by none, innodb or crc32
#
# 						crc32 			A checksum calculated using the crc32 algo, possibly done with a  	Any of the checksums generated by none, innodb or crc32
# 											hardware assist
#
# 						strict_none 	a constant number 																	Any of the checksums generated by none, innodb or crc32.
# 																																		InnoDB prints an error message if a valid but non-matching
# 																																		checksum is encountered.
#
# 						strict_innodb 	A checksum calculated in SW, using the original algo from InnoDB 		Any of the checksums generated by none, innodb or crc32.
# 																																		InnoDB prints an error message if a valid but non-matching
# 																																		checksum is encountered.
#
# 						strict_crc32 	A checksum calculated using the crc32 algorithm, possibly done 		Any of the checksums generated by none, innodb, or crc32.
# 											with a hardware assist  															InnoDB prints an error message if a valid but non-matching
# 																																		checksum is encountered.
#
# 			) innodb_cmp_per_index_enabled
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-cmp-per-index-enabled[={OFF|ON}]
# 				Sys var 					innodb_cmp_per_index_enabled
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Enables per-index compression-related statistics in the INFORMATION_SCHEMA.INNODB_CMP_PER_INDEX table.
#
# 				Because these statistics can be expensive to gather, only enable this option on development, test, or
# 				slave instances during performance tuning related to InnoDB compressed tables.
#
# 				For more information, see SECTION 25.39.7, "THE INFORMATION_SCHEMA INNODB_CMP_PER_INDEX AND INNODB_CMP_PER_INDEX_RESET TABLES",
# 				and SECTION 15.9.1.4, "MONITORING INNODB TABLE COMPRESSION AT RUNTIME"
#
# 			) innodb_commit_concurrency
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-commit-concurrency=#
# 				Sys var 					innodb_commit_concurrency
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					0
# 				Min 						0
# 				Max 						1000
#
# 				The number of threads that can commit at the same time. A value of 0 (the default) permits any number of
# 				transactions to commit simultaneously.
#
# 				The value of innodb_commit_concurrency cannot be changed at runtime from zero to nonzero or vice versa.
#
# 				The vlaue can be changed from one nonzero value to another.
#
# 			) innodb_compress_debug
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-compress-debug=value
# 				Sys var 					innodb_compress_debug
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Enumeration
# 				Default 					None
# 				Valid 					none
# 											zlib
# 											lz4
# 											lz4hc
#
# 				Compresses all tables using a specified compression algorithm without having to define a COMPRESSION
# 				attribute for each table.
#
# 				This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.
#
# 				For related information, see SECTION 15.9.2, "InnoDB PAGE COMPRESSION"
#
# 			) innodb_compression_failure_threshold_pct
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-compression-failure-threshold-pct=#
# 				Sys var 					innodb_compression_failure_threshold_pct
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					5
# 				Min 						0
# 				Max 						100
#
# 				Defines the compression failure rate threshold for a table, as a percentage, at which point MySQL begins adding padding within
# 				compressed pages to avoid expensive compression failures.
#
# 				When this threshold is passed, MySQL begins to leave additional free space within each new compressed page, dynamically adjusting
# 				the amount of free space up to the percentage of page size specified by innodb_compression_pad_pct_max.
#
# 				A value of zero disables the mechanism that monitors compression efficiency and dynamically adjusts the padding amount.
#
# 				For more information, see SECTION 15.9.1.6, "COMPRESSION FOR OLTP WORKLOADS"
#
# 			) innodb_compression_level
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-compression-level=#
# 				System Variable 		innodb_compression_level
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					6
# 				Min 						0
# 				Max 						9
#
# 				Specifies the level of zlib compression to use for InnoDB compressed tables and indexes.
#
# 				A higher value lets you fit more data onto a storage device, at the expense of more CPU
# 				overhead during compression. A lower value lets you reduce CPU overhead when storage space is not critical,
# 				or you expect the data is not especially compressible.
#
# 				For more information, see SECTION 15.9.1.6, "COMPRESSION FOR OLTP WORKLOADS"
#
# 			) innodb_compression_pad_pct_max
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-compression-pad-pct-max=#
# 				Sys var 					innodb_compression_pad_pct_max
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						INteger
# 				Default 					50
# 				Min 						0
# 				Max 						75
#
# 				Specifies the maximum percentage that can be reserved as free space within each compressed page, allowing room to
# 				reorganize the data and modification log within the page when a compressed table or index is updated and the
# 				data might be recompressed.
#
# 				Only applies when innodb_compression_failure_threshold_pct is set to a nonzero value, and the rate of
# 				COMPRESSION FAILURES passes the cutoff point.
#
# 				For more information, see SECTION 15.9.1.6, "COMPRESSION FOR OLTP WORKLOADS"
#
# 			) innodb_concurrency_tickets
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-concurrency-tickets=#
# 				Sys var 					innodb_concurrency_tickets
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					5000
# 				Min 						1
# 				Max 						4294967295
#
# 				Determines the number of threads that can enter InnoDB concurently. A thread is placed in a queue when it tries
# 				to enter InnoDB if the number of threads has already reached the concurrency limit.
#
# 				When a thread is permitted to enter InnoDB, it is given a number of "tickets" equal to the value of innodb_concurrency_tickets,
# 				and the thread can enter and leave InnoDB freely until it has used up its tickets.
#
# 				After that point, the thread again becomes subject to the concurrency check (and possible queuing) the next time it tries to
# 				enter InnoDB. The default value is 5000.
#
# 				With a small innodb_concurrency_tickets value, small transactions that only need to process a few rows compete fairly with
# 				larger transactions that process many rows.
#
# 				The disadvantage of a small innodb_concurrency_tickets value is that large transactions must loop through the queue many times
# 				before they can complete, which extends the amount of time required to complete their task.
#
# 				With a large innodb_concurrency_tickets value, large transactions spend less time waiting for a position at the end of the
# 				queue (controlled by innodb_thread_concurrency) and more time retrieving rows. Large transactions also require fewer trips
# 				through the queue to complete their task.
#
# 				The disadvantage of a large innodb_concurrency_tickets value is that too many large transactions running at the same time can
# 				starve smaller transactions by making them wait a longer time before executing.
#
# 				With a nonzero innodb_thread_concurrency value, you may need to adjust the innodb_concurrency_tickets value up or down to
# 				find the optimal balance between larger and smaller transactions.
#
# 				The SHOW ENGINE INNODB STATUS report shows the number of tickets remaining for an executing transaction in its current pass
# 				through the queue. This data may also be obtained from the TRX_CONCURRENCY_TICKETS column of the INFORMATION_SCHEMA.INNODB_TRX
# 				table.
#
# 				For more information, see SECTION 15.8.4, "CONFIGURING THREAD CONCURRENCY FOR INNODB"
#
# 			) innodb_data_file_path
#
# 				Property 			Value
#
# 				Cmd line 			--innodb-data-file-path=file_name
# 				Sys var 				innodb_data_file_path
# 				Scope 				Global
# 				Dynamic 				No
# 				SET_VAR Hint 		No
# 				Type 					String
# 				Default 				ibdata1:12M:autoextend
#
# 				Defines the name, size, and attributes of InnoDB system tablespace data files. If you do not specify a value for
# 				innodb_data_file_path, the default behavior is to create a single auto-extending data file, slightly larger
# 				than 12MB, named ibdata1.
#
# 				The full syntax for a data file specification includes the file name, file size, and autoextend and max attributes:
#
# 					file_name:file_size[:autoextend[:max:max_file_size]]
#
# 				File sizes are specified KB, MB or GB (1024MB) by appending K, M or G to the size value. If specifying the data file size
# 				in KB, do so in multiples of 1024.
#
# 				Otherwise, the KB values are rounded to nearest megabyte (MB) boundary. The sum of the sizes of the files must be at least
# 				slightly larger than 12MB.
#
# 				A minimum file size is enforced for the first system tablespace data file to ensure that there is enough space for doublewrite
# 				buffer pages:
#
# 					) For an innodb_page_size value of 16kb or less, the minimum file size is 3MB
#
# 					) For an innodb_page_size value of 32kb, the minimum file size is 6MB
#
# 					) For an innodb_page_size value of 64kb, the minimum file size is 12MB
#
# 				The size limit of individual files is determined by your OS. You can set the file size to more than 4GB on OS's that support
# 				large files.
#
# 				You can also use raw disk partitions as data files.
#
# 				The autoextend and max attributes can be used only for the data file that is specified last in the innodb_data_file_path setting.
#
# 				For example:
#
# 					[mysqld]
# 					innodb_data_file_path=ibdata1:50M;ibdata2:12M:autoextend:max:500MB
#
# 				If you specify the autoextend option, InnoDB extends the data file if it runs out of free space. The autoextend increment is
# 				64MB by default.
#
# 				To modify the increment, change the innodb_autoextend_increment system variable.
#
# 				The full directory path for system tablespace data files is formed by concatenating the paths defined by innodb_data_home_dir
# 				and innodb_data_file_path
#
# 				For more information about configuring system tablespace data files, see SECTION 15.8.1, "InnoDB STARTUP CONFIGURATION"
#
# 			) innodb_data_home_dir
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-data-home-dir=dir_name
# 				Sys var 					innodb_data_home_dir
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Dir name
#
# 				The common part of the directory path for InnoDB system tablespace data files.
#
# 				This setting does not affect the location of file-per-table tablespaces when innodb_file_per_table is enabled.
#
# 				The default value is the MySQL data directory. If you specify the value as an empty string, you can specify an
# 				absolute file paths for innodb_data_file_path.
#
# 				A trailing slash is required when specifying a value for innodb_data_home_dir. For example:
#
# 					[mysqld]
# 					innodb_data_home_dir = /path/to/myibdata/
#
# 				For related information, see SECTION 15.8.1, "InnoDB STARTUP CONFIGURATION"
#
# 			) innodb_ddl_log_crash_reset_debug
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ddl-log-crash-reset-debug[={OFF|ON}]
# 				Introduced 				8.0.3
# 				Sys var 					innodb_ddl_log_crash_reset_debug
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Enable this debug option to reset DDL log crash injection counters to 1.
#
# 				This option is only available when debugging support is compiled in using the WITH_DEBUG 
# 				CMake option.
#
# 			) innodb_deadlock_detect
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-deadlock-detect[={OFF|ON}]
# 				Sys var 					innodb_deadlock_detect
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					ON
#
# 				This option is used to disable deadlock detection. On high concurrency systems, deadlock detection can cause a slowdown
# 				when numerous threads wait for the same lock.
#
# 				At times, it may be more efficient to disable deadlock detection and rely on the innodb_lock_wait_timeout setting for
# 				transaction rollback when a deadlock occurs.
#
# 				For related information, see SECTION 15.7.5.2, "DEADLOCK DETECTION AND ROLLBACK"
#
# 			) innodb_dedicated_server
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-dedicated-server[={OFF|ON}]
# 				Introduced 				8.0.3
# 				Sys var 					innodb_dedicated_server
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				When innodb_dedicated_server is enabled, InnoDB automatically configures the following options according to
# 				the amount of memory detected on the server:
#
# 					) innodb_buffer_pool_size
#
# 					) innodb_log_file_size
#
# 					) innodb_flush_method
#
# 				Only consider enabling this option if your MySQL instance runs on a dedicated server where the MySQL server is able
# 				to consume all available system resources.
#
# 				Enabling this option is not recommended if your MySQL instance shares system resources with other applications.
#
# 				For more information, see SECTION 15.8.12, "ENABLING AUTOMATIC CONFIGURATION FOR A DEDICATED MYSQL SERVER"
#
# 			) innodb_default_row_format
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-default-row-format=value
# 				Sys var 					innodb_default_row_format
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Enumeration
# 				Default 					DYNAMIC
# 				Valid 					DYNAMIC
# 											COMPACT
# 											REDUNDANT
#
# 				The innodb_default_row_format option defines the default row format for InnoDB tables and user-created
# 				temporary tables.
#
# 				The default setting is DYNAMIC. Other permitted values are COMPACT and REDUNDANT. The COMPRESSED row format,
# 				which is not supported for use in the system tablespace, cannot be defined as the default.
#
# 				Newly created tables use the row format defined by innodb_default_row_format when a ROW_FORMAT option is not
# 				specified explicitly or when ROW_FORMAT=DEFAULT is used.
#
# 				When a ROW_FORMAT option is not specified explicitly, or when ROW_FORMAT=DEFAULT is used, any operation that
# 				rebuilds a table also silently changes the row format of the table to the format defined by innodb_default_row_format.
#
# 				For more information, see DEFINING THE ROW FORMAT OF A TABLE.
#
# 				Internal InnoDB temporary tables created by the server to process queries use the DYNAMIC row format,
# 				regardless of the innodb_default_row_format setting.
#
# 			) innodb_directories
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-directories=dir_name
# 				Introduced 				8.0.4
# 				Sys var 					innodb_directories
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Directory name
#
# 				Defines directories to scan at startup for tablespace files. This option is used when moving or restoring tablespace
# 				files to a new location while the server is offline.
#
# 				It is also used to specify directories of tablespace files created using an absolute path or that reside outside of the
# 				data directory (see SECTION 15.6.3.6, "CREATING A TABLESPACE OUTSIDE OF THE DATA DIRECTORY")
#
# 				Tablespace discovery during crash recovery relies on the innodb_directories setting to identify tablespaces referenced
# 				in the redo logs.
#
# 				For more information, see TABLESPACE DISCOVERY DURING CRASH RECOVERY.
#
# 				Directories defined by innodb_data_home_dir, innodb_undo_directory, and datadir are automatically appended to the
# 				innodb_directories argument value, regardless of whether the innodb_directories option is specified explicitly.
#
# 				innodb_directories may be specified as an option in a startup command or in a MySQL option file. Quotes are used around
# 				the argument value because otherwise a semicolon (;) is interpreted as a special char by some command interpreters.
#
# 				(unix shells treat it as a command terminator, for example)
#
# 				Startup command:
#
# 					mysqld --innodb-directories="directory_path_1;directory_path_2"
#
# 				MySQL option file:
#
# 					[mysqld]
# 					innodb_directories="directory_path_1;directory_path_2"
#
# 				Wildcard expressions cannot be used to specify directories.
#
# 				The innodb_directories scan also traverses the subdirectories of specified directories.
#
# 				Duplicate directories and subdirectories are discarded from the list of directories to be scanned.
#
# 				For more information, see SECTION 15.6.3.8, "MOVING TABLESPACE FILES WHILE THE SERVER IS OFFLINE"
#
# 			) innodb_disable_sort_file_cache
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-disable-sort-file-cache[={OFF|ON}]
# 				Sys var 						innodb_disable_sort_file_cache
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						OFF
#
# 				Disables the OS file system cache for merge-sort temporary files. The effect is to open such files with the
# 				equivalent of O_DIRECT.
#
# 			) innodb_doublewrite
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-doublewrite[={OFF|ON}]
# 				Sys var 						innodb_doublewrite
# 				Scope 						Global
# 				Dynamic 						No
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						ON
#
# 				When enabled (the default), InnoDB stores all data twice, first to the doublewrite buffer, then to the actual data files.
#
# 				This variable can be turned off with --skip-innodb-doublewrite for benchmarks or cases when top performance is needed rather
# 				than concern for data integrity or possible failures.
#
# 				If system tablesapce data files (ibdata* files) are located on Fusion-io devices that support atomic writes, doublewrite
# 				buffering is automatically disabled and Fusion-io atomic writes are used for all data files.
#
# 				Because the doublewrite buffer setting is global, doublewrite buffering is also disabled for data files residing on non-Fusion-io
# 				hardware.
#
# 				This feature is only supported on Fusion-io hardware and only enabled for Fusion-io NVMFS on Linux. To take full advantage
# 				of this feature, an innodb_flush_method setting of O_DIRECT is recommended.
#
# 				For related information, see SECTION 15.6.4, "DOUBLEWRITE BUFFER"
#
# 			) innodb_fast_shutdown
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-fast-shutdown=#
# 				System variable 			innodb_fast_shutdown
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						1
# 				Valid 						0
# 												1
# 												2
#
# 				The InnoDB shutdown mode.
#
# 				If the value is 0, INnoDB does a slow shutdown, a full purge and a change buffer merge before shutting down.
#
# 				If the value is 1, (the default), InnoDB skips these operations at shutdown, a process known as a fast shutdown.
#
# 				If the value is 2, InnoDB flushes its logs and shuts down cold, as if MySQL had crashed; no committed transactions
# 				are lost, but the crash recovery operation makes the next startup take longer.
#
# 				The slow shutdown can take minutes, or even hours in extreme cases where substantial amounts of data are still
# 				buffered. Use the slow shutdown technique before upgrading or downgrading between MySQL major releases, so that
# 				all data files are fully prepared in case the upgrade process updates the file format.
#
# 				Use innodb_fast_shutdown=2 in emergency or troubleshooting situations, to get the absolute fastest shutdown
# 				if data is at risk of corruption.
#
# 			) innodb_fil_make_page_dirty_debug
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-fil-make-page-dirty-debug=#
# 				Sys var 						innodb_fil_make_page_dirty_debug
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						0
# 				Max 							2**32-1
#
# 				By default, setting innodb_fil_make_page_dirty_debug to the ID of a tablespace immediately dirties
# 				the first page of the tablespace.
#
# 				If innodb_saved_page_number_debug is set to a non-default value, setting innodb_fil_make_page_dirty_debug
# 				dirties the specified page.
#
# 				The innodb_fil_make_page_dirty_debug option is only available if debugging support is compiled in using the
# 				WITH_DEBUG CMake option.
#
# 			) innodb_file_per_table
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-file-per-table[={OFF|ON}]
# 				Sys var 						innodb_file_per_table
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						ON
#
# 				When innodb_file_per_table is enabled, tables are created in file-per-table tablespaces by default.
#
# 				When disabled, tables are created in the system tablespace by default.
#
# 				For information about file-per-table tablespaces, see SECTION 15.6.3.2, "FILE-PER-TABLE TABLESPACES"
#
# 				For information about the InnoDB system tablespace, see SECTION 15.6.3.1, "THE SYSTEM TABLESPACE"
#
# 				The innodb_file_per_table variable can be configured at runtime using a SET_GLOBAL statement, specified
# 				on the command line at startup, or specified in an option file.
#
# 				Configuration at runtime requires privileges sufficient ot set global system variables (see SECTION 5.1.9.1,
# 				"SYSTEM VARIABLE PRIVILGES") and immediately affects the operation of all connections.
#
# 				When a table that resides in a file-per-table tablespace is truncated or dropped, the freed space is returned to
# 				the OS.
#
# 				Truncating or dropping a table that resides in the system tablespace only frees space in the system tablespace.
#
# 				Freed space in the system tablespace can be used again for InnoDB data but is not returned to the OS, as
# 				system tablespace data files never shrink.
#
# 				The innodb_file_per-table setting does not affect the creation of temporary tables.
#
# 				As of MySQL 8.0.14, temporary tables are created in session temporary tablespaces, and in the
# 				global temporary tablespace before that.
#
# 				See SECTION 15.6.3.5, "TEMPORARY TABLESPACES"
#
# 			) innodb_fill_factor
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-fill-factor=#
# 				Sys var 						innodb_fill_factor
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						100
# 				Min 							10
# 				Max 							100
#
# 				InnoDB performs a bulk load when creating or rebuilding indexes. This method of index creation is known as 
# 				a "sorted index build"
#
# 				innodb_fill_factor defines the percentage of space on each B-tree page that is filled during a sorted index
# 				build, with the remaining space reserved for future index growth.
#
# 				For example, setting innodb_fill_factor to 80 reserves 20 percent of the space on each B-tree page for future
# 				index growth.
#
# 				Actualy percentage may vary. The innodb_fill_factor setting is interpreted as a hint rather than a hard limit.
#
# 				An innodb_fill_factor setting of 100 leaves 1/16 of the space in clustered index pages free for future index growth.
#
# 				innodb_fill_factor applies to both B-tree leaf and non-leaf pages. It does not apply to external pages used for
# 				TEXT or BLOB entries.
#
# 				For more information, see SECTION 15.6.2.3, "SORTED INDEX BUILDS"
#
# 			) innodb_flush_log_at_timeout
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-flush-log-at-timeout=#
# 				Sys var 						innodb_flush_log_at_timeout
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						1
# 				Min 							1
# 				Max 							2700
#
# 				Write and flush the logs every N seconds. innodb_flush_log_at_timeout allows the timeout period between flushes
# 				to be increased in order to reduce flushing and avoid impacting performance of binary log group commit.
#
# 				The default setting for innodb_flush_log_at_timeout is once per second.
#
# 			) innodb_flush_log_at_trx_commit
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-flush-log-at-trx-commit=#
# 				Sys var 						innodb_flush_log_at_trx_commit
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Enumeration
# 				Default 						1
# 				Valid 						0
# 												1
# 												2
#
# 				Controls the balance between strict ACID compliance for commit operations and higher performance that is possible
# 				when commit-related I/O operations are rearranged and done in batches.
#
# 				You can achieve better performance by changing the default value but then you can lose transactions in a crash.
#
# 					) The default setting of 1 is required for full ACID compliance. Logs are written and flushed to disk at each transaction commit.
#
# 					) With a setting of 0, logs are written and flushed to disk once per second. Transactions for which logs have not been flushed can be lost in a crash.
#
# 					) With a setting of 2, logs are written after each transaction commit and flushed to disk once per second. Transactions for which logs have not been
# 						flushed can be lost in a crash.
#
# 					) For settings 0 and 2, once-per-second flushing is not 100% guaranteed. Flushing may occur more frequently due to DDL changes and other internal
# 						InnoDB activities that cause logs to be flushed independently of the innodb_flush_log_at_trx_commit setting, and sometimes less frequently
# 						due to scheduling issues.
#
# 						If logs are flushed once per second, up to one second of transactions can be lost in a crash. If logs are flushed more or less frequently than
# 						once per second, the amount of transactions that can be lost varies accordingly.
#
# 					) Log flushing frequency is controlled by innodb_flush_log_at_timeout, which allows you to set log flushing frequency to N seconds
# 						(where N is 1 ... 2700, with a default value of 1)
#
# 						However, any mysqld process crash can erase up to N seconds of transactions.
#
# 					) DDL changes and other internal InnoDB activities flush the log independently of the innodb_flush_log_at_trx_commit setting.
#
# 					) InnoDB crash recovery works regardless of the innodb_flush_log_at_trx_commit setting. Transactions are either applied entirely
# 						or erased entirely.
#
# 				For durability and consistency in a replication setup that uses InnoDB with transactions:
#
# 					) If binary logging is enabled, set sync_binlog=1
#
# 					) Always set innodb_flush_log_at_trx_commit=1
#
# 					CAUTION:
#
# 						Many operating systems and some disk hardware fool the flush-to-disk operation. They may tell mysqld that the flush
# 						has taken place, even though it has not.
#
# 						In this case, the durability of transactions is not guaranteed even with the recommended settings, and in the worst case,
# 						a power outage can corrupt InnoDB data.
#
# 						Using a battery-backed disk cache in the SCSI disk controller or in the disk itself speeds up file flushes, and
# 						makes the operation safer. You can also try to disable the caching of disk writes in hardware caches.
#
# 			) innodb_flush_method
#
# 				Property 			Value
#
# 				Cmd line 			--innodb-flush-method=value
# 				Sys var 				innodb_flush_method
# 				Scope 				Global
# 				Dynamic 				No
# 				SET_VAR Hint 		No
# 				Type 					String
# 				Default (Windows) unbuffered
# 				Default (Unix) 	fsync
# 				Valid (Windows) 	unbuffered
# 										normal
#
# 				Valid (Unix) 		fsync
# 										O_DSYNC
# 										littlesync
# 										nosync
# 										O_DIRECT
# 										O_DIRECT_NO_FSYNC
#
# 				Defines the method used to flush data to InnoDB data files and log files, which can affect I/O throughput.
#
# 				On Unix-like systems, the default value is fsync. On Windows, the default value is unbuffered.
#
# 					NOTE:
#
# 						In MySQL 8.0, innodb_flush_method options may be specified numerically.
#
# 				The innodb_flush_method options for Unix-like systems include:
#
# 					) fsync or 0: InnoDB uses the fsync() system call to flush both the data and log files. fsync is the default setting.
#
# 					) O_DSYNC or 1: InnoDB uses O_SYNC to open and flush the log files, and fsync() to flush the data files. InnoDB does not use
# 						O_DSYNC directly because there have been problems with it on many varities of Unix.
#
# 					) littlesync or 2: This option is used for internal performance testing and is currently unsupported. Use at your own risk.
#
# 					) nosync or 3: This option is used for internal performance testing and is currently unsupported. Use at your own risk.
#
# 					) O_DIRECT or 4: InnoDB uses O_DIRECT (or directio() on Solaris) to open the data files, and uses fsync() to flush both the
# 						data and log files.
#
# 						THis option is available on some GNU/Linux versions, FreeBSD, and Solaris.
#
# 					) O_DIRECT_NO_FSYNC: InnoDB uses O_DIRECT during flushing I/O, but skips the fsync() system call after each write operation.
#
# 						Prior to MySQL 8.0.14, this setting is not suitable for file systems such as XFS and EXT4, which require an fsync() system
# 						call to synchronize file system metadata changes.
#
# 						If you are not sure whether your file system requires an fsync() system call to synchronize file system metadata changes,
# 						use O_DIRECT instead.
#
# 						As of MySQL 8.0.14, fsync() is called after creating a new file, after increasing file size, and after closing a file,
# 						to ensure that file system metadata changes are synchronized.
#
# 						The fsync() system call is still skipped after each write operation.
#
# 						On storage devices with cache, data loss is possible if data files and redo log files reside on different storage devices,
# 						and a crash occurs before data file writes are flushed from the device cache.
#
# 						If you use or intend to use different storage devices for redo logs and data files, use O_DIRECT instead.
#
# 				The innodb_flush_method options for Windows systems include:
#
# 					) unbuffered or 0: InnoDB uses simulated Asynch I/O and non-buffered I/O
#
# 					) normal or 1: InnoDB uses simulated Asynch I/O and buffered I/O
#
# 				How each setting affects performance depends on hardware configuration and workload. Benchmark your particular configuration
# 				to decide which setting to use, or whether to keep the default setting.
#
# 				Examine the Innodb_data_fsync status variable to see the overall number of fsync() calls for each setting.
#
# 				The mix of read and write operations in your workload can affect how a setting performs. For example, on a system with a hardware
# 				RAID controller and battery-backed write cache, O_DIRECT can help to avoid double buffering between the InnoDB buffer pool and
# 				the operating system file system cache.
#
# 				On some systems where InnoDB data and log files are located on a SAN, the default value or O_DSYNC might be faster for a read-heavy
# 				workload with mostly SELECT statements.
#
# 				Always test this parameter with hardware and workload that reflect your production environment. For general I/O tuning advice,
# 				see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 				If innodb_dedicated_server is enabled, the innodb_flush_method value is automatically configured if it is not explicitly defined.
#
# 				For more information, see SECTION 15.8.12, "ENABLING AUTOMATIC CONFIGURATION FOR A DEDICATED MYSQL SERVER"
#
# 			) innodb_flush_neighbors
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-flush-neighbors=#
# 				Sys var 					innodb_flush_neighbors
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Enumeration
# 				Default (>= 8.0.3) 	0
# 				Default (<= 8.0.2) 	1
# 				Valid 					0
# 											1
# 											2
#
# 				Specifies whether flushing a page from the InnoDB buffer pool also flushes other dirty pages in the same extent.
#
# 					) A setting of 0 turns innodb_flush_neighbors off and no other dirty pages are flushed from the buffer pool.
#
# 					) A setting of 1 flushes contiguous dirty pages in the same extent from the buffer pool.
#
# 					) A setting of 2 flushes dirty pages in the same extent from the buffer pool.
#
# 				When the table data is stored on a traditional HDD storage device, flushing such neighbor pages in one operation reduces
# 				I/O overhead (primarily for disk seek operations) compared to flushing individual pages at different times.
#
# 				For table data stored on SSD, seek time is not a significant factor and you can set this option to 0 to spread out write
# 				operations.
#
# 				For related information, see SECTION 15.8.3.6, "FINE-TUNING INNODB BUFFER POOL FLUSHING"
#
# 			) innodb_flush_sync
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-flush-sync[={OFF|ON}]
# 				Sys var 					innodb_flush_sync
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					ON
#
# 				The innodb_flush_sync parameter, which is enabled by default, causes the innodb_io_capacity setting to be ignored
# 				for bursts of I/O activity that occur at checkpoints.
#
# 				To adhere to the limit on InnoDB background I/O activity defined by the innodb_io_capacity setting, disable
# 				innodb_flush_sync
#
# 				For related information, see SECTION 15.8.7, "CONFIGURING THE INNODB MASTER THREAD I/O RATE"
#
# 			) innodb_flushing_avg_loops
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-flushing-avg-loops=#
# 				Sys var 					innodb_flushing_avg_loops
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					30
# 				Min 						1
# 				Max 						1000
#
# 				Number of iterations for which InnoDB keeps the previously calculated snapshot of the flushing state, controlling how quickly
# 				adaptive flushing responds to changing workloads.
#
# 				Increasing the value makes the rate of flush operations change smoothly and gradually as the workload changes.
#
# 				Decreasing the value makes adaptive flushing adjust quickly to workload changes, which can cause spikes in flushing
# 				activity if the workload increases and decreases suddenly.
#
# 				For related information, see SECTION 15.8.3.6, "FINE-TUNING INNODB BUFFER POOL FLUSHING"
#
# 			) innodb_force_load_corrupted
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-force-load-corrupted[={OFF|ON}]
# 				Sys var 					innodb_force_load_corrupted
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Permits InnoDB to load tables at startup that are marked as corrupted. Use only during troubleshooting,
# 				to recover data that is otherwise inaccessible.
#
# 				When troubleshooting is complete, disable this setting and restart the server.
#
# 			) innodb_force_recovery
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-force-recovery=#
# 				Sys var 					innodb_force_recovery
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					0
# 				Min 						0
# 				Max 						6
#
# 				The crash recovery mode, typically only changed in serious troubleshooting situations. Possible values are
# 				from 0 to 6. For the meanings of these values and important information about innodb_force_recovery,
# 				see SECTION 15.20.2, "FORCING INNODB RECOVERY"
#
# 				WARNING:
#
# 					Only set this variable to a value greater than 0 in an emergency situation so that you can start InnoDB
# 					and dump your tables.
#
# 					As a safety measure, InnoDB prevents INSERT, UPDATE, or DELETE operations when innodb_force_recovery is
# 					greater than 0.
#
# 					An innodb_force_recovery setting of 4 or greater places InnoDB into read-only mode.
#
# 					These restrictions may cause replication administration commands to fail with an error, as replication
# 					stores the slave status logs in InnoDB tables.
#
# 			) innodb_fsync_threshold
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-fsync-threshold=#
# 				Introduced 				8.0.13
# 				Sys var 					innodb_fsync_threshold
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					0
# 				Min 						0
# 				Max 						2**64-1
#
# 				By default, when InnoDB creates a new data file, such as a new log file or tablespace file, it flushes the contents
# 				of the write buffer to disk only after the file is fully written, which can cause a large amount of disk write
# 				activity to occur at once.
#
# 				To force smaller, periodic flushes, use innodb_fsync_threshold to define a threshold size for the write buffer,
# 				in bytes. The contents of the write buffer are flushed to disk when the threshold size is reached.
#
# 				The default value of 0 forces the default behavior.
#
# 				Specifying a write buffer threshold size to force smaller, periodic flushes may be beneficial in cases where
# 				multiple MySQL instances use the same storage devices.
#
# 				For example, creating a new MySQL instance and its associated data files could cause large surges of disk write
# 				activity, impeding the performance of other MySQL instances that use the same storage devices.
#
# 				Configuring a write buffer threshold size helps avoid such surges in disk write activity.
#
# 			) innodb_ft_aux_table
#
# 				Property 				Value
#
# 				Sys var 					innodb_ft_aux_table
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						String
#
# 				Specifies the qualified name of an InnoDB table containing a FULLTEXT index. This variable is intended for diagnostic
# 				purposes and can only be set at runtime.
#
# 				For example:
#
# 					SET GLOBAL innodb_ft_aux_table = 'test/t1';
#
# 				After you set this variable to a name in the format db_name/table_name, the INFORMATION_SCHEMA tables INNODB_FT_INDEX_TABLE,
# 				INNODB_FT_INDEX_CACHE, INNODB_FT_CONFIG, INNODB_FT_DELETED, and INNODB_FT_BEING_DELETED show information about the
# 				search index for the specified table.
#
# 				For more information, see SECTION 15.14.4, "InnoDB INFORMATION_SCHEMA FULLTEXT INDEX TABLES"
#
# 			) innodb_ft_cache_size
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-cache-size=#
# 				Sys var 					innodb_ft_cache_size
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					8000000
# 				Min 						1600000
# 				Max 						80000000
#
# 				The memory allocated, in bytes, for the InnoDB FULLTEXT seach index cache, which holds a parsed document in memory
# 				while creating an InnoDB FULLTEXT index.
#
# 				Index inserts and updates are only committed to disk when the innodb_ft_cache_size size limit is reached.
#
# 				innodb_ft_cache_size defines the cache size on a per table basis.
#
# 				To set a global limit for all tables, see innodb_ft_total_cache_size
#
# 				For more information, see InnoDB FULL-TEXT INDEX CACHE
#
# 			) innodb_ft_enable_diag_print
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-enable-diag-print[={OFF|ON}]
# 				Sys var 					innodb_ft_enable_diag_print
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Whether to enable additional full-text search (FTS) diagnostic output. This option is primarily intended for advanced
# 				FTS debugging and will not be of interest to most users.
#
# 				Output is printed to the error log and includes information such as:
#
# 					) FTS index sync progress (when the FTS cache limit is reached). For example:
#
# 						FTS SYNC for table test, deleted count: 100 size: 10000 bytes
# 						SYNC words: 100
#
# 					) FTS optimize progress. For example:
#
# 						FTS start optimize test
# 						FTS_OPTIMIZE: optimize "mysql"
# 						FTS_OPTIMIZE: processed "mysql"
#
# 					) FTS index build progress. For example:
#
# 						Number of doc processed: 1000
#
# 					) For FTS queries, the query parsing tree, word weight, query processing time and memory usage are printed. For example:
#
# 						FTS Search Processing time: 1 secs: 100 millisec: row(s) 10000
# 						Full Search Memory: 245666 (bytes), Row: 10000
#
# 			) innodb_ft_enable_stopword
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-enable-stopword[={OFF|ON}]
# 				Sys var 					innodb_ft_enable_stopword
# 				Scope 					Global, Session
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					ON
#
# 				Specifies that a set of stopwords is associated with an InnoDB FULLTEXT index at the time the index is created.
#
# 				If the innodb_ft_user_stopword_table option is set, the stopwords are taken from that table. Else, if the innodb_ft_server_stopword_table
# 				option is set, the stopwords are taken from that table. Otherwise, a built-in set of default stopwords is used.
#
# 				For more information, see SECTION 12.9.4, "FULL-TEXT STOPWORDS"
#
# 			) innodb_ft_max_token_size
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-max-token-size=#
# 				Sys var 					innodb_ft_max_token_size
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					84
# 				Min 						10
# 				Max 						84
#
# 				Maximum character length of words that are stored in an InnoDB FULLTEXT index. Setting a limit on this value reduces the size
# 				of the index, thus speeding up queries, by omitting long keywords or arbitrary collections of letters that are not real words
# 				and are not likely to be search terms.
#
# 				For more information, see SECTION 12.9.6, "FINE-TUNING MYSQL FULL-TEXT SEARCH"
#
# 			) innodb_ft_min_token_size
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-min-token-size=#
# 				Sys var 					innodb_ft_min_token_size
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					3
# 				Min 						0
# 				Max 						16
#
# 				Min length of words that are stored in an InnoDB FULLTEXT index. Increasing this value reduces the size of the index,
# 				thus speeding up queries, by omitting common words that are unlikely to be significant in a search context, such as
# 				the english word "a" and "to".
#
# 				For content using a CJK (Chinese, Japansese, korean) char set, specify a value of 1.
#
# 				For more information, see SECTION 12.9.6, "FINE-TUNING MYSQL FULL-TEXT SEARCH"
#
# 			) innodb_ft_num_word_optimize
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-num-word-optimize=#
# 				Sys var 					innodb_ft_num_word_optimize
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					2000
#
# 				Number of words to process during each OPTIMIZE_TABLE operation on an InnoDB FULLTEXT index.
#
# 				Because a bulk insert or update operation to a table containing a full-text search index could require
# 				substantial index maintenance to incorporate all changes, you might do a series of OPTIMIZE_TABLE statements,
# 				each picking up where the last left off.
#
# 				For more information, see SECTION 12.9.6, "FINE-TUNING MySQL FULL-TEXT SEARCH"
#
# 			) innodb_ft_result_cache_limit
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-result-cache-limit=#
# 				Sys var 					innodb_ft_result_cache_limit
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					2000000000
# 				Min 						1000000
# 				Max 						2**32-1
#
# 				The InnoDB full-text search query result cache limit (defined in bytes) per full-text search query or per thread.
# 				Intermediate and final InnoDB full-text search query results are handled in memory.
#
# 				Use innodb_ft_result_cache_limit to place a size limit on the full-text search query result cache to avoid
# 				excessive memory consumption in case of very large InnoDB full-text search query results (millions or hundreds of mil of rows,
# 				for example)
#
# 				Memory is allocated as required when a full-text search query is processed. If the result cache size limit is reached, an error
# 				is returned indicating that hte query exceeds the maximum allowed memory.
#
# 				The maximum value of innodb_ft_result_cache_limit for all platforms types and bit sizes is 2**32-1
#
# 			) innodb_ft_server_stopword_table
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-server-stopword-table=db_name/table_name
# 				Sys var 					innodb_ft_server_stopword_table
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						String
# 				Default 					NULL
#
# 				This option is used to specify your own InnoDB FULLTEXT index stopword list for all InnoDB tables.
#
# 				To configure your own stopword list for a specific InnoDB table, use innodb_ft_user_stopword_table
#
# 				Set innodb_ft_server_stopword_table to the name of the table containing a list of stopwords, in the format db_name/table_name
#
# 				The stopword table must exist before you configure innodb_ft_server_stopword_table.innodb_ft_enable_stopword must be enabled
# 				and innodb_ft_server_stopword_table option must be configured before you create the FULLTEXT index.
#
# 				The stopword table must be an InnoDB table, containing a single VARCHAR column named value.
#
# 				For more information, see SECTION 12.9.4, "FULL-TEXT STOPWORDS"
#
# 			) innodb_ft_sort_pll_degree
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-sort-pll-degree=#
# 				Sys var 					innodb_ft_sort_pll_degree
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					2
# 				Min 						1
# 				Max 						32
#
# 				Number of threads used in parallel to index and tokenize text in an InnoDB FULLTEXT index when building a search index.
#
# 				For related information, see SECTION 15.6.2.4, "InnoDB FULLTEXT INDEXES", and innodb_sort_buffer_size
#
# 			) innodb_ft_total_cache_size
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-total-cache-size=#
# 				Sys Var 					innodb_ft_total_cache_size
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					640000000
# 				Min 						32000000
# 				Max 						1600000000
#
# 				The total memory allocated, in bytes, for the InnoDB full-text search index cache for all tables.
#
# 				Creating numerous tables, each with a FULLTEXT search index, could consume a significant portion of
# 				available memory.
#
# 				innodb_ft_total_cache_size defines a global memory limit for all full-text search indexes to help
# 				avoid excessive memory consumption.
#
# 				If the global limit is reached by an index operation, a forced sync is triggered.
#
# 				For more information, see InnoDB FULL-TEXT INDEX CACHE.
#
# 			) innodb_ft_user_stopword_table
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-ft-user-stopword-table=db_name/table_name
# 				Sys var 					innodb_ft_user_stopword_table
# 				Scope 					Global, Session
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						String
# 				Default 					NULL
#
# 				This option is used to specify your own InnoDB FULLTEXT index stopword list on a specific table.
#
# 				To configure your own stopword list for all InnoDB tables, use innodb_ft_server_stopword_table
#
# 				Set innodb_ft_user_stopword_table to the name of the table containing a list of stopwords, in the format db_name/table_name
#
# 				The stopword table must exist before you configure innodb_ft_user_stopword_table.innodb_ft_enable_stopword must be enabled
# 				and innodb_ft_user_stopword_table must be configured before you create the FULLTEXT index.
#
# 				The stopword table must be an InnoDB table, containing a single VARCHAR column named value
#
# 				For more information, see SECTION 12.9.4, "FULL-TEXT STOPWORDS"
#
# 			) innodb_io_capacity
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-io-capacity=#
# 				Sys var 					innodb_io_capacity
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					200
# 				Min 						100
# 				Max(64-bit) 			2**64-1
# 				Max(32-bit) 			2**32-1
#
# 				The innodb_io_capacity parameter sets an upper limit on the number of I/O operations performed per second by
# 				InnoDB background tasks, such as flushing pages from the buffer pool and merging data from the change buffer.
#
# 				The innodb_io_capacity limit is a total limit for all buffer pool instances. When dirty pages are flushed, the
# 				limit is divided equally among buffer pool instances.
#
# 				innodb_io_capacity should be set to approximately the number of I/O operations that the system can perform per
# 				second.
#
# 				Ideally, keep the setting as low as practical, but not so low that background activities fall behind. If the value
# 				is too high, data is removed from the buffer pool and insert buffer too quickly for caching to provide a significant benefit.
#
# 				The default value is 200. For busy systems capable of higher I/O rates, you can set a higher value to help the server
# 				handle the background maintenance work associated with a high rate of row changes.
#
# 				In general, you can increase the value as a function of the number of drives used for InnoDB I/O.
#
# 				For example, you can increase the value on systems that use multiple disks or solid-state disks (SSD)
#
# 				The default setting of 200 is generally sufficient for a lower-end SSD. For a higher-end, bus-attached SSD,
# 				consider a higher setting such as 1000, for example.
#
# 				For systems with individual 5400 RPM or 7200 RPM drives, you might lower the value to 100, which represents
# 				an estimated proportion of the I/O operations per second (IOPS) available to older-generation disk drives
# 				that can perform about 100 IOPS.
#
# 				Although you can specify a very high value such as one million, in practice such large values have little if
# 				any benefit. Generally, a value of 20000 or higher is not recommended unless you have proven that lower values
# 				are insufficient for your workload.
#
# 				Consider write workload when tuning innodb_io_capacity. Systems with large write workloads are likely to benefit
# 				from a higher setting. A lower setting may be sufficient for systems with a small write workload.
#
# 				You can set innodb_io_capacity to any number 100 or greater to a maximum defined by innodb_io_capacity_max.
#
# 				innodb_io_capacity can be set in the MySQL option file (my.cnf or my.ini) or changed dynamically using a SET
# 				GLOBAL statement, which requires privileges sufficient to set global system variables.
#
# 				See SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES"
#
# 				The innodb_flush_sync variable causes the innodb_io_capacity setting to be ignored during bursts of I/O
# 				activity that occur at checkpoints. innodb_flush_sync is enabled by default.
#
# 				See SECTION 15.8.7, "CONFIGURING THE INNODB MASTER THREAD I/O RATE" for more information. For general
# 				information about InnoDB I/O performance, see SECTION 8.5.8, "OPTIMIZING InnoDB DISK I/O"
#
# 			) innodb_io_capacity_max
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-io-capacity-max=#
# 				Sys var 					innodb_io_capacity_max
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					see Description
# 				Min 						100
# 				Max (64-bit, win) 	2**32-1
# 				Max (64-bit, Unix) 	2**64-1
# 				Max (32-bit) 			2**32-1
#
# 				If flushing activity falls behind, InnoDB can flush more aggressively than the limit imposed by innodb_io_capacity.
#
# 				innodb_io_capacity_max defines an upper limit the number of I/O operations performed per second by InnoDB background
# 				tasks in such situations.
#
# 				The innodb_io_capacity_max setting is a total limit for all buffer pool instances.
#
# 				If you specify an innodb_io_capacity setting at startup but do not specify a value for innodb_io_capacity_max,
# 				innodb_io_capacity_max defaults to twice the value of innodb_io_capacity, with a minimum value of 2000.
#
# 				When configuring innodb_io_capacity_max, twice the innodb_io_capacity is often a good starting point.
#
# 				The default value of 2000 is intended for workloads that use a solid-state disk (SSD) or more than one regular disk drive.
# 				A setting of 2000 is likely too high for workloads that do not use SSD or multiple disk drives, and could allow too much
# 				flushing.
#
# 				For a single regular disk drive, a setting between 200 and 400 is recommended. For a high-end, bus-attached SSD, consider
# 				a higher setting such as 2500.
#
# 				As with the innodb_io_capacity setting, keep the setting as low as practical, but not so low that InnoDB cannot sufficiently
# 				extend beyond the innodb_io_capacity limit, if necessary.
#
# 				Consider write workload when tuning innodb_io_capacity_max. Systems with large write workloads may benefit from a higher setting.
#
# 				A lower setting may be sufficient for systems with a small write workload.
#
# 				innodb_io_capacity_max cannot be set to a value lower than the innodb_io_capacity value.
#
# 				Setting innodb_io_capacity_max to DEFAULT using a SET statement (SET GLOBAL innodb_io_capacity_max=DEFAULT) sets
# 				innodb_io_capacity_max to the maximum value.
#
# 				For related information, see SECTION 15.8.3.6, "FINE-TUNING INNODB BUFFER POOL FLUSHING"
#
# 			) innodb_limit_optimistic_insert_debug
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-limit-optimistic-insert-debug=#
# 				Sys var 						innodb_limit_optimistic_insert_debug
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						0
# 				Min 							0
# 				Max 							2**32-1
#
# 				Limits the number of records per B-tree page. A default value of 0 means that no limit is imposed.
#
# 				This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.
#
# 			) innodb_lock_wait_timeout
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-lock-wait-timeout=#
# 				Sys var 						innodb_lock_wait_timeout
# 				Scope 						Global, Session
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						50
# 				Min 							1
# 				Max 							1073741824
#
# 				The length of time in seconds an InnoDB transaction waits for a row lock before giving up.
#
# 				The default vlaue is 50 seconds. A transaction that tries to access a row that is locked by another
# 				INnoDB transaction waits at most this many seconds for write access to the row before issuing the following
# 				error:
#
# 					ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction
#
# 				When a lock wait timeout occurs, the current statement is rolled back (not the entire transaction).
#
# 				To have the entire transaction roll back, start the server with the --innodb-rollback-on-timeout
# 				option. See also SECTION 15.20.4, "InnoDB ERROR HANDLING"
#
# 				You might decrease this value for highly interactive applications or OLTP systems, to display user feedback
# 				quickly or put the update into a queue for processing later.
#
# 				You might increase this value for long-running back-end operations, such as a transform step in a data warehouse
# 				that waits for other large insert or update operations to finish.
#
# 				innodb_lock_wait_timeout applies to InnoDB row locks. A MySQL table lock does not happen inside InnoDB and this
# 				timeout does not apply to waits for table locks.
#
# 				The lock wait timeout value does not apply to deadlocks when innodb_deadlock_detect is enabled (the default)
# 				because InnoDB detects deadlocks immediately and rolls back one of the deadlocked transactions.
#
# 				When innodb_deadlock_detect is disabled, InnoDB relies on innodb_lock_wait_timeout for transaction rollback
# 				when a deadlock occurs.
#
# 				See SECTION 15.7.5.2, "DEADLOCK DETECTION AND ROLLBACK"
#
# 				innodb_lock_wait_timeout can be set at runtime with the SET GLOBAL or SET SESSION statement.
#
# 				Changing the GLOBAL Setting requires privileges sufficient to set global system variables
# 				(see SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES") and affects the operation of all clients
# 				that subsequently connect.
#
# 				Any client can change the SESSION setting for innodb_lock_wait_timeout, which affects only that client.
#
# 			) innodb_log_buffer_size
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-log-buffer-size=#
# 				Sys Var (>= 8.0.11) 		innodb_log_buffer_size
# 				Sys Var (<= 8.0.4) 		innodb_log_buffer_size
# 				Scope (>= 8.0.11) 		Global
# 				SCope (<= 8.0.4) 			Global
# 				Dynamic (>= 8.0.11) 		Yes
# 				Dynamic (<= 8.0.4) 		No
# 				SET_VAR Hint (>= 8.0.11)No
# 				SET_VAR Hint (<= 8.0.4) No
# 				Type 							Integer
# 				Default 						16777216
# 				Min 							1048576
# 				Max 							4294967295
#
# 				The size in bytes of the buffer that InnoDB uses to write to the log files on disk.
# 				The default is 16 MB. A large log buffer enables large transactions to run without the
# 				need to write the log to disk before the transactions commit.
#
# 				Thus, if you have transactions that update, insert, or delete many rows, making the log
# 				buffer larger saves disk I/O.
#
# 				For related information, see Memory Configuration, and Section 8.5.4, "Optimizing InnoDB Redo Logging"
#
# 				For general I/O tuning advice, see SECTION 8.5.8, "Optimizing InnoDB Disk I/O"
#
# 			) innodb_log_checkpoint_fuzzy_now
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-log-checkpoint-fuzzy-now[={OFF|ON}]
# 				Intro 						8.0.13
# 				Sys var 						innodb_log_checkpoint_fuzzy_now
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						OFF
#
# 				Enable this debug option to force InnoDB to write a fuzzy checkpoint. This option is only available if debugging
# 				support is compiled in using the WITH_DEBUG Cmake option.
#
# 			) innodb_log_checkpoint_now
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-log-checkpoint-now[={OFF|ON}]
# 				Sys var 						innodb_log_checkpoint_now
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						OFF
#
# 				Enable this debug option to force InnoDB to write a checkpoint. This option is only available if debugging support is compiled
# 				in using the WITH_DEBUG CMake option.
#
# 			) innodb_log_checksums
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-log-checksums[={OFF|ON}]
# 				Sys var 						innodb_log_checksums
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR hint 				No
# 				Type 							Boolean
# 				Default 						ON
#
# 				Enables or disables checksums for redo log pages.
#
# 				innodb_log_checksums=ON enables the CRC-32C checksum algorithm for redo log pages.
#
# 				When innodb_log_checksums is disabled, the contents of the redo log page checksum field are ignored.
#
# 				Checksums on the redo log header page and redo log checkpoint pages are never disabled.
#
# 			) innodb_log_compressed_pages
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-log-compressed-pages[={OFF|ON}]
# 				Sys var 						innodb_log_compressed_pages
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						ON
#
# 				Specifies whether images of re-compressed pages are written to the redo log. Re-compression may occur when changes
# 				are made to compressed data.
#
# 				innodb_log_compressed_pages is enabled by default to prevent corruption that could occur if a different version of the
# 				zlib compression algorithm is used during recovery.
#
# 				If you are certain that the zlib version will not change, you can disable innodb_log_compressed_pages to reduce redo log
# 				generation for workloads that modify compressed data.
#
# 				To measure the effect of enabling or disabling innodb_log_compressed_pages, compare redo log generation for both settings
# 				under the same workload.
#
# 				Options for measuring redo log generation include observing the Log sequence number (LSN) in the LOG section of SHOW_ENGINE_INNODB_STATUS
# 				output, or monitoring Innodb_os_log_written status for the number of bytes written to the redo log files.
#
# 				For related information, see SECTION 15.9.1.6, "COMPRESSION FOR OLTP WORKLOADS"
#
# 			) innodb_log_file_size
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-log-file-size=#
# 				Sys var 						innodb_log_file_size
# 				Scope 						Global
# 				Dynamic 						No
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						50331648
# 				Min 							4194304
# 				Max 							512GB / innodb_log_files_in_group
#
# 				The size in bytes of each log file in a log group. The combined size of log files (innodb_log_file_size * innodb_log_files_in_group)
# 				cannot exceed a maximum value that is slightly less than 512GB. A pair of 255 GB log files, for example, approaches the limit but
# 				does not exceed it.
#
# 				The default value is 48MB.
#
# 				Generally, the combined size of the log files should be large enough that the server can smooth out peaks and troughs in workload
# 				activity, which often means that there is enough redo log space to handle more than an hour of write activity.
#
# 				The larger the value, the less checkpoint flush activity is required in the buffer pool, saving disk I/O.
#
# 				Larger log files also make crash recovery slower, although improvements to recovery performance make log file size less of a consideration
# 				than it was in earlier versions of MySQL.
#
# 				The minimum innodb_log_file_size is 4MB.
#
# 				For related information, see Redo Log File Configuration. For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING InnoDB DISK I/O"
#
# 				If innodb_dedicated_server is enabled, the innodb_log_file_size value is automatically configured if it is not explicitly defined.
#
# 				For more information, see SECTION 15.8.12, "ENABLING AUTOMATIC CONFIGURATION FOR A DEDICATED MYSQL SERVER"
#
# 			) innodb_log_files_in_group
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-log-files-in-group=#
# 				Sys var 							innodb_log_files_in_group
# 				Scope 							Global
# 				Dynamic 							No
# 				SET_VAR Hint 					No
# 				Type 								Integer
# 				Default 							2
# 				Min 								2
# 				Max 								100
#
# 				The number of log files in the log group. InnoDB writes to the files in a circular fashion. The default (and recommended)
# 				value is 2.
#
# 				The location of the files is specified by innodb_log_group_home_dir. The combined size of log files (innodb_log_file_size * innodb_log_files_in_group)
# 				can be up to 512GB.
#
# 				For related information, see REDO LOG FILE CONFIGURATION.
#
# 			) innodb_log_group_home_dir
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-log-group-home-dir=dir_name
# 				Sys var 							innodb_log_group_home_dir
# 				Scope 							Global
# 				Dynamic 							No
# 				SET_VAR Hint 					No
# 				Type 								Dir name
#
# 				The directory path to the InnoDB redo log files, whose number is specified by innodb_log_files_in_group.
#
# 				If you do not specify any InnoDB log variables, the default is to create two files named ib_logfile0 and
# 				ib_logfile1 in the MySQL data directory.
#
# 				Log file size is given by the innodb_log_file_size system variable.
#
# 				For related information, see Redo Log File Configuration
#
# 			) innodb_log_spin_cpu_abs_lwm
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-log-spin-cpu-abs-lwm=#
# 				Intro 							8.0.11
# 				Sys var 							innodb_log_spin_cpu_abs_lwm
# 				Scope 							Global
# 				Dynamic 							Yes
# 				SET_VAR Hint 					No
# 				Type 								Integer
# 				Default 							80
# 				Min 								0
# 				Max 								4294967295
#
# 				Defines the minimum amount of CPU usage below which user threads no longer spin while waiting for flushed redo.
#
# 				The value is expressed as a sum of CPU core usage. For example, the default value of 80 is 80% of a single CPU core.
#
# 				On a system with a multi-core processor, a value of 150 represents 100% usage of one CPU core plus 50% usage
# 				of a second CPU core.
#
# 				For related information, see SECTION 8.5.4, "OPTIMIZING INNODB REDO LOGGING"
#
# 			) innodb_log_spin_cpu_pct_hwm
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-log-spin-cpu-pct-hwm=#
# 				Intro 							8.0.11
# 				Sys var 							innodb_log_spin_cpu_pct_hwm
# 				Scope 							Global
# 				Dynamic 							Yes
# 				SET_VAR Hint 					No
# 				Type 								Integer
# 				Default 							50
# 				Min 								0
# 				Max 								100
#
# 				Defines the maximum amount of CPU usage above which user threads no longer spin while waiting for flushed redo.
#
# 				The value is expressed as a percentage of the combined total processing power of all CPU cores. The default value is 50%.
#
# 				For example, 100% usage of two CPU cores is 50% of the combined CPU processing power on a server with four CPU cores.
#
# 				The innodb_log_spin_cpu_pct_hwm variable respects processor affinity. For example, if a server has 48 cores but the mysqld
# 				process is pinned to only four CPU cores, the other 44 CPU cores are ignored.
#
# 				For related information, see SECTION 8.5.4, "OPTIMIZING INNODB REDO LOGGING"
#
# 			) innodb_log_wait_for_flush_spin_hwm
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-log-wait-for-flush-spin-hwm=#
# 				Intro 							8.0.11
# 				Sys var 							innodb_log_wait_for_flush_spin_hwm
# 				Scope 							Global
# 				Dynamic 							Yes
# 				SET_VAR Hint 					No
# 				Type 								Integer
# 				Default 							400
# 				Min 								0
# 				Max (64-bit) 					2**64-1
# 				Max (32-bit) 					2**32-1
#
# 				Defines the maximum average log flush time beyond which user threads no longer spin while waiting for flushed redo.
#
# 				The default value is 400 ms.
#
# 				For related information, see SECTION 8.5.4, "OPTIMIZING INNODB REDO LOGGING"
#
# 			) innodb_log_write_ahead_size
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-log-write-ahead-size=#
# 				Sys var 							innodb_log_write_ahead_size
# 				Scope 							Global
# 				Dynamic 							Yes
# 				SET_VAR Hint 					No
# 				Type 								Integer
# 				Default 							8192
# 				Min 								512 (log file block size)
# 				Max 								Equal to innodb_page_size
#
# 				Defines the write-ahead block size for the redo log, in bytes. To avoid "read-on-write", set innodb_log_write_ahead_size
# 				to match the operating system or file system cache block size.
#
# 				The default setting is 8192 bytes. Read-on-write occurs when redo log blocks are not entirely cached to the operating system
# 				or file system due to a mismatch between write-ahead block size for the redo log and operating system or file system cache block size.
#
# 				Valid values for innodb_log_write_ahead_size are multiples of the InnoDB log file block size (2^n)
#
# 				The minimum value is the InnoDB log file block size (512). Write-ahead does not occur when the minimum value is specified.
#
# 				The maximum value is equal to the innodb_page_size value. If you specify a value for innodb_log_write_ahead_size that is larger
# 				than the innodb_page_size value, the innodb_log_write_ahead_size setting is truncated to the innodb_page_size value.
#
# 				Setting the innodb_log_write_ahead_size value too low in relation to the operating system or file system cache block size
# 				results in "read-on-write". Setting the value too high may have a slight impact on fsync performance for log file writes
# 				due to several blocks being written at once.
#
# 				For related information, see SECTION 8.5.4, "OPTIMIZING INNODB REDO LOGGING"
#
# 			) innodb_lru_scan_depth
#
# 				Property 						Value
#
#				Cmd line 						--innodb-lru-scan-depth=#
# 				Sys var 							innodb_lru_scan_depth
# 				Scope 							Global
# 				Dynamic 							Yes
# 				SET_VAR Hint 					No
# 				Type 								Integer
# 				Default 							1024
# 				Min 								100
# 				Max (64-bit) 					2**64-1
# 				Max (32-bit) 					2**32-1
#
# 				A parameter that influences the algorithms and heuristics for the flush operation for the InnoDB buffer pool.
#
# 				Primarily of interest to performance experts tuning I/O-intensive workloads. It specifies, per buffer pool instance,
# 				how far down the buffer pool LRU page list the page cleaner thread scans looking for dirty pages to flush.
#
# 				This is a background operation performed once per second.
#
# 				A setting smaller than the default is generally suitable for most workloads. A value that is much higher than necessary
# 				may impact performance. Only consider increasing the value if you have spare I/O capacity under a typical workload.
#
# 				Conversely, if a write-intensive workload saturates your I/O capacity, decrease the value, especially in the case
# 				of a large buffer pool.
#
# 				When tuning innodb_lru_scan_depth, start with a low value and configure the setting upward with the goal of rarely
# 				seeing zero free pages. Also, consider adjusting innodb_lru_scan_depth when changing the number of buffer pool instances,
# 				since innodb_lru_scan_depth * innodb_buffer_pool_instances defines the amount of work performed by the page
# 				cleaner thread each second.
#
# 				For related information, see SECTION 15.8.3.6, "FINE-TUNING InnoDB BUFFER POOL FLUSHING". For general I/O tuning advice,
# 				see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 			) innodb_max_dirty_pages_pct
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-max-dirty-pages-pct=#
# 				Sys var 						innodb_max_dirty_pages_pct
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Numeric
# 				Default (>= 8.0.3) 		90
# 				Default (<= 8.0.2) 		75
# 				Min 							0
# 				Max 							99.99
#
# 				InnoDB tries to flush data from the buffer pool so that the percentage of dirty pages does not exceed this value.
#
# 				The innodb_max_dirty_pages_pct setting establishes a target for flushing activity. It does not affect the rate of flushing.
#
# 				For information about managing the rate of flushing, see SECTION 15.8.3.5, "CONFIGURING INNODB BUFFER POOL FLUSHING"
#
# 				For related information, see SECTION 15.8.3.6, "FINE-TUNING INNODB BUFFER POOL FLUSHING". For general I/O tuning advice,
# 				see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 			) innodb_max_dirty_pages_pct_lwm
#
# 				Property						Value
#
# 				Cmd line 					--innodb-max-dirty-pages-pct-lwm=#
# 				Sys var 						innodb_max_dirty_pages_pct_lwm
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Numeric
# 				Default (>= 8.0.3) 		10
# 				Default (<= 8.0.2) 		0
# 				Min 							0
# 				Max 							99.99
#
# 				Defines a low water mark representing the percentage of dirty pages at which preflushing is enabled to control the dirty
# 				page ratio. A value of 0 disables the pre-flushing behavior entirely.
#
# 				For more information, see SECTION 15.8.3.6, "FINE-TUNING InnoDB BUFFER POOL FLUSHING"
#
# 			) innodb_max_purge_lag
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-max-purge-lag=#
# 				Sys var 						innodb_max_purge_lag
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						0
# 				Min 							0
# 				Max 							4294967295
#
# 				Defines the maximum length of the purge queue. The default value of 0 indicates no limit (no delays)
#
# 				Use this option to impose a delay for INSERT, UPDATE and DELETE operations when purge operations are lagging
# 				(see SECTION 15.3, "InnoDB MULTI-VERSIONING")
#
# 				The InnoDB transaction system maintains a list of transactions that have index records delete-marked by UPDATE
# 				or DELETE operations. The length of the list represents the purge_lag value.
#
# 				When purge_lag exceeds innodb_max_purge_lag, INSERT, UPDATE and DELETE operations are delayed. Prior to MySQL
# 				8.0.14, the delay calculation is (purge_lag/innodb_max_purge_lag - 0.5) * 10000, which results in a minimum delay
# 				of 5000 microseconds.
#
# 				As of MySQL 8.0.14, the delay calculation is (purge_lag/innodb_max_purge_lag - 0.9995) * 10000, which results in a
# 				minimum delay of 5 microseconds.
#
# 				To prevent excessive delays in extreme situations where purge_lag becomes huge, you can limit the delay by setting
# 				the innodb_max_purge_lag_delay variable.
#
# 				The delay is computed at the beginning of a purge batch.
#
# 				A typical setting for a problematic workload might be 1 million, assuming that transactions are small, only
# 				100 bytes in size, and it is permissible to have 100MB of unpurged InnoDB table rows.
#
# 				The lag value is displayed as the history list length in the TRANSACTIONS section of InnoDB Monitor output.
#
# 				The lag value is 20 in this example output:
#
# 					----------------
# 					TRANSACTIONS
# 					----------------
# 					Trx id counter 0 290328385
# 					Purge done for trx's n:o < 0 290315608 undo n:o < 0 17
# 					History list length 20
#
# 				For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 			) innodb_max_purge_lag_delay
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-max-purge-lag-delay=#
# 				Sys var 					innodb_max_purge_lag_delay
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					0
# 				Min 						0
#
# 				Specifies the maximum delay in microseconds for the delay imposed by the innodb_max_purge_lag variable.
#
# 				The specified value is the upper limit on the delay period computed from the formula based on the value
# 				of innodb_max_purge_lag.
#
# 				For general I/O tuning advice, see SECTION 8.5.8 "OPTIMIZING INNODB DISK I/O"
#
# 			) innodb_max_undo_log_size
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-max-undo-log-size=#
# 				Sys var 					innodb_max_undo_log_size
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					1073741824
# 				Min 						10485760
# 				Max 						2**64-1
#
# 				Defines a threshold size for undo tablespaces. If an undo tablespace exceeds the threshold, it can be marked
# 				for truncation when innodb_undo_log_truncate is enabled.
#
# 				The default value is 1073741824 bytes (1024 MiB)
#
# 				For more information, see TRUNCATING UNDO TABLESPACES
#
# 			) innodb_merge_threshold_set_all_debug
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-merge-threshold-set-all-debug=#
# 				Sys var 					innodb_merge_threshold_set_all_debug
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					50
# 				Min 						1
# 				Max 						50
#
# 				Defines a page-full percentage value for index pages that overrides the current MERGE_THRESHOLD setting
# 				for all indexes that are currently in the dictionary cache.
#
# 				This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.
#
# 				For related information, see SECTION 15.8.11, "CONFIGURING THE MERGE THRESHOLD FOR INDEX PAGES"
#
# 			) innodb_monitor_disable
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-monitor-disable={counter|module|pattern|all}
# 				Sys var 					innodb_monitor_disable
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						String
#
# 				Disable InnoDB metrics counters. Counter data may be queried using the INFORMATION_SCHEMA.INNODB_METRICS table.
#
# 				For usage information, see SECTION 15.14.6, "InnoDB INFORMATION_SCHEMA METRICS TABLE"
#
# 				innodb_monitor_disable='latch' disables statistics collection for SHOW_ENGINE_INNODB_MUTEX.
#
# 				For more information, see SECTION 13.7.6.15, "SHOW ENGINE SYNTAX"
#
# 			) innodb_monitor_enable
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-monitor-enable={counter|module|pattern|all}
# 				Sys var 					innodb_monitor_enable
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						String
#
# 				Enables InnoDB metrics counters. Counter data may be queried using the INFORMATION_SCHEMA.INNODB_METRICS
# 				table.
#
# 				For usage information, see SECTION 15.14.6, "InnoDB INFORMATION_SCHEMA METRICS TABLE"
#
# 				innodb_monitor_enable='latch' enables statistics collections for SHOW_ENGINE_INNODB_MUTEX.
#
# 				For more information, see SECTION 13.7.6.15, "SHOW ENGINE SYNTAX"
#
# 			) innodb_monitor_reset
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-monitor-reset={counter|module|pattern|all}
# 				Sys var 					innodb_monitor_reset
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Enumeration
# 				Default 					empty string
# 				Valid 					counter
# 											module
# 											pattern
# 											all
#
# 				Resets the count value for InnoDB metrics counters to zero. Counter data may be queried using the
# 				INFORMATION_SCHEMA.INNODB_METRICS table.
#
# 				For usage information, see SECTION 15.14.6, "InnoDB INFORMATION_SCHEMA METRICS TABLE"
#
# 				innodb_monitor_reset='latch' resets statistics reported by SHOW_ENGINE_INNODB_MUTEX. 
#
# 				For more information, see SECTION 13.7.6.15, "SHOW ENGINE SYNTAX"
#
# 			) innodb_monitor_reset_all
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-monitor-reset-all={counter|module|pattern|all}
# 				Sys var 					innodb_monitor_reset_all
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Enumeration
# 				Default 					empty string
# 				Valid 					counter
# 											module
# 											pattern
# 											all
#
# 				Resets all values (minimum, maximum and so on) for InnoDB metrics counters.
#
# 				Counter data may be queried using the INFORMATION_SCHEMA.INNODB_METRICS table.
#
# 				For usage information, see SECTION 15.14.6, "InnoDB INFORMATION_SCHEMA METRICS TABLE"
#
# 			) innodb_numa_interleave
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-numa-interleave[={OFF|ON}]
# 				Sys var 					innodb_numa_interleave
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Enables the NUMA interleave memory policy for allocation of the InnoDB buffer pool.
#
# 				When innodb_numa_interleave is enabled, the NUMA memory policy is set to MPOL_INTERLEAVE
# 				for the mysqld process.
#
# 				After the InnoDB buffer pool is allocated, the NUMA memory policy is set back to MPOL_DEFAULT.
#
# 				For the innodb_numa_interleave option to be available, MySQL must be compiled on a NUMA-enabled
# 				Linux system.
#
# 				CMake sets the default WITH_NUMA value based on whether the current platform has NUMA support.
#
# 				For more information, see SECTION 2.9.4, "MySQL SOURCE-CONFIGURATION OPTIONS"
#
# 			) innodb_old_blocks_pct
#
# 				Property 			Value
#
# 				Cmd line 			--innodb-old-blocks-pct=#
# 				Sys var 				innodb_old_blocks_pct
# 				Scope 				Global
# 				Dynamic 				Yes
# 				SET_VAR Hint 		No
# 				Type 					Integer
# 				Default 				37
# 				Min 					5
# 				Max 					95
#
# 				Specifies the approximate percentage of the InnoDB buffer pool used for the old block sublist.
#
# 				The range of values is 5 to 95. The default value is 37 (that is, 3/8 of the pool)
#
# 				Often used in combination with innodb_old_blocks_time
#
# 				For more information, see SECTION 15.8.3.3, "MAKING THE BUFFER POOL SCAN RESISTANT" 
#
# 				For information about buffer pool management, the LRU algorithm, and eviction policies,
# 				see SECTION 15.5.1, "BUFFER POOL"
#
# 			) innodb_old_blocks_time
#
# 				Property 			Value
#
# 				Cmd line 			--innodb-old-blocks-time=#
# 				Sys var 				innodb_old_blocks_time
# 				Scope 				Global
# 				Dynamic 				Yes
# 				SET_VAR Hint 		No
# 				Type 					Integer
# 				Default 				1000
# 				Min 					0
# 				Max 					2**32-1
#
# 				Non-zero values protect against the buffer pool being filled by data that is referenced only for a brief period,
# 				such as during a full table scan. Increasing this value offers more protection against full table scans interfering
# 				with data cached in the buffer pool.
#
# 				Specifies how long in milliseconds a block inserted into the old sublist must stay there after its first access
# 				before it can be moved to the new sublist.
#
# 				If the value is 0, a block inserted into the old sublist moves immediately to the new sublist the first time it is
# 				accessed, no matter how soon after insertion the access occurs.
#
# 				If the value is greater than 0, blocks remain in the old sublist until an access occurs at least that many miliseconds
# 				after the first access. For example, a value of 1000 causes blocks to stay in the old sublist for 1 second after the first
# 				access before they become eligible to move to the new sublist.
#
# 				The default value is 1000.
#
# 				This variable is often used in combination with innodb_old_blocks_pct. For more information, see SECTION 15.8.3.3,
# 				"MAKING THE BUFFER POOL SCAN RESISTANT"
#
# 				For information about buffer pool management, the LRU algorithm, and eviction policies, see SECTION 15.5.1,
# 				"BUFFER POOL"
#
# 			) innodb_online_alter_log_max_size
#
# 				Property 			Value
#
# 				Cmd line 			--innodb-online-alter-log-max-size=#
# 				Sys var 				innodb_online_alter_log_max_size
# 				Scope 				Global
# 				Dynamic 				Yes
# 				SET_VAR Hint 		No
# 				Type 					Integer
# 				Default 				134217728
# 				Min 					65536
# 				Max 					2**64-1
#
# 				Specifies an upper limit in bytes on the size of the temporary log files used during online DDL operations for InnoDB tables.
#
# 				There is one such log file for each index being created or table being altered. This log file stores data inserted, updated,
# 				or deleted in the table during the DDL operation.
#
# 				THe temporary log file is extended when needed by the value of innodb_sort_buffer_size, up to the maximum specified by 
# 				innodb_online_alter_log_max_size. If a temporary log file exceeds the upper size limit, the ALTER_TABLE operation fails
# 				and all uncommitted concurrent DML operations are rolled back.
#
# 				Thus, a large value for this option allows more DML to happen during an online DDL operation, but also extends the period of
# 				time at the end of the DDL operation when the table is locked to apply the data from the log.
#
# 			) innodb_open_files
#
# 				Property 			Value
#
# 				Cmd line 			--innodb-open-files=#
# 				Sys var 				innodb_open_files
# 				Scope 				Global
# 				Dynamic 				No
# 				SET_VAR Hint 		No
# 				Type 					Integer
# 				Default 				-1 (signifies autosizing; do not assign this literal value)
# 				Min 					10
# 				Max 					4294967295
#
# 				This variable is only relevant if you use multiple InnoDB tablespaces. It specifies the maximum number of .ibd
# 				files that MySQL can keep open at one time.
#
# 				The minimum value is 10. The default value is 300 if innodb_file_per_table is not enabled, and the higher of
# 				300 and table_open_cache otherwise.
#
# 				The file descriptors used for .ibd files are for InnoDB tables only. They are independent of those specified by the
# 				open_files_limit system variable, and do not affect the operation of the table cache.
#
# 				For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 			) innodb_optimize_fulltext_only
#
# 				Property 			Value
#
# 				Cmd line 			--innodb-optimize-fulltext-only[={OFF|ON}]
# 				Sys var 				innodb_optimize_fulltext_only
# 				Scope 				Global
# 				Dynamic 				Yes
# 				SET_VAR Hint 		No
# 				Type 					Boolean
# 				Default 				OFF
#
# 				Changes the way OPTIMIZE_TABLE operates on InnoDB tables. Intended to be enabled temporarily, during maintenance
# 				operations for InnoDB tables with FULLTEXT indexes.
#
# 				By default, OPTIMIZE_TABLE reorganizes data in the clustered index of the table. When this option is enabled,
# 				OPTIMIZE_TABLE skips the reorganization of table data, and instead processes newly added, deleted and updated
# 				token data for InnoDB FULLTEXT indexes.
#
# 				For more information, see OPTIMIZING INNODB FULL-TEXT INDEXES
#
# 			) innodb_page_cleaners
#
# 				Property 			Value
#
# 				Cmd line 			--innodb-page-cleaners=#
# 				Sys var 				innodb_page_cleaners
# 				Scope 				Global
# 				Dynamic 				No
# 				SET_VAR Hint 		No
# 				Type 					Integer
# 				Default 				4
# 				Min 					1
# 				Max 					64
#
# 				The number of page cleaner threads that flush dirty pages from buffer pool instances. Page cleaner threads
# 				perform flush list and LRU flushing.
#
# 				When there are multiple page cleaner threads, buffer pool flushing tasks for each buffer pool instance are
# 				dispatched to idle page cleaner threads.
#
# 				The innodb_page_cleaners default value is 4.
#
# 				If the number of page cleaner threads exceeds the number of buffer pool instances, innodb_page_cleaners
# 				is automatically set to the same value as innodb_buffer_pool_instances
#
# 				If your workload is write-IO bound when flushing dirty pages from buffer pool instances to data files,
# 				and if your system hardware has available capacity, increasing the number of page cleaner threads
# 				may help improve write-IO throughput.
#
# 				Multithreaded page cleaner support extends to shutdown and recovery phases.
#
# 				The setpriority() system call is used on Linux platforms where it is supported, and where the mysqld execution
# 				user is authorized to give page_cleaner threads priority over other MySQL and InnoDB threads to help page flushing
# 				keep pace with the current workload.
#
# 				setpriority() support is indicated by this InnoDB startup message:
#
# 					[Note] InnoDB: If the mysqld execution user is authorized, page cleaner
# 					thread priority can be changed. See the man page of setpriority()
#
# 				For systems where server startup and shutdown is not managed by systemd, mysqld execution user authorization
# 				can be configured in /etc/security/limits.conf
#
# 				For example, if mysqld is run under the mysql user, you can authorize the mysql user by adding these lines to
# 				/etc/security/limits.conf:
#
# 					mysql 			hard 	nice 		-20
# 					mysql 			soft  nice 		-20
#
# 				For systemd managed systems, the same can be achieved by specifying LimitNICE=-20 in a localized systemd configuration
# 				file. For example, create a file named override.conf in /etc/systemd/system/mysqld.service.d/override.conf and add this entry:
#
# 					[Service]
# 					LimitNICE=-20
#
# 				After creating or changing override.conf, reload the systemd configuration, then tell systemd to restart the MySQL service:
#
# 					systemctl daemon-reload
# 					systemctl restart mysqld 	#RPM platforms
# 					systemctl restart mysql 	# Debian platforms
# 	
# 				For more information about using a localized systemd configuration file, see CONFIGURING SYSTEMD FOR MYSQL
#
# 				After authorizing the mysqld execution user, use the cat command to verify the configured Nice limits for
# 				the mysqld process:
#
# 					shell> cat /proc/mysqld_pid/limits | grep nice
# 					Max nice priority 			<limit numerals> <limit numerals>
#
# 			) innodb_page_size
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-page-size=#
# 				Sys var 					innodb_page_size
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Enumeration
# 				Default 					16384
# 				Valid 					4096
# 											8192
# 											16384
# 											32768
# 											65536
#
# 				Specifies the page size for InnoDB tablespaces. Values can be specified in bytes or kilobytes. For example, 
# 				a 16 kilobyte page size value can be specified as 16384, 16kb, or 16k.
#
# 				innodb_page_size can only be configured prior to initializing the MySQL instance and cannot be changed afterward.
#
# 				If no value is specified, the instance is initialized using the default page size. See SECTION 15.8.1, "InnoDB STARTUP CONFIGURATION"
#
# 				For both 32kb and 64kb page sizes, the maximum row length is approximately 16000 bytes. ROW_FORMAT=COMPRESSED is not supported when
# 				innodb_page_size is set to 32kb or 64kb.
#
# 				For innodb_page_size=32kb, extent size is 2MB. For innodb_page_size=64kb, extent size is 4MB.
#
# 				innodb_log_buffer_size should be set to at least 16M (the default) when using 32kb or 64kb page sizes.
#
# 				The default 16kb page size or larger is appropriate for a wide range of workloads, particularly for queries involving
# 				table scans and DML operations involving bulk updates.
#
# 				Smaller page sizes might be more efficient for OLTP workloads involving many small writes, where contention can be
# 				an issue when single pages contain many rows.
#
# 				Smaller pages might also be efficient with SSD storage devices, which typically use small block sizes. Keeping the 
# 				InnoDB page size close to the storage device block size minimizes the amount of unchanged data that is rewritten to disk.
#
# 				The minimum file size for the first system tablespace data file (ibdata1) differs depending on the innodb_page_size value.
#
# 				See the innodb_data_file_path option description for more information.
#
# 				For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 			) innodb_parallel_read_threads
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-parallel-read-threads=#
# 				Introduced 					8.0.14
# 				Sys var 						innodb_parallel_read_threads
# 				Scope 						Session
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						4
# 				Min 							1
# 				Max 							256
#
# 				Defines the number of threads that can be used for parallel clustered index reads. Parallel scanning of partitions
# 				is supported as of MySQL 8.0.17. Parallel read threads can improve CHECK_TABLE performance.
#
# 				InnoDB reads the clustered index twice during a CHECK_TABLE operation. The second read can be performed in parallel.
# 				This feature does not apply to secondary index scans. The innodb_parallel_read_threads session variable must be set
# 				to a value greater than 1 for parallel clustered index reads to occur.
#
# 				The actual number of threads used to perform a parallel clustered index read is determined by the innodb_parallel_read_threads
# 				setting or the number of index subtrees to scan, whichever is smaller.
#
# 				The pages read into the buffer pool during the scan are kept at the tail of the buffer pool LRU list so that
# 				they can be discarded quickly when free buffer pool pages are required.
#
# 				As of MySQL 8.0.17, the maximum number of parallel read threads (256) is the total number of threads for all
# 				client connections.
#
# 				If the thread limit is reached, connections fall back to using a single thread.
#
# 			) innodb_print_all_deadlocks
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-print-all-deadlocks[={OFF|ON}]
# 				Sys var 						innodb_print_all_deadlocks
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						OFF
#
# 				When this option is enabled, information about all deadlocks in InnoDB user transactions is recorded in the mysqld error log.
#
# 				Otherwise, you see information about only the last deadlock, using the SHOW ENGINE INNODB STATUS command.
#
# 				An occasional InnoDB deadlock is not necessarily an issue, because InnoDB detects the condition immediately and rolls
# 				back one of the transactions automatically.
#
# 				You might use this option to troubleshoot why deadlocks are occurring if an application does not have appropriate
# 				error-handling logic to detect the rollback and retry its operation.
#
# 				A large number of deadlocks might indicate the need to restructure transactions that issue DML or SELECT /etc/ FOR UPDATE
# 				statements for multiple tables, so that each transaction accesses the tables in the same order, thus avoiding
# 				the deadlock condition.
#
# 				For related information, see SECTION 15.7.5, "DEADLOCKS IN INNODB"
#
# 			) innodb_print_ddl_logs
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-print-ddl-logs[={OFF|ON}]
# 				Introduced 					8.0.3
# 				Sys var 						innodb_print_ddl_logs
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						OFF
#
# 				Enabling this option causes MySQL to write DDL logs to stderr. For more information, see Viewing DDL Logs.
#
# 			) innodb_purge_batch_size
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-purge-batch-size=#
# 				Sys var 						innodb_purge_batch_size
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						300
# 				Min 							1
# 				Max 							5000
#
# 				Defines the number of undo log pages that purge parses and processes in one batch from the history list.
#
# 				In a multithreaded purge configuration, the coordinator purge thread divides innodb_purge_batch_size by 
# 				innodb_purge_threads and assigns that number of pages to each purge thread.
#
# 				The innodb_purge_batch_size option also defines the number of undo log pages that purge frees after
# 				every 128 iterations through the undo logs.
#
# 				The innodb_purge_batch_size option is intended for advanced performance tuning in combination with the
# 				innodb_purge_threads setting.
#
# 				Most MySQL users need not change innodb_purge_batch_size from its default value.
#
# 				For related information, see SECTION 15.8.9, "CONFIGURING INNODB PURGE SCHEDULING"
#
# 			) innodb_purge_threads
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-purge-threads=#
# 				Sys var 						innodb_purge_threads
# 				Scope 						Global
# 				Dynamic 						No
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						4
# 				Min 							1
# 				Max 							32
#
# 				The number of background threads devoted to the InnoDB purge operation. A minimum value of 1 signifies that the
# 				purge operation is always performed by a background thread, never as part of the master thread.
#
# 				Running the purge operation in one or more background threads helps reduce internal contention within InnoDB,
# 				improving scalability. Increasing the value to greater than 1 creates that many separate purge threads, which can
# 				improve efficiency on systems where DML operations are performed on multiple tables.
#
# 				The maximum is 32.
#
# 				For related information, see SECTION 15.8.9, "CONFIGURING INNODB PURGE SCHEDULING"
#
# 			) innodb_purge_rseg_truncate_frequency
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-purge-rseg-truncate-frequency=#
# 				Sys var 						innodb_purge_rseg_truncate_frequency
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						128
# 				Min 							1
# 				Max 							128
#
# 				Defines the frequency with which the purge system frees rollback segments in terms of the number of times
# 				that purge is invoked.
#
# 				An undo tablespace cannot be truncated until its rollback segments are freed.
#
# 				Normally, the purge system frees rollback segments once every 128 times that purge is invoked.
#
# 				The default value is 128.
#
# 				Reducing this value increases the frequency with which the purge thread frees rollback segments.
#
# 				innodb_purge_rseg_truncate_frequency is intended for use with innodb_undo_log_truncate.
# 				For more information, see TRUNCATING UNDO TABLESPACES.
#
# 			) innodb_random_read_ahead
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-random-read-ahead[={OFF|ON}]
# 				Sys var 						innodb_random_read_ahead
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						OFF
#
# 				Enables the random read-ahead technique for optimizing InnoDB I/O.
#
# 				For details about performance considerations for different types of read-ahead requests,
# 				see SECTION 15.8.3.4, "CONFIGURING INNODB BUFFER POOL PREFETCHING (READ-AHEAD)"
#
# 				For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 			) innodb_read_ahead_threshold
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-read-ahead-threshold=#
# 				Sys var 						innodb_read_ahead_threshold
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						56
# 				Min 							0
# 				Max 							64
#
# 				Controls the sensitivity of linear read-ahead that InnoDB uses to prefetch pages into the buffer pool.
#
# 				If InnoDB reads at least innodb_read_ahead_threshold pages sequentially from an extent (64 pages),
# 				it initiates an asynch read for the entire following extent.
#
# 				The permissible range of values is 0 to 64.
#
# 				A value of 0 disables read-ahead. For the default of 56, InnoDB must read at least 56 pages sequentially
# 				from an extent to initiate an asynch read for the following extent.
#
# 				Knowing how many pages are read through the read-ahead mechanism, and how many of these pages are evicted
# 				from the buffer pool without ever being accessed, can be useful when fine-tuning the innodb_read_ahead_threshold
# 				setting.
#
# 				SHOW_ENGINE_INNODB_STATUS output displays counter information from the Innodb_buffer_pool_read_ahead and
# 				Innodb_buffer_pool_read_ahead_evicted global status variables, which report the number of pages brought into the
# 				buffer pool by read-ahead requests, and the number of such pages evicted from the buffer pool without ever
# 				being accessed, respectively.
#
# 				The status variables report global values since the last server restart.
#
# 				SHOW_ENGINE_INNODB_STATUS also shows the rate at which the read-ahead pages are read and the rate at which
# 				such pages are evicted without being accessed.
#
# 				The per-second averages are based on the statistics collected since the last invocation of SHOW ENGINE INNODB STATUS
# 				and are displayed in the BUFFER POOL AND MEMORY section of the SHOW_ENGINE_INNODB_STATUS output.
#
# 				For more information, see SECTION 15.8.3.4, "CONFIGURING InnoDB BUFFER POOL PREFETCHING (READ-AHEAD)"
#
# 				For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 			) innodb_read_io_threads
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-read-io-threads=#
# 				Sys var 						innodb_read_io_threads
# 				Scope 						Global
# 				Dynamic 						No
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						4
# 				Min 							1
# 				Max 							64
#
# 				The number of I/O threads for read operations in InnoDB. Its counterpart for write threads is
# 				innodb_write_io_threads.
#
# 				For more information, see SECTION 15.8.5, "CONFIGURING THE NUMBER OF BACKGROUND INNODB I/O THREADS"
#
# 				For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 				NOTE:
#
# 					On Linux systems, running multiple MySQL servers (typically more than 12) with default settings for 
# 					innodb_read_io_threads, innodb_write_io_threads, and the Linux aio-max-nr setting can exceed system
# 					limits.
#
# 					Ideally, increase the aio-max-nr setting; as a workaround, you might reduce the settings for one or both
# 					of the MySQL variables.
#
# 			) innodb_read_only
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-read-only[={OFF|ON}]
# 				Sys var 						innodb_read_only
# 				Scope 						Global
# 				Dynamic 						No
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						OFF
#
# 				Starts InnoDB in read-only mode. For distributing database applications or data sets on read-only media.
#
# 				Can also be used in data warehouses to share the same data directory between multiple instances.
# 				For more information, see SECTION 15.8.2, "CONFIGURING INNODB FOR READ-ONLY OPERATION"
#
# 				Previously, enabling the innodb_read_only system variable prevented creating and dropping tables only
# 				for the InnoDB storage engine.
#
# 				As of MySQL 8.0, enabling innodb_read_only prevents these operations for all storage engines.
# 				Table creation and drop operations for any storage engine modify data dictionary tables in
# 				the mysql system database, but those tables use the InnoDB storage engine and cannot be modified
# 				when innodb_read_only is enabled.
#
# 				The same principle applies to other table operations that require modifying data dictionary tables.
#
# 				Examples:
#
# 					) if the innodb_read_only system variable is enabled, ANALYZE_TABLE may fail because it cannot update
# 						statistics tables in the data dictionary, which uses InnoDB.
#
# 						For ANALYZE_TABLE operations that update the key distribution, failure may occur even if the 
# 						operation updates the table itself (for example, if it is a MyISAM table)
#
# 						To obtain the updated distribution statistics, set information_schema_stats_expiry=0
#
# 					) ALTER_TABLE_tbl_name_ENGINE=engine_name fails because it updates the storage engine designation,
# 						which is stored in the data dictionary.
#
# 				In addition, other tables in the mysql system database use the InnoDB storage engine in MySQL 8.0
#
# 				Making those tables read only results in restrictions on operations that modify them. Examples:
#
# 					) Account-management statements such as CREATE_USER and GRANT fail because the grant tables use InnoDB
#
# 					) The INSTALL_PLUGIN and UNINSTALL_PLUGIN plugin-management statements fail because the plugin table uses InnoDB
#
# 					) The CREATE_FUNCTION and DROP_FUNCTION UDF-management statements fail because the func table uses InnoDB.
#
# 			) innodb_redo_log_archive_dirs
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-redo-log-archive-dirs
# 				Introduced 				8.0.17
# 				Sys var 					innodb_redo_log_archive_dirs
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						String
# 				Default 					NULL
#
# 				Defines labeled directories where redo log archive files can be created. You can define multiple
# 				labeled directories in a semicolon-separated list. For example:
#
# 					innodb_redo_log_archive_dirs='label1:/backups1;label2:/backups2'
#
# 				A label can be any string of characters, with the exception of colons (:), which are not permitted.
# 				An empty label is also permitted, but the colon (:) is still required in this case.
#
# 				A path must be specified, and the directory must exist. The path can contain colons (':'), but semicolons
# 				(;) are not permitted.
#
# 			) innodb_redo_log_encrypt
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-redo-log-encrypt[={OFF|ON}]
# 				Introduced 				8.0.1
# 				Sys var 					innodb_redo_log_encrypt
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Controls encryption of redo log data for tables encrypted using the InnoDB data-at-rest encryption feature.
#
# 				Encryption of redo log data is disabled by default. For more information, see Redo Log Encryption.
#
# 			) innodb_replication_delay
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-replication-delay=#
# 				Sys var 					innodb_replication_delay
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					0
# 				Min 						0
# 				Max 						4294967295
#
# 				The replication thread delay in milliseconds on a slave server if innodb_thread_concurrency is reached.
#
# 			) innodb_rollback_on_timeout
#
# 				Property 				Value
#
# 				cmd line 				--innodb-rollback-on-timeout[={OFF|ON}]
# 				Sys var 					innodb_rollback_on_timeout
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				InnoDB rolls back only the last statement on a transaction timeout by default.
#
# 				If --innodb-rollback-on-timeout is specified, a transaction timeout causes InnoDB to
# 				abort and roll back the entire transaction.
#
# 				NOTE:
#
# 					If the start-transaction statement was START_TRANSACTION or BEGIN statement, rollback does not
# 					cancel that statement.
#
# 					Further SQL statements become part of the transaction until the occurrence of COMMIT, ROLLBACK
# 					or some SQL statement that causes an implicit commit.
#
# 				For more information, see SECTION 15.20.4, "InnoDB ERROR HANDLING"
#
# 			) innodb_rollback_segments
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-rollback-segments=#
# 				Sys var 					innodb_rollback_segments
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					128
# 				Min 						1
# 				Max 						128
#
# 				innodb_rollback_segments defines the number of rollback segments allocated to each undo tablespace
# 				and the global temporary tablespace for transactions that generate undo records.
#
# 				The number of transactions that each rollback segment supports depends on the InnoDB page size
# 				and the number of undo logs assigned to each transaction.
#
# 				For more information, see SECTION 15.6.6, "UNDO LOGS"
#
# 				For related information, see SECTION 15.3, "InnoDB MULTI-VERSIONING"
#
# 				For information about undo tablespaces, see SECTION 15.6.3.4, "UNDO TABLESPACES"
#
# 			) innodb_scan_directories
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-scan-directories=dir_name
# 				Introduced 				8.0.2
# 				Removed 					8.0.4
# 				Sys var 					innodb_scan_directories
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Dir name
# 				Default 					NULL
#
# 				This variable was replaced by innodb_directories in MySQL 8.0.4
#
# 			) innodb_saved_page_number_debug
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-saved-page-number-debug=#
# 				Sys var 					innodb_saved_page_number_debug
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					0
# 				Max 						2**23-1
#
# 				Saves a page number. Setting the innodb_fil_make_page_dirty_debug option dirties the page defined by
# 				innodb_saved_page_number_debug.
#
# 				The innodb_saved_page_number_debug option is only available if debugging support is compiled in using
# 				the WITH_DEBUG CMake option.
#
# 			) innodb_sort_buffer_size
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-sort-buffer-size=#
# 				Sys var 					innodb_sort_buffer_size
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					1048576
# 				Min 						65536
# 				Max 						67108864
#
# 				Specifies the size of sort buffers used to sort data during creation of an InnoDB index.
#
# 				The specified size defines the amount of data that is read into memory for internal sorting
# 				and then written out to disk.
#
# 				This process is referred to as a "run". During the merge phase, pairs of buffers of the 
# 				specified size are read and merged.
#
# 				The larger the setting, the fewer runs and merges there are.
#
# 				This sort area is only used for merge sorts during index creation, not during later index
# 				maintenance operations. Buffers are deallocated when index creation completes.
#
# 				The value of this option also controls the amount by which the temporary log file is extended
# 				to record concurrent DML during online DDL operations.
#
# 				Before this setting was made configurable, the size was hardcoded to 1048576 (1MB), which remains
# 				the default.
#
# 				During an ALTER_TABLE or CREATE_TABLE statement that creates an index, 3 buffers are allocated,
# 				each with a size defined by this option.
#
# 				Additionally, auxiliary pointers are allocated to rows in the sort buffer so that the sort can run
# 				on pointers (as opposed to moving rows during the sort operation)
#
# 				For a typical sort operation, a formula such as this one can be used to estimate memory consumption:
#
# 					(6 /*FTS_NUM_AUX_INDEX*/ * (3**@@GLOBAL.innodb_sort_buffer_size)
# 					+ 2 * number_of_partitions * number_of_secondary_indexes_created
# 					* (@@GLOBAL.innodb_sort_buffer_size/dict_index_get_min_size(index)*/)
# 					* 8 /*64-bit sizeof *buf->tuples*/")
#
# 				@@GLOBAL.innodb_sort_buffer_size/dict_index_get_min_size(index) indicates the maximum tuples held.
#
# 				2 * (@@GLOBAL.innodb_sort_buffer_size/*dict_index_get_min_size(index)*/) * 8 /*64-bit size of *buf->tuples*/
# 				indicates auxiliary pointers allocated.
#
# 				NOTE:
#
# 					For 32-bit, multiply by 4 instead of 8.
#
# 				For parallel sortts on a full-text index, multiply by the innodb_ft_sort_pll_degree setting:
#
# 					(6 /*FTS_NUM_AUX_INDEX*/ * @@GLOBAL.innodb_ft_sort_pll_degree)
#
# 			) innodb_spin_wait_delay
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-spin-wait-delay=#
# 				Sys var 							innodb_spin_wait_delay
# 				Scope 							Global
# 				Dynamic 							Yes
# 				SET_VAR Hint 					No
# 				Type 								Integer
# 				Default 							6
# 				Min 								0
# 				Max (64-bit <= 8.0.13) 		2**64-1
# 				Max (32-bit <= 8.0.13) 		2**32-1
# 				Max (>= 8.0.14) 				1000
#
# 				The maximum delay between polls for a spin lock. The low-level implementation of this mechanism
# 				varies depending on the combination of hardware and operating system, so the delay does not
# 				correspond to a fixed time interval.
#
# 				Can be used in combination with the innodb_spin_wait_pause_multiplier variable for greater control over
# 				the duration of spin-lock polling delays.
#
# 				For more information, see SECTION 15.8.8, "CONFIGURING SPIN LOCK POLLING"
#
# 			) innodb_spin_wait_pause_multiplier
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-spin-wait-pause-multiplier=#
# 				Introduced 						8.0.16
# 				Sys var 							innodb_spin_wait_pause_multiplier
# 				Scope 							Global
# 				Dynamic 							Yes
# 				SET_VAR Hint 					No
# 				Type 								Integer
# 				Default 							50
# 				Min 								1
# 				Max 								100
#
# 				Defines a multiplier value used to determine the number of PAUSE instructions in spin-wait loops that
# 				occur when a thread waits to acquire a mutex or rw-lock.
#
# 				For more information, see SECTION 15.8.8, "CONFIGURING SPIN LOCK POLLING"
#
# 			) innodb_stats_auto_recalc
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-stats-auto-recalc[={OFF|ON}]
# 				Sys var 							innodb_stats_auto_recalc
# 				Scope 							Global
# 				Dynamic 							Yes
# 				SET_VAR Hint 					No
# 				Type 								Boolean
# 				Default 							ON
#
# 				Causes InnoDB to automatically recalculate persistent statistics after the data in a table is changed
# 				substantially. The threshold value is 10% of the rows in the table.
#
# 				This setting applies to tables created when the innodb_stats_persistent option is enabled. Automatic
# 				statistics recalculation may also be configured by specifying STATS_PERSISTENT=1 in a CREATE_TABLE
# 				or ALTER_TABLE statement.
#
# 				The amount of data sampled to produce the statistics is controlled by the innodb_stats_persistent_sample_pages
# 				variable.
#
# 				For more information, see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 			) innodb_stats_include_delete_marked
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-stats-include-delete-marked[={OFF|ON}]
# 				Introduced 						8.0.1
# 				Sys var 							innodb_stats_include_delete_marked
# 				Scope 							Global
# 				Dynamic 							Yes
# 				SET_VAR Hint 					No
# 				Type 								Boolean
# 				Default 							OFF
#
# 				By default, InnoDB reads uncommitted data when calculating statistics.
#
# 				In the case of an uncommitted transaction that deletes rows from a table, InnoDB excludes
# 				records that are delete-marked when calculating row estimates and index statistics, which can lead
# 				to non-optimal execution plans for other transactions that are operating on the table concurrently
# 				using a transaction isolation level other than READ_UNCOMMITTED.
#
# 				To avoid this scenario, innodb_stats_include_delete_marked can be enabled to ensure that InnoDB
# 				includes delete-marked records when calculating persistent optimizer statistics.
#
# 				When innodb_stats_include_delete_marked is enabled, ANALYZE_TABLE considers delete-marked records when
# 				recalculating statistics.
#
# 				innodb_stats_include_delete_marked is a global setting that affects all InnoDB tables. It is only applicable
# 				to persistent optimizer statistics.
#
# 				For related information, see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 			) innodb_stats_method
#
# 				Property 						Value
#
# 				Cmd line 						--innodb-stats-method=value
# 				Sys var 							innodb_stats_method
# 				Scope 							Global
# 				Dynamic 							Yes
# 				SET_VAR Hint 					No
# 				Type 								Enumeration
# 				Default 							nulls_equal
# 				Valid 							nulls_equal
# 													nulls_unequal
# 													nulls_ignored
#
# 				How the server treats NULL values when collecting statistics about the distribution of index values for InnoDB tables.
#
# 				Permitted values are nulls_equal, nulls_unequal and nulls_ignored. For nulls_equal, all NULL index values are considered
# 				equal and form a single value group with a size equal to the number of NULL values.
#
# 				For nulls_unequal, NULL values are considered unequal, and each NULL forms a distinct value group of size 1.
#
# 				For nulls_ignored, NULL values are ignored.
#
# 				The method used to generate table statistics influences how the optimizer chooses indexes for query execution,
# 				as described in SECTION 8.3.8, "InnoDB and MyISAM INDEX STATISTICS COLLECTION"
#
# 			) innodb_stats_on_metadata
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-stats-on-metadata[={OFF|ON}]
# 				Sys var 						innodb_stats_on_metadata
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						OFF
#
# 				This option only applies when optimizer statistics are configured to be non-persistent. Optimizer statistics are not persisted
# 				to disk when innodb_stats_persistent is disabled or when individual tables are created or altered with STATS_PERSISTENT=0
#
# 				For more information, see SECTION 15.8.10.2, "CONFIGURING NON-PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 				When innodb_stats_on_metadata is enabled, InnoDB updates non-persistent statistics when metadata statements such as
# 				SHOW_TABLE_STATUS or when accessing the INFORMATION_SCHEMA.TABLES or INFORMATION_SCHEMA.STATISTICS tables.
#
# 				(These updates are similar to what happens for ANALYZE_TABLE). When disabled, InnoDB does not update statistics during
# 				these operations. Leaving the setting disabled can improve access speed for schemas that have a large number of tables
# 				or indexes.
#
# 				It can also improve the stability of execution plans for queries that involve InnoDB tables.
#
# 				To change the setting, issue the statement SET GLOBAL innodb_stats_on_metadata=mode, where mode is either ON
# 				or OFF (or 1 or 0). Changing the setting requires privileges sufficient to set global system variables.
#
# 				(See SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES") and immediately affects the operation of all connections.
#
# 			) innodb_stats_persistent
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-stats-persistent[={OFF|ON}]
# 				Sys var 						innodb_stats_persistent
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Boolean
# 				Default 						ON
#
# 				Specifies whether InnoDB index statistics are persisted to disk. Otherwise, statistics may be recalculated
# 				frequently which can lead to variations in query execution plans.
#
# 				This setting is stored with each table when the table is created. You can set innodb_stats_persistent at
# 				the global level before creating a table, or use the STATS_PERSISTENT clause of the CREATE_TABLE and ALTER_TABLE
# 				statements to override the system-wide setting and configure persistent statistics for individual tables.
#
# 				For more information, see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 			) innodb_stats_persistent_sample_pages
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-stats-persistent-sample-pages=#
# 				Sys var 						innodb_stats_persistent_sample_pages
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						20
#
# 				The number of index pages to sample when estimating cardinality and other statistics for an indexed column,
# 				such as those calculated by ANALYZE_TABLE.
#
# 				Increasing the value improves the accuracy of index statistics, which can improve the query execution plan,
# 				at the expense of increased I/O during the execution of ANALYZE_TABLE for an InnoDB table.
#
# 				For more information, see SECTION 15.8.10.1, "CONFIGURING PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 				NOTE:
#
# 					Setting a high value for innodb_stats_persistent_sample_pages could result in lengthy ANALYZE_TABLE
# 					execution time.
#
# 					To estimate the number of database pages accessed by ANALYZE_TABLE, see SECTION 15.8.10.3, "ESTIMATING ANALYZE TABLE COMPLEXITY FOR INNODB TABLES"
#
# 				innodb_stats_persistent_sample_pages only applies when innodb_stats_persistent is enabled for a table;
# 				when innodb_stats_persistent is disabled, innodb_stats_transient_sample_pages applies instead.
#
# 			) innodb_stats_transient_sample_pages
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-stats-transient-sample-pages=#
# 				Sys var 						innodb_stats_transient_sample_pages
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						8
#
# 				The number of index pages to sample when estimating cardinality and other statistics for an indexed column,
# 				such as those calculated by ANALYZE_TABLE.
#
# 				The default value is 8.
#
# 				Increasing the value improves the accuracy of index statistics, which can improve the query execution plan,
# 				at the expense of increased I/O when opening an InnoDB table or recalculating statistics.
#
# 				For more information, see SECTION 15.8.10.2, "CONFIGURING NON-PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 				NOTE:
#
# 					Setting a high value for innodb_stats_transient_sample_pages could result in lengthy ANALYZE_TABLE
# 					execution time. To estimate the number of database pages accessed by ANALYZE_TABLE.
#
# 					See SECTION 15.8.10.3, "ESTIMATING ANALYZE TABLE COMPLEXITY FOR INNODB TABLES"
#
# 				innodb_stats_transient_sample_pages only applies when innodb_stats_persistent is disabled for a table;
# 				when innodb_stats_persistent is enabled, innodb_stats_persistent_sample_pages applies instead.
#
# 				Takes the place of innodb_stats_sample_pages. For more information, see SECTION 15.8.10.2,
# 				"CONFIGURING NON-PERSISTENT OPTIMIZER STATISTICS PARAMETERS"
#
# 			) innodb_status_output
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-status-output[={OFF|ON}]
# 				Sys var 					innodb_status_output
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Enables or disables periodic output for the standard InnoDB Monitor. ALso used in combination
# 				with innodb_status_output_locks to enable or disable periodic output for the InnoDB lock Monitor.
#
# 				For more information, see SECTION 15.16.2, "ENABLING INNODB MONITORS"
#
# 			) innodb_status_output_locks
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-status-output-locks[={OFF|ON}]
# 				Sys var 					innodb_status_output_locks
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Enables or disables the InnoDB Lock Monitor. When enabled, the InnoDB Lock Monitor prints additional
# 				information about locks in SHOW ENGINE INNODB STATUS output and periodic output printed to the MySQL
# 				error log.
#
# 				Periodic output for the InnoDB Lock Monitor is printed as part of the standard InnoDB Monitor output.
#
# 				The standard InnoDB Monitor must therefore be enabled for the InnoDB Lock Monitor to print data to the
# 				MySQL error log periodically. For more information, see SECTION 15.16.2, "ENABLING INNODB MONITORS"
#
# 			) innodb_strict_mode
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-strict-mode[={OFF|ON}]
# 				Sys var 					innodb_strict_mode
# 				Scope 					Global, Session
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					ON
#
# 				When innodb_strict_mode is enabled, InnoDB returns errors rather than warnings for certain conditions.
#
# 				Stirct mode helps guard against ignored typos and syntax errors in SQL, or other unintended consequences
# 				of various combinations of operational modes and SQL statements.
#
# 				When innodb_strict_mode is enabled, InnoDB raises error conditions in certain cases, rather than issuing
# 				a warning and processing the specified statement (perhaps with unintended behavior)
#
# 				This is analogous to sql_mode in MySQL, which controls what SQL syntax MySQL accepts, and determines
# 				whether it silently ignores errors, or validates input syntax and data values.
#
# 				The innodb_strict_mode setting affects the handling of syntax errors for CREATE_TABLE, ALTER_TABLE,
# 				CREATE_INDEX and OPTIMIZE_TABLE statements.
#
# 				innodb_strict_mode also enables a record size check, so that an INSERT or UPDATE never fails due to the
# 				record being too large for the selected page size.
#
# 				Oracle recommends enabling innodb_strict_mode when using ROW_FORMAT and KEY_BLOCK_SIZE clauses in 
# 				CREATE_TABLE, ALTER_TABLE and CREATE_INDEX statements.
#
# 				When innodb_strict_mode is disabled, InnoDB ignores conflicting clauses and creates the table or index
# 				with only a warning in the message log.
#
# 				The resulting table might have different characteristics than intended, such as lack of compression
# 				support when attempting to create a compressed table.
#
# 				When innodb_strict_mode is enabled, such problems generate an immediate error and the table or index
# 				is not created.
#
# 				You can enable or disable innodb_strict_mode on the command line when starting mysqld, or in a MySQL
# 				configuration file. You can also enable or disable innodb_strict_mode at runtime with the statement
# 				SET [GLOBAL|SESSION] innodb_strict_mode=mode, where mode is either ΟΝ or OFF.
#
# 				Changing the GLOBAL setting requires privileges sufficient to set global system variables (see SECTION 5.1.9.1, "SYSTEM VARIABLE PRIVILEGES")
# 				and affects the operation of all clients that subsequently connect.
#
# 				Any client can change the SESSION setting for innodb_strict_mode, and the setting affects only that client.
#
# 				innodb_strict_mode is not applicable to general tablespaces. Tablespace management rules for general tablespaces
# 				are strictly enforced independently of innodb_strict_mode.
#
# 				For more information, see SECTION 13.1.21, "CREATE TABLESPACE SYNTAX"
#
# 			) innodb_sync_array_size
#
# 				Property 				Value
# 				
# 				Cmd line 				--innodb-sync-array-size=#
# 				Sys var 					innodb_sync_array_size
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					1
# 				Min 						1
# 				Max 						1024
#
# 				Defines the size of the mutex/lock wait array. Increasing the value splits the internal data structure used to 
# 				coordinate threads, for higher concurrency in workloads with large numbers of waiting threads.
#
# 				This setting must be configured when the MySQL instance is starting up, and cannot be changed afterward.
#
# 				Increasing the value is recommended for workloads that frequently produce a large number of waiting
# 				threads, typically greater than 768.
#
# 			) innodb_sync_spin_loops
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-sync-spin-loop=#
# 				Sys var 					innodb_sync_spin_loops
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					30
# 				Min 						0
# 				Max 						4294976295
#
# 				The number of times a thread waits for an InnoDB mutex to be freed before the thread is suspended.
#
# 			) innodb_sync_debug
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-sync-debug[={OFF|ON}]
# 				Sys var 					innodb_sync_debug
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Enables sync debug checking for the InnoDB storage engine. This option is only available if debugging
# 				support is compiled in using the WITH_DEBUG CMake option.
#
# 			) innodb_table_locks
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-table-locks[={OFF|ON}]
# 				Sys var 					innodb_table_locks
# 				Scope 					Global, Session
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					ON
#
# 				If autocommit = 0, InnoDB honors LOCK_TABLES; MySQL does not return from LOCK TABLES /etc/ WRITE until all other
# 				threads have released all their locks to the table.
#
# 				The default value of innodb_table_locks is 1, which means that LOCK_TABLES causes InnoDB to lock a table internally
# 				if autocommit = 0
#
# 				In MySQL 8.0, innodb_table_locks = 0 has no effect for tables locked explicitly with LOCK_TABLES_/ETC/_WRITE
#
# 				It does have an effect for tables locked for read or write by LOCK_TABLES_/ETC/_WRITE implicitly (for example,
# 				through triggers) or by LOCK_TABLES_/ETC/_READ
#
# 				For related information, see SECTION 15.7, "InnoDB LOCKING AND TRANSACTION MODEL"
#
# 			) innodb_temp_data_file_path
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-temp-data-file-path=file_name
# 				Sys var 					innodb_temp_data_file_path
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						String
# 				Default 					ibtmp1:12M:autoextend
#
# 				Defines the relative path, name, size and attributes of global temporary tablespace data files.
#
# 				The global temporary tablespace stores rollback segments for changes made to user-created
# 				temporary tables.
#
# 				If no value is specified for innodb_temp_data_file_path, the default behavior is to create a single 
# 				auto-extending data file named ibtmp1 in the innodb_data_home_dir directory.
#
# 				The initial file size is slightly larger than 12MB.
#
# 				The syntax for a global temporary tablespace data file specification includes the file name,
# 				file size, and autoextend and max attributes:
#
# 					file_name:file_size[:autoextend[:max:max_file_size]]
#
# 				The global temporary tablespace data file cannot have the same name as another InnoDB data file.
#
# 				Any inability or error creating the global temporary tablespace data file is treated as fatal
# 				and server startup is refused.
#
# 				File sizes are specified in KB, MB or GB by appending K, M or G to the size value.
#
# 				The sum of file sizes must be slightly larger than 12MB.
#
# 				The size limit of individual files is determined by the operating system. File size can be more
# 				than 4GB on operating systems that support large files.
#
# 				Use of raw disk partitions for global temporary tablespace data files is not supported.
#
# 				The autoextend and max attributes can be used only for the data file specified last in the
# 				innodb_temp_data_file_path setting.
#
# 				For example:
#
# 					[mysqld]
# 					innodb_temp_data_file_path=ibtmp1:50M;ibtmp2:12M:autoextend:max:500MB
#
# 				The autoextend option causes the data file to automatically increase in size when it runs out of
# 				free space.
#
# 				The autoextend increment is 64MB by default. To modify the increment, change the innodb_autoextend_increment
# 				variable setting.
#
# 				The directory path for global temporary tablespace data files is formed by concatenating the paths defined by
# 				innodb_data_home_dir and innodb_temp_data_file_path
#
# 				Before running InnoDB in read-only mode, set innodb_temp_data_file_path to a location outside of the data directory.
#
# 				The path must be relative to the data directory. For example:
#
# 					--innodb-temp-data-file-path=etc/etc/etc/tmp/ibtmp1:12M:autoextend
#
# 				For more information, see GLOBAL TEMPORARY TABLESPACE.
#
# 			) innodb_temp_tablespaces_dir
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-temp-tablespaces-dir=dir_name
# 				Introduced 					8.0.13
# 				Sys var 						innodb_temp_tablespaces_dir
# 				Scope 						Global
# 				Dynamic 						No
# 				SET_VAR Hint 				No
# 				Type 							Directory Name
# 				Default 						#innodb_temp
#
# 				Defines the location where InnoDB creates a pool of session temporary tablespaces at startup. 
# 				The default location is the #innodb_temp directory in the data directory.
#
# 				A fully qualified path or path relative to the data directory is permitted.
#
# 				As of MySQL 8.0.16, session temporary tablespaces always store user-created temporary tables and internal
# 				temporary tables created by the optimizer using InnoDB.
#
# 				(Previously, the on-disk storage engine for internal temporary tables was determined by the internal_tmp_disk_storage_engine
# 				system variable, which is no longer supported.
#
# 				See STORAGE ENGINE FOR ON-DISK INTERNAL TEMPORARY TABLES)
#
# 				For more information, see SESSION TEMPORARY TABLESPACES
#
# 			) innodb_thread_concurrency
#
# 				Property 					Value
#
# 				Cmd line 					--innodb-thread-concurrency=#
# 				Sys var 						innodb_thread_concurrency
# 				Scope 						Global
# 				Dynamic 						Yes
# 				SET_VAR Hint 				No
# 				Type 							Integer
# 				Default 						0
# 				Min 							0
# 				Max 							1000
#
# 				InnoDB tries to keep the number of operating system threads concurrently inside InnoDB less than or equal
# 				to the limit given by this variable (InnoDB uses operating system threads to process user transactions)
#
# 				Once the number of threads reaches this limit, additional threads are placed into a wait state within
# 				a "First in, First Out" (FIFO) queue for execution.
#
# 				Threads waiting for locks are not counted in the number of concurrently executing threads.
#
# 				The range of this variable is 0 to 1000. A value of 0 (the default) is interpreted as infinite concurrency
# 				(no concurrency checking). Disabling thread concurrency checking enables InnoDB to create as many threads
# 				as it needs.
#
# 				A value of 0 also disables the queries inside InnoDB and queries in queue counters in the ROW OPERATIONS
# 				section of SHOW ENGINE INNODB STATUS output.
#
# 				Consider setting this variable if your MySQL instance shares CPU resources with other applications, or if your
# 				workload or number of concurrent users is growing.
#
# 				The correct setting depends on workload, computing environment, and the version of MySQL that you are running.
#
# 				You will need to test a range of values to determine the setting that provides the best performance. innodb_thread_concurrency
# 				is a dynamic variable, which allows you to experiment with different settings on a live test system.
#
# 				If a particular setting performs poorly, you can quickly set innodb_thread_concurrency back to 0.
#
# 				Use the following guidelines to help find and maintain an appropriate setting:
#
# 					) If the number of concurrent user threads for a workload is less than 64, set innodb_thread_concurrency=0
#
# 					) If your workload is consistently heavy or occasionally spikes, start by setting innodb_thread_concurrency=128
# 						and then lowering the value to 96, 80, 64 and so on, until you find the number of threads that provides the
# 						best performance.
#
# 						For example, suppose your system typically has 40 to 50 users, but periodically the number increases to
# 						60, 70 or even 200.
#
# 						You find that performance is stable at 80 concurrent users but starts to show a regression above this number.
# 						In this case, you would set innodb_thread_concurrency=80 to avoid impacting performance.
#
# 					) If you do not want InnoDB to use more than a certain number of virtual CPUs for user threads (20 virtual CPUs,
# 						for example), set innodb_thread_concurrency to this number (or possibly lower, depending on performance results)
#
# 						If your goal is to isolate MySQL from other applications, you may consider binding the mysqld process exclusively
# 						to the virtual CPUs.
#
# 						Be aware, however, that exclusive binding could result in non-optimal hardware usage if the mysqld process is not
# 						consistently busy.
#
# 						In this case, you might bind the mysqld process to the virtual CPUs but also allow other applications to use some
# 						or all of the virtual CPUs.
#
# 						NOTE:
#
# 							From an operating system perspective, using a resource management solution to manage how CPU time is shared
# 							among applications may be preferable to binding the mysqld process.
#
# 							For example, you could assign 90% of virtual CPU time to a given application while other critical processes
# 							are not running, and scale that value back to 40% when other critical processes are running.
#
# 					) innodb_thread_concurrency values that are too high can cause performance regression due to increased contention on system
# 						internals and resources.
#
# 					) In some cases, the optimal innodb_thread_concurrency setting can be smaller than the number of virtual CPUs.
#
# 					) Monitor and analyze your system regularly. Changes to workload, number of users, or computing environment may require
# 						that you adjust the innodb_thread_concurrency setting.
#
# 				For related information, see SECTION 15.8.4, "CONFIGURING THREAD CONCURRENCY FOR INNODB"
#
# 			) innodb_thread_sleep_delay
#
# 				Property 			Value
#
# 				Cmd line 			--innodb-thread-sleep-delay=#
# 				Sys var 				innodb_thread_sleep_delay
# 				Scope 				Global
# 				Dynamic 				Yes
# 				SET_VAR Hint 		No
# 				Type 					Integer
# 				Default 				10000
# 				Min 					0
# 				Max 					1000000
#
# 				How long InnoDB threads sleep before joining the InnoDB queue, in microseconds. The default value is 10000.
#
# 				A value of 0 disables sleep. You can set innodb_adaptive_max_sleep_delay to the highest value you would
# 				allow for innodb_thread_sleep_delay, and InnoDB automatically adjusts innodb_thread_sleep_delay up or down
# 				depending on current thread-scheduling activity.
#
# 				This dynamic adjustment helps the thread scheduling mechanism to work smoothly during times when the system
# 				is lightly loaded or when it is operating near full capacity.
#
# 				For more information, see SECTION 15.8.4, "CONFIGURING THREAD CONCURRENCY FOR INNODB"
#
# 			) innodb_tmpdir
#
# 				Property 			Value
#
# 				Cmd line 			--innodb-tmpdir=dir_name
# 				Sys var 				innodb_tmpdir
# 				Scope 				Global, Session
# 				Dynamic 				Yes
# 				SET_VAR Hint 		No
# 				Type 					Dir name
# 				Default 				NULL
#
# 				Used to define an alternate directory for temporary sort files created during online ALTER_TABLE operations that
# 				rebuild the table.
#
# 				Online ALTER_TABLE operations that rebuild the table also create an intermediate table file in the same directory
# 				as the original table. The innodb_tmpdir option is not applicable to intermediate table files.
#
# 				A valid value is any directory path other than the MySQL data directory path. If the value is NULL (the default),
# 				temporary files are created MySQL temporary directory ($TMPDIR on Unix, %TEMP% on Windows, or the directory specified
# 				by the --tmpdir configuration option).
#
# 				If a directory is specified, existence of the directory and permissions are only checked when innodb_tmpdir is configured
# 				using a SET statement. If a symlink is provided in a directory string, the symlink is resolved and stored as an absolute path.
#
# 				The path should not exceed 512 bytes. An online ALTER_TABLE operation reports an error if innodb_tmpdir is set to an invalid
# 				directory.
#
# 				innodb_tmpdir overrides the MySQL tmpdir setting but only for online ALTER_TABLE operations.
#
# 				The FILE privilege is required to configure innodb_tmpdir.
#
# 				The innodb_tmpdir option was introduced to help avoid overflowing a temporary file directory located on a tmpfs file system.
#
# 				Such overflows could occur as a result of large temporary sort files created during online ALTER_TABLE operations
# 				that rebuild the table.
#
# 				In replication environments, only consider replicating the innodb_tmpdir setting if all servers have the same operating
# 				system environment. Otherwise, replicating the innodb_tmpdir setting could result in a replication failure when running
# 				online ALTER_TABLE operations that rebuild the table.
#
# 				If server operating environments differ, it is recommended that you configure innodb_tmpdir on each server individually.
#
# 				For more information, see SECTION 15.12.3, "ONLINE DDL SPACE REQUIREMENTS". For information about online ALTER_TABLE 
# 				operations, see SECTION 15.12, "InnoDB AND ONLINE DDL"
#
# 			) innodb_trx_purge_view_update_only_debug
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-trx-purge-view-update-only-debug[={OFF|ON}]
# 				Sys var 					innodb_trx_purge_view_update_only_debug
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Pauses purging of delete-marked records while allowing the purge view to be updated. This option artifically
# 				creates a situation in which the purge view is updated but purges have not yet been performed.
#
# 				This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.
#
# 			) innodb_trx_rseg_n_slots_debug
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-trx-rseg-n-slots-debug=#
# 				Sys var 					innodb_trx_rseg_n_slots_debug
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					0
# 				Max 						1024
#
# 				Sets a debug flag that limits TRX_RSEG_N_SLOTS to a given value for the trx_rsegf_undo_find_free function
# 				that looks for free slots for undo log segments.
#
# 				This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.
#
# 			) innodb_undo_directory
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-undo-directory=dir_name
# 				Sys var 					innodb_undo_directory
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Dir name
#
# 				The path where InnoDB creates undo tablespaces. Typically used to place undo tablespaces on a different
# 				storage device.
#
# 				There is no default value (it is NULL). If the innodb_undo_directory variable is undefined, undo tablespaces
# 				are created in the data directory.
#
# 				The default undo tablespaces (innodb_undo_001 and innodb_undo_002) created when the MySQL instance is initialized
# 				always reside in the directory defined by the innodb_undo_directory variable.
#
# 				Undo tablespaces created using CREATE_UNDO_TABLESPACE syntax are created in the directory defined by the innodb_undo_directory
# 				variable if a different path is not specified.
#
# 				For more information, see SECTION 15.6.3.4, "UNDO TABLESPACES"
#
# 			) innodb_undo_log_encrypt
#
# 				Property 				Value
#
# 				cmd line 				--innodb-undo-log-encrypt[={OFF|ON}]
# 				Introduced 				8.0.1
# 				Sys var 					innodb_undo_log_encrypt
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR HInt 			No
# 				Type 						Boolean
# 				Default 					OFF
#
# 				Controls encryption of undo log data for tables encrypted using the InnoDB data-at-rest encryption feature.
#
# 				Only applies to undo logs that reside in separate undo tablespaces. See SECTION 15.6.3.4, "UNDO TABLESPACES"
# 				Encryption is not supported for undo log data that resides in the system tablespace.
#
# 				For more information, see UNDO LOG ENCRYPTION.
#
# 			) innodb_undo_log_truncate
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-undo-log-truncate[={OFF|ON}]
# 				Sys var 					innodb_undo_log_truncate
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default (>= 8.0.2) 	ON
# 				Default (<= 8.0.1) 	OFF
#
# 				When enabled, undo tablespaces that exceed the threshold value defined by innodb_max_undo_log_size are marked
# 				for truncation. Only undo tablespaces can be truncated.
#
# 				Truncating undo logs that reside in the system tablespace is not supported. For truncation to occur, there must
# 				be at least two undo tablespaces.
#
# 				The innodb_purge_rseg_truncate_frequency variable can be used to expedite truncation of undo tablespaces.
#
# 				For more information, see TRUNCATING UNDO TABLESPACES
#
# 			) innodb_undo_logs
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-undo-logs=#
# 				Deprecated 				Yes (removed in 8.0.2)
# 				Sys var 					innodb_undo_logs
# 				Scope 					Global
# 				Dynamic 					Yes
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					128
# 				Min 						1
# 				Max 						128
#
# 				NOTE:
#
# 					innodb_undo_logs was removed in MySQL 8.0.2
#
# 				The innodb_undo_logs option is an alias for innodb_rollback_segments. For more information,
# 				see the description of innodb_rollback_segments.
#
# 			) innodb_undo_tablespaces
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-undo-tablespaces=#
# 				Deprecated 				8.0.4
# 				Sys var 					innodb_undo_tablespaces
# 				Scope 					Global
# 				Dynamic (>= 8.0.2) 	Yes
# 				Dynamic (<= 8.0.1) 	No
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default (>= 8.0.2) 	2
# 				Default (<= 8.0.1) 	0
# 				Min (>= 8.0.3) 		2
# 				Min (<= 8.0.2) 		0
# 				Max (>= 8.0.2) 		127
# 				Max (<= 8.0.1) 		95
#
# 				Defines the number of undo tablespaces used by InnoDB. The default and minimum value is 2.
#
# 				NOTE:
#
# 					The innodb_undo_tablespaces variable is deprecated and is no longer configurable as of MySQL
# 					8.0.14. It will be removed in a future release.
#
# 				For more infromation, see SECTION 15.6.3.4, "UNDO TABLESPACES"
#
# 			) innodb_use_native_aio
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-use-native-aio[={OFF|ON}]
# 				Sys var 					innodb_use_native_aio
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Boolean
# 				Default 					ON
#
# 				Specifies whether to use the Linux asynch I/O subsystem. This variable applies to Linux systems only, and cannot be changed
# 				while the server is running.
#
# 				Normally, you do not need to configure this option, because it is enabled by default.
#
# 				The asynch I/O capability that INnoDB has on Windows systems is available on Linux systems. (Other Unix-like systems
# 				continue to use synch I/O calls)
#
# 				This feature improves the scalability of heavily I/O-bound systems, which typically show many pending reads/writes in
# 				SHOW ENGINE INNODB STATUS\G output.
#
# 				Running with a large number of InnoDB I/O threads, and especially running multiple such instances on the same server
# 				machine, can exceed capacity limits on Linux systems.
#
# 				In this case, you may receive the following error:
#
# 					EAGAIN: The specified maxevents exceeds the user's limit of available events
#
# 				You can typically address this error by writing a higher limit to /proc/sys/fs/aio-max-nr
#
# 				However, if a problem with the asynch I/O subsystem in the OS prevents InnoDB from starting, you can start
# 				the server with innodb_use_native_AIO=0. This option may also be disabled automatically during startup if
# 				InnoDB detects a potential problem such as a combination of tmpdir location, tmpfs file system, and Linux
# 				kernel that does not support AIO on tmpfs.
#
# 				For more information, see SECTION 15.8.6, "USING ASYNCH I/O ON LINUX"
#
# 			) innodb_version
#
# 				The InnoDB version number. In MySQL 8.0, separate version numbering for InnoDB does not apply and this value
# 				is the same the version number of the server.
#
# 			) innodb_write_io_threads
#
# 				Property 				Value
#
# 				Cmd line 				--innodb-write-io-threads=#
# 				Sys var 					innodb_write_io_threads
# 				Scope 					Global
# 				Dynamic 					No
# 				SET_VAR Hint 			No
# 				Type 						Integer
# 				Default 					4
# 				Min 						1
# 				Max 						64
#
# 				The number of I/O threads for write operations in InnoDB. The default value is 4. Its counterpart for read threads
# 				is innodb_read_io_threads. For more information, see SECTION 15.8.5, "CONFIGURING THE NUMBER OF BACKGROUND INNODB I/O THREADS"
#
# 				For general I/O tuning advice, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 				NOTE:
#
# 					ON Linux systems, running multiple MySQL servers (typically more than 12) with default settings for innodb_read_io_threads,
# 					innodb_write_io_threads, and the Linux aio-max-nr setting can exceed system limits. Ideally, increase the aio-max-nr setting;
# 					as a workaround, you might reduce the settings for one or both of the MySQL variables.
#
# 				ALso take into consideration the value of sync_binlog, which controls synch of teh binary log to disk.
#
# 				For general i/O tuning advice, see SECTION 8.5.8, "OPTIMIZING INNODB DISK I/O"
#
# 15.14 INNODB INFORMATION_SCHEMA TABLES
#
# 		15.14.1 InnoDB INFORMATION_SCHEMA TABLES ABOUT COMPRESSION
# 		15.14.2 InnoDB INFORMATION_SCHEMA TRANSACTION AND LOCKING INFORMATION
# 		15.14.3 InnoDB INFORMATION_SCHEMA SCHEMA OBJECT TABLES
# 		15.14.4 InnoDB INFORMATION_SCHEMA FULLTEXT INDEX TABLES
# 		15.14.5 InnoDB INFORMATION_SCHEMA BUFFER POOL TABLES
# 		15.14.6 InnoDB INFORMATION_SCHEMA METRICS TABLE
# 		15.14.7 InnoDB INFORMATION_SCHEMA TEMPORARY TABLE INFO TABLE
# 		15.14.8 RETRIEVING InnoDB TABLESPACE METADATA FROM INFORMATION_SCHEMA.FILES
#		
#
# 		This section provides information and usage examples for InnoDB INFORMATION_SCHEMA tables.
#
# 		InnoDB INFORMATION_SCHEMA tables provide metadata, status information, and statistics about various aspects
# 		of the InnoDB storage engine.
#
# 		You can view a list of InnoDB INFORMATION_SCHEMA tables by issuing a SHOW_TABLES statement on the INFORMATION_SCHEMA database:
#
# 			mysql> SHOW TABLES FROM INFORMATION_SCHEMA LIKE 'INNODB%';
#
# 		For table definitions, see SECTION 25.39, "INFORMATION_SCHEMA INNODB TABLES". For general information regarding the MySQL
# 		INFORMATION_SCHEMA database, see CHAPTER 25, INFORMATION_SCHEMA TABLES
#
# 15.14.1 InnoDB INFORMATION_SCHEMA TABLES ABOUT COMPRESSION
#
# 		15.14.1.1 InnoDB_CMP AND INNODB_CMP_RESET
# 		15.14.1.2 InnoDB_CMPMEM AND INNODB_CMPMEM_RESET
# 		15.14.1.3 USING THE COMPRESSION INFORMATION SCHEMA TABLES
#
# 		There are two pairs of InnoDB INFORMATION_SCHEMA tables about compression that can provide insight into how well compression
# 		is working overall:
#
# 			) INNODB_CMP and INNODB_CMP_RESET provide information about the number of compression operations and the amount of time spent
# 				performing compression.
#
# 			) INNODB_CMPMEM and INNODB_CMP_RESET provide information about the way memory is allocated for compression.
#
# 15.14.1.1 INNODB_CMP AND INNODB_CMP_RESET
#
# 		The INNODB_CMP and INNODB_CMP_RESET tables provide status information about operations related to compressed tables,
# 		which are described in SECTION 15.9, "InnoDB TABLE AND PAGE COMPRESSION".
#
# 		The PAGE_SIZE column reports the compressed page size.
#
# 		These two tables have identical contents, but reading from INNODB_CMP_RESET resets the statistics on compression
# 		and uncompression operations. For example, if you archive the output of INNODB_CMP_RESET every 60 minutes, you see
# 		the statistics for each hourly period..
#
# 		If you monitor the output of INNODB_CMP (making sure never to read INNODB_CMP_RESET), you see the cumulative statistics
# 		since InnoDB was started.
#
# 		For the table definition, see SECTION 25.39.5, "THE INFORMATION_SCHEMA INNODB_CMP AND INNODB_CMP_RESET TABLES"
#
# 15.14.1.2 INNODB_CMPMEM AND INNODB_CMPMEM_RESET
#
# 		The INNODB_CMPMEM and INNODB_CMPMEM_RESET tables provides status information about compressed pages that reside
# 		in the buffer pool.
#
# 		Please consult SECTION 15.9, "INNODB TABLE AND PAGE COMPRESSION" for further information on compressed tables
# 		and the use of the buffer pool.
#
# 		The INNODB_CMP and INNODB_CMP_RESET tables should provide more useful statistics on compression.
#
# 		INTERNAL DETAILS
#
# 			InnoDB uses a buddy allocator system to manage memory allocated to pages of various sizes, from 1KB to 16KB.
#
# 			Each row of the two tables described here corresponds to a single page size.
#
# 			The INNODB_CMPMEM and INNODB_CMPMEM_RESET tables have identical contents, but reading from INNODB_CMPMEM_RESET
# 			resets the statistics on relocation operations.
#
# 			For example, if every 60 minutes you archived the output of INNODB_CMPMEM_RESET, it would show the hourly stats.
#
# 			If you never read INNODB_CMPMEM_RESET and monitored the output of INNODB_CMPMEM instead, it would show the cumulative
# 			statistics since InnoDB was started.
#
# 			For the table definition, see SECTION 25.39.6, "THE INFORMATION_SCHEMA INNODB_CMPMEM AND INNODB_CMPMEM_RESET TABLES"
#
# 15.14.1.3 USING THE COMPRESSION INFORMATION SCHEMA TABLES
#
# 		Example 15.1 Using the Compression Information Schema Tables
#
# 		The following is sample output from a database that contains compressed tables (see SECTION 15.9, "InnoDB TABLE AND PAGE COMPRESSION",
# 		INNODB_CMP, INNODB_CMP_PER_INDEX, and INNODB_CMPMEM)
#
# 		The following table shows the contents of INFORMATION_SCHEMA.INNODB_CMP under a light workload. The only compressed page size that the
# 		buffer pool contains is 8K.
#
# 		Compressing or uncompressing pages has consumed less than a second since the time the statistics were reset, because the columns
# 		COMPRESS_TIME and UNCOMPRESS_TIME are zero.
#
# 			PAGE SIZE 			COMPRESS OPS 		COMPRESS OPS OK 		COMPRESS TIME 			UNCOMPRESS OPS 		UNCOMPRESS TIME
#
# 			1024 					0 						0							0 							0 							0
# 			2048 					0 						0 							0 							0 							0
# 			4096 					0 						0 							0 							0 							0
# 			8192 					1048 					921 						0 							61 						0
# 			16384 				0 						0 							0 							0 							0
#
# 		According to INNODB_CMPMEM, there are 6169 compressed 8KB pages in the buffer pool. The only other allocated block size is 64 bytes.
#
# 		The smallest PAGE_SIZE in INNODB_CMPMEM is used for block descriptors of those compressed pages for which no uncompressed page
# 		exists in the buffer pool. We see that there are 5910 such pages.
#
# 		Indirectly, we see that 259 (6169-5910) compressed pages also exist in the buffer pool in uncompressed form.
#
# 		The following table shows the contents of INFORMATION_SCHEMA.INNODB_CMPMEM under a light workload. Some memory is unusable due to
# 		fragmentation of the memory allocator for compressed pages: SUM(PAGE_SIZE*PAGES_FREE)=6784.
#
# 		This is because small memory allocation requests are fullfilled by splitting bigger blocks, starting from the 16K blocks that are
# 		allocated from the main buffer pool, using the buddy allocation system.
#
# 		The fragmentation is this low because some allocated blocks have been relocated (copied) to form bigger adjacent free blocks.
#
# 		This copying of SUM(PAGE_SIZE*RELOCATION_OPS) bytes has consumed less than a second (SUM(RELOCATION_TIME)=0)
#
# 			PAGE SIZE 		PAGES USED 			PAGES FREE 			RELOCATION OPS 			RELOCATION TIME
#
# 			64 				5910 					0 						2436 							0
# 			128 				0 						1 						0 								0
# 			256 				0 						0 						0 								0
# 			512 				0 						1 						0 								0
# 			1024 				0 						0	 					0 								0
# 			2048 				0 						1 						0 								0
# 			4096 				0 						1 						0 								0
# 			8192 				6169 					0 						5 								0
# 			16384 			0 						0 						0 								0
#
# 15.14.2 INNODB INFORMATION_SCHEMA TRANSACTION AND LOCKING INFORMATION
#
# 	15.14.2.1 USING INNODB TRANSACTION AND LOCKING INFORMATION
# 	15.14.2.2 INNODB LOCK AND LOCK-WAIT INFORMATION
# 	15.14.2.3 PERSISTENCE AND CONSISTENCY OF INNODB TRANSACTION AND LOCKING INFORMATION
#
# 	NOTE:
#
# 		This section describes locking information as exposed by the Performance Schema data_locks
# 		and data_lock_waits tables, which supersede the INFORMATION_SCHEMA INNODB_LOCKS and
# 		INNODB_LOCK_WAITS tables in MySQL 8.0 
#
# 		For similar discussion written in terms of the older INFORMATION_SCHEMA tables,
# 		see INNODB INFORMATION_SCHEMA TRANSACTION AND LOCKING INFORMATION in MYSQL 5.7 REFERENCE MANUAL
#
# One INFORMATION_SCHEMA table and two Performance Schema tables enable you to monitor InnoDB transactions
# and diagnose potential locking problems:
#
# 		) INNODB_TRX: This INFORMATION_SCHEMA table provides information about every transaction currently executing
# 			inside InnoDB, including the transaction state (for example, whether it is running or waiting for a lock),
# 			when the transaction started, and the particular SQL statement the transaction is executing.
#
# 		) data_locks: This Performance Schema table contains a row for each hold lock and each lock request that is
# 			blocked waiting for a held lock to be released:
#
# 				) There is one row for each held lock, whatever the state of the transaction that holds the lock
#
# 					(INNODB_TRX.TRX_STATE is RUNNING, LOCK WAIT, ROLLING BACK or COMMITTING)
#
# 				) Each transaction in InnoDB that is waiting for another transaction to release a lock (INNODB_TRX.TRX_STATE is LOCK WAIT)
# 					is blocked by exactly one blocking lock request.
#
# 					That blocking lock request is for a row or table lock held by another transaction in an incompatible mode.
#
# 					A lock request always has a mode that is incompatible with the mode of the held lock that blocks the request
# 					(read vs. write, shared vs. exclusive)
#
# 					The blocked transaction cannot proceed until the other transaction commits or rolls back, thereby releasing
# 					the requested lock. For every blocked transaction, data_locks contains one row that describes each lock
# 					the transaction has requested, and for which it is waiting.
#
# 			) data_lock_waits: This Performance Schema table indicates which transactions are waiting for a given lock, or for which
# 				lock a given transaction is waiting.
#
# 				This table contains one or more rows for each blocked transaction, indicating the lock it has requested and any locks
# 				that are blocking that request.
#
# 				The REQUESTING_ENGINE_LOCK_ID value refers to the lock requested by a transaction, and the BLOCKING_ENGINE_LOCK_ID
# 				value refers to the lock (held by another transaction) that prevents the first transaction from proceeding.
#
# 				For any given blocked transaction, all rows in data_lock_waits have the same value for REQUESTING_ENGINE_LOCK_ID
# 				and different values for BLOCKING_ENGINE_LOCK_ID
#
# For more information about the preceding tables, see SECTION 25.39.29, "THE INFORMATION_SCHEMA INNODB_TRX TABLE", SECTION 
# 26.12.12.1, "THE DATA_LOCKS TABLE" and SECTION 26.12.12.2, "THE DATA_LOCK_WAITS TABLE"
#
# 15.14.2.1 USING INNODB TRANSACTION AND LOCKING INFORMATION
#
# NOTE:
#
# 		This section describes locking information as exposed by the Performance Schema data_locks and data_lock_waits tables,
# 		which supersede the INFORMATION_SCHEMA INNODB_LOCKS and INNODB_LOCK_WAITS tables in MySQL 8.0
#
# 		For similar discussion written in terms of the older INFORMATION_SCHEMA tables, see USING INNODB TRANSACTION AND
# 		LOCKING INFORMATION in MYSQL 5.7 REFERENCE MANUAL
#
# IDENTIFYING BLOCKING TRANSACTIONS
#
# It is sometimes helpful to identify which transaction blocks another. The tables that contain information about InnoDB
# transactions and data locks enable you to determine which transaction is waiting for another, and which resource is being
# requested.
#
# (For descriptions of these tables, see SECTION 15.14.2, "INNODB INFORMATION_SCHEMA TRANSACTION AND LOCKING INFORMATION")
#
# Suppose that three sessions are running concurrently. Each session corresponds to a MySQL thread, and executes one transaction
# after another.
#
# Consider the state of the system when these sessions have issued the following statements, but none has yet committed
# its transaction:
#
# 		) Session A:
#
# 			BEGIN;
# 			SELECT a FROM t FOR UPDATE;
# 			SELECT SLEEP(100);
#
# 		) Session B:
#
# 			SELECT b FROM t FOR UPDATE;
#
# 		) Session C:
#
# 			SELECT c FROM t FOR UPDATE;
#
# In this scenario, use the following query to see which transactions are waiting and which transactions are blocking them:
#
# 		SELECT
# 			r.trx_id waiting_trx_id,
# 			r.trx_mysql_thread_id waiting_thread,
# 			r.trx_query waiting_query,
# 			b.trx_id blocking_trx_id,
# 			b.trx_mysql_thread_id blocking_thread,
# 			b.trx_query blocking_query
# 		FROM 		performance_schema.data_lock_waits w
# 		INNER JOIN information_schema.innodb_trx b
# 		ON b.trx_id = w.blocking_engine_transaction_id
# 		INNER JOIN information_schema.innodb_trx r
# 		ON r.trx_id = w.requesting_engine_transaction_id;
#
# Or, more simply, use  the sys Schema innodb_lock_waits view:
#
# 		SELECT
# 			waiting_trx_id,
# 			waiting_pid,
# 			waiting_query,
# 			blocking_trx_id,
# 			blocking_pid,
# 			blocking_query
# 		FROM sys.innodb_lock_waits;
#
# If a NULL value is reported for the blocking query, see IDENTIFYING A BLOCKING QUERY AFTER THE ISSUING SESSION BECOMES IDLE.
#
# 		waiting trx id 		waiting thread 		Waiting query 							blocking trx id 		blocking thread 		blocking query
#
# 		A4 						6 							SELECT b FROM t FOR UPDATE 		A3 						5 							SELECT SLEEP(100)
# 		A5 						7 							SELECT c FROM t FOR UPDATE 		A3 						5 							SELECT SLEEP(100)
# 		A5 						7 							SELECT c FROM t FOR UPDATE 		A4 						6 							SELECT b FROM t FOR UPDATE
#
# In the preceding table, you can identify sessions by the "waiting query" or "blocking query" columns. As you can see:
#
# 		) Session B (trx id A4, thread 6) and Session C (trx id A5, thread 7) are both waiting for Session A (trx id A3, thread 5)
#
# 		) Session C is waiting for Session B as well as Session A.
#
# You can see the underlying data in the INFORMATION_SCHEMA INNODB_TRX table and Performance Schema data_locks and data_lock_waits
# tables.
#
# The following table shows some sample contents of the INNODB_TRX table.
#
# 		trx id 			trx state 				trx started 					trx requested lock id 				trx wait started 			trx weight 	trx mysql thread id 		trx query
#
# 		A3 				RUNNING 					2008-01-15 16:44:54 			NULL 										NULL 							2 				5 								SELECT SLEEP(100)
# 		A4 				LOCK WAIT 				2008-01-15 16:45:09 			A4:1:3:2 								2008-01-15 16:45:09 		2 				6 								SELECT b FROM t FOR UPDATE
# 		A5 				LOCK WAIT 				2008-01-15 16:45:14 			A5:1:3:2 								2008-01-15 16:45:14 		2 				7 								SELECT c FROM t FOR UPDATE
#
# The following table shows some sample contents of the data_locks table.
#
# 		lock id 			Lock trx id 			Lock mode 						Lock type 								Lock schema 				lock table 	lock index 					lock data
# 		
# 		A3:1:3:2 		A3 						X 									RECORD 									test 							t 				PRIMARY 						0x0200
# 		A4:1:3:2 		A4 						X 									RECORD 									test 							t 				PRIMARY 						0x0200
# 		A5:1:3:2 		A5 						X 									RECORD 									test 							t 				PRIMARY 						0x0200
#
# The following table shows some sample contents of the data_lock_waits table.
#
# 		requesting trx id 			requested lock id 				blocking trx id 					blocking lock id
#
# 		A4 								A4:1:3:2 							A3 									A3:1:3:2
# 		A5 								A5:1:3:2 							A3 									A3:1:3:2
# 		A5 								A5:1:3:2 							A4 									A4:1:3:2
#
# IDENTIFYING A BLOCKING QUERY AFTER THE ISSUING SESSION BECOMES IDLE
#
# When identifying blocking transactions, a NULL value is reported for the blocking query if the session that issued the query
# has become idle.
#
# In this case, use the following steps to determine the blocking query:
#
# 		1. Identify the processlist ID of the blocking transaction. In the sys.innodb_lock_waits table, the processlist ID of the blocking transaction is the blocking_pid value
#
# 		2. Using the blocking_pid, query the MySQL Performance Schema threads table to determine the THREAD_ID of the blocking transaction.
#
# 			For example, if the blocking_pid is 6, issue this query:
#
# 				SELECT THREAD_ID FROM performance_schema.threads WHERE PROCESSLIST_ID = 6;
#
# 		3. Using the THREAD_ID, query the Performance Schema events_statements_current table to determine the last query executed by the thread.
#
# 			For example, if the THREAD_ID is 28, issue this query:
#
# 				SELECT THREAD_ID, SQL_TEXT FROM performance_schema.events_statements_current
# 				WHERE THREAD_ID = 28\G
#
# 		4. If the last query executed by the thread is not enough information to determine why a lock is held, you can query the Performance
# 			Schema events_statements_history table to view the last 10 statements executed by the thread.
#
# 				SELECT THREAD_ID, SQL_TEXT FROM performance_schema.events_statements_history
# 				WHERE THREAD_ID = 28 ORDER BY EVENT_ID;
#
# CORRELATING INNODB TRANSACTIONS WITH MYSQL SESSIONS
#
# Sometimes it is useful to correlate internal InnoDB locking information with the session-level information maintained
# by MySQL. For example, you might like to know, for a given InnoDB transaction ID, the corresponding MySQL session ID 
# and name of the session that may be holding a lock, and thus blocking other transactions.
#
# The following output from the INFORMATION_SCHEMA INNODB_TRX table and Performance Schema data_locks and data_lock_waits
# tables is taken from a somewhat loaded system.
#
# As can be seen, there are several transactions running.
#
# The following data_locks and data_lock_waits tables show that:
#
# 		) Transaction 77F (executing an INSERT) is waiting for transactions 77E, 77D and 77B to commit.
#
# 		) Transaction 77E (executing an INSERT) is waiting for transactions 77D and 77B to commit.
#
# 		) Transaction 77D (executing an INSERT) is waiting for transaction 77B to commit
#
# 		) Transaction 77B (executing an INSERT) is waiting for transaction 77A to commit.
#
# 		) Transaction 77A is running, currently executing SELECT
#
# 		) Transaction E56 (executing an INSERT) is waiting for transaction E55 to commit
#
# 		) Transaction E55 (executing an INSERT) is waiting for transaction 19C to commit
#
# 		) Transaction 19C is running, currently executing an INSERT
#
# NOTE:
#
# 		There may be inconsistencies between queries shown in the INFORMATION_SCHEMA PROCESSLIST and INNODB_TRX tables.
#
# 		For an explanation, see SECTION 15.14.2.3, "PERSISTENCE AND CONSISTENCY OF INNODB TRANSACTION AND LOCKING INFORMATION"
#
# The following table shows the contents of the PROCESSLIST table for a system running a heavy workload.
#
# 		ID 		USER 			HOST 			DB 		COMMAND 		TIME 		STATE 			INFO
#
# 		384 		root 			localhost 	test 		Query 		10 		update 			INSERT INTO t2 VALUES /etc/
#
# 		257 		root 			localhost 	test 		Query 		3 			update 			INSERT INTO t2 VALUES /etc/
#
# 		130 		root 			localhost 	test 		Query 		0 			update 			INSERT INTO t2 VALUES /etc/
#
# 		61 		root 			localhost 	test 		Query 		1 			update 			INSERT INTO t2 VALUES /etc/
#
# 		8 			root 			localhost 	test 		Query 		1 			update 			INSERT INTO t2 VALUES /etc/
#
# 		4 			root 			localhost 	test 		Query 		0 			preparing 		SELECT * FROM PROCESSLIST
#
# 		2 			root 			localhost 	test 		Sleep 		566 		- 					NULL
#
# The following table shows the contents of the INNODB_TRX table for a system running a heavy workload.
#
# 		Trx id 		Trx State 		Trx started		 		Trx requested lock id 		Trx wait started 		Trx weight 		Trx mysql thread id 		trx Query
#
# 		77F 			LOCK WAIT 		2008-01-15 13:10:16 	77F 								2008-01-15 13:10:16 	1 					876 							INSERT INTO t09(D,B,C) VALUES /etc/
#
# 		77E 			LOCK WAIT 		2008-01-15 13:10:16 	77E 								2008-01-15 13:10:16 	1 					875 							INSERT INTO t09(D,B,C) VALUES /etc/
#
# 		77D 			LOCK WAIT 		2008-01-15 13:10:16 	77D 								2008-01-15 13:10:16 	1 					874 							INSERT INTO t09(D,B,C) VALUES /etc/
#
# 		77B 			LOCK WAIT 		2008-01-15 13:10:16 	77B:733:12:1 					2008-01-15 13:10:16 	4 					873 							INSERT INTO t09(D,B,C) VALUES /etc/
#
# 		77A 			RUNNING 			2008-01-15 13:10:16 	NULL 								NULL 						4 					872 							SELECT b,c FROM t09 WHERE /etc/
#
# 		E56 			LOCK WAIT 		2008-01-15 13:10:06 	E56:743:6:2 					2008-01-15 13:10:06 	5 					384 							INSERT INTO t2 VALUES /etc/
#
# 		E55 			LOCK WAIT 		2008-01-15 13:10:06 	E55:743:38:2 					2008-01-15 13:10:13 	965 				257 							INSERT INTO t2 VALUES /etc/
#
# 		19C 			RUNNING 			2008-01-15 13:09:10 	NULL 								NULL 						2900				130 							INSERT INTO t2 VALUES /etc/
#
# 		E15 			RUNNING 			2008-01-15 13:08:59 	NULL 								NULL 						5395 				61 							INSERT INTO t2 VALUES /etc/
#
# 		51D 			RUNNING 			2008-01-15 13:08:47 	NULL 								NULL 						9807 				8 								INSERT INTO t2 VALUES /etc/
#
# The following table shows the contents of the data_lock_waits table for a system running a heavy workload.
#
# 		requesting trx id 		requested lock id 		blocking trx id 		blocking lock id
#
# 		77F 							77F:806 						77E 						77E:806 (1)
#
# 		77F 							77F:806 						77D 						77D:806 (2)
# 
# 		77F 							77F:806 						77B 						77B:806 (3)
#
# 		77E 							77E:806 						77D 						77D:806 (4)
#
# 		77E 							77E:806 						77B 						77B:806 (5)
#
# 		77D 							77D:806 						77B 						77B:806 (6)
#
# 		77B 							77B:733:12:1 				77A 						77A:733:12:1
# 
# 		E56 							E56:743:6:2 				E55 						E55:743:6:2
#
# 		E55 							E55:743:38:2 				19C 						19C:743:38:2
#
# The following table shows the contents of the data_locks table for a system running a heavy workload.
#
# 		lock id 						Lock trx id 				lock mode 				lock type 			lock schema 		lock table 		lock index 				lock data
#
# 		77F:806 						77F 							AUTO_INC 				TABLE 				TEST 					T09 				NULL 						NULL
#
# 		77E:806 						77E 							AUTO_INC 				TABLE 				TEST 					T09 				NULL 						NULL
#
# 		77D:806 						77D 							AUTO_INC 				TABLE 				TEST 					T09 				NULL 						NULL
#
# 		77B:806 						77B 							AUTO_INC 				TABLE 				TEST 					T09 				NULL 						NULL
#
# 		77B:733:12:1 				77B 							X 							RECORD 				TEST 					T09 				PRIMARY 					SUPREMUM PSEUDO-RECORD
#
# 		77A:733:12:1 				77A 							X 							RECORD 				TEST 					T09 				PRIMARY 					SUPREMUM PSEUDO-RECORD
#
# 		E56:743:6:2 				E56 							S 							RECORD 				TEST 					T2 				PRIMARY 					0, 0
#
# 		E55:743:6:2 				E55 							X 							RECORD 				TEST 					T2 				PRIMARY 					0, 0
#
# 		E55:743:38:2 				E55 							S 							RECORD 				TEST 					T2 				PRIMARY 					1922, 1922
#
# 		19C:743:38:2 				19C 							X 							RECORD 				TEST 					T2 				PRIMARY 					1922, 1922
#
# 15.14.2.2 INNODB LOCK AND LOCK-WAIT INFORMATION
#
# NOTE:
#
# 		This section describes locking information as exposed by the Performance Schema data_locks and data_lock_waits
# 		tables, which supersede the INFORMATION_SCHEMA INNODB_LOCKS and INNODB_LOCK_WAITS tables in MySQL 8.0
#
# 		For similar discussion written in terms of the older INFORMATION_SCHEMA tables, see InnoDB LOCK AND LOCK-WAIT INFORMATION
# 		in MySQL 5.7 REFERENCE MANUAL
#
# When a transaction updates a row in a table, or locks it with SELECT FOR UPDATE, InnoDB establishes a list or queue of locks
# on that row.
#
# Similarly, InnoDB maintains a list of locks on a table for table-level locks. If a second transaction wants to update a row or
# lock a table already locked by a prior transaction in an incompatible mode, InnoDB adds a lock request for the row to the
# corresponding queue.
#
# For a lock to be acquired by a transaction, all incompatible block lock requests previously entered into the lock queue
# for that row or table must be removed (which occurs when the transactions holding or requesting those locks either
# commit or roll back)
#
# A transaction may have any number of lock requests for different rows or tables. At any given time, a transaction may request
# a lock that is held by another transaction, in which case it is blocked by that other transaction.
#
# The requesting transaction must wait for the transaction that holds the blocking lock to commit or roll back.
#
# If a transaction is not waiting for a lock, it is in a RUNNING state. If a transaction is waiting for a lock, it is
# in a LOCK WAIT state. (The INFORMATION_SCHEMA INNODB_TRX table indicates transaction state values)
#
# The Performance Schema data_locks table holds one or more rows for each LOCK WAIT transaction, indicating any lock
# requests that prevent its progress. This table also contains one row describing each lock in a queue of locks
# pending for a given row or table.
#
# The Performance Schema data_lock_waits table shows which locks already held by a transaction are blocking
# locks requested by other transactions.
#
# 15.4.2.3 PERSISTENCE AND CONSISTENCY OF INNODB TRANSACTION AND LOCKING INFORMATION
#
# NOTE:
#
# 		This section describes locking information as exposed by the Performance Schema data_locks and
# 		data_lock_waits tables, which supersede the INFORMATION_SCHEMA INNODB_LOCKS and INNODB_LOCK_WAITS
# 		tables in MySQL 8.0
#
# 		For similar discussion written in terms of the older INFORMATION_SCHEMA tables, see 
# 		PERSISTENCE AND CONSISTENCY OF INNODB TRANSACTION AND LOCKING INFORMATION in
# 		MYSQL 5.7 REFERENCE MANUAL
#
# The data exposed by the transaction and locking tables (INFORMATION_SCHEMA INNODB_TRX table,
# Performance Schema data_locks and data_lock_waits tables) represents a glimpse into fast-changing data.
#
# THis is not like user tables, where the data changes only when application-initiated updates occur.
#
# The underlying data is internal system-managed data, and can change very quickly:
#
# 		) Data might not be consistent between the INNODB_TRX, data_locks and data_lock_waits tables.
#
# 			The data_locks and data_lock_waits tables expose live data from the InnoDB storage engine,
# 			to provide lock information about the transactions in the INNODB_TRX table.
#
# 			Data retrieved from the lock tables exists when the SELECT is executed, but might be gone
# 			or changed by the time the query results is consumed by the client.
#
# 			Joining data_locks with data_lock_waits can show rows in data_lock_waits that identify a
# 			parent row in data_locks that no longer exists or does not exist yet.
#
# 		) Data in the transaction and locking tables might not be consistent with data in the INFORMATION_SCHEMA PROCESSLIST
# 			table or Performance Schema threads table.
#
# 			For example, you should be careful when comparing data in the InnoDB transaction and locking tables with data
# 			in the PROCESSLIST table.
#
# 			Even if you issue a single SELECT (joining INNODB_TRX and PROCESSLIST, for example), the content of those tables
# 			is generally not consistent.
#
# 			It is possible for INNODB_TRX to reference rows that are not present in PROCESSLIST or for the currently executing
# 			SQL query of a transaction shown in INNODB_TRX.TRX_QUERY to differ from the one in PROCESSLIST.INFO
#
# 15.14.3 INNODB INFORMATION_SCHEMA SCHEMA OBJECT TABLES
#
# You can extract metadata about schema objects managed by InnoDB using InnoDB INFORMATION_SCHEMA tables. This information
# comes from the data dictionary.
#
# Traditionally, you would get this type of information using the techniques from SECTION 15.16, "InnoDB MONITORS",
# setting up InnoDB monitors and parsing the output from the SHOW_ENGINE_INNODB_STATUS statement.
#
# The InnoDB INFORMATION_SCHEMA table interface allows you to query this data using SQL.
#
# InnoDB INFORMATION_SCHEMA schema object tables include the tables listed below.
#
# 		INNODB_DATAFILES
# 		INNODB_TABLESTATS
# 		INNODB_FOREIGN
# 		INNODB_COLUMNS
# 		INNODB_INDEXES
# 		INNODB_FIELDS
# 		INNODB_TABLESPACES
# 		INNODB_TABLESPACES_BRIEF
# 		INNODB_FOREIGN_COLS
# 		INNODB_TABLES
#
# The table names are indicative of the type of data provided:
#
# 		) INNODB_TABLES provides metadata about InnoDB tables
#
# 		) INNODB_COLUMNS provides metadata about InnoDB table columns
#
# 		) INNODB_INDEXES provides metadata about InnoDB indexes.
#
# 		) INNODB_FIELDS provides metadata about the key columns (fields) of InnoDB indexes.
#
# 		) INNODB_TABLESTATS provides a view of low-level status information about InnoDB tables that is derived
# 			from in-memory data structures.
#
# 		) INNODB_DATAFILES provides data file path information for InnoDB file-per-table and general tablespaces.
#
# 		) INNODB_TABLESPACES provides metadata about InnoDB file-per-table, general, and undo tablespaces.
#
# 		) INNODB_TABLESPACES_BRIEF provides a subset of metadata about InnoDB tablespaces.
#
# 		) INNODB_FOREIGN provides metadata about foreign keys defined on InnoDB tables.
#
# 		) INNODB_FOREIGN_COLS provides metadata about the columns of foreign keys that are defined on InnoDB tables.
#
# InnoDB INFORMATION_SCHEMA schema object tables can be joined together through fields such as TABLE_ID, INDEX_ID,
# and SPACE, allowing you to easily retrieve all available data for an object you want to study or monitor.
#
# Refer to the InnoDB INFORMATION_SCHEMA documentation for information about the columns of each table.
#
# EXAMPLE 15.2 InnoDB INFORMATION_SCHEMA SCHEMA OBJECT TABLES
#
# This example uses a simple table (t1) with a single index (i1) to demonstrate the type of metadata found in
# the InnoDB INFORMATION_SCHEMA schema object tables.
#
# 		1. Creates a test database and table t1:
#
# 			mysql> CREATE DATABASE test;
#
# 			mysql> USE test;
#
# 			mysql> CREATE TABLE t1 (
# 					 col1 INT,
# 					 col2 CHAR(10),
# 					 col3 VARCHAR(10))
# 					 ENGINE = InnoDB;
#
# 			mysql> CREATE INDEX i1 ON t1(col1);
#
# 		2. After creating the table t1, query INNODB_TABLES to locate the metadata for test/t1:
#
# 			mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_TABLES WHERE NAME='test/t1' \G
# 			************************** 1. row ********************************
# 							TABLE_ID: 71
# 								NAME : test/t1
# 								FLAG : 1
# 							N_COLS  : 6
# 								SPACE: 57
# 						ROW_FORMAT : Compact
# 					ZIP_PAGE_SIZE : 0
# 					INSTANT_COLS  : 0
#
# 			Table t1 has a TABLE_ID of 71. The FLAG field provides bit level information about table format and storage characteristics.
#
# 			There are six columns, three of which are hidden columns created by InnoDB(DB_ROW_ID, DB_TRX_ID and DB_ROLL_PTR)
#
# 			The ID of the table's SPACE is 57 (a value of 0 would indicate that the table resides in the system tablespace)
#
# 			The ROW_FORMAT is Compact. ZIP_PAGE_SIZE only applies to tables with a Compressed row format. INSTANT_COLS
# 			shows number of columns in the table prior to adding the first instant column using ALTER TABLE /etc/ ADD COLUMN
# 			with ALGORITHM=INSTANT
#
# 		3. Using the TABLE_ID information from INNODB_TABLES, query the INNODB_COLUMNS table for information about the
# 			table's columns.
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_COLUMNS where TABLE_ID = 71\G
# 				********************************** 1. row ***************************************
# 								TABLE_ID: 71
# 								  NAME  : col1
# 								    POS : 0
# 								MTYPE   : 6
# 								PRTYPE  : 1027
# 									LEN  : 4
# 							HAS_DEFAULT: 0
# 						DEFAULT_VALUE : NULL
# 				********************************** 2. row *****************************************
# 								TABLE_ID: 71
# 									NAME : col2
# 									  POS: 1
# 									MTYPE: 2
# 								PRTYPE  : 524542
# 									LEN  : 10
# 							HAS_DEFAULT: 0
# 						DEFAULT_VALUE : NULL
# 				********************************** 3. row *****************************************
# 								TABLE_ID: 71
# 									NAME : col3
# 								     POS: 2
# 									MTYPE: 1
# 								PRTYPE  : 524303
# 								LEN 	  : 10
# 							HAS_DEFAULT: 0
# 						DEFAULT_VALUE : NULL
#
# 			In addition to the TABLE_ID and column NAME, INNODB_COLUMNS provides the ordinal position (POS) of each column
# 			(starting from 0 and incrementing sequentially), the column MTYPE or "main type" (6 = INT, 2 = CHAR, 1 = VARCHAR),
# 			the PRTYPE or "precise type" (a binary value with bits that represent the MySQL data type, char set code and nullability),
# 			and the column length (LEN).
#
# 			The HAS_DEFAULT and DEFAULT_VALUE columns only apply to columns added instantly using ALTER TABLE /etc/ ADD COLUMN
# 			with ALGORITHM=INSTANT
#
# 		4. Using the TABLE_ID information from INNODB_TABLES once again, query INNODB_INDEXES for information about the indexes
# 				associated with table t1.
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_INDEXES WHERE TABLE_ID = 71 \G
# 				******************************** 1. row *********************************************
# 								INDEX_ID: 111
# 								  NAME  : GEN_CLUST_INDEX
# 								TABLE_ID: 71
# 									TYPE : 1
# 								N_FIELDS: 0
# 							PAGE_NO    : 3
# 								   SPACE: 57
# 					MERGE_THRESHOLD  : 50
# 				******************************** 2. row **********************************************
# 								INDEX_ID: 112
# 									NAME : i1
# 							TABLE_ID   : 71
# 									TYPE : 0
# 								N_FIELDS: 1
# 							PAGE_NO    : 4
# 									SPACE: 57
# 					MERGE_THRESHOLD  : 50
#
# 			INNODB_INDEXES returns data for two indexes. The first index is GEN_CLUST_INDEX, which is a clustered index
# 			created by InnoDB if the table does not have a user-defined clustered index.
#
# 			The second index (i1) is the user-defined secondary index.
#
# 			The INDEX_ID is an identifier for the index that is unique across all databases in an instance. The TABLE_ID
# 			identifies the table that the index is associated with.
#
# 			The index TYPE value indicates the type of index (1 = Clustered Index, 0 = Secondary Index).
#
# 			The N_FIELDS value is the number of fields that comprise the index. PAGE_NO is the root page number of the
# 			index B-tree, and SPACE is the ID of the tablespace where the index resides.
#
# 			A nonzero value indicates that the index does not reside in the system tablespace.
#
# 			MERGE_THRESHOLD defines a percentage threshold value for the amount of data in an index page. If the amount
# 			of data in an index page falls below the this value (the default is 50%) when a row is deleted or when a row
# 			is shortened by an update operation, InnoDB attempts to merge the index page with a neighboring index page.
#
# 		5. Using the INDEX_ID information from INNODB_INDEXES, query INNODB_FIELDS for information about the fields of index i1.
#
# 			mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_FIELDS where INDEX_ID = 112 \G
# 			*********************************** 1. row ************************************
# 			INDEX_ID: 112
# 				NAME : col1
# 				  POS: 0
#
# 			INNODB_FIELDS provides the NAME of the indexed field and its ordinal position within the index.
#
# 			If the index (i1) had been defined on multiple fields, INNODB_FIELDS would provide metadata for
# 			each of the indexed fields.
#
# 		6. Using the SPACE information from INNODB_TABLES, query INNODB_TABLESPACES table for information about
# 			the table's tablespace.
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_TABLESPACES WHERE SPACE = 57 \G
# 				******************************** 1. row *************************************
# 											SPACE: 57
# 											NAME : test/t1
# 											FLAG : 16417
# 									ROW_FORMAT : Dynamic
# 									 PAGE_SIZE : 16384
# 							ZIP_PAGE_SIZE    : 0
# 								SPACE_TYPE    : Single
# 								FS_BLOCK_SIZE : 4096
# 								FILE_SIZE     : 114688
# 							ALLOCATED_SIZE   : 98304
# 								SERVER_VERSION: 8.0.4
# 								SPACE_VERSION : 1
# 									ENCRYPTION : N
#
# 			In addition to the SPACE ID of the tablespace and the NAME of the associated table, INNODB_TABLESPACES
# 			provides tablespace FLAG data, which is bit level information about tablespace format and storage characteristics.
#
# 			Also provided are tablespace ROW_FORMAT, PAGE_SIZE, and several other tablespace metadata items.
#
# 		7. Using the SPACE information from INNODB_TABLES once again, query INNODB_DATAFILES for the location of the tablespace data file.
#
# 			mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_DATAFILES WHERE SPACE = 57 \G
# 			******************************** 1. row ****************************************
# 			SPACE: 57
# 			 PATH: ./test/t1.ibd
#
# 			The datafile is located in the test directory under MySQL's data directory. If a file-per-table tablespace were created
# 			in a location outside the MySQL data directory using the DATA DIRECTORY clause of the CREATE_TABLE statement, the
# 			tablespace PATH would be a fully qualified directory path.
#
# 		8. As a final step, insert a row into table t1(TABLE_ID = 71) and view the data in the INNODB_TABLESTATS table.
#
# 			The data in this table is used by the MySQL optimizer to calculate which index to use when querying an InnoDB table.
#
# 			This information is derived from in-memory data structures.
#
# 				mysql> INSERT INTO t1 VALUES(5, 'abc', 'def');
# 				Query OK, 1 row affected (0.06 sec)
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_TABLESTATS where TABLE_ID = 71 \G
# 				******************************* 1. row **************************************
# 									TABLE_ID: 71
# 									    NAME: test/t1
# 						STATS_INITIALIZED: Initialized
# 								NUM_ROWS   : 1
# 						 CLUST_INDEX_SIZE: 1
# 						OTHER_INDEX_SIZE : 0
# 						MODIFIED_COUNTER : 1
# 						AUTOINC 			  : 0
# 								REF_COUNT  : 1
#
# 			The STATS_INITIALIZED field indicates whether or not statistics have been collected for the table.
#
# 			NUM_ROWS is the current estimated number of rows in the table. The CLUST_INDEX_SIZE and OTHER_INDEX_SIZE
# 			fields report the number of pages on disk that store clustered and secondary indexes for the table,
# 			respectively.
#
# 			The MODIFIED_COUNTER value shows the number of rows modified by DML operations and cascade operations
# 			from foreign keys. The AUTOINC value is the next number to be issued for any autoincrement-based operation.
#
# 			There are no autoincrement columns defined on table t1, so the value is 0. The REF_COUNT value is a counter.
# 			When the counter reaches 0, it signifies that the table metadata can be evicted from the table cache.
#
# EXAMPLE 15.3 FOREIGN KEY INFORMATION_SCHEMA SCHEMA OBJECT TABLES
#
# The INNODB_FOREIGN and INNODB_FOREIGN_COLS tables provide data about foreign key relationships. This example uses a
# parent table and child table with a foreign key relationship to demonstrate the data found in the INNODB_FOREIGN
# and INNODB_FOREIGN_COLS tables.
#
# 		1. Create the test database with parent and child tables:
#
# 			mysql> CREATE DATABASE test;
#
# 			mysql> USE test;
#
# 			mysql> CREATE TABLE parent (id INT NOT NULL,
# 					 PRIMARY KEY (id)) ENGINE=INNODB;
#
# 			mysql> CREATE TABLE child (id INT, parent_id INT,
# 					 INDEX par_ind (parent_id),
# 					 CONSTRAINT fk1
# 					 FOREIGN KEY (parent_id) REFERENCES parent(id)
# 					 ON DELETE CASCADE) ENGINE=INNODB;
#
# 		2. After the parent and child tables are created, query INNODB_FOREIGN and locate the foreign key data for the
# 			test/child and test/parent foreign key relationship:
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_FOREIGN \G
# 				************************** 1. row ******************************
# 								ID: test/fk1
# 						FOR_NAME: test/child
# 						REF_NAME: test/parent
# 						N_COLS  : 1
# 							TYPE : 1
#
# 			Metadata includes the foreign key ID(fk1), which is named for the CONSTRAINT that was defined on the child table.
#
# 			The FOR_NAME is the name of the child table where the foreign key is defined. REF_NAME is the name of the parent
# 			table (the "referenced" table). N_COLS is the number of columns in the foreign key index. 
#
# 			TYPE is a numerical value representing bit flags that provide additional information about the foreign key column.
#
# 			In this case, the TYPE value is 1, which indicates that the ON DELETE CASCADE option was specified for the foreign
# 			key.
#
# 			See the INNODB_FOREIGN table definition for more information about TYPE values.
#
# 		3. Using the foreign key ID, query INNODB_FOREIGN_COLS to view data about the columns of the foreign key
#
# 			mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_FOREIGN_COLS WHERE ID = 'test/fk1' \G
# 			************************************* 1. row *********************************
# 									ID: test/fk1
# 					FOR_COL_NAME  : parent_id
# 				REF_COL_NAME 	  : id
# 								 POS : 0
#
# 			FOR_COL_NAME is the name of the foreign key column in the child table, and REF_COL_NAME is the name of the
# 			referenced column in the parent table. The POS Value is the ordinal position of the key field within the
# 			foreign key index, starting at zero.
#
# EXAMPLE 15.4 JOINING INNODB INFORMATION_SCHEMA SCHEMA OBJECT TABLES
#
# This example demonstrates joining three InnoDB INFORMATION_SCHEMA schema object tables (INNODB_TABLES, INNODB_TABLESPACES,
# and INNODB_TABLESTATS) to gather file format, row format, page size, and index size information about tables in the
# employees sample database.
#
# The following table name aliases are used to shorten the query string:
#
# 		) INFORMATION_SCHEMA.INNODB_TABLES: a
# 	
# 		) INFORMATION_SCHEMA.INNODB_TABLESPACES: b
#
# 		) INFORMATION_SCHEMA.INNODB_TABLESTATS: c
#
# An IF() control flow function is used to account for compressed tables. If a table is compressed, the index size is calculated
# using ZIP_PAGE_SIZE rather than PAGE_SIZE.
#
# CLUST_INDEX_SIZE and OTHER_INDEX_SIZE, which are reported in bytes, are divided by 1024*1024 to provide index sizes in megabytes
# (MBs). MB values are rounded to zero decimal spaces using the ROUND() function.
#
# 		mysql> SELECT a.NAME, a.ROW_FORMAT,
# 				 @page_size :=
# 					IF(a.ROW_FORMAT='Compressed',
# 					 b.ZIP_PAGE_SIZE, b.PAGE_SIZE)
# 					 AS page_size,
# 					ROUND((@page_size * c.CLUST_INDEX_SIZE)
# 					 /(1024*1024)) AS pk_mb,
# 					ROUND((@page_size * c.OTHER_INDEX_SIZE)
# 					 /(1024*1024)) AS secidx_mb
# 					FROM INFORMATION_SCHEMA.INNODB_TABLES a
# 					INNER JOIN INFORMATION_SCHEMA.INNODB_TABLESPACES b on a.NAME = b.NAME
# 					INNER JOIN INFORMATION_SCHEMA.INNODB_TABLESTATS c on b.NAME = c.NAME
# 					WHERE a.NAME LIKE 'employees/%'
# 					ORDER BY a.NAME DESC;
# 		+-------------------------------+------------+-------------------+-----------+-------+----------+
# 		| NAME 								  | ROW_FORMAT 						  | page_size | pk_mb | secidx_mb|
# 		+-------------------------------+--------------------------------+-----------+-------+----------+
# 		| employees/titles 				  | Dynamic 							  | 16384 	  | 20 	 | 	11 	|
# 		| employees/salaries 			  | Dynamic 							  | 16384 	  | 93 	 | 	34 	|
# 		| employees/employees 			  | Dynamic 							  | 16384 	  | 15 	 | 	0 		|
# 		| employees/dept_manager 		  | Dynamic 							  | 16384 	  | 0 	 | 	0 	   |
# 		| employees/dept_emp 			  | Dynamic 							  | 16384 	  | 12 	 | 	10 	|
# 		| employees/departments 		  | Dynamic 							  | 16384 	  | 0 	 | 	0 		|
# 		+-------------------------------+--------------------------------+-----------+-------+----------+
#
# 15.14.4 INNODB INFORMATION_SCHEMA FULLTEXT INDEX TABLES
#
# The following tables provide metadata for FULLTEXT indexes:
#
# 		mysql> SHOW TABLES FROM INFORMATION_SCHEMA LIKE 'INNODB_FT%';
# 		+------------------------------------------+
# 		| Tables_in_INFORMATION_SCHEMA (INNODB_FT%)|
# 		+------------------------------------------+
# 		| INNODB_FT_CONFIG 								 |
# 		| INNODB_FT_BEING_DELETED 						 |
# 		| INNODB_FT_DELETED 								 |
# 		| INNODB_FT_DEFAULT_STOPWORD 					 |
# 		| INNODB_FT_INDEX_TABLE 						 |
# 		| INNODB_FT_INDEX_CACHE 						 |
# 		+------------------------------------------+
#
# TABLE OVERVIEW
#
# 	) INNODB_FT_CONFIG: Provides metadata about the FULLTEXT index and associated processing for an InnoDB table.
#
# 	) INNODB_FT_BEING_DELETED: Provides a snapshot of the INNODB_FT_DELETED table; it is used only during an OPTIMIZE_TABLE
# 		maintenance operation. When OPTIMIZE_TABLE is run, the INNODB_FT_BEING_DELETED table is emptied, and DOC_ID values are
# 		removed from the INNODB_FT_DELETED table.
#
# 		Because the contents of INNODB_FT_BEING_DELETED typically have a short lifetime, this table has limited utility
# 		for monitoring or debugging. For information about running OPTIMIZE_TABLE on tables with FULLTEXT indexes,
# 		see SECTION 12.9.6, "FINE-TUNING MYSQL FULL-TEXT SEARCH"
#
# ) INNODB_FT_DELETED: Stores rows that are deleted from the FULLTEXT index for an InnoDB table. To avoid expensive index
# 		reorganization during DML operations for an InnoDB FULLTEXT index, the information about newly deleted words is stored
# 		separately, filtered out of search results when you do a text search, and removed from the main search index only
# 		when you issue an OPTIMIZE_TABLE statement for the InnoDB table.
#
# ) INNODB_FT_DEFAULT_STOPWORD: Holds a list of stopwords that are used by default when creating a FULLTEXT index on InnoDB tables.
#
# 		For information about the INNODB_FT_DEFAULT_STOPWORD table, see SECTION 12.9.4, "FULL-TEXT STOPWORDS"
#
# ) INNODB_FT_INDEX_TABLE: Provides information about the inverted index used to process text searches against the FULLTEXT index of an
# InnoDB table.
#
# ) INNODB_FT_INDEX_CACHE: Provides token information about newly inserted rows in a FULLTEXT index. To avoid expensive index reorganization
# during DML operations, the information about newly indexed words is stored seperately, and combined with the main search index only
# when OPTIMIZE_TABLE is run, when the server is shut down, or when the cache size exceeds a limit defined by the innodb_ft_cache_size
# or innodb_ft_total_cache_size system variable.
#
# NOTE:
#
# 		With the exception of the INNODB_FT_DEFAULT_STOPWORD table, these tables are empty initially.
#
# 		Before querying any of them, set the value of the innodb_ft_aux_table system variable to the name
# 		(including the database name) of the table that contains the FULLTEXT index (for example,
# 		test/articles)
#
# EXAMPLE 15.5 INNODB FULLTEXT INDEX INFORMATION_SCHEMA TABLES
#
# This example uses a table with a FULLTEXT index to demonstrate the data contained in the FULLTEXT index
# INFORMATION_SCHEMA tables.
#
# 		1. Create a table with a FULLTEXT index and insert some data:
#
# 			mysql> CREATE TABLE articles (
# 						id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY,
# 						title VARCHAR(200),
# 						body TEXT,
# 						FULLTEXT (title, body)
# 					) ENGINE=InnoDB;
#
# 			mysql> INSERT INTO articles (title,body) VALUES
# 					  //values//
#
# 		2. Set the innodb_ft_aux_table variable to the name of the table with the FULLTEXT index.
#
# 			If this variable is not set, the InnoDB FULLTEXT INFORMATION_SCHEMA tables are empty,
# 			with the exception of INNODB_FT_DEFAULT_STOPWORD.
#
# 				mysql> SET GLOBAL innodb_ft_aux_table = 'test/articles';
#
# 		3. Query the INNODB_FT_INDEX_CACHE table, which shows information about newly inserted rows in a FULLTEXT index.
#
# 			To avoid expensive index reorganization during DML operations, data for newly inserted rows remains in the
# 			FULLTEXT index cache until OPTIMIZE_TABLE is run (or until the server is shut down or cache limits are exceeded)
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_FT_INDEX_CACHE LIMIT 5;
# 				+-----------------+---------------+----------------+----------------+---------------+------------------+
# 				| WORD 				| FIRST_DOC_ID  | LAST_DOC_ID 	| DOC_COUNT 	  | DOC_ID 			| POSITION 			 |
# 				+-----------------+---------------+----------------+----------------+---------------+------------------+
# 				| 1001 				| 5 				 | 5 					| 1 				  | 5 				| 0 					 |
# 				| after 				| 3 				 | 3 					| 1 				  | 3 				| 22 					 |
# 				| comparison 		| 6 				 | 6 					| 1 				  | 6 				| 44 					 |
# 				| configured 		| 7 				 | 7 					| 1 				  | 7 			   | 20 					 |
# 				| database 			| 2 				 | 6 					| 2 				  | 2 				| 31 					 |
# 				+-----------------+---------------+----------------+----------------+---------------+------------------+
#
# 		4. Enable the innodb_optimize_fulltext_only system variable and run OPTIMIZE_TABLE on the table that contains the
# 			FULLTEXT index.
#
# 			This operation flushes the contents of the FULLTEXT index cache to the main FULLTEXT index. innodb_optimize_fulltext_only
# 			changes the way the OPTIMIZE_TABLE statement operates on InnoDB tables, and is intended to be enabled temporarily,
# 			during maintenance operations on InnoDB tables with FULLTEXT indexes.
#
# 				mysql> SET GLOBAL innodb_optimize_fulltext_only=ON;
#
# 				mysql> OPTIMIZE TABLE articles;
# 				+-------------------+-----------------+---------------+---------------+
# 				| Table 				  | Op 				  | Msg_type 		| Msg_text 		 |
# 				+-------------------+-----------------+---------------+---------------+
# 				| test.articles 	  | optimize 		  | status 			| OK 				 |
# 				+-------------------+-----------------+---------------+---------------+
#
# 		5. Query the INNODB_FT_INDEX_TABLE table to view information about data in the main FULLTEXT index, including
# 			information about the data that was just flushed from the FULLTEXT index cache.
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_FT_INDEX_TABLE LIMIT 5;
# 				+----------------+----------------+---------------+-------------+----------------+---------------------+
# 				| WORD 			  | FIRST_DOC_ID 	 | LAST_DOC_ID   | DOC_COUNT   | DOC_ID 			| POSITION 				 |
# 				+----------------+----------------+---------------+-------------+----------------+---------------------+
# 				| 1001 			  | 		5 			 | 		5 		  | 	1 			 | 	5 				| 0 						 |
# 				| after 			  | 		3 			 | 		3 		  | 	1 			 | 	3 				| 22 						 |
# 				| comparison 	  | 		6 			 | 		6 		  | 	1 			 | 	6 				| 44 						 |
# 				| configured 	  | 		7 			 | 		7 		  | 	1 			 | 	7 				| 20 						 |
# 				| database 		  | 		2 			 | 		6 		  | 	2 			 | 	2 				| 31 						 |
# 				+----------------+----------------+---------------+-------------+----------------+---------------------+
#
# 			The INNODB_FT_INDEX_CACHE table is now empty since the OPTIMIZE_TABLE operation flushed the FULLTEXT index cache.
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_FT_INDEX_CACHE LIMIT 5;
# 				Empty set (0.00 sec)
#
# 		6. Delete some records from the test/articles table.
#
# 			mysql> DELETE FROM test.articles WHERE id < 4;
#
# 		7. Query the INNODB_FT_DELETED table. This table records rows that are deleted from the FULLTEXT index.
#
# 			To avoid expensive index reorganization during DML operations, information about newly deleted
# 			records is stored separately, filtered out of search results when you do a text search, and removed
# 			from the main search index when you run OPTIMIZE_TABLE.
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_FT_DELETED;
# 				+-------+
# 				| DOC_ID|
# 				+-------+
# 				| 2 	  |
# 				| 3 	  |
# 				| 4 	  |
# 				+-------+
#
# 		8. Run OPTIMIZE_TABLE to remove the deleted records.
#
# 				mysql> OPTIMIZE TABLE articles;
# 				+-------------------+----------+---------------+--------------+
# 				| Table 				  | Op 		 | Msg_type 	  | Msg_text 	  |
# 				+-------------------+----------+---------------+--------------+
# 				| test.articles 	  | optimize | status 		  | OK 			  |
# 				+-------------------+----------+---------------+--------------+
#
# 			The INNODB_FT_DELETED table should now be empty.
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_FT_DELETED;
# 				Empty set (0.00 sec)
#
# 		9. Query the INNODB_FT_CONFIG table. This table contains metadata about the FULLTEXT index and related processing:
#
# 			) optimize_checkpoint_limit: The number of seconds after which an OPTIMIZE_TABLE run stops.
#
# 			) synced_doc_id: The next DOC_ID to be issued
#
# 			) stopword_table_name: The database/table name for a user-defined stopword table. The VALUE column is empty if there
# 				is no user-defined stopword table.
#
# 			) use_stopword: indicates whether a stopword table is used, which is defined when the FULLTEXT index is created.
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_FT_CONFIG;
# 				+--------------------------+-----------+
# 				| KEY 							| VALUE 	   |
# 				+--------------------------+-----------+
# 				| optimize_checkpoint_limit| 180 		|
# 				| synced_doc_id 				| 8 			|
# 				| stopword_table_name 		|  			|
# 				| use_stopword 				| 1 			|
# 				+--------------------------+-----------+
#
# 		10. Disable innodb_optimize_fulltext_only, since it is intended to be enabled only temporarily:
#
# 			mysql> SET GLOBAL innodb_optimize_fulltext_only=OFF;
#
# 15.14.5 INNODB INFORMATION_SCHEMA BUFFER POOL TABLES
#
# The InnoDB INFORMATION_SCHEMA buffer pool tables provide buffer pool status information and metadata about the pages within the
# InnoDB buffer pool.
#
# The InnoDB INFORMATION-SCHEMA buffer pool tables include those listed below:
#
# 		mysql> SHOW TABLES FROM INFORMATION_SCHEMA LIKE 'INNODB_BUFFER%';
# 		+-------------------------------------------------------+
# 		| Tables_in_INFORMATION_SCHEMA (INNODB_BUFFER%)			  |
# 		+-------------------------------------------------------+
# 		| INNODB_BUFFER_PAGE_LRU 										  |
# 		| INNODB_BUFFER_PAGE 											  |
# 		| INNODB_BUFFER_POOL_STATS 									  |
# 		+-------------------------------------------------------+
#
# TABLE OVERVIEW
#
# 	) INNODB_BUFFER_PAGE: Holds information about each page in the InnoDB buffer pool
#
# 	) INNODB_BUFFER_PAGE_LRU: Holds information about the pages in the InnoDB buffer pool, in particular how they are ordered
# 		in the LRU list that determines which pages to evict from the buffer pool when it becomes full.
#
# 		The INNODB_BUFFER_PAGE_LRU table has the same columns as the INNODB_BUFFER_PAGE table, except that the 
# 		INNODB_BUFFER_PAGE_LRU table has an LRU_POSITION column instead of a BLOCK_ID column.
#
# 	) INNODB_BUFFER_POOL_STATS: Provides buffer pool status information. Much of the same information is provided by SHOW_ENGINE_INNODB_STATUS
# 		output, or may be obtained using InnoDB buffer pool server status variables.
#
# 		WARNING:
#
# 			Querying the INNODB_BUFFER_PAGE or INNODB_BUFFER_PAGE_LRU table can affect performance. Do not query these tables on a production
# 			system unless you are aware of the performance impact and have determined it to be acceptable.
#
# 			To avoid impacting performance on a production system, reproduce the issue you want to investigate and query buffer pool
# 			statistics on a test instance.
#
# EXAMPLE 15.6 QUERYING SYSTEM DATA IN THE INNODB_BUFFER_PAGE TABLE
#
# This query provides an approximate count of pages that contain system data by excluding pages where the TABLE_NAME value
# is either NULL or includes a slash / or period . in the table name, which indicates a user-defined table.
#
# 		mysql> SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
# 				 WHERE TABLE_NAME IS NULL OR (INSTR(TABLE_NAME '/') = 0 AND INSTR(TABLE_NAME, '.') = 0);
# 		+---------------+
# 		| COUNT(*) 		 |
# 		+---------------+
# 		| 1516 			 |
# 		+---------------+
#
# This query returns the approximate number of pages that contain system data, the total number of buffer pool pages, and an approximate
# percentage of pages that contain system data.
#
# 		mysql> SELECT
# 				 (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
# 				 WHERE TABLE_NAME IS NULL OR (INSTR(TABLE_NAME, '/') = 0 AND INSTR(TABLE_NAME, '.') = 0)
# 				 ) AS system_pages,
# 				 (
# 				 SELECT COUNT(*)
# 				 FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
# 				 ) AS total_pages,
# 				 (
# 				 SELECT ROUND((system_pages/total_pages) * 100)
# 				 ) AS system_page_percentage;
# 		+---------------+----------------+-------------------------------+
# 		| system_pages  | total_pages 	| system_page_percentage 		  |
# 		+---------------+----------------+-------------------------------+
# 		| 295 			 | 	8192 		   | 					4 					  |
# 		+---------------+----------------+-------------------------------+
#
# The type of system data in the buffer pool can be determined by querying the PAGE_TYPE value.
#
# For example, the following query returns eight distinct PAGE_TYPE values among the pages that contain
# system data:
#
# 		mysql> SELECT DISTINCT PAGE_TYPE FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
# 				 WHERE TABLE_NAME IS NULL OR (INSTR(TABLE_NAME, '/') = 0 AND INSTR(TABLE_NAME, '.') = 0);
# 		+---------------------+
# 		| PAGE_TYPE 			 |
# 		+---------------------+
# 		| SYSTEM 				 |
# 		| IBUF_BITMAP 			 |
# 		| UNKNOWN 				 |
# 		| FILE_SPACE_HEADER   |
# 		| INODE 					 |
# 		| UNDO_LOG 				 |
# 		| ALLOCATED 			 |
# 		+---------------------+
#
# EXAMPLE 15.7 QUERYING USER DATA IN THE INNODB_BUFFER_PAGE TABLE
#
# This query provides an approximate count of pages containing user data by counting pages where the
# TABLE_NAME value is NOT NULL and NOT LIKE '%INNODB_TABLES%'
#
# 		mysql> SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
# 				 WHERE TABLE_NAME IS NOT NULL AND TABLE_NAME NOT LIKE '%INNODB_TABLES%';
# 		+---------+
# 		| COUNT(*)|
# 		+---------+
# 		| 7897 	 |
# 		+---------+
#
# This query returns the approximate number of pages that contain user data, the total number of buffer pool pages,
# and an approximate percentage of pages that contain user data.
#
# 		mysql> SELECT
# 				 (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
# 				 WHERE TABLE_NAME IS NOT NULL AND (INSTR(TABLE_NAME, '/') > 0 OR INSTR(TABLE_NAME, '.') > 0)
# 				 ) AS user_pages,
# 				 (
# 				 SELECT COUNT(*)
# 				 FROM information_schema.INNODB_BUFFER_PAGE
# 				 ) AS total_pages,,
# 				 (
# 				 SELECT ROUND((user_pages/total_pages) * 100)
# 				 ) AS user_page_percentage;
# 		+------------+---------------+-----------------------------+
# 		| user_pages | total_pages   | user_page_percentage 		  |
# 		+------------+---------------+-----------------------------+
# 		| 7897 		 | 8192 			  | 			96 					  |
# 		+------------+---------------+-----------------------------+
#
# This query identifies user-defined tables with pages in the buffer pool:
#
# 		mysql> SELECT DISTINCT TABLE_NAME FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
# 				 WHERE TABLE_NAME IS NOT NULL AND (INSTR(TABLE_NAME, '/') > 0 OR INSTR(TABLE_NAME, '.') > 0)
# 				 AND TABLE_NAME NOT LIKE '`mysql`.`innodb_%';
# 		+-------------------------+
# 		| TABLE_NAME 				  |
# 		+-------------------------+
# 		| `employees`.`salaries`  |
# 		| `employees`.`employees` |
# 		+-------------------------+
#
# EXAMPLE 15.8 QUERYING INDEX DATA IN THE INNODB_BUFFER_PAGE TABLE
#
# For information about index pages, query the INDEX_NAME column using the name of the index. For example,
# the following query returns the number of pages and total data size of pages for the emp_no index that is
# defined on the employees.salaries table:
#
# 		mysql> SELECT INDEX_NAME, COUNT(*) AS Pages,
# 		ROUND(SUM(IF(COMPRESSED_SIZE = 0, @@GLOBAL.innodb_page_size, COMPRESSED_SIZE))/1024/1024)
# 		AS 'Total Data (MB)'
# 		FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
# 		WHERE INDEX_NAME='emp_no' AND TABLE_NAME = '`employees`.`salaries`';
# 		+---------------+--------+--------------------+
# 		| INDEX_NAME    | Pages  | Total Data (MB)    |
# 		+---------------+--------+--------------------+
# 		| emp_no 		 | 1609   | 		25 			 |
# 		+---------------+--------+--------------------+
#
# This query returns the number of pages and total data size of pages for all indexes defined on the employees.salaries table:
#
# 		mysql> SELECT INDEX_NAME, COUNT(*) AS Pages,
# 				 ROUND(SUM(IF(COMPRESSED_SIZE = 0, @@GLOBAL.innodb_page_size, COMPRESSED_SIZE))/1024/1024)
# 				 AS 'Total Data (MB)'
# 				 FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE
# 				 WHERE TABLE_NAME = '`employees`.`salaries`'
# 				 GROUP BY INDEX_NAME;
#
# 		+--------------+----------+--------------------+
# 		| INDEX_NAME   | Pages    | Total Data (MB)    |
# 		+--------------+----------+--------------------+
# 		| emp_no 		| 	1608    | 		25 			  |
# 		| PRIMARY 		|  6086 	  | 		95 			  |
# 		+--------------+----------+--------------------+
#
# EXAMPLE 15.9 QUERYING LRU_POSITION DATA IN THE INNODB_BUFFER_PAGE_LRU TABLE
#
# The INNODB_BUFFER_PAGE_LRU table holds information about the pages in the InnoDB buffer pool, in particular
# how they are ordered that determines which pages to evict from the buffer pool when it becomes full.
#
# The definition for this page is the same as for INNODB_BUFFER_PAGE, except this table has an LRU_POSITION
# column instead of a BLOCK_ID column.
#
# This query counts the number of positions at a specific location in the LRU list occupied by pages of
# the employees.employees table.
#
# 		mysql> SELECT COUNT(LRU_POSITION) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE_LRU
# 				 WHERE TABLE_NAME='`employees`.`employees`' AND LRU_POSITION < 3072;
# 		+-------------------------+
# 		| COUNT(LRU_POSITION) 	  |
# 		+-------------------------+
# 		| 548 						  |
# 		+-------------------------+
#
# EXAMPLE 15.10 QUERYING THE INNODB_BUFFER_POOL_STATS TABLE
#
# The INNODB_BUFFER_POOL_STATS table provides information similar to SHOW_ENGINE_INNODB_STATUS and InnoDB buffer pool status variables.
#
# 		mysql> SELECT * FROM information_schema.INNODB_BUFFER_POOL_STATS \G
# 		***************************** 1. row *****************************
# 									POOL_ID: 0
# 								POOL_SIZE : 8192
# 						FREE_BUFFERS    : 1
# 						  DATABASE_PAGES: 8173
# 					OLD_DATABASE_PAGES : 3014
# 			  MODIFIED_DATABASE_PAGES: 0
#			 		PENDING_DECOMPRESS : 0
# 						PENDING_READS   : 0
# 					PENDING_FLUSH_LRU  : 0
# 					PENDING_FLUSH_LIST : 0
# 					PAGES_MADE_YOUNG   : 15907
# 				  PAGES_NOT_MADE_YOUNG: 3803101
# 				 PAGES_MADE_YOUNG_RATE: 0
# 			PAGES_MADE_NOT_YOUNG_RATE: 0
# 				NUMBER_PAGES_READ     : 3270
# 				NUMBER_PAGES_CREATED  : 13176
# 				NUMBER_PAGES_WRITTEN  : 15109
# 				PAGES_READ_RATE 		 : 0
# 				PAGES_CREATE_RATE 	 : 0
# 				PAGES_WRITTEN_RATE    : 0
# 				NUMBER_PAGES_GET 		 : 33069332
# 							HIT_RATE     : 0
# 		YOUNG_MAKE_PER_THOUSAND_GETS: 0
# NOT_YOUNG_MAKE_PER_THOUSAND_GETS: 0
# 			NUMBER_PAGES_READ_AHEAD  : 2713
# 			NUMBER_READ_AHEAD_EVICTED: 0
# 			READ_AHEAD_RATE 			 : 0
# 			READ_AHEAD_EVICTED_RATE  : 0
# 					LRU_IO_TOTAL 		 : 0
# 				LRU_IO_CURRENT 		 : 0
# 				UNCOMPRESS_TOTAL 		 : 0
# 				UNCOMPRESS_CURRENT    : 0
#
# For comparison, SHOW_ENGINE_INNODB_STATUS output and InnoDB buffer pool status variable output is shown below,
# based on the same data set.
#
# For more information about SHOW_ENGINE_INNODB_STATUS output, see SECTION 15.16.3, "InnoDB STANDARD MONITOR AND LOCK MONITOR OUTPUT"
#
# 		mysql> SHOW ENGINE INNODB STATUS \G
# 		-
# 		-------------------------
# 		BUFFER POOL AND MEMORY
# 		-------------------------
# 		Total large memory allocated 137428992
# 		Dictionary memory allocated  579084
# 		Buffer pool size 	8192
# 		Free buffers 		1
# 		Database pages 	8173
# 		Old database pages 3014
# 		Modified db pages 0
# 		Pending reads 		0
# 		Pending writes: LRU 0, flush list 0, single page 0
# 		Pages made young 15907, not young 3803101
# 		0.00 young/s, 0.00 non-youngs/s
# 		Pages read 3270, created 13176, written 15109
# 		0.00 reads/s, 0.00 creates/s, 0.00 writes/s
# 		No buffer pool page gets since the last printout
# 		Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s
# 		LRU len: 8173, unzip_LRU len: 0
# 		I/O sum[0]:cur[0], unzip sum[0]:cur[0]
# 		-
#
# For status variable descriptions, see SECTION 5.1.10, "SERVER STATUS VARIABLES"
#
# 		mysql> SHOW STATUS LIKE 'Innodb_buffer%';
# 		+-----------------------------------------+--------------+
# 		| Variable_name 									| Value 		   |
# 		+-----------------------------------------+--------------+
# 		| INnodb_buffer_pool_dump_status 			| not started  |
# 		| Innodb_buffer_pool_load_status 			| not started  |
# 		| Innodb_buffer_pool_resize_status 			| not started  |
# 		| Innodb_buffer_pool_pages_data 				| 8173 			|
# 		| Innodb_buffer_pool_bytes_data 				| 133906432 	|
# 		| Innodb_buffer_pool_pages_dirty 			| 0 				|
# 		| Innodb_buffer_pool_bytes_dirty 			| 0 				|
# 		| Innodb_buffer_pool_pages_flushed 			| 15109 			|
# 		| Innodb_buffer_pool_pages_free 				| 1 				|
# 		| Innodb_buffer_pool_pages_misc 				| 18 				|
# 		| Innodb_buffer_pool_pages_total 			| 8192 			|
# 		| Innodb_buffer_pool_read_ahead_rnd 		| 0 				|
# 		| Innodb_buffer_pool_read_ahead 	 			| 2713 			|
# 		| Innodb_buffer_pool_read_ahead_evicted   | 0 				|
# 		| Innodb_buffer_pool_read_requests 			| 33069332 		|
# 		| Innodb_buffer_pool_reads 					| 558 		   |
# 		| Innodb_buffer_pool_wait_free 				| 0 				|
# 		| Innodb_buffer_pool_write_requests 		| 11985961 		|
# 		+-----------------------------------------+--------------+
#
# 15.14.6 INNODB INFORMATION_SCHEMA METRICS TABLE
#
# The INNODB_METRICS table provides information about InnoDB performance and resource-related counters.
#
# INNODB_METRICS table columns are shown below. For column descriptions, see SECTION 25.39.22, "THE INFORMATION_SCHEMA INNODB_METRICS TABLE"
#
# 		mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_METRICS WHERE NAME="dml_inserts" \G
# 		***************************** 1. row ******************************
# 						NAME: dml_inserts
# 				SUBSYSTEM : dml
# 					COUNT  : 46273
# 				MAX_COUNT : 46273
# 				MIN_COUNT : NULL
# 				AVG_COUNT : 492.2659574468085
# 			COUNT_RESET  : 46273
# 		 MAX_COUNT_RESET: 46273
# 		MIN_COUNT_RESET : NULL
# 		AVG_COUNT_RESET : NULL
# 			TIME_ENABLED : 2014-11-28 16:07:53
# 		TIME_DISABLED   : NULL
# 			TIME_ELAPSED : 94
# 			TIME_RESET   : NULL
# 					STATUS : enabled
# 					TYPE   : status_counter
# 					COMMENT: Number of rows inserted
#
# ENABLING, DISABLING AND RESETTING COUNTERS
#
# You can enable, disable and reset counters using the following variables:
#
# 		) innodb_monitor_enable: Enables counters
#
# 			SET GLOBAL innodb_monitor_enable = [counter-name|module_name|pattern|all];
#
# 		) innodb_monitor_disable: Disables counters
#
# 			SET GLOBAL innodb_monitor_disable = [counter-name|module_name|pattern|all];
#
# 		) innodb_monitor_reset: Resets counter values to zero
#
# 			SET GLOBAL innodb_monitor_reset = [counter-name|module_name|pattern|all];
#
# 		) innodb_monitor_reset_all: Resets all counter values. A counter must be disabled before using innodb_monitor_reset_all
#
# 			SET GLOBAL innodb_monitor_reset_all = [counter-name|module_name|pattern|all];
#
# Counters and counter modules can also be enabled at startup using the MySQL server configuration file.
#
# For example, to enable the log module, metadata_table_handles_opened and metadata_table_handles_closed counters,
# enter the following line in the [mysqld] section of the MySQL server configuration file.
#
# 		[mysqld]
# 		innodb_monitor_enable = module_recovery, metadata_table_handles_opened,metadata_table_handles_closed
#
# When enabling multiple counters or modules in a configuration file, specify the innodb_monitor_enable variable followed
# by counter and module names separated by a comma, as shown above.
#
# Only the innodb_monitor_enable variable can be used in a configuration file. The innodb.monitor_disable and
# innodb_monitor_reset variables are supported on the command line only.
#
# NOTE:
#
# 		Because each counter adds a degree of runtime overhead, use counters conservatively on production servers to diagnose
# 		specific issues or monitor specific functionality.
#
# 		A test or development server is recommended for more extensive use of counters.
#
# COUNTERS
#
# The list of available counters is subject to change. Query the INFORMATION_SCHEMA.INNODB_METRICS table for counters
# available in your MySQL server version.
#
# The counters enabled by default correspond to those shown in SHOW_ENGINE_INNODB_STATUS output. Counters shown in
# SHOW_ENGINE_INNODB_STATUS output are always enabled at a system level but can be disabled for  the INNODB_METRICS
# table.
#
# Counter status is not persistent. Unless configured otherwise, counters revert to their default enabled or disabled
# status when the server is restarted.
#
# If you run programs that would be affected by the addition or removal of counters, it is recommended that you review
# the release notes and query the INNODB_METRICS table to identify those changes as part of your upgrade process.
#
# 	mysql> SELECT name, subsystem, status FROM INFORMATION_SCHEMA.INNODB_METRICS ORDER BY NAME;
# 	+-------------------------------------------------------------+--------------------+-------------+
# 	| name 																		  | subsystem 		     | status 		 |
# 	+-------------------------------------------------------------+--------------------+-------------+
# 	| adaptive_hash_pages_added 											  | adaptive_hash_index| disabled 	 |
# 	| adaptive_hash_pages_removed 										  | adaptive_hash_index| disabled 	 |
# 	| adaptive_hash_rows_added 											  | adaptive_hash_index| disabled 	 |
# 	| adaptive_hash_rows_deleted_no_hash_entry 						  | adaptive_hash_index| disabled 	 |
# 	| adaptive_hash_rows_removed 											  | adaptive_hash_index| disabled 	 |
# 	| adaptive_hash_rows_updated 											  | adaptive_hash_index| disabled 	 |
# 	| adaptive_hash_searches 												  | adaptive_hash_index| enabled     |
# 	| adaptive_hash_searches_btree 										  | adaptive_hash_index| enabled     |
# 	| buffer_data_reads 														  | buffer 				  | enabled 	 |
# 	| buffer_data_written 													  | buffer 				  | enabled 	 |
# 	| buffer_flush_adaptive 												  | buffer 				  | disabled 	 |
# 	| buffer_flush_adaptive_avg_pass 									  | buffer 				  | disabled 	 |
# 	| buffer_flush_adaptive_avg_time_est 								  | buffer 				  | disabled 	 |
# 	| buffer_flush_adaptive_avg_time_slot 								  | buffer 				  | disabled 	 |
# 	| buffer_flush_adaptive_avg_time_thread 							  | buffer 				  | disabled    |
# 	| buffer_flush_adaptive_pages 										  | buffer 				  | disabled 	 |
# 	| buffer_flush_adaptive_total_pages 								  | buffer 				  | disabled 	 |
# 	| buffer_flush_avg_page_rate 											  | buffer 				  | disabled 	 |
# 	| buffer_flush_avg_pass 												  | buffer 				  | disabled 	 |
# 	| buffer_flush_avg_time 												  | buffer 				  | disabled 	 |
# 	| buffer_flush_background 												  | buffer 				  | disabled 	 |
# 	| buffer_flush_background_pages 										  | buffer 				  | disabled 	 |
# 	| buffer_flush_background_total_pages 								  | buffer 				  | disabled 	 |
# 	| buffer_flush_batches 													  | buffer 				  | disabled 	 |
# 	| buffer_flush_batch_num_scan 										  | buffer 				  | disabled 	 |
# 	| buffer_flush_batch_pages 											  | buffer 				  | disabled 	 |
# 	| buffer_flush_batch_scanned 											  | buffer 				  | disabled 	 |
# 	| buffer_flush_batch_scanned_per_call 								  | buffer 				  | disabled    |
# 	| buffer_flush_batch_total_pages 									  | buffer 				  | disabled 	 |
# 	| buffer_flush_lsn_avg_rate 											  | buffer 				  | disabled 	 |
# 	| buffer_flush_neighbor 												  | buffer 				  | disabled 	 |
# 	| buffer_flush_neighbor_pages 										  | buffer 				  | disabled 	 |
# 	| buffer_flush_neighbor_total_pages 								  | buffer 				  | disabled 	 |
# 	| buffer_flush_n_to_flush_by_age 									  | buffer 				  | disabled 	 |
# 	| buffer_flush_n_to_flush_requested 								  | buffer 				  | disabled 	 |
# 	| buffer_flush_pct_for_dirty 											  | buffer 				  | disabled 	 |
# 	| buffer_flush_pct_for_lsn 											  | buffer 				  | disabled 	 |
# 	| buffer_flush_sync 														  | buffer 				  | disabled 	 |
# 	| buffer_flush_sync_pages 												  | buffer 				  | disabled 	 |
# 	| buffer_flush_sync_total_pages 										  | buffer 				  | disabled 	 |
# 	| buffer_flush_sync_waits 												  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batches_evict 											  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batches_flush 											  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batch_evict_pages 										  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batch_evict_total_pages 								  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batch_flush_avg_pass 									  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batch_flush_avg_time_est 								  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batch_flush_avg_time_slot 							  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batch_flush_avg_time_thread 							  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batch_flush_pages 										  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batch_flush_total_pages 								  | buffer 				  | disabled 	 |
#  | buffer_LRU_batch_num_scan 											  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batch_scanned 											  | buffer 				  | disabled 	 |
# 	| buffer_LRU_batch_scanned_per_call 								  | buffer 				  | disabled 	 |
# 	| buffer_LRU_get_free_loops 											  | buffer 				  | disabled 	 |
# 	| buffer_LRU_get_free_search 											  | buffer 				  | disabled 	 |
# 	| buffer_LRU_get_free_waits 											  | buffer 				  | disabled 	 |
# 	| buffer_LRU_search_num_scan 											  | buffer 				  | disabled 	 |
# 	| buffer_LRU_search_scanned 											  | buffer 				  | disabled 	 |
# 	| buffer_LRU_search_scanned_per_call 								  | buffer 				  | disabled 	 |
# 	| buffer_LRU_single_flush_failure_count 							  | buffer 				  | disabled 	 |
# 	| buffer_LRU_single_flush_num_scan 									  | buffer 				  | disabled 	 |
# 	| buffer_LRU_single_flush_scanned 									  | buffer 				  | disabled 	 |
# 	| buffer_LRU_single_flush_scanned_per_call 						  | buffer 				  | disabled 	 |
# 	| buffer_LRU_unzip_search_num_scan 									  | buffer 				  | disabled 	 |
# 	| buffer_LRU_unzip_search_scanned 									  | buffer 				  | disabled 	 |
# 	| buffer_LRU_unzip_search_scanned_per_call 						  | buffer 				  | disabled 	 |
# 	| buffer_pages_created 													  | buffer 				  | enabled 	 |
# 	| buffer_pages_read 														  | buffer 				  | enabled 	 |
# 	| buffer_pages_written 													  | buffer 				  | enabled 	 |
# 	| buffer_page_read_blob 												  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_fsp_hdr 											  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_ibuf_bitmap 										  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_ibuf_free_list 									  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_index_ibuf_leaf 									  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_index_ibuf_non_leaf 							  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_index_inode 										  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_index_leaf 										  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_index_non_leaf 									  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_other 												  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_system_page 										  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_trx_system 										  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_undo_log 											  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_xdes 												  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_zblob 												  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_read_zblob2 												  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_blob 											  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_fsp_hdr 										  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_ibuf_bitmap 									  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_ibuf_free_list 								  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_index_ibuf_leaf 								  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_index_ibuf_non_leaf 						  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_index_inode 									  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_index_leaf 									  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_index_non_leaf 								  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_other 											  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_system_page 									  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_trx_system 									  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_undo_log 										  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_xdes 											  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_zblob 											  | buffer_page_io 	  | disabled 	 |
# 	| buffer_page_written_zblob2 											  | buffer_page_io 	  | disabled 	 |
# 	| buffer_pool_bytes_data 												  | buffer			 	  | enabled 	 |
# 	| buffer_pool_bytes_dirty 												  | buffer				  | enabled 	 |
# 	| buffer_pool_pages_data 												  | buffer 				  | enabled 	 |
# 	| buffer_pool_pages_dirty 												  | buffer 				  | enabled 	 |
# 	| buffer_pool_pages_free 												  | buffer 				  | enabled 	 |
# 	| buffer_pool_pages_misc 												  | buffer 				  | enabled 	 |
# 	| buffer_pool_pages_total 												  | buffer 				  | enabled 	 |
# 	| buffer_pool_reads 														  | buffer 				  | enabled 	 |
# 	| buffer_pool_read_ahead 												  | buffer 				  | enabled 	 |
# 	| buffer_pool_read_ahead_evicted 									  | buffer 				  | enabled 	 |
# 	| buffer_pool_read_requests 											  | buffer 				  | enabled 	 |
# 	| buffer_pool_size 														  | server 				  | enabled 	 |
# 	| buffer_pool_wait_free 												  | buffer 				  | enabled 	 |
# 	| buffer_pool_write_requests 											  | buffer 				  | enabled 	 |
# 	| compression_pad_decrements 											  | compression 	     | disabled 	 |
# 	| compression_pad_increments 											  | compression 	 	  | disabled 	 |
# 	| compress_pages_compressed 											  | compression 		  | disabled 	 |
# 	| compress_pages_decompressed 										  | compression 		  | disabled 	 |
# 	| ddl_background_drop_indexes 										  | ddl 					  | disabled 	 |
# 	| ddl_background_drop_tables 											  | ddl 					  | disabled 	 |
# 	| ddl_log_file_alter_table 											  | ddl 					  | disabled 	 |
# 	| ddl_online_create_index 												  | ddl 					  | disabled 	 |
# 	| ddl_pending_alter_table 												  | ddl 					  | disabled 	 |
# 	| ddl_sort_file_alter_table 											  | ddl 					  | disabled 	 |
# 	| dml_deletes 																  | dml 					  | enabled 	 |
# 	| dml_inserts 																  | dml 					  | enabled 	 |
# 	| dml_reads 																  | dml 					  | disabled 	 |
# 	| dml_updates 																  | dml 					  | enabled 	 |
# 	| file_num_open_files 													  | file_system 		  | enabled 	 |
# 	| ibuf_merges 																  | change_buffer 	  | enabled 	 |
# 	| ibuf_merges_delete 													  | change_buffer 	  | enabled 	 |
# 	| ibuf_merges_delete_mark 												  | change_buffer 	  | enabled 	 |
# 	| ibuf_merges_discard_delete 											  | change_buffer 	  | enabled 	 |
# 	| ibuf_merges_discard_delete_mark 									  | change_buffer 	  | enabled 	 |
# 	| ibuf_merges_discard_insert 											  | change_buffer 	  | enabled 	 |
# 	| ibuf_merges_insert 													  | change_buffer 	  | enabled 	 |
# 	| ibuf_size 																  | change_buffer 	  | enabled 	 |
# 	| icp_attempts 															  | icp 					  | disabled 	 |
# 	| icp_match 																  | icp 					  | disabled 	 |
# 	| icp_no_match 															  | icp 					  | disabled 	 |
# 	| icp_out_of_range 														  | icp 					  | disabled 	 |
# 	| index_page_discards 													  | index 				  | disabled 	 |
# 	| index_page_merge_attempts 											  | index 				  | disabled 	 |
# 	| index_page_merge_successful 										  | index 				  | disabled 	 |
# 	| index_page_reorg_attempts 											  | index 				  | disabled 	 |
# 	| index_page_reorg_successful 										  | index 				  | disabled 	 |
# 	| index_page_splits 														  | index 				  | disabled 	 |
# 	| innodb_activity_count 												  | server 				  | enabled 	 |
# 	| innodb_background_drop_table_usec 								  | server 				  | disabled 	 |
# 	| innodb_checkpoint_usec 												  | server 				  | disabled 	 |
# 	| innodb_dblwr_pages_written 											  | server 				  | enabled 	 |
# 	| innodb_dblwr_writes 													  | server 				  | enabled 	 |
# 	| innodb_dict_lru_count 												  | server 				  | disabled 	 |
# 	| innodb_dict_lru_usec 													  | server 				  | disabled 	 |
# 	| innodb_ibuf_merge_usec 												  | server 				  | disabled 	 |
# 	| innodb_log_flush_usec 												  | server 				  | disabled 	 |
# 	| innodb_master_active_loops 											  | server 				  | disabled 	 |
# 	| innodb_master_idle_loops 											  | server 				  | disabled 	 |
# 	| innodb_master_purge_usec 											  | server 				  | disabled 	 |
# 	| innodb_master_thread_sleeps 										  | server 				  | disabled 	 |
# 	| innodb_mem_validate_usec 											  | server 				  | disabled 	 |
# 	| innodb_page_size 														  | server 				  | enabled 	 |
# 	| innodb_rwlock_sx_os_waits 											  | server 				  | enabled 	 |
# 	| innodb_rwlock_sx_spin_rounds 										  | server 				  | enabled 	 |
# 	| innodb_rwlock_sx_spin_waits 										  | server 				  | enabled 	 |
# 	| innodb_rwlock_s_os_waits 											  | server 				  | enabled 	 |
# 	| innodb_rwlock_s_spin_rounds 										  | server 				  | enabled 	 |
# 	| innodb_rwlock_s_spin_waits 											  | server 				  | enabled 	 |
# 	| innodb_rwlock_x_os_waits 											  | server 				  | enabled 	 |
# 	| innodb_rwlock_x_spin_rounds 										  | server 				  | enabled 	 |
# 	| innodb_rwlock_x_spin_waits 											  | server 				  | enabled 	 |
# 	| lock_deadlocks 															  | lock 				  | enabled 	 |
# 	| lock_rec_locks 															  | lock 				  | disabled 	 |
# 	| lock_rec_lock_created 												  | lock 				  | disabled 	 |
# 	| lock_rec_lock_removed 												  | lock 				  | disabled 	 |
# 	| lock_rec_lock_requests 												  | lock 				  | disabled 	 |
# 	| lock_rec_lock_waits 													  | lock 				  | disabled 	 |
# 	| lock_row_lock_current_waits 										  | lock 				  | enabled 	 |
# 	| lock_row_lock_time 													  | lock 				  | enabled 	 |
# 	| lock_row_lock_time_avg 												  | lock 				  | enabled 	 |
# 	| lock_row_lock_time_max 												  | lock 				  | enabled 	 |
# 	| lock_row_lock_waits 													  | lock 				  | enabled 	 |
# 	| lock_table_locks 														  | lock 				  | disabled 	 |
# 	| lock_table_lock_created 												  | lock 				  | disabled 	 |
# 	| lock_table_lock_removed 												  | lock 				  | disabled 	 |
# 	| lock_table_lock_waits 												  | lock 				  | disabled 	 |
# 	| lock_timeouts 															  | lock 				  | enabled 	 |
# 	| log_checkpoints 														  | recovery 			  | disabled 	 |
# 	| log_lsn_buf_pool_oldest 												  | recovery 			  | disabled 	 |
# 	| log_lsn_checkpoint_age 												  | recovery 			  | disabled 	 |
# 	| log_lsn_current 														  | recovery 			  | disabled 	 |
# 	| log_lsn_last_checkpoint 												  | recovery 			  | disabled 	 |
# 	| log_lsn_last_flush 													  | recovery 			  | disabled 	 |
# 	| log_max_modified_age_async 										     | recovery 			  | disabled    |
# 	| log_max_modified_age_sync 											  | recovery 			  | disabled 	 |
# 	| log_num_log_io 															  | recovery 			  | disabled 	 |
# 	| log_padded 																  | recovery 			  | enabled 	 |
# 	| log_pending_checkpoint_writes 										  | recovery 			  | disabled 	 |
# 	| log_pending_log_flushes 												  | recovery 			  | disabled 	 |
# 	| log_waits 																  | recovery 			  | enabled 	 |
# 	| log_writes 																  | recovery 			  | enabled 	 |
# 	| log_write_requests 													  | recovery 			  | enabled 	 |
# 	| metadata_table_handles_closed 										  | metadata 			  | disabled 	 |
# 	| metadata_table_handles_opened 										  | metadata 			  | disabled 	 |
# 	| metadata_table_reference_count 									  | metadata 			  | disabled 	 |
# 	| os_data_fsyncs 															  | os 					  | enabled 	 |
# 	| os_data_reads 															  | os 					  | enabled 	 |
# 	| os_data_writes 															  | os 					  | enabled 	 |
# 	| os_log_bytes_written 													  | os 					  | enabled 	 |
# 	| os_log_fsyncs 															  | os 					  | enabled 	 |
# 	| os_log_pending_fsyncs 												  | os 					  | enabled 	 |
# 	| os_log_pending_writes 												  | os 					  | enabled 	 |
# 	| os_pending_reads 														  | os 					  | disabled 	 |
# 	| os_pending_writes 														  | os 					  | disabled 	 |
# 	| purge_del_mark_records 												  | purge 				  | disabled 	 |
# 	| purge_dml_delay_usec 													  | purge 				  | disabled 	 |
# 	| purge_invoked 															  | purge 				  | disabled 	 |
# 	| purge_resume_count 													  | purge 				  | disabled 	 |
# 	| purge_stop_count 														  | purge 				  | disabled 	 |
# 	| purge_undo_log_pages 													  | purge 				  | disabled 	 |
# 	| purge_upd_exist_or_extern_records 								  | purge 				  | disabled 	 |
# 	| trx_active_transactions 												  | transaction 		  | disabled 	 |
# 	| trx_commits_insert_update 											  | transaction 		  | disabled 	 |
# 	| trx_nl_ro_commits 														  | transaction 		  | disabled 	 |
# 	| trx_rollbacks 															  | transaction 	 	  | disabled 	 |
# 	| trx_rollbacks_savepoint 												  | transaction 		  | disabled 	 |
# 	| trx_rollback_active 													  | transaction 		  | disabled 	 |
# 	| trx_ro_commits 															  | transaction 	     | disabled 	 |
# 	| trx_rseg_current_size 												  | transaction 		  | disabled 	 |
# 	| trx_rseg_history_len 													  | transaction 		  | enabled 	 |
# 	| trx_rw_commits 															  | transaction 		  | disabled 	 |
# 	| trx_undo_slots_cached 												  | transaction 		  | disabled 	 |
# 	| trx_undo_slots_used 													  | transaction 		  | disabled 	 |
# 	+-------------------------------------------------------------+--------------------+-------------+
# 	235 rows in set (0.01 sec)
#
# COUNTER MODULES
#
# Each counter is associated with a particular module. Module names can be used to enable, disable or reset all counters
# for a particular subsystem. For example, use module_dml to enable all counters associated with the dml subsystem.
#
# 		mysql> SET GLOBAL innodb_monitor_enable = module_dml;
#
# 		mysql> SELECT name, subsystem, status FROM INFORMATION_SCHEMA.INNODB_METRICS
# 				 WHERE subsystem = 'dml',
# 		+-----------------+-------------+--------------+
# 		| name 			   | subsystem   | status 		  |
# 		+-----------------+-------------+--------------+
# 		| dml_reads 		| dml 		  | enabled 	  |
# 		| dml_inserts 	   | dml 		  | enabled 	  |
# 		| dml_deletes 	   | dml 		  | enabled 	  |
# 		| dml_updates 	   | dml 		  | enabled 	  |
# 		+-----------------+-------------+--------------+
#
# MOdule names can be used with innodb_monitor_enable and related variables.
#
# Module names and corresponding SUBSYSTEM names are listed below.
#
# 		) module_adaptive_hash(subsystem = adaptive_hash_index)
#
# 		) module_buffer(subsystem = buffer)
#
# 		) module_buffer_page(subsystem = buffer_page_io)
#
# 		) module_compress (subsystem = compression)
#
# 		) module_ddl (subsystem = ddl)
#
# 		) module_dml (subsystem = dml)
#
# 		) module_file (subsystem = file_system)
#
# 		) module_ibuf_system (subsystem = change_buffer)
#
# 		) module_icp (subsystem = icp)
#
# 		) module_index (subsystem = index)
#
# 		) module_innodb (subsystem = innodb)
#
# 		) module_lock (subsystem = lock)
#
# 		) module_log (subsystem = recovery)
#
# 		) module_metadata (subsystem = metadata)
#
# 		) module_os (subsystem = os)
#
# 		) module_purge (subsystem = purge)
#
# 		) module_trx (subsystem = transaction)
#
# 		) module_undo (subsystem = undo)
#
# EXAMPLE 15.11 WORKING WITH INNODB_METRICS TABLE COUNTERS
#
# This example demonstrates enabling, disbaling and resetting a counter, and querying counter data in the INNODB_METRICS table.
#
# 		1. Create a simple InnoDB table:
#
# 			mysql> USE test;
# 			Database changed
#
# 			mysql> CREATE TABLE t1 (c1 INT) ENGINE=INNODB;
# 			Query OK, 0 rows affected (0.02 sec)
#
# 		2. Enable the dml_inserts counter.
#
# 			mysql> SET GLOBAL innodb_monitor_enable = dml_inserts;
# 			Query OK, 0 rows affected (0.01 sec)
#
# 			A description of the dml_inserts counter can be found in the COMMENT column of the INNODB_METRICS table:
#
# 				mysql> SELECT NAME, COMMENT FROM INFORMATION_SCHEMA.INNODB_METRICS WHERE NAME="dml_inserts";
# 				+--------------+-------------------------+
# 				| NAME 		   | COMMENT 			 		  |
# 				+--------------+-------------------------+
# 				| dml_inserts 	| Number of rows inserted |
# 				+--------------+-------------------------+
#
# 		3. Query the INNODB_METRICS table for the dml_inserts counter data. because no DML operations have been performed,
# 			the counter values are zero or NULL.
#
# 			The TIME_ENABLED and TIME_ELAPSED values indicate when the counter was last enabled and how many seconds have elapsed
# 			since that time.
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_METRICS WHERE NAME="dml_inserts" \G
# 				*************************** 1. row **************************************
# 								NAME: dml_inserts
# 						SUBSYSTEM : dml
# 						     COUNT: 0
# 						 MAX_COUNT: 0
# 						MIN_COUNT : NULL
# 					 	AVG_COUNT : 0
# 					  COUNT_RESET: 0
# 				MAX_COUNT_RESET : 0
# 				MIN_COUNT_RESET : NULL
# 				AVG_COUNT_RESET : NULL
# 					TIME_ENABLED : 2014-12-04 14:18:28
# 					TIME_DISABLED: NULL
# 					TIME_ELAPSED : 28
# 					TIME_RESET   : NULL
# 							STATUS : enabled
# 							TYPE   : status_counter
# 							COMMENT: Number of rows inserted
#
# 		4. Insert three rows of data into the table
#
# 			mysql> INSERT INTO t1 values(1);
# 			Query OK, 1 row affected (0.00 sec)
#
# 			mysql> INSERT INTO t1 values(2);
# 			Query OK, 1 row affected (0.00 sec)
#
# 			mysql> INSERT INTO t1 VALUES(3);
# 			Query OK, 1 row affected (0.00 sec)
#
# 		5. Query the INNODB_METRICS table again for the dml_inserts counter data. A number of counter values
# 			have now incremented including COUNT, MAX_COUNT, AVG_COUNT, and COUNT_RESET.
#
# 			Refer to the INNODB_METRICS table definition for descriptions of these values.
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_METRICS WHERE NAME="dml_inserts"\G
# 				******************************** 1. row **********************************
# 								NAME: dml_inserts
# 						SUBSYSTEM : dml
# 						COUNT  	 : 3
# 						MAX_COUNT : 3
# 						MIN_COUNT : NULL
# 						AVG_COUNT : 0.046153846153846156
# 					COUNT_RESET  : 3
# 				MAX_COUNT_RESET : 3
# 				MIN_COUNT_RESET : NULL
# 				AVG_COUNT_RESET : NULL
# 					TIME_ENABLED : 2014-12-04 14:18:28
# 				TIME_DISABLED   : NULL
# 					TIME_ELAPSED : 65
# 					TIME_RESET   : NULL
# 						STATUS 	 : enabled
# 						TYPE 		 : status_counter
# 						COMMENT   : Number of rows inserted
#
# 		6. Reset the dml_inserts counter and query the INNODB_METRICS table again for the dml_inserts counter data.
#
# 			The %_RESET values that were reported previously, such as COUNT_RESET and MAX_RESET, are set back to zero.
# 			Values such as COUNT, MAX_COUNT, and AVG_COUNT, which cumulatively collect data from the time the counter is
# 			enabled, are unaffected by the reset.
#
# 				mysql> SET GLOBAL innodb_monitor_reset = dml_inserts;
# 				Query OK, 0 rows affected (0.00 sec)
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_METRICS WHERE NAME="dml_inserts"\G
# 				*************************************** 1. row *********************************************
# 											NAME: dml_inserts
# 									SUBSYSTEM : dml
# 										COUNT  : 3
# 									MAX_COUNT : 3
# 									MIN_COUNT : NULL
# 									AVG_COUNT : 0.03529411764705882
# 								COUNT_RESET  : 0
# 							MAX_COUNT_RESET : 0
# 							MIN_COUNT_RESET : NULL
# 							AVG_COUNT_RESET : 0
# 								TIME_ENABLED : 2014-12-04 14:18:28
# 								TIME_DISABLED: NULL
# 								TIME_ELAPSED : 85
# 								TIME_RESET   : 2014-12-04 14:19.44
# 									STATUS    : enabled
# 											TYPE: status_counter
# 										COMMENT: Number of rows inserted
#
# 		7. To reset all counter values, you must first disable the counter. Disabling the counter sets the STATUS value to disabled.
#
# 				mysql> SET GLOBAL innodb_monitor_disable = dml_inserts;
# 				Query OK, 0 rows affected (0.00 sec)
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_METRICS WHERE NAME="dml_inserts"\G
# 				*********************************** 1. row *************************************
# 											NAME: dml_inserts
# 									SUBSYSTEM : dml
# 										COUNT  : 3
# 									MAX_COUNT : 3
# 									MIN_COUNT : NULL
# 									AVG_COUNT : 0.030612244897959183
#								COUNT_RESET  : 0
# 							MAX_COUNT_RESET : 0
# 							MIN_COUNT_RESET : NULL
# 							AVG_COUNT_RESET : 0
# 								TIME_ENABLED : 2014-12-04 14:18:28
# 							TIME_DISABLED   : 2014-12-04 14:20:06
# 							TIME_ELAPSED 	 : 98
# 								TIME_RESET 	 : NULL
# 								STATUS 		 : disabled
# 									TYPE 		 : status_counter
# 									COMMENT 	 : Number of rows inserted
#
# 			NOTE:
#
# 				Wildcard match is supported for counter and module names. For example, instead of specifying the full dml_inserts counter name,
# 				you can specify dml_i%. You can also enable, disable or reset multiple counters or modules at once using a wildcard match.
#
# 				For example, specify dml_% to enable, disable or reset all counters that begin with dml_
#
# 		8. After the counter is disabled, you can reset all counter values using the innodb_monitor_reset_all option. All values are set to zero or NULL.
#
# 				mysql> SET GLOBAL innodb_monitor_reset_all = dml_inserts;
# 				Query OK, 0 rows affected (0.00 sec)
#
# 				mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_METRICS WHERE NAME="dml_inserts"\G
# 				***************************** 1. row *******************************
# 									NAME: dml_inserts
# 							SUBSYSTEM : dml
# 								COUNT  : 0
# 							MAX_COUNT : NULL
# 							MIN_COUNT : nULL
# 							AVG_COUNT : NULL
# 						COUNT_RESET  : 0
# 					MAX_COUNT_RESET : NULL
# 					MIN_COUNT_RESET : NULL
# 					AVG_COUNT_RESET : NULL
# 					 	TIME_ENABLED : NULL
# 						TIME_DISABLED: NULL
# 						TIME_ELAPSED : NULL
# 						TIME_RESET   : NULL
# 								STATUS : disabled
# 								TYPE 	 : status_counter
# 								COMMENT: Number of rows inserted
#
# 15.14.7 INNODB INFORMATION_SCHEMA TEMPORARY TABLE INFO TABLE
#
# INNODB_TEMP_TABLE_INFO provides information about user-created InnoDB temporary tables that are active in the InnoDB
# instance. It does not provide information about internal InnoDB temporary tables used by the optimizer.
#
# 		mysql> SHOW TABLES FROM INFORMATION_SCHEMA LIKE 'INNODB_TEMP%';
# 		+-------------------------------------------------------------+
# 		| Tables_in_INFORMATION_SCHEMA (INNODB_TEMP%) 					  |
# 		+-------------------------------------------------------------+
# 		| INNODB_TEMP_TABLE_INFO 												  |
# 		+-------------------------------------------------------------+
#
# For the table definition, see SECTION 25.39.28, "THE INFORMATION_SCHEMA INNODB_TEMP_TABLE_INFO TABLE"
#
# EXAMPLE 15.12 INNODB_TEMP_TABLE_INFO
#
# This example demonstrates characteristics of the INNODB_TEMP_TABLE_INFO table
#
# 1. Create a simple InnoDB temporary table:
#
# 		mysql> CREATE TEMPORARY TABLE t1 (c1 INT PRIMARY KEY) ENGINE=INNODB;
#
# 2. Query INNODB_TEMP_TABLE_INFO to view the temporary table metadata.
#
# 		mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_TEMP_TABLE_INFO\G
# 		********************** 1. row *****************************
# 					TABLE_ID: 194
# 					NAME 	  : #sql17a79_1_0
# 					N_COLS  : 4
# 					SPACE   : 182
#
# 		The TABLE_ID is a unique identifier for the temporary table. The NAME column displays the system-generated name for the temporary table.
# 		Which is prefixed with "#sql".
#
# 		The number of columns (N_COLS) is 4 rather than 1 because InnoDB always creates three hidden table columns (DB_ROW_ID, DB_TRX_ID and  DB_ROLL_PTR)
#
# 3. Restart MySQL and query INNODB_TEMP_TABLE_INFO
#
# 		mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_TEMP_TABLE_INFO\G
#
# 		An empty set is returned because INNODB_TEMP_TABLE_INFO and its data are not persisted to disk when the server is shut down.
#
# 4. Create a new temporary table.
#
# 		mysql> CREATE TEMPORARY TABLE t1 (c1 INT PRIMARY KEY) ENGINE=INNODB;
#
# 5. Query INNODB_TEMP_TABLE_INFO to view the temporary table metadata.
#
# 		mysql> SELECT * FROM INFORMATION_SCHEMA.INNODB_TEMP_TABLE_INFO\G
# 		**************************** 1. row ********************************
# 						TABLE_ID: 196
# 						NAME    : #sql17b0e_1_0
# 						N_COLS  : 4
# 						 SPACE  : 184
#
# 		the SPACE ID may be different because it is dynamically generated when the server is started.
#
# 15.14.8 RETRIEVING INNODB TABLESPACE METADATA FROM INFORMATION_SCHEMA.FILES
#
# The INFORMATION_SCHEMA.FILES table provides metadata about all InnoDB tablespace types including file-per-table tablespaces,
# general tablespaces, the system tablespace, temporary table tablespaces, and undo tablespaces (if present)
#
# This section provides InnoDB-specific usage examples. For more information about data provided by the INFORMATION_SCHEMA.FILES
# table, see SECTION 25.11, "THE INFORMATION_SCHEMA FILES TABLE"
#
# NOTE:
#
# 		The INNODB_TABLESPACES and INNODB_DATAFILES tables also provide metadata about InnoDB tablespaces, but data is limited
# 		to file-per-table, general, and undo tablespaces.
#
# This query retrieves metadata about the InnoDB system tablespace from fields of the INFORMATION_SCHEMA.FILES table that are
# pertinent to InnoDB tablespaces.
#
# INFORMATION_SCHEMA.FILES fields that are not relevant to InnoDB always return NULL, and are excluded from the query.
#
# 		mysql> SELECT FILE_ID, FILE_NAME, FILE_TYPE, TABLESPACE_NAME, FREE_EXTENTS,
# 				 TOTAL_EXTENTS, EXTENT_SIZE, INITIAL_SIZE, MAXIMUM_SIZE, AUTOEXTEND_SIZE, DATA_FREE, STATUS ENGINE
# 				 FROM INFORMATION_SCHEMA.FILES WHERE TABLESPACE_NAME LIKE 'innodb_system' \G
# 		***********************************1. row **************************************
# 								FILE_ID: 0
# 							FILE_NAME : ./ibdata1
# 							FILE_TYPE : TABLESPACE
# 					TABLESPACE_NAME : innodb_system
# 					FREE_EXTENTS 	 : 0
# 					TOTAL_EXTENTS   : 12
# 					EXTENT_SIZE 	 : 1048576
# 					INITIAL_SIZE    : 12582912
# 					MAXIMUM_SIZE    : NULL
# 					AUTOEXTEND_SIZE : 67108864
# 						DATA_FREE    : 4194304
# 							ENGINE 	 : NORMAL
#
# This query retrieves the FILE_ID (equivalent to the space ID) and the FILE_NAME (which includes path information) for InnoDB
# file-per-table and general tablespaces.
#
# File-per-table and general tablespaces have a .ibd file extension.
#
# 		mysql> SELECT FILE_ID, FILE_NAME FROM INFORMATION_SCHEMA.FILES
# 				 WHERE FILE_NAME LIKE '%.ibd%' ORDER BY FILE_ID;
# 				+------------+------------------------------------------------+
# 				| FILE_ID    | FILE_NAME 												  |
# 				+------------+------------------------------------------------+
# 				| 2 			 | ./mysql/plugin.ibd 									  |
# 				| 3 			 | ./mysql/servers.ibd 									  |
# 				| 4 		    | ./mysql/help_topic.ibd 								  |
# 				| 5 			 | ./mysql/help_category.ibd 							  |
# 				| 6 			 | ./mysql/help_relation.ibd 							  |
# 				| 7 			 | ./mysql/help_keyword.ibd 							  |
# 				| 8 			 | ./mysql/time_zone_name.ibd 						  |
# 				| 9 			 | ./mysql/time_zone.ibd 								  |
# 				| 10 			 | ./mysql/time_zone_transition.ibd 				  |
# 				| 11 			 | ./mysql/time_zone_transition_type.ibd 			  |
# 				| 12 			 | ./mysql/time_zone_leap_second.ibd 				  |
# 				| 13 			 | ./mysql/innodb_table_stats.ibd 					  |
# 				| 14 			 | ./mysql/innodb_index_stats.ibd 					  |
# 				| 15 			 | ./mysql/slave_relay_log_info.ibd 				  |
# 				| 16 		    | ./mysql/slave_master_info.ibd 					  |
# 				| 17 			 | ./mysql/slave_worker_info.ibd 					  |
# 				| 18 			 | ./mysql/gtid_executed.ibd 							  |
# 				| 19	 		 | ./mysql/server_cost.ibd 							  |
# 				| 20 			 | ./mysql/engine_cost.ibd 							  |
# 				| 21 			 | ./sys/sys_config.ibd 								  |
# 				| 23 			 | ./test/t1.ibd 											  |
# 				| 26 			 | /home/user/test/test/t2.ibd 						  |
# 				+------------+------------------------------------------------+
#
# This query retrieves the FILE_ID and FILE_NAME for the InnoDB global temporary tablespace. Global temporary tablespace file names
# are prefixed by ibtmp.
#
# 		mysql> SELECT FILE_ID, FILE_NAME FROM INFORMATION_SCHEMA.FILES
# 				 WHERE FILE_NAME LIKE '%ibtmp%';
# 		+-------------+-------------+
# 		| FILE_ID 	  | FILE_NAME 	 |
# 		+-------------+-------------+
# 		| 22 			  | ./ibtmp1 	 |
# 		+-------------+-------------+
#
# Similarly, InnoDB undo tablespace file names are prefixed by undo. The following query returns the FILE_ID and FILE_NAME
# for InnoDB undo tablespaces.
#
# 		mysql> SELECT FILE_ID, FILE_NAME FROM INFORMATION_SCHEMA.FILES
# 				 WHERE FILE_NAME LIKE '%undo%';
#
# 15.15 INNODB INTEGRATION WITH MYSQL PERFORMANCE SCHEMA
#
# 15.15.1 MONITORING ALTER TABLE PROGRESS FOR INNODB TABLES USING PERFORMANCE SCHEMA
# 15.15.2 MONITORING INNODB MUTEX WAITS USING PERFORMANCE SCHEMA
#
# This section provides a brief introduction to InnoDB integration with Performance Schema. For comprehensive Performance Schema
# documentation, see CHAPTER 26, MYSQL PERFORMANCE SCHEMA.
#
# You can profile certain internal InnoDB operations using the MySQL Performance Schema Feature. This type of tuning is primarily
# for expert users who evaluate optimization strategies to overcome performance bottlenecks.
#
# DBAs can also use this feature for capacity planning, to see whether their typical workload encounters any
# performance bottlenecks with a particular combination of CPU, RAM and disk storage; and if so, to judge
# whether performance can be improved by increasing the capacity of some part of the system.
#
# To use this feature to examine InnoDB performance:
#
# 		) You must be generally familiar with how to use the Performance Schema Feature. For example, you should know how
# 			to enable instruments and consumers, and how to query performance_schema tables to retrieve data.
#
# 			For an introductory overview, see SECTION 26.1, "PERFORMANCE SCHEMA QUICK START"
#
# 		) You should be familiar with Performance Schema instruments that are available for InnoDB. To view InnoDB- related
# 			instruments, you can query the setup_instruments table for instrument names that contain 'innodb'
#
# 			mysql> SELECT *
# 					 FROM performance_schema.setup_instruments
# 					 WHERE NAME LIKE '%innodb%';
# 			+----------------------------------------------------------------+--------------+-----------------+
# 			| NAME 																			  | ENABLED 	  | TIMED 			  |
# 			+----------------------------------------------------------------+--------------+-----------------+
# 			| wait/synch/mutex/innodb/commit_cond_mutex 							  | NO 			  | NO 				  |
# 			| wait/synch/mutex/innodb/innobase_share_mutex 						  | NO 			  | NO 				  |
# 			| wait/synch/mutex/innodb/autoinc_mutex 								  | NO 			  | NO 				  |
# 			| wait/synch/mutex/innodb/buf_pool_mutex 								  | NO 			  | NO 				  |
# 			| wait/synch/mutex/innodb/buf_pool_zip_mutex 						  | NO 			  | NO 				  |
# 			| wait/synch/mutex/innodb/cache_last_read_mutex 					  | NO 			  | NO 				  |
# 			| wait/synch/mutex/innodb/dict_foreign_err_mutex 					  | NO 			  | NO 				  |
# 			| wait/synch/mutex/innodb/dict_sys_mutex 								  | NO 			  | NO 				  |
# 			| wait/synch/mutex/innodb/recalc_pool_mutex 							  | NO 			  | NO 				  |
# 			| /etc/
# 			| wait/io/file/innodb/innodb_data_file 								  | YES 			  | YES 				  |
# 			| wait/io/file/innodb/innodb_log_file 									  | YES 			  | YES 				  |
# 			| wait/io/file/innodb/innodb_temp_file 								  | YES 			  | YES 				  |
# 			| stage/innodb/alter table (end) 										  | YES 			  | YES 				  |
# 			| stage/innodb/alter table (flush) 										  | YES 			  | YES 				  |
# 			| stage/innodb/alter table (insert) 									  | YES 			  | YES 				  |
# 			| stage/innodb/alter table (log apply index) 						  | YES 			  | YES 				  |
# 			| stage/innodb/alter table (log apply table) 						  | YES 			  | YES 				  |
# 			| stage/innodb/alter table (merge sort) 								  | YES 			  | YES 				  |
# 			| stage/innodb/alter table (read PK and internal sort) 			  | YES 			  | YES 				  |
# 			| stage/innodb/buffer pool load 											  | YES 			  | YES 				  |
# 			| memory/innodb/buf_buf_pool 												  | NO 			  | NO 				  |
# 			| memory/innodb/dict_stats_bg_recalc_pool_t 							  | NO 			  | NO 				  |
# 			| memory/innodb/dict_stats_index_map_t 								  | NO 			  | NO 				  |
# 			| memory/innodb/dict_stats_n_diff_on_level 							  | NO 			  | NO 				  |
# 			| memory/innodb/other 														  | NO 			  | NO 				  |
# 			| memory/innodb/row_log_buf 												  | NO 			  | NO 				  |
# 			| memory/innodb/row_merge_sort 											  | NO 			  | NO 				  |
# 			| memory/innodb/std 															  | NO 			  | NO 				  |
# 			| memory/innodb/sync_debug_latches 										  | NO 			  | NO 				  |
# 			| memory/innodb/trx_sys_t::rw_trx_ids 									  | NO 			  | NO 				  |
# 			// etc //
# 			+----------------------------------------------------------------+--------------+-----------------+
# 			155 rows in set (0.00 sec)
#
# For additional information about the instrumented InnoDB objects, you can query Performance Schema instances
# tables, which provide additional information about instrumented objects.
#
# Instance tables relevant to InnoDB include:
#
# 		) The mutex_instances table
# 	
# 		) The rwlock_instances table
# 
# 		) The cond_instances table
#
# 		) The file_instances table
#
# NOTE:
#
# 		Mutexes and RW-locks related to the InnoDB buffer pool are not included in this coverage; the same applies
# 		to the output of the SHOW ENGINE INNODB MUTEX command.
#
# For example, to view information about instrumented InnoDB file objects seen by the Performance Schema when executing
# file I/O instrumentation, you might issue the following query:
#
# 		mysql> SELECT *
# 				 FROM performance_schema.file_instances
# 				 WHERE EVENT_NAME LIKE '%innodb%'\G
# 		*********************************** 1. row *************************
# 			FILE_NAME: /path/to/mysql-8.0/data/ibdata1
# 		  EVENT_NAME: wait/io/file/innodb/innodb_data_file
# 		  OPEN_COUNT: 3
# 		****************************** 2. row *******************************
# 			FILE_NAME: /path/to/mysql-8.0/data/ib_logfile0
# 		  EVENT_NAME: wait/io/file/innodb/innodb_log_file
# 		  OPEN_COUNT: 2
# 		****************************** 3. row *******************************
# 		  FILE_NAME : /path/to/mysql-8.0/data/ib_logfile1
# 		  EVENT_NAME: wait/io/file/innodb/innodb_log_file
# 		  OPEN_COUNT: 2
# 		****************************** 4. row *******************************
# 		  FILE_NAME : /path/to/mysql-8.0/data/mysql/engine_cost.ibd
# 		 EVENT_NAME : wait/io/file/innodb/innodb_data_file
# 		 OPEN_COUNT : 3
# 			/ETC/
#
# 	) You should be familiar with performance_schema tables that store InnoDB event data. Tables relevant to InnoDB-related events include:
#
# 		) The Wait Event tables, which store wait events
#
# 		) The Summary tables, which provide aggregated information for terminated events over time. Summary tables include file I/O summary tables,
# 			which aggregate information about I/O operations.
#
# 		) Stage Event tables, which store event data for InnoDB ALTER_TABLE and buffer pool load operations. For more information, see SECTION
# 			15.15.1, "MONITORING ALTER TABLE PROGRESS FOR INNODB TABLES USING PERFORMANCE SCHEMA", and MONITORING BUFFER POOL LOAD PROGRESS USING
# 			PERFORMANCE SCHEMA.
#
# If you are only interested in InnoDB-related objects, use the clause WHERE EVENT_NAME LIKE '%innodb%' or WHERE NAME LIKE '%innodb%'
# (as required) when querying these tables.
#
# 15.15.1 MONITORING ALTER TABLE PROGRESS FOR INNODB TABLES USING PERFORMANCE SCHEMA
#
# You can monitor ALTER_TABLE progress for InnoDB tables using PERFORMANCE_SCHEMA
#
# There are seven stage events that represent different phases of ALTER_TABLE. Each stage event reports a running total of
# WORK_COMPLETED and WORK_ESTIMATED for the overall ALTER_TABLE operation as it progresses through its different phases.
#
# WORK_ESTIMATED is calculated using a formula that takes into account all of the work that ALTER_TABLE performs,
# and may be revised during ALTER_TABLE processing.
#
# WORK_COMPLETED and WORK_ESTIMATED values are an abstract representation of all of the work performed by ALTER_TABLE.
#
# In order of occurrence, ALTER_TABLE stage events include:
#
# 		) stage/innodb/alter table (read PK and internal sort): This stage is active when ALTER_TABLE is in the reading-primary-key
# 			phase. It starts with WORK_COMPLETED=0 and WORK_ESTIMATED set to the estimated number of pages in the primary key.
#
# 			When the stage is completed, WORK_ESTIMATED is updated to the actual number of pages in the primary key.
#
# 		) stage/innodb/alter table (merge sort): This stage is repeated for each index added by the ALTER_TABLE operation
#
# 		) stage/innodb/alter table (insert): This stage is repeated for each index added by the ALTER_TABLE operation
#
# 		) stage/innodb/alter table (log apply index): This stage includes the application of DML log generated while ALTER_TABLE was running.
#
# 		) stage/innodb/alter table (flush): Before this stage begins, WORK_ESTIMATED is updated with a more accurate estimate, based on the
# 			length of the flush list.
#
# 		) stage/innodb/alter table (log apply table): This stage includes the application of concurrent DML log generated while ALTER_TABLE
# 			was running. The duration of this phase depends on the extent of table changes.
#
# 			This phase is instant if no concurrent DML was run on the table.
#
# 		) stage/innodb/alter table (end): Includes any remaining work that appeared after the flush phase, such as reapplying DML
# 			that was executed on the table while ALTER_TABLE was running.
#
# NOTE:
#
# 		InnoDB ALTER_TABLE stage events do not currently account for the addition of spatial indexes.
#
# ALTER TABLE MONITORING EXAMPLE USING PERFORMANCE SCHEMA
#
# The following example demonstrates how to enable the stage/innodb/alter table% stage event instruments and related
# consumer tables to monitor ALTER_TABLE progress.
#
# For information about Performance Schema stage event instruments and related consumers, see SECTION 26.12.5, "PERFORMANCE SCHEMA STAGE EVENT TABLES"
#
# 		1. Enable the stage/innodb/alter% instruments:
#
# 			mysql> UPDATE performance_schema.setup_instruments
# 					 SET ENABLED = 'YES'
# 					 WHERE NAME LIKE 'stage/innodb/alter%';
# 			Query OK, 7 rows affected (0.00 sec)
# 			Rows matched: 7 	Changed: 7 	Warnings: 0
#
# 		2. Enable the stage event consumer tables, which include events_stages_current, events_stages_history, and
# 			events_stages_history_long
#
# 				mysql> UPDATE performance_schema.setup_consumers
# 						 SET ENABLED = 'YES'
# 						 WHERE NAME LIKE '%stages%';
# 				Query OK, 3 rows affected (0.00 sec)
# 				Rows matched: 3 	Changed: 	3 		Warnings: 0
#
# 		3. Run an ALTER_TABLE operation. In this example, a middle_name column is added to the employees table of the employees sample database.
#
# 				mysql> ALTER TABLE employees.employees ADD COLUMN middle_name VARCHAR(14) AFTER first_name;
# 				Query OK, 0 rows affected (9.27 sec)
# 				Records: 0 	Duplicates: 0 	Warnings: 0
#
# 		4. Check the progress of the ALTER_TABLE operation by querying the Performance Schema events_stages_current table.
#
# 			The stage event shown differs depending on which ALTER_TABLE phase is currently in progress. The WORK_COMPLETED
# 			column shows the work completed.
#
# 			The WORK_ESTIMATED column provides an estimate of the remaining work.
#
# 				mysql> SELECT EVENT_NAME, WORK_COMPLETED, WORK_ESTIMATED
# 						 FROM performance_schema.events_stages_current;
# 				+------------------------------------------------------+-------------------+-----------------------+
# 				| EVENT_NAME 													    | WORK_COMPLETED 	| WORK_ESTIMATED 		   |
# 				+------------------------------------------------------+-------------------+-----------------------+
# 				| stage/innodb/alter table (read PK and internal sort) | 280 				   | 1245 					   |
# 				+------------------------------------------------------+-------------------+-----------------------+
# 				1 row in set (0.01 sec)
#
# 			The events_stages_current table returns an empty set if the ALTER_TABLE operation has completed.
#
# 			In this case, you can check the events_stages_history table to view event data for the completed
# 			operation. For example:
#
# 				mysql> SELECT EVENT_NAME, WORK_COMPLETED, WORK_ESTIMATED
# 						 FROM performance_schema.events_stages_history;
# 				+------------------------------------------------------+-------------------+------------------------+
# 				| EVENT_NAME 														 | WORK_COMPLETED 	| WORK_ESTIMATED 			 |
# 				+------------------------------------------------------+-------------------+------------------------+
# 				| stage/innodb/alter table (read PK and internal sort) | 		886 			| 	1213 						 |
# 				| stage/innodb/alter table (flush) 						    | 		1213 		 	| 	1213 						 |
# 				| stage/innodb/alter table (log apply table) 			 | 		1597 			| 	1597 						 |
# 				| stage/innodb/alter table (end) 							 | 		1597 			| 	1597 						 |
# 				| stage/innodb/alter table (log apply table) 			 | 		1981 			| 	1981 						 |
# 				+------------------------------------------------------+-------------------+------------------------+
# 				5 rows in set (0.00 sec)
#
# 			As shown above, the WORK_ESTIMATED value was revised during ALTER TABLE processing. The estimated work
# 			after completion of the initial stage is 1213. When ALTER TABLE processing completed, WORK_ESTIMATED
# 			was set to the actual value, which is 1981.
#
# 15.15.2 MONITORING INNODB MUTEX WAITS USING PERFORMANCE SCHEMA
#
# A mutex is a synchronization mechanism used in the code to enforce that only one thread at a given time can have
# access to a common resource.
#
# When two or more threads executing in the server need to access the same resource, the threads compete against
# each other.
#
# The first thread to obtain a lock on the mutex causes the other threads to wait until the lock is released.
#
# For InnoDB mutexes that are instrumented, mutex waits can be monitored using PERFORMANCE SCHEMA. Wait event
# data collected in Performance Schema tables can help identify mutexes with the most waits or the greatest
# total wait time, for example.
#
# The following example demonstrates how to enable InnoDB mutex wait instruments, how to enable associated
# consumers, and how to query wait event data.
#
# 		1. To view available InnoDB mutex wait instruments, query the Performance Schema setup_instruments
# 			table.
#
# 			All InnoDB mutex wait instruments are disabled by default.
#
# 				mysql> SELECT * 
# 						 FROM performance_schema.setup_instruments
# 						 WHERE NAME LIKE '%wait/synch/mutex/innodb%';
# 				+--------------------------------------------------------------------+-------------+----------------+
# 				| NAME 																					| ENABLED 	  | TIMED 			 |
# 				+--------------------------------------------------------------------+-------------+----------------+
# 				| wait/synch/mutex/innodb/commit_cond_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/innobase_share_mutex 								| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/autoinc_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/autoinc_persisted_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/buf_pool_flush_state_mutex 						| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/buf_pool_LRU_list_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/buf_pool_free_list_mutex 						| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/buf_pool_zip_free_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/buf_pool_zip_hash_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/buf_pool_zip_mutex 								| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/cache_last_read_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/dict_foreign_err_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/dict_persist_dirty_tables_mutex 				| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/dict_sys_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/recalc_pool_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/fil_system_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/flush_list_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/fts_bg_threads_mutex 								| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/fts_delete_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/fts_optimize_mutex 								| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/fts_doc_id_mutex 								 	| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/log_flush_order_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/hash_table_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/ibuf_bitmap_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/ibuf_mutex 											| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/ibuf_pessimistic_insert_mutex 					| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/log_sys_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/log_sys_write_mutex 								| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/mutex_list_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/page_zip_stat_per_index_mutex 					| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/purge_sys_pq_mutex 							   | NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/recv_sys_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/recv_writer_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/redo_rseg_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/noredo_rseg_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/rw_lock_list_mutex 								| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/rw_lock_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/srv_dict_tmpfile_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/srv_innodb_monitor_mutex 						| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/srv_misc_tmpfile_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/srv_monitor_file_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/buf_dblwr_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/trx_undo_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/trx_pool_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/trx_pool_manager_mutex 							| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/srv_sys_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/lock_mutex 											| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/lock_wait_mutex 								   | NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/trx_mutex 											| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/srv_threads_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/rtr_active_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/rtr_match_mutex 									| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/rtr_path_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/rtr_ssn_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/trx_sys_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/zip_pad_mutex 										| NO 			  | NO 				 |
# 				| wait/synch/mutex/innodb/master_key_id_mutex 								| NO 			  | NO 				 |
# 				+--------------------------------------------------------------------+-------------+----------------+
#
# 	2. Some InnoDB mutex instances are created at server startup and are only instrumented if the associated instrument
# 		is also enabled at server startup.
#
# 		To ensure that all InnoDB mutex instances are instrumented and enabled, add the following performance-schema-instrument
# 		rule to your MySQL configuration file:
#
# 			performance-schema-instrument='wait/synch/mutex/innodb/%=ON'
#
# 		If you do not require wait event data for all InnoDB mutexes, you can disable specific instruments by adding additional
# 		performance-schema-instrument rules to your MySQL configuration file.
#
# 		For example, to disable InnoDB mutex wait event instruments related to full-text search, add the following rule:
#
# 			performance-schema-instrument='wait/synch/mutex/innodb/fts%=OFF'
#
# 		NOTE:
#
# 			Rules with a longer prefix such as wait/synch/mutex/innodb/fts% take precedence over rules with shorter
# 			prefixes such as wait/synch/mutex/innodb/%
#
# 		After adding the performance-schema-instrument rules to your configuration file, restart the server.
#
# 		All the InnoDB mutexes except for those related to full text search are enabled. To verify, query
# 		the setup_instruments table. The ENABLED and TIMED columns should be set to YES for the instruments
# 		that you enabled.
#
# 			mysql> SELECT *
# 					 FROM performance_schema.setup_instruments
# 					 WHERE NAME LIKE '%wait/synch/mutex/innodb%';
# 			+-------------------------------------------------------------------+---------+-------------+
# 			| NAME 																				  | ENABLED | TIMED 		  |
# 			+-------------------------------------------------------------------+---------+-------------+
# 			| wait/synch/mutex/innodb/commit_cond_mutex 								  | YES 		| YES 		  |
# 			| wait/synch/mutex/innodb/innobase_share_mutex 							  | YES 	   | YES 		  |
# 			| wait/synch/mutex/innodb/autoinc_mutex 									  | YES 		| YES 		  |
# 			/ etc /
# 			| wait/synch/mutex/innodb/master_key_id_mutex 							  | YES 	   | YES 		  |
# 			+-------------------------------------------------------------------+---------+-------------+
# 			49 rows in set (0.00 sec)
#
# 		3. Enable wait event consumers by updating the setup_consumers table. Wait event consumers are disabled by default.
#
# 			mysql> UPDATE performance_schema.setup_consumers
# 					 SET enabled = 'YES'
# 					 WHERE NAME like 'events_waits%';
# 			Query OK, 3 rows affected (0.00 sec)
# 			Rows matched: 3 	Changed: 3 		Warnings: 0
#
# 			You can verify that wait event consumers are enabled by querying the setup_consumers table.
#
# 			The events_waits_current, events_waits_history, and events_waits_history_long consumers should
# 			be enabled.
#
# 				mysql> SELECT * FROM performance_schema.setup_consumers;
# 				+---------------------------------------------------+-------------+
# 				| NAME 															 | ENABLED 	   |
# 				+---------------------------------------------------+-------------+
# 				| events_stages_current 									 | NO 			|
# 				| events_stages_history 									 | NO 			|
# 				| events_stages_history_long 								 | NO 		   |
# 				| events_statements_current 								 | YES 			|
# 				| events_statements_history 								 | YES 			|
# 				| events_statements_history_long 						 | NO 			|
# 				| events_transactions_current 							 | YES 			|
# 				| events_transactions_history 							 | YES 			|
# 				| events_transactions_history_long 						 | NO 			|
# 				| events_waits_current 										 | YES 			|
# 				| events_waits_history 										 | YES 			|
# 				| events_waits_history_long 								 | YES 			|
# 				| global_instrumentation 									 | YES 			|
# 				| thread_instrumentation 									 | YES 			|
# 				| statements_digest 											 | YES 			|
# 				+---------------------------------------------------+-------------+
# 				15 rows in set (0.00 sec)
#
# 		4. Once instruments and consumers are enabled, run the workload that you want to monitor. In this example, the mysqlslap load emulation
# 			client is used to simulate a workload.
#
# 				shell> ./mysqlslap --auto-generate-sql --concurrency=100 --iterations=10
# 						 --number-of-queries=1000 --number-char-cols=6 --number-int-cols=6;
#
#		5. Query the wait event data. In this example, wait event data is queried from the events_waits_summary_global_by_event_name
# 			table which aggregates data found in the events_waits_current, events_waits_history, and events_waits_history_long tables.
#
# 			Data is summarized by event name (EVENT_NAME), which is the name of the instrument that produced the event.
#
# 			SUmmarized data includes:
#
# 				) COUNT_STAR
#
# 					The number of summarized wait events
#
# 				) SUM_TIMER_WAIT
#
# 					The total wait time of the summarized timed wait events
#
# 				) MIN_TIMER_WAIT
#
# 					The minimum wait time of the summarized timed wait events.
#
# 				) AVG_TIMER_WAIT
#
# 					The average wait time of the summarized timed wait events.
#
# 				) MAX_TIMER_WAIT
#
# 					The maximum wait time of the summarized timed wait events.
#
# 			The following query returns the instrument name (EVENT_NAME), the number of wait events (COUNT_STAR), and the total wait time for
# 			the events for that instrument (SUM_TIMER_WAIT). Because waits are timed in picoseconds (trillionths of a second) by default,
# 			wait times are divided by (a lot) to show wait times in milliseconds.
#
# 			Data is presented in descending order, by the number of summarized wait events (COUNT_STAR). You can adjust the ORDER BY
# 			clause to order the data by total wait time.
#
# 				mysql> SELECT EVENT_NAME, COUNT_STAR, SUM_TIMER_WAIT/1000000000 SUM_TIMER_WAIT_MS
# 						 FROM performance_schema.events_waits_summary_global_by_event_name
# 						 WHERE SUM_TIMER_WAIT > 0 AND EVENT_NAME LIKE 'wait/synch/mutex/innodb/%'
# 						 ORDER BY COUNT_STAR DESC;
# 				+----------------------------------------------------------------------+--------------+---------------------------+
# 				| EVENT_NAME 																			  | COUNT_STAR   | SUM_TIMER_WAIT_MS 			|
# 				+----------------------------------------------------------------------+--------------+---------------------------+
# 				| wait/synch/mutex/innodb/trx_mutex 											  | 201111 		  | 	23.4719 					   |
# 				| wait/synch/mutex/innodb/fil_system_mutex 									  | 62244 		  | 	9.6426 					   |
# 				| wait/synch/mutex/innodb/redo_rseg_mutex 									  | 48238 		  | 	3.1135 						|
# 				| wait/synch/mutex/innodb/log_sys_mutex 										  | 46113 		  | 	2.0434 						|
# 				| wait/synch/mutex/innodb/trx_sys_mutex 										  | 35134 		  | 	1068.1588 				   |
# 				| wait/synch/mutex/innodb/lock_mutex 											  | 34872 		  | 	1039.2589 				   |
# 				| wait/synch/mutex/innodb/log_sys_write_mutex 								  | 17805 		  | 	1526.0490 				   |
# 				| wait/synch/mutex/innodb/dict_sys_mutex 										  | 14912 		  | 	1606.7348 					|
# 				| wait/synch/mutex/innodb/trx_undo_mutex 										  | 10634 		  | 	1.1424 					   |
# 				| wait/synch/mutex/innodb/rw_lock_list_mutex 								  | 8538 		  | 	0.1960 						|
# 				| wait/synch/mutex/innodb/buf_pool_free_list_mutex 						  | 5961 		  | 	0.6473 						|
# 				| wait/synch/mutex/innodb/trx_pool_mutex 										  | 4885 		  | 	8821.7496 					|
# 				| wait/synch/mutex/innodb/buf_pool_LRU_list_mutex 							  | 4364 		  | 	0.2077 					   |
# 				| wait/synch/mutex/innodb/innobase_share_mutex 								  | 3212 		  | 	0.2650 						|
# 				| wait/synch/mutex/innodb/flush_list_mutex 									  | 3178 		  | 	0.2349 					   |
# 				| wait/synch/mutex/innodb/trx_pool_manager_mutex 							  | 2495 		  | 	0.1310 						|
# 				| wait/synch/mutex/innodb/buf_pool_flush_state_mutex 						  | 1318 		  | 	0.2161 						|
# 				| wait/synch/mutex/innodb/log_flush_order_mutex 							  | 1250 		  | 	0.0893 						|
# 				| wait/synch/mutex/innodb/buf_dblwr_mutex 									  | 951 			  | 	0.0918 					   |
# 				| wait/synch/mutex/innodb/recalc_pool_mutex 									  | 670 			  | 	0.0942 						|
# 				| wait/synch/mutex/innodb/dict_persist_dirty_tables_mutex 				  | 345 			  | 	0.0414						|
# 				| wait/synch/mutex/innodb/lock_wait_mutex 									  | 303 			  | 	0.1565 						|
# 				| wait/synch/mutex/innodb/autoinc_mutex 										  | 196 			  | 	0.0213 						|
# 				| wait/synch/mutex/innodb/autoinc_persisted_mutex 							  | 196 			  | 	0.0175 						|
# 				| wait/synch/mutex/innodb/purge_sys_pq_mutex 								  | 117 			  | 	0.0308 						|
# 				| wait/synch/mutex/innodb/srv_sys_mutex 										  | 94 			  | 	0.0077 						|
# 				| wait/synch/mutex/innodb/ibuf_mutex 											  | 22 			  | 	0.0086 						|
# 				| wait/synch/mutex/innodb/recv_sys_mutex 										  | 12 			  | 	0.0008 						|
# 				| wait/synch/mutex/innodb/srv_innodb_monitor_mutex 						  | 4 			  | 	0.0009 						|
# 				| wait/synch/mutex/innodb/recv_writer_mutex 									  | 1 			  | 	0.0005 						|
# 				+----------------------------------------------------------------------+--------------+---------------------------+
#
# 			NOTE:
#
# 				The preceding result set includes wait event data produced during the startup process.
#
# 				To exclude this data, you can truncate the events_wait_summary_global_by_event_name table immediately
# 				after startup and before running your workload.
#
# 				However, the truncate operation itself may produce a negligible amount of wait event data.
#
# 					mysql> TRUNCATE performance_schema.events_waits_summary_global_by_event_name;
#
# 15.16 INNODB MONITORS
#
# 15.16.1 INNODB MONITOR TYPES
# 15.16.2 ENABLING INNODB MONITORS
# 15.16.3 INNODB STANDARD MONITOR AND LOCK MONITOR OUTPUT
#
# InnoDB monitors provide information about the InnoDB internal state. This information is useful for performance tuning.
#
# 15.16.1 INNODB MONITOR TYPES
#
# There are two types of InnoDB monitor:
#
# 		) The standard InnoDB monitor displays the following types of information:
#
# 			) Work done by the main background thread
#
# 			) Semaphore waits
#
# 			) Data about the most recent foreign key and deadlock errors
#
# 			) Lock waits for transactions
#
# 			) Table and record locks held by active transactions
#
# 			) Pending I/O operations and related statistics
#
# 			) Insert buffer and adaptive hash index statistics
#
# 			) Redo log data
#
# 			) Buffer pool statistics
#
# 			) Row operation data
#
# 		) The InnoDB Lock Monitor prints additional lock information as part of the standard InnoDB Monitor output.
#
# 15.16.2 ENABLING INNODB MONITORS
#
# When InnoDB monitors are enabled for periodic output, InnoDB writes the output to mysqld server standard error output
# (stderr) every 15 seconds, approximately.
#
# InnoDB sends the monitor output to stderr rather than to stdout or fixed-size memory buffers to avoid ptoential buffer overflows.
#
# On Windows, stderr is directed to the default log file unless configured otherwise. If you want to direct the output to the console
# window rather than to the error log, start the server from a command prompt in a console window with the --console option.
#
# For more information, see ERROR LOGGING ON WINDOWS
#
# On Unix and Unix-like systems, stderr is typically directed to the terminal unless configured otherwise. FOr more information,
# see ERROR LOGGING ON UNIX AND UNIX-LIKE SYSTEMS.
#
# InnoDB monitors should only be enabled when you actually want to see monitor information because output generation causes some
# performance decrement. Also, if monitor output is directed to the error log, the log may become quite large if you forgot
# to disable the monitor later.
#
# 		NOTE:
#
# 			To assist with troubleshooting, InnoDB temporarily enables standard InnoDB Monitor output under certain conditions.
#
# 			For more information, see SECTION 15.20, "INNODB TROUBLESHOOTING"
#
# InnoDB monitor output begins with a header containing a timestamp and the monitor name. For example:
#
# 		================
# 		2014-10-16 18:37:29 0x7fc2a95c1700 INNODB MONITOR OUTPUT
# 		================
#
# The header for the standard InnoDB Monitor (INNODB MONITOR OUTPUT) is also used for the Lock Monitor because the
# latter produces the same output with the addition of extra lock information.
#
# The innodb_status_output and innodb_status_output_locks system variables are used to enable the standard InnoDB
# monitor and InnoDB Lock Monitor.
#
# The PROCESS privilege is required to enable or disable InnoDB Monitors.
#
# ENABLING THE STANDARD INNODB MONITOR
#
# Enable the standard InnoDB Monitor by setting the innodb_status_output system variable to ON.
#
# 		SET GLOBAL innodb_status_output=ON;
#
# To disable the standard InnoDB Monitor, set innodb_status_output to OFF.
#
# When you shut down the server, the innodb_status_output variable is set to the default OFF value.
#
# ENABLING THE INNODB LOCK MONITOR
#
# InnoDB Lock Monitor data is printed with the InnoDB standard Monitor output. Both the InnoDB Standard Monitor
# and InnoDB Lock Monitor must be enabled to have InnoDB Lock Monitor data printed periodically.
#
# To enable the InnoDB Lock Monitor, set the innodb_status_output_locks system variable to ON. Both the InnoDB
# stnadard Monitor and INnoDB Lock Monitor must be enabled to have INnoDB Lock Monitor data printed periodically:
#
# 		SET GLOBAL innodb_status_output=ON;
# 		SET GLOBAL innodb_status_output_locks=ON;
#
# To disable the InnoDB Lock Monitor, set innodb_status_output_locks to OFF. Set innodb_status_output to OFF
# to also disable the InnoDB Standard Monitor.
#
# When you shut down the server, the innodb_status_output and innodb_status_output_locks variables are set to
# the default OFF value.
#
# NOTE:
#
# 		To enable the InnoDB Lock Monitor for SHOW_ENGINE_INNODB_STATUS Output, you are only required to enable innodb_status_output_locks
#
# OBTAINING STANDARD INNODB MONITOR OUTPUT ON DEMAND
#
# As an alternative to enabling the standard InnoDb Monitor for periodic output, you can obtain standard InnoDB Monitor output on demand
# using the SHOW_ENGINE_INNODB_STATUS SQL statement, which fetches the output to your client program.
#
# If you are using the mysql interactive client, the output is more readable if you replace the usual semicolon statement
# terminator with \G:
#
# 		mysql> SHOW ENGINE INNODB STATUS\G
#
# SHOW_ENGINE_INNODB_STATUS output also includes InnoDB Lock Monitor data if the InnoDB Lock Monitor is enabled.
#
# DIRECTING STANDARD INNODB MONITOR OUTPUT TO A STATUS FILE
#
# Standard InnoDB Monitor output can be enabled and directed to a status file by specifying the --innodb-status-file option
# at startup. When this option is used, InnoDB creates a file named innodb_status.pid in the data dir and writes output
# to it every 15 secs, approx.
#
# InnoDB removes the status file when the server is shut down normally. If an abnormal shutdown occurs, the status file
# may have to be removed manually.
#
# The --innodb-status-file option is intended for temporary use, as output generation can affect performance, and the 
# innodb_status.pid file can become quite large over time.
#
# 15.16.3 INNODB STANDARD MONITOR AND LOCK MONITOR OUTPUT
#
# The Lock Monitor is the same as the Standard Monitor except that it includes additional lock information. Enabling either
# monitor for periodic output turns on the same output stream, but the stream includes extra information if the Lock Monitor
# is enabled.
#
# For example, if you enable the Standard Monitor and Lock Monitor, that turns on a single output stream.
#
# This stream includes extra lock information until you disable the Lock Monitor.
#
# Standard Monitor output is limited to 1MB when produced using the SHOW_ENGINE_INNODB_STATUS statement.
# This limit does not apply to output written to server standard error output (stderr)
#
# Example Standard Monitor output:
#
# 		mysql> SHOW ENGINE INNODB STATUS\G
# 		************************* 1. row **********************
# 			Type: InnoDB
# 			Name:
# 			Status:
# 		================================
# 		2018-04-12 15:14:08 0x7f971c063700 INNODB MONITOR OUTPUT
# 		================================
# 		Per second averages calculated from the last 4 seconds.
# 		----------------
# 		BACKGROUND THREAD
# 		----------------
# 		srv_master_thread loops: 15 srv_active, 0 srv_shutdown, 1122 srv_idle
# 		srv_master_thread log flush and writes: 0
# 		----------------
# 		SEMAPHORES
# 		----------------
# 		OS WAIT ARRAY INFO: reservation count 24
# 		OS WAIT ARRAY INFO: signal count 24
# 		RW-shared spins 4, rounds 8, OS waits 4
# 		RW-excl spins 2, rounds 60, OS waits 2
# 		RW-sx spins 0, rounds 0, OS waits 0
# 		Spin rounds per wait: 2.00 RW-shared, 30.00 RW-excl, 0.00 RW-sx
# 		----------------
# 		LATEST FOREIGN KEY ERROR
# 		----------------
# 		2018-04-12 14:57:24 0x7f97a9c91700 Transaction:
# 		TRANSACTION 7717, ACTIVE 0 sec inserting
# 		mysql tables in use 1, locked 1
# 		4 lock struct(s), heap size 1136, 3 row lock(s), undo log entries 3
# 		MySQL thread id 8, OS thread handle 140289365317376, query id 14 localhost root update
# 		INSERT INTO child VALUES (NULL, 1), (NULL, 2), (NULL, 3), (NULL, 4), (NULL, 5), (NULL, 6)
# 		Foreign key constraint fails for table `test`.`child`:
# 			´
# 				CONSTRAINT `child_ibfk_1` FOREIGN KEY (`parent_id`) REFERENCES `parent` (`id`) ON DELETE
# 				CASCADE ON UPDATE CASCADE
# 		Trying to add in child table, in index par_ind tuple:
# 		DATA TUPLE: 2 fields;
# 			0: len 4; hex 80000003; asc 		;;
# 			1: len 4; hex 80000003; asc 		;;
#
# 		But in parent table `test`.`parent`, in index PRIMARY,
# 		the closest match we can find is record:
# 		PHYSICAL RECORD: n_fields 3; compact format; info bits 0
# 			0: len 4; hex 80000004; asc 		;;
# 			1: len 6; hex 000000001e19; asc 		;;
# 			2: len 7; hex 81000001110137; asc 	  7;;
#
# 		---------------
# 		TRANSACTIONS
# 		---------------
# 		Trx id counter 7748
# 		Purge done for trx's n:o < 7747 undo n:o < 0 state: running but idle
# 		History list length 19
# 		LIST OF TRANSACTIONS FOR EACH SESSION:
# 		----TRANSACTION 4217644597900000, not started
# 		0 lock struct(s), heap size 1136, 0 row lock(s)
# 		----TRANSACTION 7747, ACTIVE 23 sec starting index read
# 		mysql tables in use 1, locked 1
# 		LOCK WAIT 2 lock struct(s), heap size 1136, 1 row lock(s)
# 		MySQL thread id 9, OS thread handle 1402286987249408, query id 51 localhost root updating
# 		DELETE FROM t WHERE i = 1
# 		------------- TRX HAS BEEN WAITING 23 SEC FOR THIS LOCK TO BE GRANTED:
# 		RECORD LOCKS space id 4 page no 4 n bits 72 index GEN_CLUST_INDEX of table `test`.`t`
# 		trx id 7747 lock_mode X waiting
# 		Record lock, heap no 3 PHYSICAL RECORD: n_fields 4; compact format; info bits 0
# 			0: len 6; hex 0000000000202; asc 			;;
# 			1: len 6; hex 0000000001e41; asc 		  A;;
# 			2: len 7; hex 8200000008b0110; asc 		  	 	;;
# 			3: len 4; hex 80000001; asc 		;;
#
# 		------------------------
# 		TABLE LOCKS table `test`.`t` trx.id 7747 lock mode IX
# 		RECORD LOCKS space id 4 page no 4 n bits 72 index GEN_CLUST_INDEX of table `test`.`t`
# 		trx id 7747 lock_mode X waiting
# 		Record lock, heap no 3 PHYSICAL RECORD: n_fields 4; compact format; info bits 0
# 			0: len 6; hex 00000000000202; asc 			;;
# 			1: len 6; hex 00000000001e41; asc 			A;;
# 			2: len 7; hex 8200000000008b0110; asc 			;;
# 			3: len 4; hex 800000001; asc 		 ;;
#
# 		-------
# 		FILE I/O
# 		-------
# 		I/O thread 0 state: waiting for i/o request (insert buffer thread)
# 		I/O thread 1 state: waiting for i/o request (log thread)
# 		I/O thread 2 state: waiting for i/o request (read thread)
# 		I/O thread 3 state: waiting for i/o request (read thread)
# 		I/O thread 4 state: waiting for i/o request (read thread)
# 		I/O thread 5 state: waiting for i/o request (read thread)
# 		I/O thread 6 state: waiting for i/o request (write thread)
# 		I/O thread 7 state: waiting for i/o request (write thread)
# 		I/O thread 8 state: waiting for i/o request (write thread)
# 		I/O thread 9 state: waiting for i/o request (write thread)
# 		Pending normal aio reads: [0, 0, 0, 0] , aio writes: [0, 0, 0, 0] ,
# 			ibuf aio reads:, log i/o's:, sync i/o's:
# 		Pending flushes (fsync) log: 0; buffer pool: 0
# 		833 OS file reads, 605 OS file writes, 208 OS fsyncs
# 		0.00 reads/s, 0 avg bytes/read, 0.00 writes/s, 0.00 fsyncs/s
# 		--------------------------------------
# 		INSERT BUFFER AND ADAPTIVE HASH INDEX
# 		-------------------------------------
# 		Ibuf: size 1, free list len 0, seg size 2, 0 merges
# 		merged operations:
# 			insert 0, delete mark 0, delete 0
# 		discarded operations:
# 			insert 0, delete mark 0, delete 0
# 		Hash table size 553253, node heap has 0 buffer(s)
# 		Hash table size 553253, node heap has 1 buffer(s)
# 		Hash table size 553253, node heap has 3 buffer(s)
# 		Hash table size 553253, node heap has 0 buffer(s)
# 		Hash table size 553253, node heap has 0 buffer(s)
# 		Hash table size 553253, node heap has 0 buffer(s)
# 		Hash table size 553253, node heap has 0 buffer(s)
# 		Hash table size 553253, node heap has 0 buffer(s)
# 		0.00 hash searches/s, 0.00 non-hash searches/s
# 		---
# 		LOG
# 		---
# 		Log sequence number 				19643450
# 		Log buffer assigned up to  	19643450
# 		Log buffer completed up to 	19643450
# 		Log written up to 				19643450
# 		Log flushed up to 				19643450
# 		Added dirty pages up to 		19643450
# 		Pages flushed up to 				19643450
# 		Last checkpoint at 				19643450
# 		129 log i/o's done, 0.00 log i/o's/second
# 		----------------------
# 		BUFFER POOL AND MEMORY
# 		----------------------
# 		Total large memory allocated 2198863872
# 		Dictionary memory allocated 409606
# 		Buffer pool size 		131072
# 		Free buffers 			130095
# 		Database pages 		973
# 		Old database pages 	0
# 		Modified db pages 	0
# 		Pending reads 			0
# 		Pending writes: LRU 0, flush list 0, single page 0
# 		Pages made young 0, not young 0
# 		0.00 youngs/s, 0.00 non-youngs/s
# 		Pages read 810, created 163, written 404
# 		0.00 reads/s, 0.00 creates/s, 0.00 writes/s
# 		Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000
# 		Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s
# 		LRU len: 973, unzip_LRU len: 0
# 		I/O sum[0]:cur[0], unzip sum[0]:cur[0]
# 		------------------------
# 		INDIVIDUAL BUFFER POOL INFO
# 		------------------------
# 		---BUFFER POOL 0
# 		Buffer pool size 		65536
# 		Free buffers 			65043
# 		Database pages 		491
# 		Old database pages	0
# 		Modified db pages 	0
# 		Pending reads 			0
# 		Pending writes: LRU 0, flush list 0, single page 0
# 		Pages made young 0, not young 0
# 		0.00 youngs/s, 0.00 non-youngs/s
# 		Pages read 411, created 80, written 210
# 		0.00 reads/s, 0.00 creates/s, 0.00 writes/s
# 		Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000
# 		Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s
# 		LRU len: 491, unzip_LRU len: 0
# 		I/O sum[0]:cur[0], unzip sum[0]:cur[0]
# 		---BUFFER POOL 1
# 		Buffer pool size 		65536
# 		Free buffers 			65052
# 		Database Pages 		482
# 		Old database pages 	0
# 		Modified db pages 	0
# 		Pending reads 			0
# 		Pending writes: LRU 0, flush list 0, single page 0
# 		Pages made young 0, not young 0
# 		0.00 youngs/s, 0.00 non-youngs/s
# 		Pages read 399, created 83, written 194
# 		0.00 reads/s, 0.00 creates/s, 0.00 writes/s
# 		No buffer pool page gets since the last printout
# 		Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s
# 		LRU len: 482, unzip_LRU len: 0
# 		I/O sum[0]:cur[0], unzip sum[0]:cur[0]
# 		--------------
# 		ROW OPERATIONS
# 		--------------
# 		0 queries inside InnoDB, 0 queries in queue
# 		0 read views open inside InnoDB
# 		Process ID=5772, Main thread ID=140286437054208, state=sleeping
# 		Number of rows inserted 57, updated 354, deleted 4, read 4421
# 		0.00 inserts/s, 0.00 updates/s, 0.00 deletes/s, 0.00 reads/s
# 		--------------
# 		END OF INNODB MONITOR OUTPUT
# 		=================
#
# STANDARD MONITOR OUTPUT SECTIONS
#
# For a description of each metric reported by the Standard Monitor, refer to the Metrics chapter in the ORACLE ENTERPRISE MANGER
# FOR MYSQL DATABASE USER'S GUIDE.
#
# 		) Status
#
# 			This section shows the timestamp, the monitor name, and the number of seconds that per-second averages are based on.
#
# 			The number of seconds is the elapsed time between the current time and the last time INnoDB Monitor output was printed.
#
# 		) BACKGROUND THREAD
#
# 			The srv_master_thread lines shows work done by the main background thread
#
# 		) SEMAPHORES
#
# 			This section reports threads waiting for a semaphore and statistics on how many times threads have needed a spin
# 			or a wait on a mutex or a rw-lock semaphore.
#
# 			A large number of threads waiting for semaphores may be a result of disk I/O, or contention problems inside
# 			InnoDB.
#
# 			Contention can be due to heavy parallelism of queries or problems in operating system thread scheduling.
#
# 			Setting the innodb_thread_concurrency system variable smaller than the default value might help in
# 			such situations.
#
# 			The Spin rounds per wait line shows the number of spinlock rounds per OS wait for a mutex.
#
# 			Mutex metrics are reported by SHOW_ENGINE_INNODB_MUTEX
#
# 		) LATEST FOREIGN KEY ERROR
#
# 			This section provides information about the most recent foreign key constraint error.
#
# 			It is not present if no such error has occurred. The contents include the statement that failed
# 			as well as information about the constraint that failed and the referenced and referencing tables.
#
# 		) LATEST DETECTED DEADLOCK
#
# 			This section provides information about the most recent deadlock. It is not present if no deadlock has occurred.
#
# 			The contents show which transactions are involved, the statement each was attempting to execute, the locks
# 			they have and need, and which transaction InnoDB dediced to roll back to break the deadlock.
#
# 			The lock modes reported in this section are explained in SECTION 15.7.1, "INNODB LOCKING"
#
# 		) TRANSACTIONS
#
# 			If this section reports lock waits, your applications might have lock contention. The output can also help
# 			to trace the reasons for transaction deadlocks.
#
# 		) FILE I/O
#
# 			This section provides information about threads that InnoDB uses to perform various types of I/O.
#
# 			The first few of these are dedicated to general InnoDB processing.
#
# 			The contents also display information for pending I/O operations and statistics for I/O performance.
#
# 			The number of these threads are controlled by the innodb_read_io_threads and innodb_write_io_threads
# 			parameters.
#
# 			See SECTION 15.13, "INNODB STARTUP OPTIONS AND SYSTEM VARIABLES"
#
# 		) INSERT BUFFER AND ADAPTIVE HASH INDEX
#
# 			This section shows the status of the InnoDB insert buffer (also referred to as the change buffer) and the
# 			adaptive hash index.
#
# 			For related information, see SECTION 15.5.2, "CHANGE BUFFER", and SECTION 15.5.3, "ADAPTIVE HASH INDEX"
#
# 		) LOG
#
# 			This section displays information about the InnoDB log. The contents include the current log sequence number,
# 			how far the log has been flushed to disk, and the position at which InnoDB last took a checkpoint.
#
# 			(See SECTION 15.11.3, "InnoDB CHECKPOINTS")
#
# 			The section also displays information about pending writes and write performance statistics.
#
# 		) BUFFER POOL AND MEMORY
#
# 			This section gives you statistics on pages read and written. You can calculate from these numbers
# 			how many data file I/O operations your queries currently are doing.
#
# 			For buffer pool statistics descriptions, see MONITORING THE BUFFER POOL USING THE INNODB STANDARD MONITOR.
#
# 			For additional information about the operation of the buffer pool, see SECTION 15.5.1, "BUFFER POOL"
#
# 		) ROW OPERATIONS
#
# 			This section shows what the main thread is doing, including the number and performance rate for each
# 			type of row operation.
#
# 15.17 INNODB BACKUP AND RECOVERY
#
# 15.17.1 INNODB BACKUP
# 15.17.2 INNODB RECOVERY
#
# This section covers topics related to InnoDB backup and recovery.
#
# 		) For information about backup techniques applicable to InnoDB, see SECTION 15.17.1, "INNODB BACKUP"
#
# 		) For information about point-in-time recovery, recovery from disk failure or corruption, and how InnoDB
# 			performs crash recovery, see SECTION 15.17.2, "INNODB RECOVERY"
#
# 15.17.1 INNODB BACKUP
#
# The key to safe database management is making regular backups. Depending on your data volume, number of MySQL server,
# and database workload, you can use these backup techniques, alone or in combination: 
#
# 	Hot backup with MySQL ENTERPRISE BACKUP
#
# Cold backup by copying files while the MySQL server is shut down
#
# Logical backup with mysqldump for smaller data volumes or to record the structure of schema objects.
#
# Hot and cold backups are physical backups that copy actual data files, which can be used directly
# by the mysqld server for faster restore.
#
# Using MySQL Enterprise Backup is the recommended method for backing up InnoDB data.
#
# 	NOTE:
#
# 		InnoDB does not support databases that are restored using third-party backup tools.
#
# HOT BACKUPS
#
# The mysqlbackup command, part of the MySQL Enterprise Backup component, lets you back up a running MySQL instance,
# including InnoDB tables, with minimal disruption to operations while producing a consistent snapshot of the database.
#
# When mysqlbackup is copying InnoDB tables, reads and writes to InnoDB tables can continue.
#
# MySQL Enterprise backup can also create compressed backup files, and back up subsets of tables and databases.
#
# In conjunction with the MySQL binary log, users can perform point-in-time recovery. MySQL Enterprise Backup
# is part of the MySQL Enterprise Subscription.
#
# For more details, see SECTION 30.2, "MYSQL ENTERPRISE BACKUP OVERVIEW"
#
# COLD BACKUPS
#
# If you can shut down the MySQL server, you can make a physical backup that consists of all files used by InnoDB
# to manage its tables.
#
# Use the following procedure:
#
# 		1. Perform a slow shutdown of the MySQL server and make sure that it stops without errors.
#
# 		2. Copy all InnoDB data files (ibdata files and .ibd files) into a safe place
#
# 		3. Copy all InnoDB log files (ib_logfile files) to a safe place
#
# 		4. Copy your my.cnf configuration file or files to a safe place
#
# LOGICAL BACKUPS USING MYSQLDUMP
#
# In addition to physical backups, it is recommended that you regularly create logical backups by dumping
# your tables using mysqldump.
#
# A binary file might be corrupted without you noticing it. Dumped tables are stored into text files that are
# human-readable, so spotting table corruption becomes easier.
#
# Also, because the format is simpler, the chance for serious data corruption is smaller. mysqldump also has
# a --single-transaction option for making a consistent snapshot without locking out other clients.
#
# See SECTION 7.3.1, "ESTABLISHING A BACKUP POLICY"
#
# Replication works with InnoDB tables, so you can use MySQL replication capabilities to keep a copy of
# your database at database sites requiring high availability.
#
# See SECTION 15.18, "INNODB AND MYSQL REPLICATION"
#
# 15.17.2 INNODB RECOVERY
#
# This section describes InnoDB recovery. Topics include:
#
# 		) POINT-IN-TIME RECOVERY
#
# 		) RECOVERY FROM DATA CORRUPTION OR DISK FAILURE
#
# 		) INNODB CRASH RECOVERY
#
# 		) TABLESPACE DISCOVERY DURING CRASH RECOVERY
#
# POINT-IN-TIME RECOVERY
#
# To recover an InnoDB database to the present from the time at which the physical backup was made, you must
# run MySQL server with binary logging enabled, even before taking the backup.
#
# To achieve point-in-time recovery after restoring a backup, you can apply changes from the binary log that
# occurred after the backup was made.
#
# See SECTION 7.5, "POINT-IN-TIME (INCREMENTAL) RECOVERY USING THE BINARY LOG"
#
# RECOVERY FROM DATA CORRUPTION OR DISK FAILURE
#
# If your database becomes corrupted or disk failure occurs, you must perform the recovery using a backup.
# In the case of corruption, first find a backup that is not corrupted.
#
# After restoring the base backup, do a point-in-time recovery from the binary log files using
# mysqlbinlog and mysql to restore the changes that occurred after the backup was made.
#
# In some cases of database corruption, it is enough to dump, drop, and re-create one or a few corrupt
# tables. You can use the CHECK_TABLE statement to check whether a table is corrupt, although CHECK_TABLE
# naturally cannot detect every possible kind of corruption.
#
# In some cases, apparent database page corruption is actually due to the operating system corrupting its
# own file cache, and the data on disk may be okay.
#
# It is best to try restarting the computer first. Doing so may eliminate errors that appeared to be
# database page corruption.
#
# If MySQL still has trouble starting because of InnoDB consistency problems, see SECTION 15.20.2, "FORCING INNODB RECOVERY"
# for steps to start the instance in recovery mode, which permits you to dump the data.
#
# INNODB CRASH RECOVERY
#
# To recover from a MySQL server crash, the only requirement is to restart the MySQL server.
#
# InnoDB automatically checks the logs and performs a roll-forward of the database to the present.
# InnoDB automatically rolls back uncommitted transactions that were present at the time of the crash.
#
# During recovery, mysqld displays output similar to this:
#
# 		InnoDB: The log sequence number 664050266 in the system tablespace does not match
# 		the log sequence number 685111586 in the ib_logfiles!
# 		InnoDB: Database was not shutdown normally!
# 		InnoDB: Starting crash recovery.
# 		InnoDB: Using 'tablespaces.open.2' max LSN: 664075228
# 		InnoDB: Doing recovery: scanned up to log sequence number 690354176
# 		InnoDB: Doing recovery: scanned up to log sequence number 695597056
# 		InnoDB: Doing recovery: scanned up to log sequence number 700839936
# 		InnoDB: Doing recovery: scanned up to log sequence number 706082816
# 		InnoDB: Doing recovery: scanned up to log sequence number 711325696
# 		InnoDB: Doing recovery: scanned up to log sequence number 713458156
# 		InnoDB: Applying a batch of 1467 redo log records ...
# 		InnoDB: 10%
# 		InnoDB: 20%
# 		InnoDB: 30%
# 		InnoDB: 40%
# 		InnoDB: 50%
# 		InnoDB: 60%
# 		InnoDB: 70%
# 		InnoDB: 80%
# 		InnoDB: 90%
# 		InnoDB: 100%
# 		InnoDB: Apply batch completed
# 		InnoDB: 1 transaction(s) which must be rolled back or cleaned up in total 561887 row
# 		operations to undo
# 		InnoDB: Trx id counter is 4096
# 		/etc/
# 		InnoDB: 8.0.1 started; log sequence number 713458156
# 		InnoDB: Waiting for purge to start
# 		InnoDB: Starting in background the rollback of uncommitted transactions
# 		InnoDB: Rolling back trx with id 3596, 561887 rows to undo
# 		/etc/
# 		./mysqld: ready for connections..
#
# InnoDB crash recovery consists of several steps:
#
# 		) Tablespace discovery
#
# 			Tablespace discovery is the process that InnoDB uses to identify tablespaces that require redo log application.
#
# 			See TABLESPACE DISCOVERY DURING CRASH RECOVERY.
#
# 		) REDO LOG APPLICATION
#
# 			Redo log application is performed during initialization, before accepting any connections. If all changes are flushed
# 			from the buffer pool to the tablespaces (ibdata* and *.ibd files) at the time of the shutdown or crash, redo log
# 			application is skipped.
#
# 			InnoDB also skips redo log application if redo log files are missing at startup.
#
# 				) The current maximum auto-increment counter value is written to the redo log each time the value changes,
# 					which makes it crash-safe. During recovery, InnoDB scans the redo log to collect counter value changes
# 					and applies the changes to the in-memory table object.
#
# 					For more information about how InnoDB handles auto-increment values, see SECTION 15.6.1.4, "AUTO_INCREMENT
# 					HANDLING IN INNODB", and InnoDB AUTO_INCREMENT COUNTER INITIALIZATION.
#
# 				) When encountering index tree corruption, InnoDB writes a corruption flag to the redo log, which makes the corruption
# 					flag crash-safe. InnoDB also writes in-memory corruption flag data to an engine-private system table on each 
# 					checkpoint.
#
# 					During recovery, InnoDB reads corruption flags from both locations and merges results before marking in-memory
# 					table and index objects as corrupt.
#
# 				) Removing redo logs to speed up recovery is not recommended, even if some data loss is acceptable. Removing redo logs
# 					should only be considered after a clean shutdown, with innodb_fast_shutdown set to 0 or 1.
#
# 		) Roll back of incomplete transactions
#
# 			Incomplete transactions are any transactions that were active at the time of crash or fast shutdown.
#
# 			The time it takes to roll back an incomplete transaction can be three or four times the amount of time
# 			a transaction is active before it is interrupted, depending on server load.
#	
# 			You cannot cancel transactions that are being rolled back. In extreme cases, when rolling back transactions
# 			is expected to take an exceptionally long time, it may be faster to start InnoDB with an innodb_force_recovery
# 			setting of 3 or greater.
#
# 			See SECTION 15.20.2, "FORCING INNODB RECOVERY"
#
# 		) Change buffer merge
#
# 			Applying changes from the change buffer (part of the system tablespace) to leaf pages of secondary indexes,
# 			as the index pages are read to the buffer pool.
#
# 		) Purge
#
# 			Deleting delete-marked records that are no longer visible to active transactions.
#
# The steps that follow redo log application do not depend on the redo log (other than for logging the writes) and are
# performed in parallel with normal processing.
#
# Of these, only rollback of incomplete transactions is special to crash recovery. The insert buffer merge and the
# purge are performed during normal processing.
#
# After redo log application, InnoDB attempts to accept connections as early as possible, to reduce downtime. As part 
# of crash recovery, InnoDB rolls back transactions that were not committed or in XA PREPARE state when the server crashed.
#
# The rollback is performed by a background thread, executed in parallel with transactions from new connections.
#
# Until the rollback operation is completed, new connections may encounter locking conflicts with recovered transactions.
#
# In most situations, even if the MySQL server was killed unexpectedly in the middle of heavy activity, the recovery process
# happens automatically and no action is required of the DBA.
#
# If a hardware failure or severe system error corrupted InnoDB data, MySQL might refuse to start. In this case,
# see SECTION 15.20.2, "FORCING INNODB RECOVERY"
#
# For information about the binary log and InnoDB crash recovery, see SECTION 5.4.4, "THE BINARY LOG"
#
# TABLESPACE DISCOVERY DURING CRASH RECOVERY
#
# If, during recovery, InnoDB encounters redo logs written since the last checkpoint, the redo logs must be applied
# to affected tablespaces.
#
# The process that identifies affected tablespaces during recovery is referred to as tablespace discovery.
#
# Tablespace discovery relies on the innodb_directories setting, which defines the directories to scan at 
# startup for tablespace files. Directories defined by innodb_data_home_dir, innodb_undo_directory, and datadir
# are automatically appended to the innodb_directories argument value, regardless of whether the innodb_directories
# option is configured explicitly.
#
# Tablespace files defined with an absolute path or that reside outside of the directories automatically appended to
# the innodb_directories setting should be added to the innodb_directories setting.
#
# Recovery is terminated if any tablespace file referenced in a redo log has not been discovered previously.
#
# 15.18 INNODB AND MYSQL REPLICATION
#
# MySQL replication works for InnoDB tables as it does for MyISAM tables. It is also possible to use replication in a way
# where the storage engine on the slave is not the same as the original storage engine on the master.
#
# For example, you can replicate modifications to an InnoDB table on the master to a MyISAM table on the slave.
#
# For more information see, SECTION 17.3.4, "USING REPLICATION WITH DIFFERENT MASTER AND SLAVE STORAGE ENGINES"
#
# For information about setting up a new slave for a master, see SECTION 17.1.2.6, "SETTING UP REPLICATION SLAVES",
# and SECTION 17.1.2.5, "CHOOSING A METHOD FOR DATA SNAPSHOTS"
#
# To make a new slave without taking down the master or an existing slave, use the MySQL Enterprise Backup product.
#
# Transactions that fail on the master do not affect replication at all. MySQL replication is based on the binary log
# where MySQL writes SQL statements that modify data.
#
# A transaction that fails (for example, because of a foreign key violation, or because it is rolled back) is not written
# to the binary log, so it is not sent to slaves.
#
# See SECTION 13.3.1, "START TRANSACTION, COMMIT, AND ROLLBACK SYNTAX"
#
# Replication and CASCADE. Cascading actions for InnoDB tables on the master are replicated on the slave only if the tables
# sharing the foreign key relation use InnoDB on both the master and slave.
#
# This is true whether you are using statement-based or row-based replication. Suppose that you have started replication,
# and then create two tables on the master using the following CREATE_TABLE statements:
#
# 		CREATE TABLE fc1 (
# 			i INT PRIMARY KEY,
# 			j INT
# 		) ENGINE = InnoDB;
#
# 		CREATE TABLE fc2 (
# 			m INT PRIMARY KEY,
# 			n INT,
# 			FOREIGN KEY ni (n) REFERENCES fc1 (i)
# 				ON DELETE CASCADE
# 		) ENGINE = InnoDB;
#
# Suppose that the slave does not have InnoDB support enabled. If this is the case, then the tables on the slave
# are created, but they use the MyISAM storage engine, and the FOREIGN KEY option is ignored.
#
# Now we insert some rows into the tables on the master:
#
# 		master> INSERT INTO fc1 VALUES (1, 1), (2, 2);
# 		Query OK, 2 rows affected (0.09 sec)
# 		Records: 2 	Duplicates: 0 	Warnings: 0
#
# 		master> INSERT INTO fc2 VALUES (1, 1), (2, 2), (3, 1);
# 		Query OK, 3 rows affected (0.19 sec)
# 		Records: 3 	Duplicates: 0 	Warnings: 0
#
# At this point, on both the master and the slave, table fc1 contains 2 rows, and table fc2 contains 3 rows, as shown here:
#
# 		master> SELECT * FROM fc1;
# 		+---+----------+
# 		| i | j 			|
# 		+---+----------+
# 		| 1 |	1 			|
# 		| 2 | 2 			|
# 		+---+----------+
# 		2 rows in set (0.00 sec)
#
# 		master> SELECT * FROM fc2;
# 		+---+-----------+
# 		| m | n 			 |
# 		+---+-----------+
# 		| 1 | 1 			 |
# 		| 2 | 2 			 |
# 		| 3 | 1 			 |
# 		+---+-----------+
# 		3 rows in set (0.00 sec)
#
# 		slave> SELECT * FROM fc1;
# 		+---+-----------+
# 		| i | j 			 |
# 		+---+-----------+
# 		| 1 | 1 			 |
# 		| 2 | 2 			 |
# 		+---+-----------+
# 		2 rows in set (0.00 sec)
#
# 		slave> SELECT * FROM fc2;
# 		+---+------------+
# 		| m | n 			  |
# 		+---+------------+
# 		| 1 | 1 			  |
# 		| 2 | 2 			  |
# 		| 3 | 1 			  |
# 		+---+------------+
# 		3 rows in set (0.00 sec)
#
# Now suppose that you perform the following DELETE statement on the master:
#
# 		master> DELETE FROM fc1 WHERE i=1;
# 		Query OK, 1 row affected (0.09 sec)
#
# Due to the cascade, table fc2 on the master now contains only 1 row:
#
# 		master> SELECT * FROM fc2;
# 		+-------+----------+
# 		| m 	  | n 		 |
# 		+-------+----------+
# 		| 2 	  | 2 		 |
# 		+-------+----------+
# 		1 row in set (0.00 sec)
#
# However, the cascade does not propagate on the slave because on the slave the DELETE for fc1 deletes no rows from fc2.
# The slave's copy of fc2 still contains all of the rows that were originally inserted:
#
# 		slace> SELECT * FROM fc2;
# 		+---+----+
# 		| m | n 	|
# 		+---+----+
# 		| 1 | 1  |
# 		| 3 | 1  |
# 		| 2 | 2 	|
# 		+---+----+
# 		3 rows in set (0.00 sec)
#
# This difference is due to the fact that the cascading deletes are handled internally by the InnoDB storage engine,
# which means that none of the changes are logged.
#
# 15.19 INNODB MEMCACHED PLUGIN
#
# 15.19.1 BENEFITS OF THE INNODB MEMCACHED PLUGIN
# 15.19.2 INNODB MEMCACHED ARCHITECHTURE
# 15.19.3 SETTING UP THE INNODB MEMCACHED PLUGIN
# 15.19.4 INNODB MEMCACHED MULTIPLE GET AND RANGE QUERY SUPPORT
# 15.19.5 SECURITY CONSIDERATIONS FOR THE INNODB MEMCACHED PLUGIN
# 15.19.6 WRITING APPLICATIONS FOR THE INNODB MEMCACHED PLUGIN
# 15.19.7 THE INNODB MEMCACHED PLUGIN AND REPLICATION
# 15.19.8 INNODB MEMCACHED PLUGIN INTERNALS
# 15.19.9 TROUBLESHOOTING THE INNODB MEMCACHED PLUGIN
#
# The InnoDB memcached plugin (daemon_memcached) provides an integrated memcached daemon that automatically
# stores and retrieves data from InnoDB tables, turning the MySQL server into a fast "key-value store".
#
# Instead of formulating queries in SQL, you can use simple get, set and incr operations that avoid the
# performance overhead associated with SQL parsing and constructing a query optimization plan.
#
# You can also access the same InnoDB tables through SQL for convenience, complex queries, bulk operations,
# and other strengths of traditional database software.
#
# This "NoSQL-style" interface uses the memcached API to speed up database operations, letting InnoDB handle
# memory caching using its buffer pool mechanism.
#
# Data modified through memcached operations such as add, set and incr are stored to disk, in InnoDB tables.
#
# The combination of memcached simplicity and InnoDB reliability and consistency provides users with the best
# of both worlds, as explained in SECTION 15.19.1, "BENEFITS OF TEH INNODB MEMCACHED PLUGIN"
#
# For an architechtural overview, see SECTION 15.19.2, "INNODB MEMCACHED ARCHITECHTURE".
#
# 15.19.1 BENEFITS OF THE INNODB MEMCACHED PLUGIN
#
# This section outlines advantages the daemon_memcached plugin. The combination of InnoDB tables and memcached
# offers advantages over using either by themselves.
#
# 		) Direct access to the InnoDB storage engine avoids the parsing and planning overhead of SQL
#
# 		) Running memcached in the same process space as the MySQL server avoids the network overhead of passing
# 			requests back and forth.
#
# 		) Data written using the memcached protocol is transparently written to an InnoDB table, without going
# 			through the MySQL SQL layer. You can control frequency of writes to achieve higher raw performance
# 			when updating non-critical data.
#
# 		) Data requested through the memcached protocol is transparently queried from an InnoDB table, without
# 			going through the MySQL SQL layer.
#
# 		) Subsequent requests for the same data is served from the InnoDB buffer pool. The buffer pool handles
# 			the in-memory caching. You can tune performance of data-intensive operations using InnoDB configuration
# 			options.
#
# 		) Data can be unstructured or structured, depending on the type of application. You can create a new table for data, or use existing tables.
#
# 		) InnoDB can handle composing and decomposing multiple column values into a single memcached item value, reducing the amount
# 			of string parsing and concatenation required in your application.
#
# 			For example, you can store the string value 2|4|6|8 in the memcached cache, and have InnoDB split the value based on a
# 			separator character, then store the result in four numeric columns.
#
# 		) The transfer between memory and disk is handled automatically, simplifying application logic.
#
# 		) Data is stored in a MySQL database to protect against crashes, outages, and corruption.
#
# 		) You can access the underlying InnoDB table through SQL for reporting, analysis, ad hoc queries, bulk loading,
# 			multi-step transactional computations, set operations such as union and intersection, and other operations
# 			suited to the expressiveness and flexibility of SQL.
#
# 		) You can ensure high availability by using the daemon_memcached plugin on a master server in combination with MySQL replication.
#
# 		) The integration of memcached with MySQL provides a way to make in-memory data persistent, so you can use it for more significant
# 			kinds of data.
#
# 			You can use more add, incr, and similar write operations in your application without concern that data could be lost.
#
# 			You can stop and start the memcached server without losing updates made to cached data. To guard against unexpected
# 			outages, you can take advantage of InnoDB crash recovery, replication and backup capabilities.
#
# 		) The way InnoDB does fast primary key lookups is a natural fit for memcached single-item queries. The direct, low-level
# 			database access path used by the daemon_memcached plugin is much more efficient for key-value lookups than equivalent
# 			SQL queries.
#
# 		) The serialization features of memcached, which can turn complex data structures, binary files, or even code blocks into
# 			storeable strings, offer a simple way to get such objects into a database.
#
# 		) Because you can access the underlying data through SQL, you can produce reports, search or update across multiple keys,
# 			and call functions such as AVG() and MAX() on memcached data.
#
# 			All of these operations are expensive or complicated using memcached by itself.
#
# 		) You do not need to manually load data into memcached at startup. As particular keys are requested by an application,
# 			values are retrieved from the database automatically, and cached in memory using the InnoDB buffer pool.
#
# 		) Because memcached consumes relativiely little CPU, and its memory footprint is easy to control, it can run comfortably
# 			alongside a MySQL instance on the same system.
#
# 		) Because data consistency is enforced by mechanisms used for regular InnoDB tables, you do not have to worry about
# 			stale memcached data or fallback logic to query the database in the case of a missing key.
#
# 15.19.2 INNODB MEMCACHED ARCHITECHTURE
#
# The InnoDB memcached plugin implements memcached as a MySQL plugin daemon that accesses the InnoDB storage engine directly,
# bypassing the MySQL SQL layer.
#
# The following diagram illustrates how an application accesses data through the daemon_memcached plugin, compared with SQL.
#
# FIGURE 15.4 MYSQL SERVER WITH INTEGRATED MEMCACHED SERVER
#
# 			 [	Application ]
# 			v 					v
# 		SQL 					Memcached protocol
#
# 				
#				[	mysqld	]	
# 
#			v 						v
#
# 		[ MySQL Server ] 		[ memcached plugin ]
# 									[innodb_memcache] [local cache (optional)]
#
# 		[Handler API] 			[InnoDB API]
#
# 		[ 				InnoDB Storage Engine 				]
#
# Features of the daemon_memcached plugin:
#
# 		) memcached as a daemon plugin of mysqld. Both mysqld and memcached run in the same process space, with very low latency
# 			access to data.
#
# 		) Direct access to InnoDB tables, bypassing the SQL parser, the optimizer, and even the Handler API layer.
#
# 		) Standard memcached protocols, including the text-based protocol and the binary protocol. The daemon_memcached plugin
# 			passes all 55 compatibility tests of the memcapable command.
#
# 		) Multi-column support. You can map multiple columns into the "value" part of the key-value store, with column values
# 			delimited by a user-specified separator character.
#
# 		) By default, the memcached protocol is used to read and write data directly to InnoDB, letting MySQL manage in-memory
# 			caching using the InnoDB buffer pool.
#
# 			The default settings represent a combination of high reliability and the fewest surprises for database applications.
#
# 			For example, default settings avoid uncommitted data on the database side, or stale data returned for memcached get requests.
#
# 		) Advanced users can configure the system as a traditional memcached server, with all data cached only in the memcached engine
# 			(memory caching), or use a combination of the "memcached engine" (memory caching) and the InnoDB memcached engine
# 			(InnoDB as back-end persistent storage)
#
# 		) Control over how often data is passed back and forth between InnoDB and memcached operations through the innodb_api_bk_commit_interval,
# 			daemon_memcached_r_batch_size, and daemon_memcached_w_batch_size configuration options.
#
# 			Batch size options default to a value of 1 for maximum reliability.
#
# 		) The ability to specify memcached options through the daemon_memcached_option configuration parameter. For example, you can change
# 			the port that memcached listens on, reduce the maximum number of simultaneous connections, change the maximum memory size
# 			for a key-value pair, or enable debugging messages for the error log.
#
# 		) The innodb_api_trx_level configuration option controls the transaction isolation level on queries processed by memcached.
# 		
# 			Although memcached has no concept of transactions, you can use this option to control how soon memcached sees changes caused
# 			by SQL statements issued on the table used by the daemon_memcached plugin. By default, innodb_api_trx_level is set to READ_UNCOMMITTED.
#
# 		) The innodb_api_enable_mdl option can be used to lock the table at the MySQL level, so that the mapped table cannot be dropped or
# 			altered by DDL through the SQL interface.
#
# 			Without the lock, the table can be dropped from the MySQL layer, but kept in InnoDB storage until memcached or some other
# 			user stops using it.
#
# 			"MDL" stands for "metadata locking"
#
# DIFFERENCES BETWEEN INNODB MEMCACHED AND TRADITIONAL MEMCACHED
#
# You may already be familiar with using memcached with MySQL, as described in USING MYSQL WITH MEMCACHED. This section describes
# how features of the integrated InnoDB memcached plugin differ from traditional memcached.
#
# 		) Installation: The memcached library comes with the MySQL server, making installation and setup relatively easy.
#
# 			Installation involves running the innodb_memcached_config.sql script to create a demo_test table for memcached to use,
# 			issuing an INSTALL_PLUGIN statement to enable the daemon_memcached plugin, and adding desired memcached options to a
# 			MySQL configuration file or startup script.
#
# 			You might still install the traditional memcached distribution for additional utiltiies such as memcp, memcat,
# 			and memcapable.
#
# 			For comparison with traditional memcached, see INSTALLING MEMCACHED.
#
# 		) Deployment: With traditional memcached, it is typical to run large numbers of low-capacity memcached servers.
#
# 			A typical deployment of the daemon_memcached plugin, however, involves a smaller number of moderate or high-powered
# 			servers that are already running MySQL.
#
# 			The benefit of this configuration is in improving efficiency of individual database servers rather than exploiting
# 			unused memory or distributing lookups across large numbers of servers. In the default configuration, very little
# 			memory is used for memcached, and in-memory lookups are served from the InnoDB buffer pool, which automatically
# 			caches the most recently and frequently used data.
#
# 			As with a traditional MySQL server instance, keep the value of the innodb_buffer_pool_size configuration option
# 			as high as practical (without causing paging at the OS level), so that as much work as possible is performed
# 			in memory.
#
# 			For comparison with traditional memcached, see memcached Deployment.
#
# 		) Expiry: By default (that is, using the innodb_only caching policy), the latest data from the InnoDB table is always
# 			returned, so the expiry options have no practical effect.
#
# 			If you change the caching policy to caching or cache_only, the expiry options work as usual, but requested data
# 			might be stale if it is updated in the underlying table before it expires from the memory cache.
#
# 			For comparison with traditional memcached, see DATA EXPIRY
#
# 		) Namespaces: memcached is like a large directory where you give files elaborate names with prefixes and suffixes
# 			to keep the files from conflicting.
#
# 			The daemon_memcached plugin lets you use similar naming conventions for keys, with one addition. Key names
# 			in the format @@table_id.key.table_id are decoded to reference a specific table, using mapping data from
# 			the innodb_memcache.containers table.
#
# 			The key is looked up in or written to the specified table.
#
# 			The @@ notation only works for individual calls to get, add and set functions, but not others such as incr
# 			or delete. To designate a default table for subsequent memcached operations within a session, perform a
# 			get request using the @@ notation with a table_id, but without the key portion.
#
# 			For example:
#
# 				get @@table_id
#
# 			Subsequent get, set, incr, delete and other operations use the table designated by table_id in the innodb_memcache.containers.name
# 			column.
#
# 			For comparison with traditional memcached, see USING NAMESPACES.
#
# 		) Hashing and distribution: The default configration, which uses the innodb_only caching policy, is suitable for a traditional deployment
# 			configuration where all data is available on all servers, such as a set of replication slave servers.
#
# 			If you physically divide data, as in a sharded configuration, you can split data across several machines running the daemon_memcached plugin
# 			, and use the traditional memcached hashing mechanism to route requests to a particular machine.
#
# 			On the MySQL side, you would typically let all data be inserted by add requests to memcached so that appropriate values are stored in the
# 			database on the appropriate server.
#
# 			For comparison with tradititonal memcached, see memcached Hashing/Distribution Types.
#
# 		) Memory usage: By default (with the innodb_only caching policy), the memcached protocol passes information back and forth with InnoDB
# 			tables, and the InnoDB buffer pool handles in-memory lookups instead of memcached memory usage growing and shrinking.
#
# 			Relatively little memory is used on the memcached side.
#
# 			If you switch the caching policy to caching or cache_only, the normal rules of memcached memory usage apply. Memory for memcached
# 			data values is allocated in terms of "slabs". You can control slab size and maximum memory used for memcached.
#
# 			Either way, you can monitor and troubleshoot the daemon_memcached plugin using the familiar statistics system, accessed through
# 			the standard protocol, over a telnet session, for example.
#
# 			Extra utilities are not included with the daemon_memcached plugin. You can use the memcached-tool script to install a full
# 			memcached distribution.
#
# 			For comparison with traditional memcached, see MEMORY ALLOCATION WITHIN MEMCACHED.
#
# 		) Thread usage: MySQL threads and memcached threads co-exist on the same server. Limits imposed on threads by the operating
# 			system apply to the total number of threads.
#
# 			For comparison with traditional memcached, see memcached Thread Support.
#
# 		) Log usage: Because the memcached daemon is run alongside the MySQL server and writes to stderr, the -v, -vv and -vvv options
# 			for logging write output to the MySQL error log.
#
# 			For comparison with traditional memcached, see memcached logs.
#
# 		) memcached operations: Familiar memcached operations such as get, set, add and delete are available.
#
# 			Serialization (that is, the exact string format representing complex data structures) depends on the
# 			language interface.
#
# 			For comparison with traditional memcached, see BASIC MEMCACHED OPERATIONS.
#
# 		) Using memcached as a MySQL front end: This is the primary purpose of the InnoDB memcached plugin. 
#
# 			An integrated memcached daemon improves application performance, and having InnoDB handle data transfers
# 			between memory and disk simplifies application logic.
#
# 			For comparison with traditional memcached, see USING MEMCACHED AS A MYSQL CACHING LAYER
#
# 		) Utilities: The MySQL server includes the libmemcached library but not additional command-line utilities.
#
# 			To use commands such as memcp, memcat, and memcapable commands, install a full memcached distribution.
# 			When memrm and memflush remove items from the cache, the items are also removed from the underlying InnoDB table.
#
# 			For comparison with traditional memcached, see libmemcached Command-Line Utilities
#
# 		) Programming Interfaces: You can access the MySQL server through the daemon_memcached plugin using all supported languages:
#
# 			C and C++, Java, Perl, Python, PHP and Ruby.
#
# 			Specify the server hostname and port as with a traditional memcached server. By default, the daemon_memcached plugin listens
# 			on port 11211. You can use both the text and binary protocols.
#
# 			YOu can customize the behavior of memcached functions at runtime. Serialization (that is, the exact string format representing
# 			complex data structures) depends on the language interface.
#
# 			For comparison with traditional memcached, see DEVELOPING A MEMCACHED APPLICATION.
#
# 		) Frequently asked questions: MySQL has an extensive FAQ for traditional memcached. The FAQ is mostly applicable, except taht
# 			using InnoDB as a storage medium for memcached data means that you can use memcached for more write-intensive applications
# 			than before, rather than as a read-only cache.
#
# 			See memcached FAQ.
#
# 15.19.3 SETTING UP THE INNODB MEMCACHED PLUGIN
#
# This section describes how to set up the daemon_memcached plugin to a MySQL server. Because the memcached daemon is tightly
# integrated with the MySQL server to avoid network traffic and minimize latency, you perform this process on each MySQL
# instance that uses this feature.
#
# NOTE:
#
# 		Before setting up the daemon_memcached plugin, consult SECTION 15.19.5, "SECURITY CONSIDERATIONS FOR THE INNODB MEMCACHED PLUGIN"
# 		to understand the security procedures required to prevent unauthorized access.
#
# PREREQUISITES
#
# 		) The daemon_memcached plugin is only supported on Linux, Solaris and OS X platforms. Other operating systems are not supported.
#
# 		) When building MySQL from source, you must build with -DWITH_INNODB_MEMCACHED=ON. This build option generates two shared libraries
# 			in teh MySQL plugin directory (plugin_dir) that are required to run the daemon_memcached plugin:
#
# 			) libmemcached.so: the memcached daemon plugin to MySQL
#
# 			) innodb_engine.so: an InnoDB API plugin to memcached
#
# 		) libevent must be installed
#
# 			) if you did not build MySQL from source, the libevent library is not included in your installation.
#
# 				Use the installation method for your operating system to install libevent 1.4.12 or later.
#
# 				For example, depending on the operating system, you might use apt-get, yum or port install.
#
# 				For example, on Ubuntu Linux, use:
#
# 					sudo apt-get install libevent-dev
#
# 			) If you installed MySQL from a source code release, libevent 1.4.12 is bundled with the package and is located
# 				at the top level of the MySQL source code directory.
#
# 				If you use the bundled version of libevent, no action is required.
#
# 				If you want to use a local system version of libevent, you must build MySQL with the -DWITH_LIBEVENT 
# 				build option set to system or yes.
#
# INSTALLING AND CONFIGURING THE INNODB MEMCACHED PLUGIN
#
# 	1. Configure the daemon_memcached plugin so it can interact with InnoDB tables by running the innodb_memcached_config.sql
# 			configuration script, which is located in MYSQL_HOME/share
#
# 			This script installs the innodb_memcache database with three required tables (cache_policies, config_options,
# 			and containers)
#
# 			It also installs the demo_test sample table in the test database.
#
# 				mysql> source MYSQL_HOME/share/innodb_memcached_config.sql
#
# 			Running the innodb_memcached_config.sql script is a one-time operation. The tables remain in place if you later
# 			uninstall and re-install the daemon_memcached plugin.
#
# 				mysql> USE innodb_memcache;
# 				mysql> SHOW TABLES;
# 				+--------------------------+
# 				| Tables_in_innodb_memcache|
# 				+--------------------------+
# 				| cache_policies 			   |
# 				| config_options 			   |
# 				| containers 					|
# 				+--------------------------+
# 
# 				mysql> USE test;
# 				mysql> SHOW TABLES;
# 				+--------------------------+
# 				| Tables_in_test 			   |
# 				+--------------------------+
# 				| demo_test 					|
# 				+--------------------------+
#
# 			Of these tables, the innodb_memcache.containers table is the most important. Entires in the containers table
# 			provide a mapping to InnoDB table columns.
#
# 			Each InnoDB table used with the daemon_memcached plugin requires an entry in the containers table.
#
# 			The innodb_memcached_config.sql script inserts a single entry in the containers table that provides a mapping 
# 			for the demo_test table. It also inserts a single row of data into the demo_test table.
#
# 			This data allows you to immediately verify the installation after the setup is completed.
#
# 				mysql> SELECT * FROM innodb_memcache.containers\G
# 				*************************** 1. row **********************
# 										name: aaa
# 								db_schema : test
# 								db_table  : demo_test
# 							  key_columns: c1
# 							value_columns: c2
# 									flags  : c3
# 							cas_column 	 : c4
# 					expire_time_column : c5
# 				unique_idx_name_on_key: PRIMARY
#
# 				mysql> SELECT * FROM test.demo_test;
# 				+---------+-----------------------------+----------+-----------+----------+
# 				| c1 		 | c2 								 | c3 		| c4 			| c5 		  |
# 				+---------+-----------------------------+----------+-----------+----------+
# 				| AA 		 | HELLO, HELLO 					 | 8 			| 0 			| 0 		  |
# 				+---------+-----------------------------+----------+-----------+----------+
#
# 			For more information about innodb_memcache tables and the demo_test sample table, see SECTION 15.19.8,
# 			"INNODB MEMCACHED PLUGIN INTERNALS"
#
# 		2. Activate the daemon_memcached plugin by running the INSTALL_PLUGIN statement:
#
# 			mysql> INSTALL PLUGIN daemon_memcached soname "libmemcached.so";
#
# 			Once the plugin is installed, it is automatically activated each time the MySQL server is restarted.
#
# VERIFYING THE INNODB AND MEMCACHED SETUP
#
# To verify the daemon_memcached plugin setup, use a telnet session to issue memcached commands. By default, the memcached
# daemon listens on port 11211.
#
# 		1. Retrieve the data from the test.demo_test table. The single row of data in the demo_test table has a key value of AA.
#
# 				telnet localhost 11211
# 				Trying 127.0.0.1
# 				Connected to localhost
# 				Escape character is '^]'
# 				get AA
# 				VALUE AA 8 12
# 				HELLO, HELLO
# 				END
#
# 		2. Insert data using a set command.
#
# 				set BB 10 0 16
# 				GOODBYE, GOODBYE
#				STORED
#
# 			where:
#
# 				) set is the command to store a value
#
# 				) BB is the key
#
# 				) 10 is a flag for the operation; ignored by memcached but may be used by the client to indicate any type of information. Specify 0 if unused.
#
# 				) 0 is the expiration time (TTL); specify 0 if unused
#
# 				) 16 is the length of the supplied value block in bytes
#
# 				) GOODBYE, GOODBYE is the value that is stored
#
# 		3. Verify that the data inserted is stored in MySQL by connecting to the MySQL server and querying the test.demo_test table.
#
# 			mysql> SELECT * FROM test.demo_test;
# 			+-----+------------------+---------+----------+-------------+
# 			| c1  | c2 					 | c3 	  | c4 		 | c5 			|
# 			+-----+------------------+---------+----------+-------------+
# 			| AA  | HELLO, HELLO 	 | 8 		  | 0 		 | 0 				|
# 			| BB  | GOODBYE, GOODBYE | 10 	  | 1 		 | 0 				|
# 			+-----+------------------+---------+----------+-------------+
#
# 		4. Return to the telnet session and retrieve the data that you inserted earlier using key BB.
#
# 			get BB
# 			VALUE BB 10 16
# 			GOODBYE, GOODBYE
# 			END
# 			quit
#
# If you shut down the MySQL server, which also shuts off the integrated memcached server, further attempts to access the
# memcached data fail with a connection error.
#
# Normally, the memcached data also disappears at this point, and you would require application logic to load the data back
# into memory when memcached is restarted.
#
# However, the InnoDB memcached plugin automates this process for you.
#
# When you restart MySQL, get operations once again return the key-value pairs you stored in the earlier memcached session.
#
# When a key is requested and the associated value is not already in the memory cache, the value is automatically queried
# from the MySQL test.demo_test table.
#
# CREATING A NEW TABLE AND COLUMN MAPPING
#
# This example shows how to setup your own InnoDB table with the daemon_memcached plugin.
#
# 		1. Create an InnoDB table. The table must have a key column with a unique index. The key column of the city table
# 			is city_id, which is defined as the primary key.
#
# 			The table must also include columns for flags, cas and expiry values. There may be one or more value columns.
#
# 			The city table has three value columns (name, state, country)
#
# 			NOTE:
#
# 				There is no special requirement with respect to column names as long as a valid mapping is added to the
# 				innodb_memcache.containers table.
#
# 			mysql> CREATE TABLE city (
# 					 city_id VARCHAR(32),
# 					 name VARCHAR(1024),
# 					 state VARCHAR(1024),
# 					 country VARCHAR(1024),
# 					 flags INT,
# 					 cas BIGINT UNSIGNED,
# 					 expiry INT,
# 					primary key(city_id)
# 					) ENGINE=InnoDB;
#
# 		2. Add an entry to the innodb_memcache.containers table so that hte daemon_memcached plugin knows how to access
# 			the InnoDB table.
#
# 			The entry must satisfy the innodb_memcache.containers table definition.
#
# 			For a description of each field, see SECTION 15.19.8, "INNODB MEMCACHED PLUGIN INTERNALS"
#
# 				mysql> DESCRIBE innodb_memcache.containers;
# 				+------------------------------------+-----------------------+-------+-----+--------+------------+
# 				| Field 										 | Type 						 | Null 	| Key | Default| Extra 	    |
# 				+------------------------------------+-----------------------+-------+-----+--------+------------+
# 				| name 										 | varchar(50) 			 | NO 	| PRI | NULL 	|  			 |
# 				| db_schema 								 | varchar(250) 			 | NO 	| 	   | NULL 	| 			    |
# 				| db_table 									 | varchar(250) 			 | NO 	| 		| NULL 	| 				 |
# 				| key_columns 								 | varchar(250) 			 | NO 	| 	   | NULL   |  			 |
# 				| value_columns 							 | varchar(250) 			 | YES 	| 		| NULL 	|  			 |
# 				| flags 										 | varchar(250) 			 | NO 	| 		| 0		| 				 |
# 				| cas_column 								 | varchar(250) 			 | YES 	| 	   | NULL   | 				 |
# 				| expire_time_column 					 | varchar(250) 			 | YES 	| 		| NULL 	| 				 |
# 				| unique_idx_name_on_key 				 | varchar(250) 			 | NO 	| 		| NULL   | 				 |
# 				+------------------------------------+-----------------------+-------+-----+--------+------------+
#
# 			The innodb_memcache.containers table entry for the city table is defined as:
#
# 				mysql> INSERT INTO `innodb_memcache`.`containers` (
# 						 `name`, `db_schema`, `db_table`, `key_columns`, `value_columns`,
# 						 `flags`, `cas_column`, `expire_time_column`, `unique_idx_name_on_key`)
# 						 VALUES ('default', 'test', 'city', 'city_id', 'name|state|country',
# 						 'flags', 'cas', 'expiry', 'PRIMARY');
#
# 				) default is specified for the containers.name column to configure the city table as the default InnoDB table
# 					to be used with the daemon_memcached plugin.
#
# 				) Multiple InnoDB table columns (name, state, country) are mapped to containers.value_columns using a "|" delimiter.
#
# 				) The flags, cas_column, and expire_time_column fields of the innodb_memcache.containers table are typically not
# 					significant in applications using the daemon_memcached plugin.
#
# 					However, a designated InnoDB table column is required for each. When inserting data, specify 0 for these
# 					columns if they are unused.
#
# 		3. After updating the innodb_memcache.containers table, restart the daemon_memcache plugin to apply the changes.
#
# 			mysql> UNINSTALL PLUGIN daemon_memcached;
#
# 			mysql> INSTALL PLUGIN daemon_memcached soname "libmemcached.so";
#
# 		4. Using telnet, insert data into the city table using a memcached set command.
#
# 			telnet localhost 11211
# 			Trying 127.0.0.1
# 			Connected to localhost
# 			Escpae character is '^]'
# 			set B 0 0 22
# 			BANGALORE|BANGALORE|IN
# 			STORED
#
# 		5. Using MySQL, query the test.city table to verify that the data you inserted was stored.
#
# 			mysql> SELECT * FROM test.city;
# 			+----------+-------------+------------+------------+----------+---------+-----------+
# 			| city_id  | name 		 | state 	  | country 	| flags    | cas 	   | expiry 	|
# 			+----------+-------------+------------+------------+----------+---------+-----------+
# 			| B 		  | BANGALORE 	 | BANGALORE  | 	IN 		| 0 		  | 3 		| 0 			|
# 			+----------+-------------+------------+------------+----------+---------+-----------+
#
# 		6. Using MySQL, insert additional data into the test.city table
#
# 			mysql> INSERT INTO city VALUES ('C', 'CHENNAI', 'TAMIL BLA', 'IN', 0, 0, 0);
# 			/etc/
#
# 		NOTE:
#
# 			It is recommended that you specify a value of 0 for the flags, cas_column and expire_time_column fields if they are unused.
#
# 		7. Using telnet, issue a memcached get command to retrieve data you inserted using MySQL
#
# 			get H
# 			VALUE H 0 22
# 			//VALUES//
# 			END
#
# CONFIGURING THE INNODB MEMCACHED PLUGIN
#
# Traditional memcached configuration options may be specified in a MySQL configuration file or a mysqld startup string, encoded
# in the argument of the daemon_memcached_option configuration parameter. 
#
# memcached configuration options take effect when the plugin is loaded, which occurs each time the MySQL server is started.
#
# For example, to make memcached listen on port 11222 instead of hte default port 11211, specify -p11222 as an argument of the
# daemon_memcached_option configuration option:
#
# 		mysqld /etc/ --daemon_memcached_option="-p11222"
#
# Other memcached options can be encoded in the daemon_memcached_option string. For example, you can specify options to reduce
# the maximum number of simultaneous connections, change the maximum memory size for a key-value pair, or enable debugging
# messages for the error log, and so on.
#
# There are also configuration options specific to the daemon_memcached plugin. These include:
#
# 		) daemon_memcached_engine_lib_name: Specifies the shared library that implements the InnoDB memcached plugin.
#
# 			The default setting is innodb_engine.so
#
# 		) daemon_memcached_engine_lib_path: The path of the directory containing the shared library that implements the
# 			InnoDB memcached plugin. The default is NULL, representing the plugin directory.
#
# 		) daemon_memcached_r_batch_size: Defines the batch commit size for read operations (get)
#
# 			It specifies the number of memcached read operations after which a commit occurs.
#
# 			daemon_memcached_r_batch_size is set to 1 by default so that every get request accesses the most
# 			recently committed data in the InnoDB table, whether the data was updated through memcached or by SQL.
#
# 			When the value is greater than 1, the counter for read operations is incremented with each get call.
#
# 			A flush_all call resets both read and write counters.
#
# 		) daemon_memcached_w_batch_size: Defines the batch commit size for write operations (set, replace, append, prepend,
# 			incr, decr and so on)
#
# 			daemon_memcached_w_batch_size is set to 1 by default so that no uncommitted data is lost in case of an outage,
# 			and so that SQL queries on the underlying table access the most recent data.
#
# 			When the value is greater than 1 , the counter for write operations is incremented for each add, set, incr,
# 			decr and delete call.
#
# 			A flush_all call resets both read and write counters.
#
# By default, you do not need to modify daemon_memcached_engine_lib_name or daemon_memcached_engine_lib_path.
#
# You might configure these options if, for example, you want to use a different storage engine for memcached
# (such as the NDB memcached engine)
#
# daemon_memcached plugin configuration parameters may be specified in the MySQL configuration file or in a mysqld startup string.
#
# They take effect when you load the daemon_memcached plugin.
#
# When making changes to daemon_memcached plugin configuration, reload the plugin to apply the changes. To do so, issue the
# following statements:
#
# 		mysql> UNINSTALL PLUGIN daemon_memcached;
#
# 		mysql> INSTALL PLUGIN daemon_memcached soname "libmemcached.so";
#
# Configuration settings, required tables, and data are preserved when the plugin is restarted.
#
# For additional information about enabling and disabling plugins, see SECTION 5.6.1, "INSTALLING AND UNINSTALLING PLUGINS"
#
# 15.19.4 INNODB MEMCACHED MULTIPLE GET AND RANGE QUERY SUPPORT
#
# The daemon_memcached plugin supports multiple get operations (fetching multiple key-value pairs in a single memcached query)
# and range queries.
#
# MULTIPLE GET OPERATIONS
#
# The ability to fetch multiple key-value pairs in a single memcached query improves read performance by reducing communication
# traffic between the client and the server.
#
# For InnoDB, it means fewer transactions and open-table operations.
#
# The following example demonstrates multiple-get support. The example uses the test.city table described in 
# CREATING A NEW TABLE AND COLUMN MAPPING.
#
# 		mysql> USE test;
# 		mysql> SELECT * FROM test.city;
# 		+---------+-------------+---------------+---------------+-------------+------------+------------+
# 		| city_id | name 			| state 			 | country 		  | flags 		 | cas 		  | expiry 		|
# 		+---------+-------------+---------------+---------------+-------------+------------+------------+
# 		| B 		 | BANGALORE 	| BANGALORE 	 | IN 			  | 0 			 | 1 			  | 0 			|
# 		// etc //
#
# Run a get command to retrieve all values from the city table. The results are returned in a key-value pair
# sequence.
#
# 		telnet 127.0.0.1 11211
# 		Trying 127.0.0.1
# 		Connected to 127.0.0.1
# 		Escape character is '^]'
# 		get B C D H M
# 		VALUE B 0 22
# 		BANGALORE|BANGALORE|IN
# 		VALUE C 0 21
# 		CHENNAI|TAMIL NADU|IN
# 		VALUE D 0 14
# 		DELHI|DELHI|IN
# 		VALUE H 0 22
# 		HYDERABAD|TELANGANA|IN
# 		VALUE M 0 21
# 		MUMBAI|MAHARASHTRA|IN
# 		END
#
# When retrieving multiple values in a single get command, you can switch tables (using @@containers.name notation) to retireve
# the value for the first key, but you cannot switch tables for subsequent keys. For example, the table switch in this example
# is valid:
#
# 		get @@aaa.AA BB
# 		VALUE @@aaa.AA 8 12
# 		HELLO HELLO
# 		VALUE BB 10 16
# 		GOODBYE, GOODBYE
# 		END
#
# Attempting to switch tables again in the same get command to retrieve a key value from a different table is not supported.
#
# RANGE QUERIES
#
# For range queries, the daemon_memcached plugin supports the following comparison operators: <,>,<=,>=.
#
# An operator must be preceded by an @ symbol. When a range query finds multiple matching key-value pairs,
# results are returned in a key-value pair sequence.
#
# The following examples demonstrate range query support. The examples use the test.city table described in 
# CREATING A NEW TABLE AND COLUMN MAPPING
#
# 		mysql> SELECT * FROM test.city;
# 		+-----------+-------------+------------------+---------------+-----------+-----------+-----------+
# 		| city_id 	| name 		  | state 				| country 		 | flags 	 | cas 		 | expiry 	 |
# 		+-----------+-------------+------------------+---------------+-----------+-----------+-----------+
# 		| B 			| BANGALORE   | BANGALORE 			| IN 				 | 0 			 | 1 			 | 0 			 |
# 		| C 			| CHENNAI 	  | TAMIL NADU 		| IN 				 | 0 			 | 0 			 | 0 			 |
# 		| D 			| DELHI 		  | DELHI 			   | IN 				 | 0 			 | 0 			 | 0	 		 |
# 		| H 			| HYDERABAD   | TELANGANA 			| IN 				 | 0 			 | 0 			 | 0 		    |
# 		| M 		 	| MUMBAI 	  | MAHARASHTRA 		| IN 				 | 0 			 | 0 			 | 0 			 |
# 		+-----------+-------------+------------------+---------------+-----------+-----------+-----------+
#
# Open a telnet session:
#
# 		telnet 1270.0.0.1 11211
# 		Trying 127.0.0.1
# 		Connected to 127.0.0.1
# 		Escape character is '^]'
#
# To get all values greater than B, enter get @>B:
#
# 		get @>B
# 		VALUE C 0 21
# 		CHENNAI|TAMIL NADU|IN
# 		VALUE D 0 14
# 		DELHI|DELHI|IN
# 		VALUE H 0 22
# 		HYDERABAD|TELAGANA|IN
# 		VALUE M 0 21
# 		MUMBAI|MAHARASHTRA|IN
# 		END
#
# To get all values less than M, enter get @<M
#
# 		get @<M
# 		VALUE B 0 22
# 		BANGALORE|BANGALORE|IN
# 		VALUE C 0 21
# 		CHENNAI|TAMIL NADU|IN
# 		VALUE D 0 14
# 		DELHI|DELHI|IN
# 		VALUE H 0 22
# 		HYDERABAD|TELANGANA|IN
# 		END
#
# To get all values less than and including M, enter get @<=M:
#
# 		get @<=M
# 		VALUE B 0 22
# 		BANGALORE|BANGALORE|IN
# 		VALUE C 0 21
# 		CHENNAI|TAMIL NADU|IN
# 		VALUE D 0 14
# 		DELHI|DELHI|IN
# 		VALUE H 0 22
# 		HYDERABAD|TELAGANA|IN
# 		VALUE M 0 21
# 		MUMBAI|MAHARASHTRA|IN
#
# To get values greater than B but less than M, enter get @>B@<M:
#
# 		get @>B@<M
# 		VALUE C 0 21
# 		CHENNAI|TAMIL NADU|IN
# 		VALUE D 0 14
# 		DELHI|DELHI|IN
# 		VALUE H 0 22
# 		HYDERABAD|TELANGANA|IN
# 		END
#
# A maximum of two comparison operators can be parsed, one either a 'less than' (@<) or 'less than or equal to'(@<=) operator, and the
# other being either a 'greater than' (@>) or 'greater than or equal to'(@>=) operator.
#
# ANy additional operators are assumed to be part of the key. For example, if you issue a get command with three operators,
# the third operator (@>C) is treated as part of the key, and the get command searches for values smaller than M and greater
# than B@>C
#
# 		get @<M@>B@>C
# 		VALUE C 0 21
# 		CHENNAI|TAMIL NADU|IN
# 		VALUE D 0 14
# 		DELHI|DELHI|IN
# 		VALUE H 0 22
# 		HYDERABAD|TELANGANA|IN
#
# 15.19.5 SECURITY CONSIDERATIONS FOR THE INNODB MEMCACHED PLUGIN
#
# CAUTION:
#
# 		Consult this section before deploying the daemon_memcached plugin on a production server, or even on a test
# 		server if the MySQL instance contains sensitive data.
#
# Because memcached does not use an authentication mechanism by default, and the optional SASL authentication is not
# as strong as traditional DBMS security measures, only keep non-sensitive data in the MySQL instance that uses the
# daemon_memcached plugin, and wall off any servers that use this configuration from potentital intruders.
#
# Do not allow memcached access to these servers from the Internet; only allow access from within a firewall intranet,
# ideally from a subnet whose membership you can restrict.
#
# PASSWORD-PROTECTING MEMCACHED USING SASL
#
# SASL support provides the capability to protect your MySQL database from unauthenticated access through memcached
# clients.
#
# This section explains how to enable SASL with the daemon_memcached plugin. The steps are almost identical to those
# performed to enable SASL for a traditional memcached server.
#
# SASL stands for "SImple Authentication and Security Layer", a standard for adding authentication support to connection
# based protocols.
#
# memcached added SASL support in version 1.4.3
#
# SASL authentication is only supported with the binary protocol.
#
# memcached clients are only able to access InnoDB tables that are registered in the innodb_memcache.containers table.
#
# Even though a DBA can place access restrictions on such tables, acess through memcached applications cannot be controlled.
# For this reason, SASL support is provided to control access to InnoDB tables associated with the daemon_memcached plugin.
#
# The following section shows how to build, enable and test an SASL-enabled daemon_memcached plugin.
#
# BUILDING AND ENABLING SASL WITH THE INNODB MEMCACHED PLUGIN
#
# By default, an SASL-enabled daemon_memcached plugin is not included in MySQL release packages, since an SASL-enabled
# daemon_memcached plugin requires building memcached with SASL libraries.
#
# To enable SASL support, download the MySQL source and rebuild the daemon_memcached plugin after downloading the SASL
# libraries:
#
# 		1. Install the SASL development and utility libraries. For example, on Ubuntu, use apt-get to obtain the libraries:
#
# 			sudo apt-get -f install libsasl2-2 sasl2-bin libsasl2-2 libsasl2-dev libsasl2-modules
#
# 		2. Build the daemon_memcached plugin shared libraries with SASL capability by adding ENABLE_MEMCACHED_SASL=1 to your
# 			cmake options.
#
# 			memcached also provides simple cleartext password support, which facilitates testing. To enable simple cleartext
# 			password support, specify the ENABLE_MEMCACHED_SASL_PWDB=1 cmake option.
#
# 			In summary, add following three cmake options:
#
# 				cmake /etc/ -DWITH_INNODB_MEMCACHED=1 -DENABLE_MEMCACHED_SASL=1 -DENABLE_MEMCACHED_SASL_PWDB=1
#
# 		3. Install the daemon_memcached plugin, as described in SECTION 15.19.3, "SETTING UP THE INNODB MEMCACHED PLUGIN"
#
# 		4. Configure a user name and password file. (This example uses memcached simple cleartext password support)
#
# 			a. In a file, create a user named testname and define the password as testpasswd:

# 				echo "testname:testpasswd::::::::" >/home/jy/memcached-sasl-db
#
# 			b. Configure the MEMCACHED_SASL_PWDB environment variable to inform memcached of the user name and password file:
#
# 				export MEMCACHED_SASL_PWDB=/home/jy/memcached-sasl-db
#
# 			c. Inform memcached that a cleartext password is used:
#
# 				echo "mech_list: plain" > /home/jy/work2/msasl/clients/memcached.conf
# 				export SASL_CONF_PATH=/home/jy/work2/msasl/clients
#
# 		5. Enable SASL by restarting the MySQL server with the memcached -S option encoded in the daemon_memcached_option configuration parameter:
#
# 			mysqld /etc/ --daemon_memcached_option="-S"
#
# 		6. To test the setup, use an SASL-enabled client such as SASL-enabled libmemcached
#
# 			memcp --servers=localhost:11211 --binary --username=testname
# 				--password=password myfile.txt
#
# 			memcat --servers=localhost:11211 --binary --username=testname
# 				--password=password myfile.txt
#
# 			If you specify an incorrect user name or password, the operation is rejected with a memcache error AUTHENTICATION FAILURE message.
# 			In this case, examine the cleartext password set in the memcached-sasl-db file to verify that the credentials you supplied
# 			are correct.
#
# There are other methods to test SASL authentication with memcached, but the method described above is the most straightforward.
#
# 15.19.6 WRITING APPLICATIONS FOR THE INNODB MEMCACHED PLUGIN
#
# 15.19.6.1 ADAPTING AN EXISTING MYSQL SCHEMA FOR THE INNODB MEMCACHED PLUGIN
# 15.19.6.2 ADAPTING A MEMCACHED APPLICATION FOR THE INNODB MEMCACHED PLUGIN
# 15.19.6.3 TUNING INNODB MEMCACHED PLUGIN PERFORMANCE
# 15.19.6.4 CONTROLLING TRANSACTIONAL BEHAVIOR OF THE INNODB MEMCACHED PLUGIN
# 15.19.6.5 ADAPTING DML STATEMENTS TO MEMCACHED OPERATIONS
# 15.19.6.6 PERFORMING DML AND DDL STATEMENTS ON THE UNDERLYING INNODB TABLE
#
# Typically, writing an application for the InnoDB memcached plugin involves some degree of rewriting or adapting existing code
# that uses MySQL or the memcached API.
#
# 		) With the daemon_memcached plugin, instead of many traditional memcached servers running on low-powered machines, you have
# 			the same number of memcached servers as MySQL servers, running on relatively high-powered machines with substantial disk
# 			storage and memory.
#
# 			You might reuse some existing code that works with the memcached API, but adaptation is likely required due to the different
# 			server configuration.
#
# 		) The data stored through the daemon_memcached plugin goes into VARCHAR, TEXT or BLOB columns, and must be converted to do
# 			numeric operations.
#
# 			You can perform the conversion on the application side, or by using the CAST() function in queries.
#
# 		) Coming from a database background, you might be used to general-purpose SQL tables with many columns.
#
# 			The tables accessed by memcached code likely have only a few or even a single column holding data values.
#
# 		) You might adapt parts of your application that perform single-row queries, inserts, updates or deletes, to improve
# 			performance in critical sections of code.
#
# 			Both queries (read) and DML (write) operations can be substantially faster when performed through the InnoDB
# 			memcached interface.
#
# 			The performance improvement for writes is typically greater than the performance improvement for reads, so you might
# 			focus on adapting code that performs logging or records interactive choices on a website.
#
# The following sections explore these points in more detail.
#
# 15.19.6.1 ADAPTING AN EXISTING MYSQL SCHEMA FOR THE INNODB MEMCACHED PLUGIN
#
# Consider these aspects of memcached applications when adapting an existing MySQL schema or application to use the daemon_memcached plugin:
#
# 		) memcached keys cannot contain spaces or newlines, because these characters are used as separators in the ASCII protocol.
#
# 			If you are using lookup values that contain spaces, transform or hash them into values without spaces before using them
# 			as keys in calls to add(), set(), get() and so on. Although theoretically these characters are allowed in keys in programs
# 			that use the binary protocol, you should restrict the characters used in keys to ensure compatibility with a broad range of
# 			clients.
#
# 		) If there is a short numeric primary key column in an InnoDB table, use it as the unique lookup key for memcached by converting
# 			the integer to a string value.
#
# 			If the memcached server is used for multiple applications, or with more than one InnoDB table, consider modifying the name
# 			to ensure that it is unique.
#
# 			For example, prepend the table name, or the database name and the table name, before the numeric value.
#
# 			NOTE:
#
# 				The daemon_memcached plugin supports inserts and reads on mapped InnoDB tables that have an INTEGER defined
# 				as the primary key.
#
# 		) You cannot use a partitioned table for data queried or stored using memcached.
#
# 		) The memcached protocol passes numeric values around as strings. To store numeric values in the underlying InnoDB table,
# 			to implement counters that can be used in SQL functions such as SUM() or AVG(), for example:
#
# 			) Use VARCHAR columns with enough characters to hold all the digits of the largest expected number (and additional characters
# 				if appropriate for the negative sign, decimal point or both)
#
# 			) In any query that performs arithmetic using column values, use the CAST() function to convert the values from string to integer,
# 				or to some other numeric type.
#
# 				For example:
#
# 					#Alphabetic entries are returned as zero
#
# 					SELECT CAST(c2 AS UNSIGNED integer) FROM demo_test;
#
# 					# Since there could be numeric values of 0, can't disqualify them
# 					# Test the string values to find the ones that are integers, and average only those.
#
# 					SELECT AVG(cast(c2 AS UNSIGNED integer)) FROM demo_test
# 						WHERE c2 BETWEEN '0' and '9999999999';
#
# 					# Views let you hide the complexity of queries. The results are already converted,
# 					# no need to repeat conversion functions and WHERE clauses each time
#
# 					CREATE VIEW numbers AS SELECT c1 KEY, CAST(c2 AS UNSIGNED INTEGER) val
# 						FROM demo_test WHERE c2 BETWEEN '0' and '99999999';
# 					SELECT SUM(val) FROM numbers;
#
# 				NOTE:
#
# 					ANy alphabetic values in the result set are converted into 0 by the call to CAST(). When using functions
# 					such as AVG(), which depend on the number of rows in the result set, include WHERE clauses to filter out
# 					non-numeric values.
#
# 		) If the InnoDB column used as a key could have values longer than 250 bytes, hash the value to less than 250 bytes.
#
# 		) To use an existing table with the daemon_memcached plugin, define an entry for it in the innodb_memcache.containers table.
#
# 			To make that table the default for all memcached requests, specify a value of default in the name column, then restart
# 			the MySQL server to make the change take effect.
#
# 			If you use multiple tables for different classes of memcached data, set up multiple entries in the innodb_memcache.containers
# 			table with name values of your choice, then issue a memcached request in the form of get @@name or set @@name within the 
# 			application to specify the table to be used for subsequent memcached requests.
#
# 			For an example of using a table other than the predefined test.demo_test table, see EXAMPLE 15.13, "USING YOUR OWN TABLE WITH AN INNODB MEMCACHED APPLICATION"
#
# 			For the required table layout, see SECTION 15.19.8, "INNODB MEMCACHED PLUGIN INTERNALS"
#
# 		) To use multiple InnoDB table column values with memcached key-value pairs, specify column names separated by comma, semicolon,
# 			space, or pipe characters in the value_columns field of the innodb_memcache.containers entry for the InnoDB table.
#
# 			For example, specify col1, col2, col3 or col1|col2|col3 in the value_columns field.
#
# 			Concatenate the column values into a single string using the pipe character as a separator before passing the string to
# 			memcached add or set calls.
#
# 			The string is unpacked automatically into the correct column.
#
# 			Each get call returns a single string containing the column values that is also delimited by the pipe character.
#
# 			You can unpack the values using the appropriate language syntax.
#
# EXAMPLE 15.13 USING YOUR OWN TABLE WITH AN INNODB MEMCACHED APPLICATION
#
# This example shows how to use your own table with a sample Python application that uses memcached for data manipulatiton.
#
# The example assumes that the daemon_memcached plugin is installed as described in SECTION 15.19.3, "SETTING UP THE INNODB 
# MEMCACHED PLUGIN"
#
# It also assumes that your system is configured to run a Python script that uses the python-memcache module.
#
# 		1. Create the multicol table which stores country information including population area, and driver side data
# 			('R' for right and 'L' for left)
#
# 				mysql> USE test;
#
# 				mysql> CREATE TABLE `multicol` (
# 					`country` varchar(128) NOT NULL DEFAULT '',
# 					`population` varchar(10) DEFAULT NULL,
# 					`area_sq_km` varchar(9)  DEFAULT NULL,
# 					`drive_side` varchar(1)  DEFAULT NULL,
# 					`c3` int(11) DEFAULT NULL,
# 					`c4` bigint(20) unsigned DEFAULT NULL,
# 					`c5` int(11) DEFAULT NULL,
# 					PRIMARY KEY (`country`)
# 					) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
#
# 		2. Insert a record into the innodb_memcache.containers table so that the daemon_memcached plugin can access the multicol table.
#
# 				mysql> INSERT INTO innodb_memcache.containers
# 						 (name,db_schema,db_table,key_columns,value_columns,flags,cas_column,
# 						 expire_time_column,unique_idx_name_on_key)
# 						 VALUES
# 						 ('bbb','test','multicol','country','population,area_sq_km,drive_side',
# 						 'c3','c4','c5','PRIMARY');
# 				mysql> COMMIT;
#
# 			) The innodb_memcache.containers record for the multicol table specifies a name value of 'bbb', which is the table identifier.
#
# 				NOTE:
#
# 					If a single InnoDB table is used for all memcached applications, the name value can be set to default to avoid
# 					using @@ notation to switch tables.
#
# 			) The db_schema column is set to test, which is the name of the database where the multicol table resides.
#
# 			) The db_table column is set to multicol, which is the name of the InnoDB table
#
# 			) key_columns is set to the unique country column. The country column is defined as the primary key in the multicol table definition.
#
# 			) Rather than a single InnoDB table column to hold a composite data value, data is divided among three table columns (population, area_sq_km,
# 				and drive_side)
#
# 				To accomodate multiple value columns, a comma-separated list of columns is specified in the value_columns field. The columns defined
# 				in the value_columns field are the columns used when storing or retrieving values.
#
# 			) Values for the flags, expire_time, and cas_column fields are based on values used in the demo.test sample table. These fields are typically
# 				not significant in applications that use the daemon_memcached plugin because MySQL keeps data synchronized, and there is no need
# 				to worry about data expiring or becoming stale.
#
# 			) The unique_idx_name_on_key field is set to PRIMARY, which refers to the primary index defined on the unique country column in the multicol table.
#
# 		3. Copy the sample Python application into a file. In this example, the sample script is copied to a file named multicol.py
#
# 			The sample Python application inserts data into the multicol table and retrieves data for all keys, demonstrating how to access an InnoDB
# 			table through the daemon_memcached plugin.
#
# 				import sys, os
# 				import memcache
#
# 				def connect_to_memcached():
# 					memc = memcache.Client(['127.0.0.1:11211'], debug=0);
# 					print 'Connected to Memcached.'
# 					return memc
#
# 				def banner(message):
# 					print
# 					print "=" * len(message)
# 					print message
# 					print "=" * len(message)
#
# 				country_data = [
# 				("Canada", //etc, values//)
#				]
#
# 				def switch_table(memc,table):
# 					key = "@@" + table
# 					print "Switching default table to '" + table "' by issuing GET for '" + key + "'."
# 					result = memc.get(key)
#
# 				def insert_country_data(memc):
# 					banner("Inserting initial data via memcached interface")
# 					for item in country_data:
# 						country = item[0]
# 						population = item[1]
# 						area = item[2]
# 						drive_side = item[3]
#
# 						key = country
# 						value = "|".join([population,area,drive_side])
# 						print "Key = " + key
# 						print "Value = " + value
#
# 						if memc.add(key,value):
# 							print "Added new key, value pair."
# 						else:
# 							print "Updating value for existing key"
# 							memc.set(key,value)
#
# 				def query_country_data(memc):
# 					banner("Retrieving data for all keys (country names)")
# 					for item in country_data:
# 						key = item[0]
# 						result = memc.get(key)
# 						print "Here is the result retrieved from the database for key " + key + ":"
# 						print result
# 						(m_population, m_area, m_drive_side) = result.split("|")
# 						print "Unpacked population value: " + m_population
# 						print "Unpacked area value 	  : " + m_area
# 						print "Unpacked drive side value: " + m_drive_side
#
# 				if __name__ == '__main__':
# 
# 					memc = connect_to_memcached()
# 					switch_table(memc, "bbb")
# 					insert_country_data(memc)
# 					query_country_data(memc)
# 
# 					sys.exit(0)
#
# Sample Python application notes:
#
# 		) No database authorization is reuqired ot run the app, since data manipulation is performed through the memcached interface.
#
# 			The only required information is the port number on the local system where the memcached daemon listens.
#
# 		) To make sure the application uses the multicol table, the switch_table() function is called, which performs a dummy get or set
# 			request using @@ notation.
#
# 			The name value in the request is bbb, which is the multicol table identifier defined in the innodb_memcache.containers.name field
#
# 			A more descriptive name value might be used in a real-world application. This example simply illustrates that a table identifier
# 			is specified rather than the table name in get @@/etc/ requests.
#
# 		) The utility functions used to insert and query data demonstrate how to turn a Python data structure into pipe-separated values for
# 			sending data to MySQL with add or set requests, and how to unpack the pipe-separated values returned by get requests.
#
# 			This extra processing is only required when mapping a single memcached value to multiple MySQL table columns.
#
# 4. Run the sample Python app
#
# 		shell> python multicol.py
#
# If successful, the sample application returns this output:
#
# 		Connected to memcached
# 		Switching default table to 'bbb' by issuing GET for '@@bbb'
#
# 		===================
# 		Inserting initial data via memcached interface
# 		===================
# 		Key = Canada
# 		Value = 34820000|9984670|R
# 		Added new key, value pair
# 		Key = USA
# 		//etc.//
#
# 		===================
# 		Retrieving data for all keys (country names)
# 		===================
# 		//ETC//
#
# 5. Query the innodb_memcache.containers table to view the record you inserted earlier for the multicol table.
#
# The first record is the sample entry for the demo_test table that is created during the initial daemon_memcached
# plugin setup.
#
# The second record is the entry you inserted for the multicol table.
#
# 		mysql> SELECT * FROM innodb_memcache.containers\G
# 		*********************** 1. row *******************
# 						name: aaa
# 				db_schema : test
# 				db_table  : demo_test
# 			key_columns  : c1
# 			value_columns: c2
# 					flags  : c3
# 			cas_column   : c4
# 	 expire_time_column: c5
#unique_idx_name_on_key: PRIMARY
#
# 		*********************** 2. row *********************
# 						name: bbb
# 				db_schema : test
# 				db_table  : multicol
# 			  key_columns: country
# 			value_columns: population,area_sq_km,drive_side
# 					flags  : c3
# 			cas_column   : c4
# 	 expire_time_column: c5
#unique_idx_name_on_key: PRIMARY
#
# 6. Query the multicol table to view data inserted by the sample Python application. The data is available for MySQL queries,
# 		which demonstrates how the same data can be accessed using SQL or through applications (using the appropriate MySQL Connector or API)
#
# 			mysql> SELECT * FROM test.multicol;
# 			+----------+-----------------+--------------+--------------+----------+-----------+-----------+
# 			| country  | population 	  | area_sq_km   | drive_side   | c3 	    | c4 		 | c5 		 |
# 			+----------+-----------------+--------------+--------------+----------+-----------+-----------+
# 			| Canada   | //values//
#
# NOTE:
#
# 		Always allow sufficient size to hold necessary digits, decimal points, sign characters, leading zeros,
# 		and so on when defining the length for columns that are treated as numbers.
#
# 		Too long values in a string column such as a VARCHAR are truncated by removing some characters,
# 		which could produce nonsensical numeric values
#
# 7. Optionally, run report-type queries on the InnoDB table that stores the memcached data.
#
# 		YOu can produce reports through SQL queries, performing calculations and tests across any columns,
# 		not just the country key column.
#
# 		(Because the following examples use data from only a few countries, the numbers are for illustration purposes only)
#
# 		The following queries return the average population of countries where people drive on the right, and the average size
# 		of countries whose names start with "U"
#
# 			mysql> SELECT AVG(population) FROM multicol WHERE drive_side = 'R';
# 			+----------------+
# 			| avg(population)|
#			+----------------+
# 			| //value//
#
# 			mysql> SELECT SUM(area_sq_km) FROM multicol WHERE country LIKE '%';
# 			+-----------------+
# 			| sum(area_sq_km) |
# 			+-----------------+
# 			| //value//
#
# 		Because the population and area_sq_km columns store character data rather than strongly typed numeric data,
# 		functions such as AVG() and SUM() work by converting each value to a number first.
#
# 		This approach does not work for operators such as < or >, for example, when comparing character-based values,
# 		9 > 1000, which is not expected from a clause such as ORDER BY population DESC 
#
# 		For the most accurate type treatment, perform queries against views that cast numeric columns to the appropriate types.
#
# 		This technique lets you issue simple SELECT * queries from database applications, while ensuring that casting, filtering
# 		and ordering is correct.
#
# 		The following example shows a view that can be queried to find the top three countries in descending order of population,
# 		with the results reflecting the latest data in the multicol table, and with population and area figures treated as numbers:
#
# 			mysql> CREATE VIEW populous_countries AS
# 					 SELECT
# 					 country,
# 					 cast(population as unsigned integer) population,
# 					 cast(area_sq_km as unsigned integer) area_sq_km,
# 					 drive_side FROM multicol
# 					 ORDER BY CAST(population as unsigned integer) DESC
# 					 LIMIT 3;
#
# 			mysql> SELECT * FROM populous_countries;
# 			+-----------+---------------+------------+---------------+
# 			| country 	| population 	 | area_sq_km | drive_side 	|
# 			+-----------+---------------+------------+---------------+
# 			| China 	   | 	/values//
#
# 			mysql> DESC populous_countries;
# 			+-----------+---------------+-------+--------+----------+----------+
# 			| Field 	   | Type 			 | Null  | Key 	| Default  | Extra 	 |
# 			+-----------+---------------+-------+--------+----------+----------+
# 			| //ETC//
#
# 15.19.6.2 ADAPTING A MEMCACHED APPLICATION FOR THE INNODB MEMCACHED PLUGIN
#
# Consider these aspects of MySQL and InnoDB tables when adapting existing memcached applications to use the daemon_memcached plugin:
#
# 		) If there are key values longer than a few bytes, it may be more efficient to use a numeric auto-increment column as the primary
# 			key of the INnoDB table, and to create a unique secondary index on the column that contains the memcached key values.
#
# 			This is because InnoDB performs best for large-scale insertions if primary key values are added in sorted order (as they are with
# 			auto-increment values)
#
# 			Primary key values are included in secondary indexes, which takes up unnecessary space if the primary key is a long string value.
#
# 		) If you store several different classes of information using memcached, consider setting up a separate InnoDB table for each type
# 			of data.
#
# 			Define additional table identifiers in the innodb_memcache.containers table, and use the @@table_id.key notation to store and
# 			retrieve items from different tables.
#
# 			Physically dividing different types of information allows you to tune the characteristics of each table for optimum space utilization,
# 			performance and reliability.
#
# 			For example, you might enable compression for a table that holds blog posts, but not for a table that holds thumbnail images.
#
# 			You might back up one table more frequently than another because it holds critical data.
#
# 			You might create additional secondary indexes on tables that are frequently used to generate reports using SQL.
#
# 		) Preferably, configure a stable set of table definitions for use with the daemon_memcached plugin, and leave the tables
# 			in place permanently.
#
# 			Changes to the innodb_memcache.containers table take effect the next time the innodb_memcache.containers table is queried.
# 			Entries in the containers table are processed at startup, and are consulted whenever an unrecognized table identifier
# 			(as defined by containers.name) is requested using @@ notation.
#
# 			Thus, new entries are visible as soon as oyu use the associated table identifier, but changes to existing entries
# 			require a server restart before they take effect.
#
# 		) When you use the default innodb_only caching policy, calls to add(), set(), incr() and so on can succeed but still
# 			trigger debugging messages such as while expecting 'STORED', got unexpected response 'NOT_STORED'
#
# 			Debug messages occur because new and updated values are sent directly to the INnoDB table without being saved
# 			in the memory cache, due to the innodb_only caching policy.
#
# 15.19.6.3 TUNING INNODB MEMCACHED PLUGIN PERFORMANCE
#
# Because using InnoDB in combination with memcached involves writing all data to disk, whether immediately or sometimes
# later, raw performance is expected to be somewhat slower than using memcached by itself.
#
# When using the InnoDB memcached plugin, focus tuning goals for memcached operations on achieving better performance
# than equivalent SQL operations.
#
# Benchmarks suggest that queries and DML operations (inserts, updates and deletes) that use the memcached interface
# are faster than traditional SQL.
#
# DML operations typically see larger improvements. Therefore, consider adapting write-intensive applications to use the
# memcached interface first. Also consider prioritizing adaptation of write-intensive applications that use fast, lightweight
# mechanisms that lack reliability.
#
# ADAPTING SQL QUERIES
#
# The types of queries that are most suited to simple GET requests are those with a single clause or a set of AND conditions
# in the WHERE clause:
#
# 		SQL:
# 		SELECT col FROM tbl WHERE key = '/value/';
#
# 		memcached:
# 		get /value/
#
# 		SQL:
# 		SELECT col FROM tbl WHERE col1 = val1 and col2 = val2 and col3 = val3;
#
# 		memcached:
# 		# since you must always know these 3 values to look up the key,
# 		# combine them into a unique string and use that as the key
# 		# for all ADD, SET and GET operations.
# 		key_value = val1 + ":" + val2 + ":" + val3
# 		get key_value
#
# 		SQL:
# 		SELECT 'key exists' FROM tbl
# 			WHERE EXISTS (SELECT col1 FROM tbl WHERE KEY = '/value/') LIMIT 1;
#
# 		memcached:
# 		# Test for existence of key by asking for its value and checking if the call succeeds,
# 		# ignoring the value itself. For existence checking, you typically only store a very
# 		# short value such as "1"
# 		get key_value
#
# USING SYSTEM MEMORY
#
# 	For best performance, deploy the daemon_memcached plugin on machines that are configured as typical database servers,
# 	where the majority of system RAM is devoted to the InnoDB Buffer pool, through the innodb_buffer_pool_size configuration
# 	option.
#
# 	For systems with multi-gigabyte buffer pools, consider raising the value of innodb_buffer_pool_instances for maximum
# 	throughput when most operations involve data that is already cached in memory.
#
# REDUCING REDUNDANT I/O
#
# InnoDB has a number of settings that let you choose the balance between high reliability, in case of a crash, and the amount
# of I/O overhead during high write workloads.
#
# For example, consider setting the innodb_doublewrite to 0 and innodb_flush_log_at_trx_commit to 2.
#
# Measure performance with different innodb_flush_method settings.
#
# For other ways to reduce or tune I/O table operations, see SECTION 8.5.8 "OPTIMIZING INNODB DISK I/O"
#
# REDUCING TRANSACTIONAL OVERHEAD
#
# a default value of 1 for daemon_memcached_r_batch_size and daemon_memcached_w_batch_size is intended for maximum reliability
# of results and safety of stored or updated data.
#
# Depending on the type of application, you might increase one or both of these settings to reduce the overhead of frequent
# commit operations.
#
# On a busy system, you might increase daemon_memcached_r_batch_size, knowing that changes to data made through SQL may not become
# visible to memcached immediately (that is, until N more get oeprations are processed).
#
# When processing data where every write operation must be reliably stored, leave daemon_memcached_w_batch_size set to 1.
#
# Increase the setting when processing large numbers of updates intended only for statistical analysis, where losing the last
# N updates in a crash is an acceptable risk.
#
# For example, a system that monitors traffic crossing a bridge, recording data for approx 100K vehicles each day.
#
# If the application counts different types of vehicles to analyze traffic patterns, chagning daemon_memcached_w_batch_size
# from 1 to 100 reduces I/O overhead for commit operations by 99%. 
#
# In case of an outage, a maximum of 100 records are lost, which may be an acceptable margin of error. If instead the application
# performed automated toll collection for each car, you would set daemon_memcached_w_batch_size to 1 to ensure that each toll
# record is immediately saved to disk.
#
# Because of the way InnoDB organizes memcached key values on disk, if you have a large number of keys to create, it may be faster
# to sort the data items by key value in the application and add them in sorted order, rather than create keys in arbitrary order.
#
# The memslap command, which is part of the regular memcached distirbution but not included with the daemon_memcached plugin, 
# can be useful for benchmarking different configurations.
#
# It can also be used to generate sample key-value pairs to use in your own benchmarks. see LIBMEMCACHED command-line utilities for details.
#
# 15.19.6.4 CONTROLLING TRANSACTIONAL BEHAVIOR OF THE INNODB MEMCACHED PLUGIN
#
# UNlike traditional memcached, the daemon_memcached plugin allows you to control durability of data values produced through
# calls to add, set, incr and so on.
#
# By default, data written through the memcached interface is stored to disk, and calls to get return the most recent value from disk.
#
# Although the default behavior does not offer the best possible raw performance, it is still fast compared to the SQL interface
# for InnoDB tables.
#
# As you gain exp. using the daemon_memcached plugin, you can consider relaxing durability settings for non-critical classes of data,
# at the risk of losing some updated values in the event of an outage, oir returning data that is slightly out-of-date.
#
# FREQUENCY OF COMMITS
#
# One tradeoff between durability and raw perf. is how frequently new and changed data is committed. If data is critical, it should be
# committed immediately so that it is safe in case of a crash or outage.
#
# If data is less critical, such as counters that are reset after a crash or logging data that you can afford to lose, you might
# prefer higher raw throughput that is available with less frequent commits.
#
# When a memcached operation inserts, updates, or deletes data in the underlying InnoDB table, the change might be committed to the
# InnoDB table instantly (if daemon_memcached_w_batch_size=1) or some time later (if the daemon_memcached_w_batch_size value is greater
# than 1).
#
# In either case, the change cannot be rolled back.
#
# If oyu increase the value of daemon_memcached_w_batch_Size to avoid high I/O overhead during busy times,commits could become
# infrequent when the workload decreases.
#
# As a safety measure, a background thread automatically commits changes made through the memcached API at regular intervals.
#
# The interval controlled by the innodb_api_bk_commit_interval configuration option, which has a default setting of 5 seconds.
#
# When a memcached operation inserts or updates data in the underlying InnoDB table, the changed data is immediately visible
# to other memcached requests because the new value remains in the memory cache, even if it is not yet committed on the MySQL side.
#
# TRANSACTION ISOLATION
#
# When a memcached operation such as get or incr causes a query or DML operation on the underlying InnoDB table, you can control
# whether the operation sees the very latest data written to the table, only data that has been committed, or other variations
# of transaction isolation level.
#
# Use the innodb_api_trx_level configuration option to control this feature.
#
# The numeric values specified for this option correspond to isoaltion levels such as REPEATABLE_READ. See the description
# of the innodb_api_trx_level option for information about other settings.
#
# A strict isolation level ensures that data you retrieve is not rolled back or changed suddenly causing subsequent queries
# to return different values.
#
# However, strict isolation levels require greater locking overhead, which can cause waits. For a NoSQL-style application
# that does not use long-running transactions, you can typically use the default isolation level or switch to a less
# strict isolation level.
#
# DISABLING ROW LOCKS FOR MEMCACHED DML OPERATIONS
#
# The innodb_api_disable_rowlock option can be used to disable row locks when memcached requests through the daemon_memcached
# plugin causes DML operations.
#
# By default, innodb_api_disable_rowlock is set to OFF which means that memcached requests row locks for get and set operations.
#
# When innodb_api_disable_rowlock is set to ON, memcached requests a table lock instead of row locks.
#
# The innodb_api_disable_rowlock option is not dynamic. It must be specified at startup on the mysqld command line or entered
# in a MySQL configuration file.
#
# ALLOWING OR DISALLOWING DDL
#
# By default, you can perform DDL operations such as ALTER_TABLE on tables used by the daemon_memcached plugin.
#
# To avoid potential slowdowns when these tables are used for high-throughput applications, disable DDL operations
# on these tables by enabling innodb_api_enable_mdl at startup.
#
# THis option is less appropriate when accessing the same tables through both memcached and SQL, because it blocks
# CREATE_INDEX statements on the tables, which could be important for running reporting queries.
#
# STORING DATA ON DISK, IN MEMORY OR BOTH
#
# The innodb_memcache.cache_policies table specifies whether to store data written through the memcached interface to disk
# (innodb_only, the default); in memory only, as with traditional memcached (cache_only); or both (caching).
#
# With the caching setting, if memcached cannot find a key in memory, it searches for the value in an InnoDB table.
#
# Values returned from get calls under the caching setting could be out-of-date if the values were updated on disk
# in the InnoDB table but are not yet expired from the memory cache.
#
# The caching policy can be set independently for get, set (including incr and decr), delete and flush operations.
#
# For example, you might allow get and set operations to query or update a table and the memcached memory cache at the
# same time (using the caching setting), while making delete, flush or both operate only on the in-memory copy
# (using the cache_only setting).
#
# That way, deleting or flushing an item only expires the item from the cache, and the latest value is returned
# from the InnoDB table the next time the item is requested.
#
# 		mysql> SELECT * FROM innodb_memcache.cache_policies;
# 		+---------------+--------------+--------------+----------------+----------------+
# 		| policy name   | get_policy   | set_policy 	 | delete_policy  | flush_policy   |
# 		+---------------+--------------+--------------+----------------+----------------+
# 		| cache_policy  | innodb_only  | innodb_only  | innodb_only    | innodb_only 	  |
# 		+---------------+--------------+--------------+----------------+----------------+
#
# 		mysql> UPDATE innodb_memcache.cache_policies SET set_policy = 'caching'
# 				 WHERE policy_name = 'cache_policy';
#
# innodb_memcache.cache_policies values are only read at startup. After changing values in this table, uninstall and 
# reinstall the daemon_memcached plugin to ensure that changes take effect.
#
# 		mysql> UNINSTALL PLUGIN daemon_memcached;
#
# 		mysql> INSTALL PLUGIN daemon_memcached soname "libmemcached.so";
#
# 15.19.6.5 ADAPTING DML STATEMENTS TO MEMCACHED OPERATIONS
#
# Benchmarks suggest that the daemon_memcached plugin speeds up DML operations (inserts, updates and deletes) more than it speeds
# up queries.
#
# Therefore, consider focusing initial development efforts on write-intensive applications that are I/O bound, and look for opportunities
# to use MySQL with the daemon_memcached plugin for new write-intensive applications.
#
# Single-row DML statements are the easiest types of statements to turn into memcached operations. INSERT becomes add, UPDATE becomes set,
# incr or decr, and DELETE becomes delete.
#
# These operations are guaranteed to only affect one row when issued through the memcached interface, because the key is unique within the table.
#
# In the following SQL examples, t1 refers to the table used for memcached operations, based on the configuration in the innodb_memcache.containers
# table.
#
# Key refers to the column listed under key_columns, and val refers to the column listed under value_columns.
#
# 		INSERT INTO t1 (key,val) VALUES (some_key,some_value);
# 		SELECT val FROM t1 WHERE key = some_key;
# 		UPDATE t1 SET val = new_value WHERE key = some_key;
# 		UPDATE t1 SET val = val + x WHERE key = some_key;
# 		DELETE FROM t1 WHERE KEY = some_key;
#
# The following TRUNCATE_TABLE and DELETE statements, which remove all rows from teh table, correspond to the flush_all operation,
# where t1 is configured as the table for memcached operations, as in the previous example.
#
# 		TRUNCATE TABLE t1;
# 		DELETE FROM t1;
#
# 15.19.6.6 PERFORMING DML AND DDL STATEMENTS ON THE UNDERLYING INNODB TABLE
#
# You can access the underlying InnoDB table (which is test.demo_test by default) through standard SQL interfaces.
# However, there are some restrictions:
#
# 		) When querying a table that is also accessed through the memcached interface, remember that memcached operations
# 			can be configured to the committed periodically rather than after every write operation.
#
# 			This behavior is controlled by the daemon_memcached_w_batch_size option.
#
# 			If this option is set to a value greater than 1, use READ_UNCOMMITTED queries to find rows that were just inserted.
#
# 				mysql> SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
#
# 				mysql> SELECT * FROM demo_test;
# 				+---------+---------+---------+----------+-----------+-----------+---------+---------+---------+---------+----------+
# 				| cx 	    | cy 	  | c1 		| cz 		  | c2 		  | ca 	 		| CB 		 | c3 	  | cu 		| c4 	    | C5 		|
# 				+---------+---------+---------+----------+------------+-----------+---------+---------+---------+---------+----------+
# 				| NULL 	 | NULL 	  | a11 	   | NULL 	  | 123456789  | NULL 		| NULL 	 | 10 	  | NULL 	| 3 		 | NULL 		|
# 				+---------+---------+---------+----------+------------+-----------+---------+---------+---------+---------+----------+
#
# 		) When modifying a table using SQL that is also accessed through the memcached interface, you can configure memcached operations
# 			to start a new transaction periodically rather than for every read operation.
#
# 			This behavior is controlled by the daemon_memcached_r_batch_size option.
#
# 			If this option is set to a value greater than 1, changes made to the table using SQL are not immediately visible to memcached operations.
#
# 		) The InnoDB table is either IS (intention shared) or IX (intention exclusive) locked for all operations in a transaction.
#
# 			If you increase daemon_memcached_r_batch_size and daemon_memcached_w_batch_size substantially from their default value of 1,
# 			the table is most likely locked between each operation, preventing DDL statements on the table.
#
# 15.19.7 THE INNODB MEMCACHED PLUGIN AND REPLICATION
#
# Because the daemon_memcached plugin supports the MySQL binary log, updates made on a master server through the memcached
# interface can be replicated for backup, balancing intensive read workloads, and high availability.
#
# All memcached commands are supported with binary logging.
#
# You do not need to set up the daemon_memcached plugin on slave servers. The primary advantage of htis configuration is increased
# write throughput on the master.
#
# The speed of the replication mechanism is not affected.
#
# The following sections show how to use the binary log capability when using the daemon_memcached plugin with MySQL replication.
#
# It is assumed that you have completed the setup described in SECTION 15.19.3, "SETTING UP THE INNODB MEMCACHED PLUGIN"
#
# ENABLING THE INNODB MEMCACHED BINARY LOG
#
# 	1. TO use the daemon_memcached plugin with the MySQL binary log, enable the innodb_api_enable_binlog configuration option
# 		on the master server.
#
# 		This option can only be set at server startup. You must also enable the MySQL binary log on the master server using the
# 		--log-bin option. YOu can add these options to the MySQL configuration file, or on the mysqld command line.
#
# 			mysqld /etc/ --log-bin --innodb_api_enable_binlog=1
#
# 	2. Configure the master and slave server, as described in SECTION 17.1.2, "SETTING UP BINARY LOG FILE POSITION BASED REPLICATION"
#
# 	3. Use mysqldump to create a master data snapshot, and sync the snapshot to the slave server.
#
# 			master shell> mysqldump --all-databases --lock-all-tables > dbdump.db
# 			slave shell> mysql < dbdump.db
#
# 	4. On the master server, issue SHOW_MASTER_STATUS to obtain the master binary log coordinates.
#
# 		mysql> SHOW MASTER STATUS;
#
# 	5. On the slave server, use a CHANGE_MASTER_TO statement to set up a slave server using the master binary log coordinates.
#
# 		mysql> CHANGE MASTER TO
# 				 MASTER_HOST='localhost',
# 				 MASTER_USER='root'
# 				 MASTER_PASSWORD='',
# 				 MASTER_PORT=13000,
# 				 MASTER_LOG_FILE='0.0000001',
# 				 MASTER_LOG_POS=114;
#
# 	6. Start the slave
#
# 		mysql> START SLAVE;
#
#  	If the error log prints output similar to the following, the slave is ready for replication.
#
# 			2013-09-24T13:04:38./etc/ [Note] Slave I/O thread: connected to
# 			master 'root@localhost:13000', replication started in log '0.0000001'
# 			at position 114
#
# TESTING THE INNODB MEMCACHED REPLICATION CONFIGURATION
#
# This example demonstrates how to test the InnoDB memcached replication configuration using the memcached and telnet to insert,
# update and delete data.
#
# A MySQL client is used to verify results on the master and slave servers.
#
# The example uses the demo_test table, which was created by the innodb_memcached_config.sql configuration script during the
# initial setup of the daemon_memcached plugin.
#
# The demo_test table contains a single example record.
#
# 	1. Use the set command to insert a record with a key of test1, a flag value of 10, an expiration value of 0, a cas value of 1, and a value of t1.
#
# 		telnet 127.0.0.1 11211
# 		Trying 127.0.0.1
# 		Connected to 127.0.0.1
# 		Escape character is '^]'
# 		set test1 10 0 1
# 		t1 
# 		STORED
#
# 	2. On the master server, check that the record was inserted into the demo_test table.
#
# 		Assuming the demo_test table was not previously modified, there should be two records.
#
# 		The example record with a key of AA, and the record you just inserted, with a key of test1. The c1
# 		column maps to the key, the c2 column to the value, the c3 column to the flag value, the c4 column
# 		to the cas value, and the 5 column to the expiration time.
#
# 		The expiration time was set to 0, since it is unused.
#
# 			mysql> SELECT * FROM test.demo_test;
# 			+---------+------------------+----------+---------+-----------+
# 			| c1 		 | c2 				  | c3 		 | c4 	  | c5 		  |
# 			+---------+------------------+----------+---------+-----------+
# 			| AA 		 | HELLO, HELLO 	  | 8 		 | 0 		  | 0 		  |
# 			| test1 	 | t1 				  | 10 		 | 1 		  | 0 		  |
# 			+---------+------------------+----------+---------+-----------+
#
# 	3. Check to verify that the same record was replicated to the slave server.
#
# 			mysql> SELECT * FROM test.demo_test;
# 			+---------+-----------------+-------------+------------+---------+
# 			| c1 		 | c2 				 | c3 			| c4 			 | c5 	  |
# 			+---------+-----------------+-------------+------------+---------+
# 			| AA 		 | HELLO, HELLO 	 | 8 				| 0 			 | 0 		  |
# 			| test1 	 | t1 				 | 10 			| 1 			 | 0 		  |
# 			+---------+-----------------+-------------+------------+---------+
#
# 	4. Use the set command to update the key to a value of new
#
# 			telnet 127.0.0.1 11211
# 			Trying 127.0.0.1 
# 			Connected to 127.0.0.1
# 			Escape character is '^]'
# 			set test1 10 0 2
# 			new
# 			STORED
#
# 			The update is replicated to the slave server (notice that the cas value is also updated)
#
# 				mysql> SELECT * FROM test.demo_test;
# 				+------------+--------------+--------+-------+--------+
# 				| c1 			 | c2 			 | c3 	 | c4 	| c5 	   |
# 				+------------+--------------+--------+-------+--------+
# 				| AA 			 | HELLO, HELLO | 8 		 | 0 		| 0 		|
# 				| test1 		 | new 			 | 10 	 | 2 		| 0 		|
# 				+------------+--------------+--------+-------+--------+
#
# 	5. Delete the test1 record using a delete command
#
# 			telnet 127.0.0.1 11211
# 			Trying 127.0.0.1
# 			Connected to 127.0.0.1
# 			Escape character is '^]'
# 			delete test1
# 			DELETED
#
# 		When the delete operation is replicated to the slave, the test1 record on the slave
# 		is also deleted.
#
# 			mysql> SELECT * FROM test.demo_test;
# 			+------------+----------------+------------+----------+------------+
# 			| c1 			 | c2 				| c3 			 | c4 		| c5 			 |
# 			+------------+----------------+------------+----------+------------+
# 			| AA 			 | HELLO, HELLO 	| 8 			 | 0 			| 0 			 |
# 			+------------+----------------+------------+----------+------------+
#
# 	6. Remove all rows from the table using the flush_all command
#
# 			telnet 127.0.0.1 11211
# 			Trying 127.0.0.1
# 			Connected to 127.0.0.1
# 			Escape character is '^]'
# 			flush_all
# 			OK
#
# 			mysql> SELECT * FROM test.demo_test;
# 			Empty set (0.00 sec)
#
# 	7. Telnet to the master server and enter two new records.
#
#
# 			telnet 127.0.0.1 11211
# 			Trying 127.0.0.1
# 			Connected to 127.0.0.1
# 			Escape character is '^]'
# 			set test2 10 0 4
# 			again
# 			STORED
# 			set test3 10 0 5
# 			again1
# 			STORED
#
# 	8. Confirm that the two records were replicated to the slave server
#
# 		mysql> SELECT * FROM test.demo_test;
# 		+--------+--------------+----------+--------+---------+
# 		| c1 	   | c2 			   | c3 		  | c4 	  | c5 		|
# 		+--------+--------------+----------+--------+---------+
# 		| test2  | again 			| 10 		  | 4 	  | 0 		|
# 		| test3 	| again1 		| 10 		  | 5 	  | 0 		|
# 		+--------+--------------+----------+--------+---------+
#
# 	9. Remove all rows from the table using the flush_all command
#
# 		telnet 127.0.0.1 11211
# 		Trying 127.0.0.1
# 		Connected to 127.0.0.1
# 		Escape character is '^]'
# 		flush_all
# 		OK
#
# 	10. Check to ensure that the flush_all operation was replicated on the slave server
#
# 		mysql> SELECT * FROM test.demo_test;
# 		Empty set (0.00 sec)
#
# INNODB MEMCACHED BINARY LOG NOTES
#
# Binary Log Format:
#
# 		) Most memcached operations are mapped to DML statements (analogous to insert, delete, update). Since there is no actual
# 			SQL statement being processed by the MySQL server, all memcached commands (except for flush_all) use Row-based replication
# 			(RBR) logging, which is independent of any server binlog_format setting.
#
# 		) The memcached flush_all command is mapped to the TRUNCATE_TABLE command in MySQL 5.7 and earlier.
#
# 			Since DDL commands can only use statement-based logging, the flush_all command is replicated by sending 
# 			a TRUNCATE_TABLE statement.
#
# 			In MySQL 8.0 and later, flush_all is mapped to DELETE but is still replicated by sending a TRUNCATE_TABLE
# 			statement.
#
# TRANSACTIONS:
#
# 		) The concept of transactions has not typically been part of memcached applications. For performance considerations,
# 			daemon_memcached_r_batch_size and daemon_memcached_w_batch_size are used to control the batch size for read and write
# 			transactions.
#
# 			These settings do not affect replication. each SQL operation on the underlying InnoDB table is replicated after
# 			successful completion.
#
# 		) The default value of daemon_memcached_w_batch_size is 1, which means that each memcached write operation is commited immediately.
#
# 			This default setting incurs a certain amount of performance overhead to avoid inconsistencies in the data that is visible on the
# 			master and slave servers.
#
# 			The replicated records are always available immediately on the slave server. If you set daemon_memcached_w_batch_size to a value
# 			greater than 1, records inserted or updated through memcached are not immediately visible on the master server;
#
# 			To view the records on the master server before they are committed, issue SET_TRANSACTION_ISOLATION_LEVEL_READ_UNCOMMITTED.
#
# 15.19.8 INNODB MEMCACHED PLUGIN INTERNALS
#
# InnoDB API FOR THE INNODB MEMCACHED PLUGIN
#
# The InnoDB memcached engine accesses InnoDB through InnoDB APIs, most of which are directly adopted from embedded InnoDB.
# InnoDB API functions are passed to the InnoDB memcached engine as callback functions.
#
# InnoDB API functions access the InnoDB tables directly, and are mostly DML operations with the exception of TRUNCATE_TABLE
#
# memcached commands are implemented through the InnoDB memcached API. The following table outlines how memcached commands
# are mapped to DML or DDL operations.
#
# TABLE 15.28 MEMCACHED COMMANDS AND ASSOCIATED DML OR DDL OPERATIONS
#
# 		memcached Command 			DML or DDL Operations
#
# 		get 								a read/fetch command
#
# 		set 								a search followed by an INSERT or UPDATE (depending on whether or not a key exists)
#
# 		add 								a search followed by an INSERT or UPDATE
#
# 		replace 							a search followed by an UPDATE
#
# 		append 							a search followed by an UPDATE (appends data to the result before UPDATE)
#
# 		prepend 							a search followed by an UPDATE (prepends data to the result before UPDATE)
#
# 		incr 								a search followed by an UPDATE
#
# 		decr 								a search followed by an UPDATE
# 		
# 		delete 							a search followed by a DELETE
#
# 		flush_all 						TRUNCATE TABLE (DDL)
#
# INNODB MEMCACHED PLUGIN CONFIGURATION TABLES
#
# This section describes configuration tables used by the daemon_memcached plugin. The cache_policies table,
# config_options table and containers table are created by the innodb_memcached_config.sql configuration
# script in the innodb_memcache database.
#
# 		mysql> USE innodb_memcache;
# 		Database changed
# 		mysql> SHOW TABLES;
# 		+---------------------------+
# 		| Tables_in_innodb_memcache |
# 		+---------------------------+
# 		| cache_policies 			    |
# 		| config_options 				 |
# 		| containers 					 |
# 		+---------------------------+
#
# CACHE_POLICIES TABLE
#
# The cache_policies table defines a cache policy for the InnoDB memcached installation. You can specify individual policies
# for get, set, delete and flush operations, within a single cache policy.
#
# The default setting for all operations is innodb_only.
#
# 		) innodb_only: Use InnoDB as the data store
#
# 		) cache_only: Use the memcached engine as the data store
#
# 		) caching: Use both InnoDB and the memcached engine as data stores. In this case, if memcached cannot find a key in memory, it searches
# 			for the value in an InnoDB table.
#
# 		) disable: Disable caching
#
# TABLE 15.29 CACHE_POLICIES COLUMNS
#
# 			Column 					Description
#
# policy_name 						Name of the cache policy. The default cache policy name is cache_policy
#
# get_policy 						The cache policy for get operations. Valid values are innodb_only, cache_only,
# 										caching, or disabled. The default setting is innodb_only
#
# set_policy 						The cache policy for set operations. Valid values are innodb_only, cache_only,
# 										caching, or disabled. The default setting is innodb_only
#
# delete_policy 					The cache policy for delete operations. Valid values are innodb_only, cache_only,
# 										caching or disabled. The default setting is innodb_only
#
# flush_policy 					The cache policy for flush operations. Valid values are innodb_only, cache_only,
# 										caching, or disabled. The default setting is innodb_only.
#
# CONFIG_OPTIONS TABLE
#
# The config_options table stores memcached-related settings that can be changed at runtime using SQL. Supported
# configuration options are separator and table_map_delimiter.
#
# TABLE 15.30 CONFIG_OPTIONS COLUMNS
#
# 	Column 				Description
#
# Name 					Name of the memcached-related configuration option. The following configuration options are supported
# 							by the config_options table:
#
# 								) separator: Used to separate values of a long string into separate values when there are multiple value_columns
# 									defined.
#
# 									By default, the separator is a | character. For example, if you define col1, col2 as value columns, and you define
# 									| as the separator, you can issue the following memcached command to insert values into col1 and col2, respectively:
#
# 										set keyx 10 0 19
# 										valuecolx|valuecoly
#
# 									valuecol1x is stored in col1 and valuecoly is stored in col2.
#
# 								) table_map_delimiter: The character separating the schema name and the table name when you use the @@ notation
# 									in a key name to access a key in a specific table.
#
# 									For example, @@t1.some_key and @@t2.som_key have the same key value, but are stored in different tables.
#
# Value 					The value assigned to the memcached-related configuration option
#
# CONTAINERS TABLE
#
# The containers table is the most important of the three configuration tables. Each InnoDB table that is used to store memcached
# values must have an entry in the containers table.
#
# The entry provides a mapping between InnoDB table columns and container table columns, which is required for memcached to work
# with InnoDB tables.
#
# The containers table contains a default entry for the test.demo_test table, which is created by the innodb_memcached_config.sql
# configuration script.
#
# To use the daemon_memcached plugin with your own InnoDB table, you must create an entry in the containers table.
#
# TABLE 15.31 CONTAINERS COLUMNS
#
# 		COLUMN 					DESCRIPTION
#
# 		name 						The name given to the container. If an InnoDB table is not requested by name using @@ notation, the daemon_memcached
# 									plugin uses the InnoDB table with a containers.name value of default. If there is no such entry, the first entry in the
# 									containers table, ordered alphabetically by name (ascending), determines the default InnoDB table.
#
# 		db_schema 				The name of the database where the InnoDB table resides. This is a required value
#
# 		db_table 				The name of the InnoDB table that stores memcached values. This is a required value
#
# 		key_columns 			The column in the InnoDB table that contains lookup key values for memcached operations. This is a required value.
#
# 		value_columns 			The InnoDB table columns (one or more) that store memcached data. Multiple columns can be specified using the
# 									separator character specified in the innodb_memcached.config_options table.
#
# 									By default, the separator is a pipe character ("|"). To specify multiple columns, separate them with the defined
# 									separator character.
#
# 									For example: col1|col2|col3
#
# 									This is a required value
#
# 		flags 					The InnoDB table columns that are used as flags (a user-defined numeric value that is stored and retrieved along
# 									with the main value) for memcached.
#
# 									A flag value can be used as a column specifier for some operations (such as incr, prepend) if a memcached 
# 									value is mapped to multiple columns, so that an operation is performed on a specified column.
#
# 									For example, if you have mapped a value_columns to three InnoDB table columns, and only want the increment
# 									operation performed on one column, use the flags column to specify the column.
#
# 									If you do not use the flags column, set a value of 0 to indicate that it is unused.
#
# 		cas_column 				The InnoDB table column that stores compare-and-swap (cas) values. The cas_column value is related
# 									to the way memcached hashes requests to different servers and caches data in memory.
#
# 									Because the InnoDB memcached plugin is tightly integrated with a single memcached daemon, and the
# 									in-memory caching mechanism is handled by MySQL and the InnoDB buffer pool, this column is rarely
# 									needed.
#
# 									If you do not use this column, set a value of 0 to indicate that it is unused.
#
# 		expire_time_column 	The InnoDB table column that stores expiration values. The expire_time_column value is related to the way
# 									memcached hashes requests to different servers and caches data in memory.
#
# 									Because the InnoDB memcached plugin is tightly integrated with a single memcached daemon, and the
# 									in-memory caching mechanism is handled by MySQL and the InnoDB buffer pool, this column is rarely needed.
#
# 									If you do not use this column, set a value of 0 to indicate that the column is unused.
#
# 									The maximum expire time is defined as INT_MAX32 or 2147483647 seconds (approx 68 years)
#
# 	unique_idx_name_on_key  The name of the index on the key column. It must be a unique index. It can be the primary key or
# 									a secondary index.
#
# 									Preferably, use the primary key of the InnoDB table. Using the primary key avoids a lookup that is
# 									performed when using a secondary index.
#
# 									You cannot make a covering index for memcached lookups; InnoDB returns an error if you try to define
# 									a composite secondary index over both the key and value columns.
#
# CONTAINERS TABLE COLUMN CONSTRAINTS
#
# 		) You must supply a value for db_schema, db_name, key_columns, value_columns and unique_idx_name_on_key. Specify 0 for flags,
# 			cas_column and expire_time_column if they are unused.
#
# 			Failing to do so could cause your setup to fail.
#
# 		) key_columns: The maximum limit for a memcached key is 250 characters, which is enforced by memcached. The mapped key must be
# 			a non-Null CHAR or VARCHAR type.
#
# 		) value_columns: Must be mapped to a CHAR, VARCHAR, or BLOB column. There is no length restriction and the value can be NULL.
#
# 		) cas_column: The cas value is a 64 bit integer. It must be mapped to a BIGINT of at least 8 bytes. If you do not use this column,
# 			set a value of 0 to indicate that it is unused.
#
# 		) expiration_time_column: Must mapped to an INTEGER of at least 4 bytes. Expiration time is defined as a 32-bit integer for Unix time
# 			(the number of seconds since January 1, 1970, as a 32-bit value), or the number of seconds starting from the current time.
#
# 			For the latter, the number of seconds may not exceed 60*60*24*30 (the number of seconds in 30 days).
#
# 			If the number sent by a client is larger, the server considers it to be a real Unix time value rather than an offset from the
# 			current time.
#
# 			If you do not use this column, set a value of 0 to indicate that it is unused.
#
# 		) flags: Must be mapped to an INTEGER of at least 32-bits and can be NULL. If you do not use this column, set a value of 0 to indicate
# 			that it is unused.
#
# A pre-check is performed at plugin load time to enforce column constraints. If mismatches are found, the plugin is not loaded.
#
# MULTIPLE VALUE COLUMN MAPPING
#
# 		) During plugin initialization, when InnoDB memcached is configured with information defined in the containers table, each mapped
# 			column defined in containers.value_columns is verified against the mapped InnoDB table.
#
# 			If multiple InnoDB table columns are mapped, there is a check to ensure that each column exists and is the right type.
#
# 		) At run-time, for memcached insert operations, if there are more delimited values than the number of mapped columns,
# 			only the number of mapped values are taken.
#
# 			For example, if there are six mapped columns, and seven delimited values are provided, only the first six delimited values
# 			are taken.
#
# 			The seventh delimited value is ignored.
#
# 		) If there are fewer delimited values than mapped columns, unfilled columns are set to NULL. If an unfilled column cannot be
# 			set to NULL, insert operations fail.
#
# 		) If a table has more columns than mapped values, the extra columns do not affect results.
#
# THE DEMO_TEST EXAMPLE TABLE
#
# The innodb_memcached_config.sql configuration script creates a demo_test table in the test database, which can be used
# to verify InnoDB memcached plugin installation immediately after setup.
#
# The innodb_memcached_config.sql configuration script also creates an entry for the demo_test table in the innodb_memcache.containers table.
#
# 		mysql> SELECT * FROM innodb_memcache.containers\G
# 		*********************** 1. row *********************************
# 							name: aaa
# 					db_schema : test
# 					db_table  : demo_test
# 				key_columns  : c1
# 			value_columns   : c2
# 						flags  : c3
# 				cas_column   : c4
# 		expire_time_column : c5
# 	unique_idx_name_on_key: PRIMARY
#
# 		mysql> SELECT * FROM test.demo_test;
# 		+-------+-----------------------+---------+-----------+----------+
# 		| c1 	  | c2 						  | c3 		| c4 			| c5 		  |
# 		+-------+-----------------------+---------+-----------+----------+
# 		| AA 	  | HELLO, HELLO 			  | 8 		| 0 			| 0 		  |
# 		+-------+-----------------------+---------+-----------+----------+
#
# 15.19.9 TROUBLESHOOTING THE INNODB MEMCACHED PLUGIN
#
# This section describes issues that you may encounter when using the InnoDB memcached plugin.
#
# 		) If you encounter the following error in the MySQL error log, the server might fail to start:
#
# 				failed to set rlimit for open files. Try running as root or requesting smaller maxconns value.
#
# 			The error message is from the memcached daemon. One solution is to raise the OS limit for the number
# 			of open files.
#
# 			The commands for checking and increasing the open file limit varies by operating system.
#
# 			This example shows commands for Linux and OS X:
#
# 				# Linux
# 				shell> ulimit -n
# 				1024
# 				shell> ulimit -n 4096
# 				shell> ulimit -n
# 				4096
#
# 				# OS X
# 				shell> ulimit -n
# 				256
# 				shell> ulimit -n 4096
# 				shell> ulimit -n
# 				4096
#
# 			THe other solution is to reduce the number of concurrent connections permitted for the memcached daemon.
#
# 			To do so, encode the -c memcached option in the daemon_memcached_option configuration parameter in the
# 			MySQL configuration file. The -c option has a default value of 1024.
#
# 				[mysqld]
# 				/etc/
# 				loose-daemon_memcached_option='-c 64'
#
# 		) To troubleshoot problems where the memcached daemon is unable to store or retrieve InnoDB table data, encode the
# 			-vvv memcached option in the daemon_memcached_option configuration parameter in the MySQL configuration file.
#
# 			Examine the MySQL error log debug output related to memcached operations.
#
# 				[mysqld]
# 				/etc/
# 				loose-daemon_memcached_option='-vvv'
#
# 		) If columns specified to hold memcached values are the wrong data type, such as a numeric type instead of a string
# 			type, attempts to store key-value pairs fail with no specific error code or message.
#
# 		) If the daemon_memcached plugin causes MySQL server startup issues, you can temporarily disable the daemon_memcached
# 			plugin while troubleshooting by adding this line under the [mysqld] group in the MySQL configuration file:
#
# 				daemon_memcached=OFF
#
# 			For example, if you run the INSTALL_PLUGIN statement before running the innodb_memcached_config.sql configuration
# 			script to set up the necessary database and tables, the server might crash and fail to start.
#
# 			The server could also fail to start if you incorrectly configure an entry in the innodb_memcache.containers table.
#
# 			To uninstall the memcached plugin for a MySQL instance, issue the following statement:
#
# 				mysql> UNINSTALL PLUGIN daemon_memcached;
#
# 		) If you run more than one instance of MySQL on the same machine with the daemon_memcached plugin enabled in each
# 			instance, use the daemon_memcached_option configuration parameter to specify a unique memcached port for
# 			each daemon_memcached plugin.
#
# 		) If an SQL statement cannot find the InnoDB table or finds no data in the table, but memcached API calls retrieve
# 			the expected data, you may be missing an entry for the InnoDB table in the innodb_memcache.containers table,
# 			or you may have not switched to the correct InnoDB table by issuing a get or set request using @@table_id notation.
#
# 			This problem could also occur if you change an existing entry in the innodb_memcache.containers table without
# 			restarting the MySQL server afterward.
#
# 			The free-form storage mechanism is flexible enough that your requests to store or retrieve a multi-column value
# 			such as col1|col2|col3 may still work, even if the daemon is using the test.demo_test table which stores values
# 			in a single column.
#
# 		) When defining your own InnoDB table for use with the daemon_memcached plugin, and columns in the table are defined
# 			as NOT NULL, ensure that values are supplied for the NOT NULL columns when inserting a record for the table into
# 			the innodb_memcache.containers table.
#
# 			If the INSERT statement for the innodb_memcache.containers record contains fewer delimited values than there are
# 			mapped columns, unfilled columns are set to NULL.
#
# 			Attempting to insert a NULL value into a NOT NULL column causes the INSERT to fail, which may only become evident
# 			after you reinitialize the daemon_memcached plugin to apply changes to the innodb_memcache.containers table.
#
# 		) If cas_column and expire_time_column fields of the innodb_memcached.containers table are set to NULL, the following
# 			error is returned when attempting to load the memcached plugin:
#
# 				InnoDB_Memcached: column 6 in the entry for config table 'containers' in
# 				database 'innodb_memcache' has an invalid NULL value.
#
# 			The memcached plugin rejects usage of NULL in the cas_column and expire_time_column columns.
#
# 			Set the value of these columns to 0 when the columns are unused.
#
# 		) As the length of the memcached key and values increase, you might encounter size and length limits.
#
# 			) When the key exceeds 250 bytes, memcached operations return an error. This is currently a fixed limit within memcached.
#
# 			) InnoDB table limits may be encountered if values exceed 768 bytes in size, 3072 bytes in size, or half of the innodb_page_size
# 				value.
#
# 				These limits primarily apply if you intend to create an index on a value column to run report-generating queries on that
# 				column using SQL.
#
# 				See SECTION 15.6.1.6, "LIMITS ON INNODB TABLES" for details.
#
# 			) The maximum size for the key-value combination is 1 MB.
#
# 		) If you share configuration files across MySQL servers of different versions, using the latest configuration options for the
# 			daemon_memcached plugin could cause startup errors on older MySQL versions.
#
# 			To avoid compatibility problems, use the loose prefix with option names. For example, use loose-daemon_memcached_option='-c 64'
# 			instead of daemon_memcached_option='-c 64'
#
# 		) There is no restriction or check in place to validate character set settings. memcached stores and retrieves keys and values
# 			in bytes and is therefore not character set sensitive. However, you must ensure that the memcached client and the MySQL
# 			table use the same character set.
#
# 		) memcached connections are blocked from accessing tables that contain an indexed virtual column.
#
# 			Accessing an indexed virtual column requires a callback to the server, but a memcached connection does
# 			not have access to the server code.
#
#  
#
# 15.20 INNODB TROUBLESHOOTING
#
# 15.20.1 TROUBLESHOOTING INNODB I/O PROBLEMS
# 15.20.2 FORCING INNODB RECOVERY
# 15.20.3 TROUBLESHOOTING INNODB DATA DICTIONARY OPERATIONS
# 15.20.4 INNODB ERROR HANDLING
#
# The following general guidelines apply to troubleshooting InnoDB problems:
#
# 		) When an operation fails or you suspect a bug, look at the MySQL server error log (see SECTION 5.4.2, "THE ERROR LOG")
# 		
# 			SECTION B.3.1, "SERVER ERROR MESSAGE REFERENCE" provides troubleshooting information for some of the common
# 			InnoDB-specific errors that you may encounter.
#
# 		) If the failure is related to a deadlock, run with the innodb_print_all_deadlocks option enabled so that details about
# 			each deadlock are printed to the MySQL server error log.
#
# 			For information about deadlocks, see SECTION 15.7.5, "DEADLOCKS IN INNODB"
#
# 		) If the issue is related to the InnoDB data dictionary, see SECTION 15.20.3, "TROUBLESHOOTING INNODB DATA DICTIONARY OPERATIONS"
#
# 		) When troubleshooting, it is usually best to run the MySQL server from the command prompt, rather than through mysqld_safe or as 
# 			a Windows service.
#
# 			You can then see what mysqld prints to the console, and so have a better grasp of what is going on. On Windows, start
# 			mysqld with the --console option to direct the output to the console window.
#
# 		) Enable the InnoDB Monitors to obtain information about a problem (see SECTION 15.16, "INNODB MONITORS")
#
# 			If the problem is performance-related, or your server appears to be hung, you should enable the standard
# 			Monitor to print the information about the internal state of InnoDB.
#
# 			If the problem is with locks, enable the Lock Monitor.

# 			If the problem is with table creation, tablespaces, or data dictionary operations, refer to the InnoDB INFORMATION
# 			SCHEMA SYSTEM TABLES to examine contents of the InnoDB internal data dictionary.
#
# 			InnoDB temporarily enables standard InnoDB Monitor output under the following conditions:
#
# 				) A long semaphore wait
#
# 				) InnoDB cannot find free blocks in the buffer pool
#
# 				) Over 67% of the buffer pool is occupied by lock heaps or the adaptive hash index
#
# 		) If you suspect that a table is corrupt, run CHECK_TABLE on that table.
#
# 15.20.1 TROUBLESHOOTING INNODB I/O PROBLEMS
#
# The troubleshooting steps for InnoDB I/O problems depend on when the problem occurs: during startup of the MySQL
# server, or during normal operations when a DML or DDL statement fails due to problems at the file system level.
#
# Initialization Problems
#
# If something goes wrong when InnoDB attempts to initialize its tablespace or its log files, delete all files created
# by InnoDB: all ibdata files and all ib_logfile files.
#
# If you already created some InnoDB tables, also delete any .ibd files from the MySQL database directories.
#
# Then try the InnoDB database creation again. For easiest troubleshooting, start the MySQL server from a command
# prompt so that you see what is happening.
#
# RUNTIME PROBLEMS
#
# If InnoDB prints an operating system error during a file operation, usually the problem has one of the following solutions:
#
# 		) Make sure the InnoDB data file directory and the InnoDB log directory exist
#
# 		) Make sure mysqld has access rights to create files in those directories
#
# 		) Make sure mysqld can read the proper my.cnf or my.ini option file, so that it starts with the options that you specified
#
# 		) Make sure the disk is not full and you are not exceeding any disk quota
#
# 		) Make sure that the names you specify for subdirectories and data files do not clash
#
# 		) Doublecheck the syntax of the innodb_data_home_dir and innodb_data_file_path values. In particular, any MAX value
# 			in the innodb_data_file_path option is a hard limit, and exceeding that limit causes a fatal error.
#
# 15.20.2 FORCING INNODB RECOVERY
#
# To investigate database page corruption, you might dump your tables from the database with SELECT_/ETC/_INTO_OUTFILE.
#
# Usually, most of the data obtained in this way is intact. Serious corruption might cause SELECT * FROM tbl_name statements
# or InnoDB background operations to crash or assert, or even cause InnoDB roll-forward recovery to crash.
#
# In such cases, you can use the innodb_force_recovery option to force the InnoDB storage engine to start up while preventing
# background operations from running, so that you can dump your tables. For example, you can add the following line to the
# [mysqld] section of your option file before restarting the server:
#
# 		[mysqld]
# 		innodb_force_recovery = 1
#
# For information about using option files, see SECTION 4.2.2.2, "USING OPTION FILES"
#
# WARNING:
#
# 		Only set innodb_force_recovery to a value greater than 0 in an emergency situation, so that you can start InnoDB
# 		and dump your tables.
#
# 		Before doing so, ensure that you have a backup copy of your database in case you need to recreate it.
#
# 		Values of 4 or greater can permanently corrupt data files.
#
# 		Only use an innodb_force_recovery setting of 4 or greater on a production server instance after you have successfully
# 		tested the setting on a separate physical copy of your database.
#
# 		When forcing InnoDB recovery, you should always start with innodb_force_recovery=1 and only increase
# 		the value incrementally, as necessary.
#
# innodb_force_recovery is 0 by default (normal startup without forced recovery). The permissible nonzero values for
# innodb_force_recovery are 1 to 6. A larger value includes the functionality of lesser values.
#
# For example, a value of 3 includes all of the functionality of values 1 and 2.
#
# If you are able to dump your tables with an innodb_force_recovery value of 3 or less, then you are relatively safe that
# only some data on corrupt individual pages is lost.
#
# A value of 4 or greater is considered dangerous because data files can be permanently corrupted. A value of 6 is considered
# drastic because database pages are left in an obsolete state, which in turn may introduce more corruption into B-trees
# and other database structures.
#
# As a safety measure, InnoDB prevents INSERT, UPDATE or DELETE operations when innodb_force_recovery is greater than 0.
#
# An innodb_force_recovery setting of 4 or greater places InnoDB in read-only mode.
#
# 		) 1 (SRV_FORCE_IGNORE_CORRUPT)
#
# 			Let the server run even if it detects a corrupt page. Tries to make SELECT * FROM tbl_name jump over corrupt index
# 			records and ppages, which helps in dumping tables.
#
# 		) 2 (SRV_FORCE_NO_BACKGROUND)
#
# 			Prevents the master thread and any purge threads from running. If a crash would occur during the purge operation,
# 			this recovery value prevents it.
#
# 		) 3 (SRV_FORCE_NO_TRX_UNDO)
#
# 			Does not run transaction rollbacks after crash recovery.
#
# 		) 4 (SRV_FORCE_NO_IBUF_MERGE)
#
# 			Prevents insert buffer merge operations. If they would cause a crash, does not do them. Does not calculate
# 			table statistics.
#
# 			This value can permanently corrupt data files. After using this value, be prepared to drop and recreate all
# 			secondary indexes. Sets InnoDB to read-only.
#
# 		) 5 (SRV_FORCE_NO_UNDO_LOG_SCAN)
#
# 			Does not look at undo logs when starting the database: InnoDB treats even incomplete transactions as committed.
#
# 			This value can permanently corrupt data files. Sets InnoDB to read-only.
#
# 		) 6 (SRV_FORCE_NO_LOG_REDO)
#
# 			Does not do the redo log roll-forward in connection with recovery. This value can permanently corrupt data files.
#
# 			Leaves database pages in an obsolete state, which in turn may introduce more corruption into B-trees and other
# 			database structures. Sets InnoDB to read-only.
#
# You can SELECT from tables to dump them. With an innodb_force_recovery value of 3 or less you can DROP or CREATE tables.
#
# DROP_TABLE is also supported with an innodb_force_recovery value greater than 3. DROP_TABLE is not permitted with an
# innodb_force_recovery value greater than 4.
#
# If you know that a given table is causing a crash on rollback, you can drop it. If you encounter a runaway rollback
# caused by a failing mass import or ALTER_TABLE, you can kill the mysqld process and set innodb_force_recovery to 3
# to bring the database up without the rollback, and then DROP the table that is causing the runaway rollback.
#
# If corruption within the table data prevents you from dumping the entire table contents, a query with an ORDER BY
# primary_key DESC clause might be able to dump the portion of the table after the corrupted part.
#
# If a high innodb_force_recovery value is required to start InnoDB, there may be corrupted data structures that could
# cause complex queries (queries containing WHERE, ORDER BY or other clauses) to fail.
#
# In this case, you may only be able to run basic SELECT * FROM t queries.
#
# 15.20.3 TROUBLESHOOTING INNODB DATA DICTIONARY OPERATIONS
#
# Information about table definitions is stored in the InnoDB data dictionary. If you move data files around, dictionary
# data can become inconsistent.
#
# If a data dictionary corruption or consistency issue prevents you from starting InnoDB, see SECTION 15.20.2, "FORCING INNODB RECOVERY"
# for information about manual recovery.
#
# CANNOT OPEN DATAFILE
#
# With innodb_file_per_table enabled (the default), the following messages may appear at startup if a file-per-table tablespace
# file (.ibd file) is missing:
#
# 		[ERROR] InnoDB: Operating system error number 2 in a file operation
# 		[ERROR] InnoDB: The error means the system cannot find the path specified
# 		[ERROR] InnoDB: Cannot open datafile for read-only: './test/t1.ibd' OS error: 71
# 		[Warning] InnoDB: Ignoring tablespace `test/t1` because it could not be opened.
#
# To address these messages, issue DROP_TABLE statement to remove data about the missing table from the data dictionary.
#
# RESTORING ORPHAN FILE-PER-TABLE IBD FILES
#
# This procedure describes how to restore orphan file-per-table .ibd files to another MySQL instance. You might use this
# procedure if the system tablespace is lost or unrecoverable and you want to restore .ibd file backups on a new MySQL instance.
#
# The procedure is not supported for general tablespace .ibd files
#
# The procedure assumes that you only have .ibd file backups, you are recovering to the same version of MySQL that initially
# created the orphan .ibd files, and that .ibd file backups are clean.
#
# See SECTION 15.6.1.2, "MOVING OR COPYING INNODB TABLES" for information about creating clean backups.
#
# Tablespace copying limitations outlined in SECTION 15.6.3.7, "COPYING TABLESPACES TO ANOTHER INSTANCE" are applicable
# to this procedure.
#
# 		1. On the new MySQL instance, recreate the table in a database of the same name.
#
# 			mysql> CREATE DATABASE sakila;
#
# 			mysql> USE sakila;
#
# 			mysql> CREATE TABLE actor (
# 						actor_id SMALLINT UNSIGNED NOT NULL AUTO_INCREMENT,
# 						first_name VARCHAR(45) NOT NULL,
# 						last_name VARCHAR(45) NOT NULL,
# 						last_update TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
# 						PRIMARY KEY (actor_id),
# 						KEY idx_actor_last_name (last_name)
# 					)ENGINE=InnoDB DEFAULT CHARSET=utf8;
#
# 		2. Discard the tablespace of the newly created table.
#
# 				mysql> ALTER TABLE sakila.actor DISCARD TABLESPACE;
#
# 		3. Copy the orphan .ibd file from your backup directory to the new database directory
#
# 			shell> cp /backup_directory/actor.ibd path/to/mysql-5.7/data/sakila/
#
# 		4. Ensure that the .ibd file has the necessary file permissions.
#
# 		5. Import the orphan .ibd file. A warning is issued indicating that InnoDB will atempt to import the file without schema verification.
#
# 				mysql> ALTER TABLE sakila.actor IMPORT TABLESPACE; SHOW WARNINGS;
# 				Query OK, 0 rows affected, 1 warning (0.15 sec)
#
# 				Warning | 1810 | InnoDB: IO Read error: (2, No such file or directory)
# 				Error opening './sakila/actor.cfg', will attempt to import
# 				without schema verification
#
# 		6. Query the table to verify that the .ibd file was successfully restored.
#
# 			mysql> SELECT COUNT(*) FROM sakila.actor;
# 			+---------+
# 			| count(*)|
# 			+---------+
# 			| 200 	 |
# 			+---------+
#
# 15.20.4 InnoDB ERROR HANDLING
#
# The following items describe how InnoDB performs error handling. InnoDB sometimes rolls back only the statement that failed,
# other times it rolls back the entire transaction.
#
# 		) If you run out of file space in a tablespace, a MySQL Table is full error occurs and InnoDB rolls back the SQL statement
#
# 		) A transaction deadlock causes InnoDB to roll back the entire transaction. Retry the whole transaction when this happens.
#
# 			A lock wait timeout causes InnoDB to roll back only the single statement that was waiting for the lock and encountered the
# 			timeout.
#
# 			(To have the entire transaction roll back, start the server with the --innodb-rollback-on-timeout option)
#
# 			Retry the statement if using the current behavior, or the entire transaction if using --innodb-rollback-on-timeout
#
# 			Both deadlocks and lock wait timeouts are normal on busy servers and it is necessary for applications to be aware that
# 			they may happen and handle them by retrying.
#
# 			You can make them less likely by doing as little work as possible between the first change to data during a transaction
# 			and the commit, so the locks are held for the shortest possible time and for the smallest possible number of rows.
#
# 			Sometimes splitting work between different transactions may be practical and helpful.
#
# 			When a transaction rollback occurs due to a deadlock or lock wait timeout, it cancels the effect of the statements
# 			within the transaction.
#
# 			But if the start-transaction statement was START_TRANSACTION or BEGIN statement, rollback does not cancel that statement.
#
# 			Further SQL statements become a part of the transaction until the occurrence of COMMIT, ROLLBACK, or some SQL statement
# 			that causes an implicit commit.
#
# 		) A duplicate-key error rolls back the SQL statement, if you have not specified the IGNORE option in your statement.
#
# 		) A row too long error rolls back the SQL statement
#
# 		) Other errors are mostly detected by the MySQL layer of code (above the InnoDB storage engine level), and they roll back
# 			the corresponding SQL statement.
#
# 			Locks are not released in a rollback of a single SQL statement.
#
# During implicit rollbacks, as well as during the execution of an explicit ROLLBACK SQL statement, SHOW_PROCESSLIST displays
# Rolling back in the State column for the relevant connection.
#
# CHAPTER 16 ALTERNATIVE STORAGE ENGINES
#
# Table of Contents
#
# 16.1 SETTING THE STORAGE ENGINE
# 16.2 THE MYISAM STORAGE ENGINE
# 16.3 THE MEMORY STORAGE ENGINE
# 16.4 THE CSV STORAGE ENGINE
# 16.5 THE ARCHIVE STORAGE ENGINE
# 16.6 THE BLACKHOLE STORAGE ENGINE
# 16.7 THE MERGE STORAGE ENGINE
# 16.8 THE FEDERATED STORAGE ENGINE
# 16.9 THE EXAMPLE STORAGE ENGINE
# 16.10 OTHER STORAGE ENGINES
# 16.11 OVERVIEW OF MYSQL STORAGE ENGINE ARCHITECHTURE
#
# Storage engines are MySQL components that handle the SQL operations for different table types. InnoDB is the default and most
# genral-purpose storage engine, and Oracle recommends using it for tables except for specialized use cases.
#
# (The CREATE_TABLE statement in MySQL 8.0 creates InnoDB tables by default)
#
# MySQL Server uses a pluggable storage engine architechture that enables storage engines to be loaded into and unloaded
# from a running MySQL server.
#
# To determine which storage engines your server supports, use the SHOW_ENGINES statement. The value in the Support column
# indicates whether an engine can be used.
#
# A value of YES, NO, or DEFAULT indicates that an engine is available, not available, or available and currently set as
# the default storage engine.
#
# 		mysql> SHOW ENGINES\G
# 		*************************** 1. row *****************************
# 					Engine: PERFORMANCE_SCHEMA
# 				Support  : YES
# 				Comment  : Performance Schema
# 		Transactions   : NO
# 						XA : NO
# 		Savepoints 	   : NO
# 		*************************** 2. row ******************************
#  				Engine: InnoDB
# 				Support  : DEFAULT
# 				Comment  : Supports transactions, row-level locking, and foreign keys
# 		Transactions 	: YES
# 						XA : YES
# 			Savepoints  : YES
# 		*************************** 3. row *******************************
# 					Engine: MRG_MYISAM
# 				Support  : YES
# 				Comment  : Collection of identical MyISAM tables
# 		Transactions   : NO
# 						XA : NO
# 			Savepoints  : NO
# 		*************************** 4. row ********************************
# 				Engine   : BLACKHOLE
# 				Support  : YES
# 				  Comment: /dev/null storage engine (anything you write to it disappears)
# 			Transactions: NO
# 						XA : NO
# 			Savepoints  : NO
# 		*************************** 5. row *********************************
# 				Engine   : MyISAM
# 				Support  : YES
# 				Comment  : MyISAM storage engine
# 			Transactions: NO
# 						XA : NO
# 			Savepoints  : NO
# 		/ETC/
#
# This chapter covers use cases for special-purpose MySQL storage engines. It does not cover the default InnoDB
# storage engine or the NDB storage engine which are covered in CHAPTER 15, THE INNODB STORAGE ENGINE and 
# CHAPTER 22, MYSQL NDB CLUSTER 8.0
#
# For advanced users, it also contains a description of the pluggable storage engine architechture (see SECTION 16.11, "OVERVIEW OF MYSQL STORAGE ENGINE ARCHITECHTURE")
#
# For information about features offered in commerical MySQL Server binaries, see MySQL Editions.
#
# The storage engines available might depend on which edition of MySQL you are using.
#
# For answers to FAQ's about MySQL storage engines, see SECTION A.2, "MYSQL 8.0 FAQ: STORAGE ENGINES"
#
# MySQL 8.0 SUPPORTED STORAGE ENGINES
#
# 	) InnoDB: The default storage engine in MySQL 8.0. InnoDB is a transaction-safe (ACID compliant) storage engine for MySQL
# 					that has commit, rollback and crash-recovery capabilities to protect user data.
#
# 				InnoDB row-level locking (without escalation to coarser granularity locks) and Oracle-style consistent nonlocking
# 				reads increase multi-user concurrency and performance.
#
# 				InnoDB stores user data in clustered indexes to reduce I/O for common queries based on primary keys. To maintain
# 				data integrity, InnoDB also supports FOREIGN KEY referential-integrity constraints.
#
# 				For more information about InnoDB, see CHAPTER 15, THE INNODB STORAGE ENGINE.
#
#  ) MyISAM: These tables have a small footprint. Table-level locking limits the performance in read/write workloads, so it is often
# 				used in read-only or read-mostly workloads in Web and data warehousing configurations.
#
# 	) Memory: Stores all data in RAM, for fast access in environments that require quick lookups of non-critical data.
#
# 				This engine was formerly known as the HEAP engine. Its use cases are decreasing; InnoDB with its buffer pool
# 				memory area provides a general-purpose and durable way to keep most or all data in memory, and NDBCLUSTER
# 				provides fast key-value lookups for huge distributed data sets.
#
# 	) CSV: Its tables are really text files with comma-separated values. CSV tables let you import or dump data in CSV format,
# 				to exchange data with scripts and applications that read and write that same format.
#
# 				Because CSV tables are not indexed, you typically keep the data in InnoDB tables during normal operation,
# 				and only use CSV tables during the import or export stage.
#
#  ) Archive: These compact, unindexed tables are intended for storing and retrieving large amounts of seldom-referenced
# 					historical, archived or security audit information.
#
# ) Blackhole: The Blackhole storage engine accepts but does not store data, similar to the Unix /dev/null device.
#
# 					Queries always return an empty set. These tables can be used in replication configurations where DML
# 					statements are sent to slave servers, but the master server does not keep its own copy of the data.
#
# ) NDB (also known as NDBCLUSTER): This clustered database engine is particularly suited for applications that require the highest
# 												possible degree of uptime and availability.
#
# ) Merge: Enables a MySQL DBA or developer to logically group a series of identical MyISAM tables and reference them as one object.
# 				Good for VLDB environments such as data warehousing.
#
# ) Federated: Offers the ability to link separate MySQL servers to create one logical database from many physical servers.
#
# 					Very good for distributed or data mart environments.
#
# ) Example: This engine serves as an example in the MySQL source code that illustrates how to begin writing new storage engines.
# 				It is primarily of interest to developers.
#
# 				The storage engine is a "stub" that does nothing. You can create tables with this engine, but no data can be stored
# 				in them or retrieved from them.
#
# You are not restricted to using the same storage engine for an entire server or schema. You can specify the storage engine
# for any table.
#
# For example, an application might use mostly InnoDB tables, with one CSV table for exporting data to a spreadsheet and a few
# MEMORY tables for temporary workspaces.
#
# CHOOSING A STORAGE ENGINE
#
# The various storage engines provided with MySQL are designed with different use cases in mind. The following table provides
# an overview of some storage engines provided with MySQL, with clarifying notes following the table.
#
# TABLE 16.1 STORAGE ENGINES FEATURE SUMMARY
#
# 		FEATURE 			MyISAM 		MEMORY 		InnoDB 		Archive 		NDB
# 		
# 		B-tree indexes Yes 			Yes 			Yes 			No 			No
#
# 		Backup/ 			Yes 			Yes 			Yes 			Yes 			Yes
# 		point-in-time
# 		recovery
# 		(note 1)
#
# 		Cluster 			No 			No 			No 			No 			Yes 
# 		database
# 		support
#
# 		Clustered 		No 			No 			Yes 			No 			No
# 		indexes
#
# 		Compressed 		Yes (note 2) No 			Yes 			Yes 			No
# 		data
#
# 		Data caches 	No 		 	 N/A 		     Yes 			No 			Yes
#
# 		Encrypted 		Yes (note 3) Yes (note 3) Yes (note 4) Yes (note 3) Yes (note 3)
# 		data
#
# 		Foreign key 	No 			No 			Yes 			No 			Yes (note 5)
# 		support
#
# 		Full-text  		Yes 			No 			Yes (note 6) No 			No
# 		search indexes
#
# 		Geospatial 		Yes 			No 			Yes 			Yes 			Yes
# 		data type
# 		support
#
# 		geospatial 		Yes 			No 			Yes (note 7) No 			No
# 		indexing
# 		support
#
# 		Hash indexes 	No 			Yes 			No (Note 8) No 			Yes
#
# 		Index caches 	Yes 			N/A 			Yes 			No 			Yes
#
# 		Locking 			Table 		Table 		Row 			Row 			Row
# 		granularity
#
# 		MVCC 				No 			No 			Yes 			No 			No
#
# 		Replication 	Yes 			Limited (note 9) Yes 	Yes 			Yes 
# 		support (note 1)
#
# 		Storage limits 256TB 		RAM 			64TB 			None 			384EB
#
# 		T-tree indexes No 			No 			No 			No 			Yes
#
# 		Transactions 	No 			No 			Yes 			No 			Yes
#
# 		Update stats 	Yes 			Yes 			Yes 			Yes 			Yes
# 		for data dict.
#
# NOTES:
#
# 		1. Implemented in the server, rather than in the storage engine
#
# 		2. Compressed MyISAM tables are supported only when using the compressed row format. Tables using the compressed row format with MyISAM are read only.
#
# 		3. Implemented in the server via encryption functions
#
# 		4. Implemented in the server via encryption functions; In MySQL 5.7 and later, data-at-rest tablespace encryption is supported.
#
# 		5. Support for foreign keys is available in MySQL Cluster NDB 7.3 and later
#
# 		6. InnoDB support for FULLTEXT indexes is available in MysQL 5.6 and alter
#
# 		7. InnoDB support for geospatial indexing is available in MySQL 5.7 and later
#
# 		8. InnoDB utilizes hash indexes internally for its Adaptive Hash Index feature.
#
# 		9. See the discussion later in this section.
#
# 16.1 SETTING THE STORAGE ENGINE
#
# When you create a new table, you can specify which storage engine to use by adding an ENGINE table option to the CREATE_TABLE statement:
#
# 		-- ENGINE=INNODB not needed unless you have set a different
# 		-- default storage engine
# 		CREATE TABLE t1 (i INT) ENGINE = INNODB;
# 		-- Simple table definition can be switched from one to another
# 		CREATE TABLE t2 (i INT) ENGINE = CSV;
# 		CREATE TABLE t3 (i INT) ENGINE = MEMORY;
#
# When you omit the ENGINE option, the default storage engine is used. The default engine is InnoDB in mySQL 8.0
#
# You can specify the default engine by using the --default-storage-engine server startup option, or by setting the
# default-storage-engine option in the my.cnf configuration file.
#
# You can set the default storage engine for the current session by setting the default_storage_engine variable:
#
# 		SET default_storage_engine=NDBCLUSTER;
#
# The storage engine for TEMPORARY tables created with CREATE_TEMPORARY_TABLE can be set separately from the engine
# for permanent tables by setting the default_tmp_storage_engine, either at startup or at runtime.
#
# To convert a table from one storage engine to another, use an ALTER_TABLE statement that indicates the new engine.
#
# 		ALTER TABLE t ENGINE = InnoDB;
#
# See SECTION 13.1.20, "CREATE TABLE SYNTAX", and SECTION 13.1.9, "ALTER TABLE SYNTAX"
#
# If you try to use a storage engine that is not compiled in or that is compiled in but deactivated, MySQL instead creates 
# a table using the default storage engine.
#
# For example, in a replication setup, perhaps your master server uses InnoDB tables for maximum safety, but the slave servers
# use other storage engines for speed at the expense of durability or concurrency.
#
# By default, a warning is generated whenever CREATE_TABLE or ALTER_TABLE cannot use the default storage engine. To prevent
# confusing, unintended behavior if the desired engine is unavailable, enable the NO_ENGINE_SUBSTITUTION SQL mode.
#
# If the desired engine is unavailable, this setting produces an error instead of a warning, and the table is not created
# or altered. See SECTION 5.1.11, "SERVER SQL MODES"
#
# MySQL may store a table's index and data in one or more other files, depending on the storage engine. Table and column
# definitions are stored in the MySQL data dictionary.
#
# Individual storage engines create any additional files required for the tables that they manage. If a table name contains
# special characters, the names for the table files contain encoded versions of those characters as described in SECTION
# 9.2.3, "MAPPING OF IDENTIFIERS TO FILE NAMES"
#
# 16.2 THE MYISAM STORAGE ENGINE
#
# 16.2.1 MYISAM STARTUP OPTIONS
# 16.2.2 SPACE NEEDED FOR KEYS
# 16.2.3 MYISAM TABLE STORAGE FORMATS
# 16.2.4 MYISAM TABLE PROBLEMS
#
# MyISAM is based on the older (and no longer available) ISAM storage engine but has many useful extensions.
#
# TABLE 16.2 MYISAM STORAGE ENGINE FEATURES
#
# 				FEATURE 																										Support
#
# 	B-tree indexes																							 				Yes
# 	Backup/point-in-time recovery (implemented in the server, rather than in the storage engine) Yes
# 
# 	Cluster database support 																							No
# 	Clustered indexes 																									No
#
# 	Compressed Data 																										Yes (Compressed MyISAM tables are supported only when using the compressed row format.
# 																																	Tables using the compressed row format with MyISAM are read only)
#
# 	Data caches 																											No
# 	Encrypted Data 																										Yes (implemented in the server via encryption functions)
#
# 	Foreign key support 																									No
# 	Full-text search indexes 																							Yes
#
# 	Geospatial data type support 																						Yes
# 	Geospatial indexing support 																						Yes
#
# 	Hash indexes 																											No
# 	INdex caches 																											Yes
#
# 	Locking granularity 																									Table
# 	MVCC 																														No
#
# 	Replication support (implemented in the server, rather than in the storage engine) 				Yes
# 	Storage limits 																										256TB
#
# 	T-tree indexes 																										No
# 	Transactions 																											No
#
# 	Update statistics for data dictionary 																			Yes
#
# Each MyISAM table is stored on disk in two files. The files have names that begin with the table name and have
# an extension to indicate the file type.
#
# The data file has an .MYD (MYData) extension. The index file has an .MYI (MYIndex) extension. The table definition
# is stored in the MySQL data dictionary.
#
# To specify explicitly that you want a MyISAM table, indicate that with an ENGINE table option:
#
# 		CREATE TABLE t (i INT) ENGINE = MYISAM;
#
# https://dev.mysql.com/doc/refman/8.0/en/myisam-storage-engine.html
#